<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2311.01007</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#26469;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effective Human-AI Teams via Learned Natural Language Rules and Onboarding. (arXiv:2311.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#21644;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#21644;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25551;&#36848;&#65292;&#25105;&#20204;&#25945;&#23548;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;AI&#20195;&#29702;&#26469;&#24110;&#21161;&#20182;&#20204;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#20154;&#31867;&#24517;&#39035;&#30693;&#36947;&#20309;&#26102;&#20381;&#36182;&#20110;&#20195;&#29702;&#65292;&#19982;&#20195;&#29702;&#21512;&#20316;&#25110;&#24573;&#30053;&#20854;&#24314;&#35758;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21306;&#22495;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#23398;&#20064;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20197;&#35828;&#26126;&#20154;&#31867;&#24212;&#35813;&#22914;&#20309;&#19982;AI&#21512;&#20316;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#21306;&#22495;&#21457;&#29616;&#31639;&#27861;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#25968;&#25454;&#30340;&#23616;&#37096;&#21306;&#22495;&#20316;&#20026;&#37051;&#22495;&#65292;&#32416;&#27491;&#20102;&#20154;&#31867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#21306;&#22495;&#37117;&#36890;&#36807;&#36845;&#20195;&#21644;&#23545;&#27604;&#36807;&#31243;&#36827;&#34892;&#25551;&#36848;&#65292;&#20854;&#20013;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25551;&#36848;&#35813;&#21306;&#22495;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#24341;&#23548;&#38454;&#27573;&#23558;&#36825;&#20123;&#35268;&#21017;&#25945;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#26356;&#21152;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#20998;&#21035;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#21306;&#22495;&#21457;&#29616;&#21644;&#25551;&#36848;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00322</link><description>&lt;p&gt;
&#22122;&#22768;&#22270;&#20013;&#30340;&#40065;&#26834;&#22270;&#32858;&#31867;&#36890;&#36807;&#20803;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#22122;&#22768;&#36793;&#19978;&#40065;&#26834;&#22320;&#25214;&#21040;&#22270;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65311;&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#22312;&#22270;&#32858;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;GNN-based&#22270;&#32858;&#31867;&#30340;MetaGC&#12290;MetaGC&#37319;&#29992;&#21487;&#20998;&#35299;&#30340;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#33410;&#28857;&#23545;&#20043;&#38388;&#25439;&#22833;&#30340;&#27714;&#21644;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#20803;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#65292;&#20351;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#22686;&#21152;&#65292;&#32780;&#19981;&#37027;&#20040;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#65288;&#20363;&#22914;&#22122;&#22768;&#36793;&#65289;&#30340;&#26435;&#37325;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MetaGC&#25353;&#29031;&#39044;&#26399;&#23398;&#20064;&#26435;&#37325;&#65292;&#24182;&#19988;&#22240;&#27492;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperfor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18471</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#22240;&#26524;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal disentanglement of multimodal data. (arXiv:2310.18471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21457;&#29616;&#20102;&#25968;&#25454;&#30340;&#36739;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;&#21487;&#20197;&#23545;&#22240;&#26524;&#20851;&#31995;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65307;&#30001;&#20110;&#23454;&#29616;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35768;&#22810;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#20102;&#25351;&#31034;&#20808;&#39564;&#20449;&#24687;&#30340;&#20803;&#32032;&#65292;&#20363;&#22914;&#65288;&#32447;&#24615;&#65289;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#12289;&#24178;&#39044;&#25968;&#25454;&#25110;&#24369;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22312;&#25506;&#32034;&#24615;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#36825;&#20123;&#20803;&#32032;&#21644;&#20808;&#39564;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#29992;&#25110;&#19981;&#21512;&#36866;&#12290;&#30456;&#21453;&#65292;&#31185;&#23398;&#25968;&#25454;&#38598;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#25110;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#32422;&#26463;&#65292;&#24182;&#19988;&#24050;&#32463;&#35777;&#26126;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#36825;&#31181;&#31185;&#23398;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21487;&#20197;&#25913;&#21892;&#22240;&#26524;&#20998;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;causalPIMA&#65289;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#24050;&#30693;&#30340;&#29289;&#29702;&#23398;&#30693;&#35782;&#21457;&#29616;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#31639;&#27861;&#21033;&#29992;&#26032;&#30340;&#21487;&#24494;&#21442;&#25968;&#21270;&#26469;&#23398;&#20064;&#36825;&#31181;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal representation learning algorithms discover lower-dimensional representations of data that admit a decipherable interpretation of cause and effect; as achieving such interpretable representations is challenging, many causal learning algorithms utilize elements indicating prior information, such as (linear) structural causal models, interventional data, or weak supervision. Unfortunately, in exploratory causal representation learning, such elements and prior information may not be available or warranted. Alternatively, scientific datasets often have multiple modalities or physics-based constraints, and the use of such scientific, multimodal data has been shown to improve disentanglement in fully unsupervised settings. Consequently, we introduce a causal representation learning algorithm (causalPIMA) that can use multimodal data and known physics to discover important features with causal relationships. Our innovative algorithm utilizes a new differentiable parametrization to lear
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.13032</link><description>&lt;p&gt;
AI&#21453;&#39304;&#20419;&#36827;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity through AI Feedback. (arXiv:2310.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13032
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#21453;&#39304;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QDAIF&#65289;&#31639;&#27861;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#35780;&#20272;&#21019;&#36896;&#24615;&#20889;&#20316;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#19981;&#20165;&#20559;&#22909;&#21333;&#19968;&#22238;&#22797;&#65292;&#32780;&#26159;&#24076;&#26395;&#24471;&#21040;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#36755;&#20986;&#20197;&#20379;&#36873;&#25321;&#12290;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;QD&#65289;&#25628;&#32034;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#19981;&#26029;&#25913;&#36827;&#21644;&#22810;&#26679;&#21270;&#20505;&#36873;&#20154;&#32676;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;QD&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#31561;&#36136;&#24615;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#31639;&#27861;&#25351;&#23450;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#36890;&#36807;AI&#21453;&#39304;&#25351;&#23548;&#25628;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;LMs&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#34987;&#25552;&#31034;&#26469;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#24615;&#26041;&#38754;&#12290;&#20511;&#21161;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;AI&#21453;&#39304;&#23454;&#29616;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#31639;&#27861;&#65288;QDAIF&#65289;&#65292;&#20854;&#20013;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;LMs&#26469;&#29983;&#25104;&#21464;&#24322;&#24182;&#35780;&#20272;&#20505;&#36873;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#22312;&#21019;&#20316;&#24615;&#20889;&#20316;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;&#19982;&#38750;QDAIF&#31639;&#27861;&#30456;&#27604;&#65292;QDAIF&#26356;&#24191;&#27867;&#22320;&#35206;&#30422;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25351;&#23450;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at such outcomes, by continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Interestingly, recent developments in language models (LMs) have enabled guiding search through AI feedback, wherein LMs are prompted in natural language to evaluate qualitative aspects of text. Leveraging this development, we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. When assessed on creative writing domains, QDAIF covers more of a specified search space with high-quality samples than do non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.05036</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#31574;&#30053;&#65306;&#35780;&#20272;&#22312;Avalon&#28216;&#25103;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
From Text to Tactic: Evaluating LLMs Playing the Game of Avalon. (arXiv:2310.05036v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Avalon&#28216;&#25103;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;AvalonBench&#26469;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#23384;&#22312;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29609;&#31574;&#30053;&#31038;&#20132;&#25512;&#29702;&#28216;&#25103;Resistance Avalon&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#29609;&#23478;&#19981;&#20165;&#38656;&#35201;&#26681;&#25454;&#21160;&#24577;&#21457;&#23637;&#30340;&#28216;&#25103;&#38454;&#27573;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#36824;&#38656;&#35201;&#21442;&#19982;&#35752;&#35770;&#65292;&#22312;&#35752;&#35770;&#20013;&#24517;&#39035;&#27450;&#39575;&#12289;&#25512;&#29702;&#21644;&#19982;&#20854;&#20182;&#29609;&#23478;&#36827;&#34892;&#35848;&#21028;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;Avalon&#25104;&#20026;&#30740;&#31350;LLM&#20195;&#29702;&#30340;&#20915;&#31574;&#21644;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#26377;&#36259;&#35797;&#39564;&#24179;&#21488;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AvalonBench&#8212;&#8212;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22810;&#20195;&#29702;LLM&#20195;&#29702;&#30340;&#20840;&#38754;&#28216;&#25103;&#29615;&#22659;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#65306;&#65288;1&#65289;Avalon&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#65288;2&#65289;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#20316;&#20026;&#22522;&#20934;&#23545;&#25163;&#65292;&#20197;&#21450;&#65288;3&#65289;&#38024;&#23545;&#27599;&#20010;&#35282;&#33394;&#20855;&#26377;&#23450;&#21046;&#25552;&#31034;&#30340;ReAct-style LLM&#20195;&#29702;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;AvalonBench&#30340;&#35780;&#20272;&#31361;&#20986;&#26174;&#31034;&#20102;&#26126;&#26174;&#30340;&#33021;&#21147;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#20687;ChatGPT&#36825;&#26679;&#22312;&#22909;&#35282;&#33394;&#20013;&#30340;&#27169;&#22411;&#23545;&#25112;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#20154;&#30340;&#32988;&#29575;&#20026;22.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Large Language Models (LLMs) Agents in playing the strategic social deduction game, Resistance Avalon. Players in Avalon are challenged not only to make informed decisions based on dynamically evolving game phases, but also to engage in discussions where they must deceive, deduce, and negotiate with other players. These characteristics make Avalon a compelling test-bed to study the decision-making and language-processing capabilities of LLM Agents. To facilitate research in this line, we introduce AvalonBench - a comprehensive game environment tailored for evaluating multi-agent LLM Agents. This benchmark incorporates: (1) a game environment for Avalon, (2) rule-based bots as baseline opponents, and (3) ReAct-style LLM agents with tailored prompts for each role. Notably, our evaluations based on AvalonBench highlight a clear capability gap. For instance, models like ChatGPT playing good-role got a win rate of 22.2% against rule-based bots play
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03912</link><description>&lt;p&gt;
RTDK-BO&#65306;&#20855;&#26377;Reinforced Transformer&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RTDK-BO: High Dimensional Bayesian Optimization with Reinforced Transformer Deep kernels. (arXiv:2310.03912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#39640;&#36125;&#21494;&#26031;&#20248;&#21270;&#27169;&#22411;&#24314;&#27169;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#28145;&#24230;&#26680;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20195;&#29702;&#25351;&#23548;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#23545;&#20110;&#39640;&#32500;&#40657;&#30418;&#20248;&#21270;&#38750;&#24120;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#31185;&#23398;&#35745;&#31639;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20989;&#25968;&#20248;&#21270;&#21644;&#23569;&#26679;&#26412;&#22810;&#30446;&#26631;&#20248;&#21270;&#19978;&#24341;&#20837;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#23569;&#26679;&#26412;&#25216;&#26415;&#20063;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#32039;&#23494;&#30456;&#20851;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;GP&#20195;&#29702;&#30340;&#24314;&#27169;&#33021;&#21147;&#19982;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;DKL&#20013;&#26469;&#25913;&#36827;&#20803;&#23398;&#20064;BO&#20195;&#29702;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#22312;BO&#36807;&#31243;&#20013;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;Transformer&#28145;&#24230;&#26680;&#26041;&#27861;&#19982;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#25552;&#39640;BO&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO), guided by Gaussian process (GP) surrogates, has proven to be an invaluable technique for efficient, high-dimensional, black-box optimization, a critical problem inherent to many applications such as industrial design and scientific computing. Recent contributions have introduced reinforcement learning (RL) to improve the optimization performance on both single function optimization and \textit{few-shot} multi-objective optimization. However, even few-shot techniques fail to exploit similarities shared between closely related objectives. In this paper, we combine recent developments in Deep Kernel Learning (DKL) and attention-based Transformer models to improve the modeling powers of GP surrogates with meta-learning. We propose a novel method for improving meta-learning BO surrogates by incorporating attention mechanisms into DKL, empowering the surrogates to adapt to contextual information gathered during the BO process. We combine this Transformer Deep Kern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.00034</link><description>&lt;p&gt;
PB-LLM: &#37096;&#20998;&#20108;&#20540;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PB-LLM: Partially Binarized Large Language Models. (arXiv:2310.00034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;PB-LLM&#26159;&#19968;&#31181;&#37096;&#20998;&#20108;&#20540;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#36890;&#36807;&#21518;&#35757;&#32451;&#37327;&#21270;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31561;&#26041;&#27861;&#24674;&#22797;&#37327;&#21270;LLMM&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32593;&#32476;&#20108;&#20540;&#21270;&#65292;&#19968;&#31181;&#21387;&#32553;&#27169;&#22411;&#26435;&#37325;&#20026;&#21333;&#20010;&#27604;&#29305;&#30340;&#37327;&#21270;&#30340;&#28608;&#36827;&#24418;&#24335;&#65292;&#19987;&#38376;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21387;&#32553;&#12290;&#30001;&#20110;&#20043;&#21069;&#30340;&#20108;&#20540;&#21270;&#26041;&#27861;&#20250;&#23548;&#33268;LLMs&#23849;&#28291;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#37096;&#20998;&#20108;&#20540;&#21270;LLM&#65288;PB-LLM&#65289;&#65292;&#21487;&#20197;&#23454;&#29616;&#26497;&#20302;&#27604;&#29305;&#37327;&#21270;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#37327;&#21270;LLMs&#30340;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#29616;&#26377;&#20108;&#20540;&#21270;&#31639;&#27861;&#30340;&#21407;&#29983;&#24212;&#29992;&#30340;&#26080;&#25928;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#26174;&#33879;&#26435;&#37325;&#22312;&#23454;&#29616;&#20302;&#20301;&#37327;&#21270;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;PB-LLM&#22312;&#20108;&#36827;&#21046;&#21270;&#36807;&#31243;&#20013;&#36807;&#28388;&#20102;&#19968;&#23567;&#37096;&#20998;&#26174;&#33879;&#26435;&#37325;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#39640;&#20301;&#23384;&#20648;&#20013;&#65292;&#21363;&#37096;&#20998;&#20108;&#20540;&#21270;&#12290;PB-LLM&#22312;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#30340;&#35282;&#24230;&#20998;&#26512;&#21518;&#65292;&#25193;&#23637;&#20102;&#24674;&#22797;&#37327;&#21270;LLMM&#23481;&#37327;&#30340;&#33021;&#21147;&#12290;&#22312;PTQ&#19979;&#65292;&#32467;&#21512;&#20102;GPTQ&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.08637</link><description>&lt;p&gt;
TextBind: &#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
TextBind: Multi-turn Interleaved Multimodal Instruction-following. (arXiv:2309.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08637
&lt;/p&gt;
&lt;p&gt;
TextBind&#26159;&#19968;&#20010;&#27880;&#37322;&#26497;&#23569;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#36171;&#20104;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#36825;&#20010;&#26694;&#26550;&#23545;&#20110;&#35299;&#20915;&#23454;&#38469;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#20854;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25968;&#25454;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#12290;&#24403;&#28041;&#21450;&#21040;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TextBind&#65292;&#36825;&#26159;&#19968;&#20010;&#20960;&#20046;&#19981;&#38656;&#35201;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36171;&#20104;&#36739;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#20132;&#38169;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65292;&#24182;&#20174;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#36718;&#22810;&#27169;&#24577;&#25351;&#20196;-&#22238;&#24212;&#23545;&#35805;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#28436;&#31034;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#36319;&#38543;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models with instruction-following abilities have revolutionized the field of artificial intelligence. These models show exceptional generalizability to tackle various real-world tasks through their natural language interfaces. However, their performance heavily relies on high-quality exemplar data, which is often difficult to obtain. This challenge is further exacerbated when it comes to multimodal instruction following. We introduce TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities. Our approach requires only image-caption pairs and generates multi-turn multimodal instruction-response conversations from a language model. We release our dataset, model, and demo to foster future research in the area of multimodal instruction following.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;</title><link>http://arxiv.org/abs/2309.08589</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#26159;&#19968;&#31181;&#31574;&#30053;&#25913;&#36827;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Reasoning is a Policy Improvement Operator. (arXiv:2309.08589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08589
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;SECToR&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#20196;&#20154;&#36190;&#21497;&#30340;&#26032;&#33021;&#21147;&#20196;&#19990;&#30028;&#20026;&#20043;&#24778;&#21497;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30446;&#21069;&#32570;&#20047;&#33258;&#25105;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#25509;&#21463;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SECToR&#65288;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#23454;&#29616;&#33258;&#25105;&#25945;&#32946;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#25104;&#21151;&#22320;&#33258;&#23398;&#26032;&#25216;&#33021;&#12290;&#21463;&#21040;&#20197;&#21069;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;Silver&#31561;&#20154;&#65292;2017&#65289;&#21644;&#20154;&#31867;&#35748;&#30693;&#65288;Kahneman&#65292;2011&#65289;&#20013;&#30340;&#30456;&#20851;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;SECToR&#39318;&#20808;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#36880;&#28176;&#24605;&#32771;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SECToR&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30456;&#21516;&#30340;&#31572;&#26696;&#65292;&#36825;&#27425;&#19981;&#20877;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#25512;&#29702;&#12290;&#36890;&#36807;SECToR&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#23398;&#20250;&#20102;&#36827;&#34892;&#22810;&#36798;29&#20301;&#25968;&#23383;&#30340;&#21152;&#27861;&#36816;&#31639;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#36229;&#36807;6&#20301;&#25968;&#23383;&#30340;&#22522;&#20934;&#30495;&#23454;&#31034;&#20363;&#65292;&#20165;&#36890;&#36807;&#21021;&#22987;&#30340;&#30417;&#30563;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Large language models have astounded the world with fascinating new capabilities. However, they currently lack the ability to teach themselves new skills, relying instead on being trained on large amounts of human-generated data. We introduce SECToR (Self-Education via Chain-of-Thought Reasoning), a proof-of-concept demonstration that language models can successfully teach themselves new skills using chain-of-thought reasoning. Inspired by previous work in both reinforcement learning (Silver et al., 2017) and human cognition (Kahneman, 2011), SECToR first uses chain-of-thought reasoning to slowly think its way through problems. SECToR then fine-tunes the model to generate those same answers, this time without using chain-of-thought reasoning. Language models trained via SECToR autonomously learn to add up to 29-digit numbers without any access to any ground truth examples beyond an initial supervised fine-tuning phase consisting only of numbers with 6 or fewer digits. Our central hypot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07670</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#19988;&#37096;&#20998;&#23458;&#25143;&#31471;&#20855;&#26377;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;FedDaDiL&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#20195;&#34920;&#30528;&#29305;&#23450;&#30340;&#39046;&#22495;&#65292;&#32780;FedDaDiL&#21017;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#32852;&#37030;&#32463;&#39564;&#20998;&#24067;&#23383;&#20856;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#19978;&#35774;&#35745;&#20102;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#25152;&#36873;&#25321;&#30340;&#21327;&#35758;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#25552;&#39640;&#20102;&#25972;&#20307;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Caltech-Office&#12289;TEP&#21644;CWRU&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#38598;&#20013;&#24335;&#26041;&#27861;&#21644;&#20854;&#20182;&#32852;&#37030;&#39046;&#22495;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
&lt;/p&gt;</description></item><item><title>Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06597</link><description>&lt;p&gt;
Rank2Tell: &#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Rank2Tell: A Multimodal Driving Dataset for Joint Importance Ranking and Reasoning. (arXiv:2309.06597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06597
&lt;/p&gt;
&lt;p&gt;
Rank2Tell&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32852;&#21512;&#37325;&#35201;&#24615;&#25490;&#24207;&#21644;&#25512;&#29702;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#29992;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21644;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#21487;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31038;&#20250;&#23545;&#23427;&#20204;&#30340;&#25509;&#21463;&#31243;&#24230;&#65292;&#32780;&#23545;&#39569;&#36710;&#20154;&#26469;&#35828;&#65292;&#23427;&#20204;&#34987;&#35270;&#20026;&#21487;&#20449;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20010;&#20219;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#29616;&#20195;&#33258;&#20027;&#31995;&#32479;&#36719;&#20214;&#20005;&#37325;&#20381;&#36182;&#20110;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;Rank2Tell&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#37325;&#35201;&#24615;&#32423;&#21035;&#25490;&#24207;&#21644;&#21407;&#22240;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#21508;&#31181;&#38381;&#21512;&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22797;&#26434;&#20132;&#36890;&#24773;&#26223;&#20013;&#21508;&#31181;&#37325;&#35201;&#23545;&#35937;&#30340;&#21508;&#31181;&#35821;&#20041;&#12289;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20851;&#31995;&#23646;&#24615;&#30340;&#23494;&#38598;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#30340;&#23494;&#38598;&#27880;&#37322;&#21644;&#29420;&#29305;&#23646;&#24615;&#20351;&#20854;&#25104;&#20026;&#20174;&#20107;&#35270;&#35273;&#22330;&#26223;&#29702;&#35299;&#21644;&#30456;&#20851;&#39046;&#22495;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#34920;&#31034;&#21644;&#25512;&#29702;&#37325;&#35201;&#24615;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of commercial autonomous vehicles (AVs) and advanced driver assistance systems (ADAS) may largely depend on their acceptance by society, for which their perceived trustworthiness and interpretability to riders are crucial. In general, this task is challenging because modern autonomous systems software relies heavily on black-box artificial intelligence models. Towards this goal, this paper introduces a novel dataset, Rank2Tell, a multi-modal ego-centric dataset for Ranking the importance level and Telling the reason for the importance. Using various close and open-ended visual question answering, the dataset provides dense annotations of various semantic, spatial, temporal, and relational attributes of various important objects in complex traffic scenarios. The dense annotations and unique attributes of the dataset make it a valuable resource for researchers working on visual scene understanding and related fields. Further, we introduce a joint model for joint i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06202</link><description>&lt;p&gt;
&#22312;&#26816;&#27979;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#20013;&#25506;&#32034;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring Predicate Visual Context in Detecting of Human-Object Interactions. (arXiv:2308.06202v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#31561;&#26041;&#24335;&#26469;&#25913;&#36827;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DETR&#26694;&#26550;&#24050;&#25104;&#20026;&#20154;&#29289;&#21644;&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#30740;&#31350;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#20004;&#38454;&#27573;&#21464;&#25442;&#22120;&#30340;HOI&#26816;&#27979;&#22120;&#26159;&#24615;&#33021;&#26368;&#22909;&#21644;&#35757;&#32451;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20197;&#32570;&#20047;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#29289;&#20307;&#29305;&#24449;&#20316;&#20026;HOI&#20998;&#31867;&#30340;&#26465;&#20214;&#65292;&#32780;&#24573;&#35270;&#20102;&#23039;&#21183;&#21644;&#26041;&#21521;&#20449;&#24687;&#65292;&#32780;&#26356;&#27880;&#37325;&#20851;&#20110;&#29289;&#20307;&#36523;&#20221;&#21644;&#36793;&#30028;&#30340;&#35270;&#35273;&#25552;&#31034;&#12290;&#36825;&#33258;&#28982;&#22320;&#38459;&#30861;&#20102;&#23545;&#22797;&#26434;&#25110;&#27169;&#31946;&#20132;&#20114;&#30340;&#35782;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#23454;&#39564;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#24341;&#20837;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#25913;&#36827;&#20102;&#26597;&#35810;&#35774;&#35745;&#65292;&#24191;&#27867;&#25506;&#32034;&#20102;&#38190;&#21644;&#20540;&#65292;&#20197;&#21450;&#20351;&#29992;&#30418;&#23376;&#37197;&#23545;&#20301;&#32622;&#23884;&#20837;&#20316;&#20026;&#31354;&#38388;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#35859;&#35789;&#35270;&#35273;&#32972;&#26223;&#65288;PViC&#65289;&#27169;&#22411;&#22312;HICO-DET&#21644;V-COCO&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the DETR framework has emerged as the dominant approach for human--object interaction (HOI) research. In particular, two-stage transformer-based HOI detectors are amongst the most performant and training-efficient approaches. However, these often condition HOI classification on object features that lack fine-grained contextual information, eschewing pose and orientation information in favour of visual cues about object identity and box extremities. This naturally hinders the recognition of complex or ambiguous interactions. In this work, we study these issues through visualisations and carefully designed experiments. Accordingly, we investigate how best to re-introduce image features via cross-attention. With an improved query design, extensive exploration of keys and values, and box pair positional embeddings as spatial guidance, our model with enhanced predicate visual context (PViC) outperforms state-of-the-art methods on the HICO-DET and V-COCO benchmarks, while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.01098</link><description>&lt;p&gt;
&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21033;&#29992;&#22810;&#19987;&#23478;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#26356;&#22909;&#30340;&#26597;&#35810;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search. (arXiv:2308.01098v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65288;KC&#65289;&#65292;&#36890;&#36807;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#26597;&#35810;&#20998;&#31867;&#24615;&#33021;&#65292;&#22312;&#20140;&#19996;&#24191;&#21578;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#20998;&#31867;&#20316;&#20026;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#30830;&#20445;&#26356;&#20302;&#30340;&#24310;&#36831;&#65292;&#24120;&#20351;&#29992;&#27973;&#23618;&#27169;&#22411;&#65288;&#22914;FastText&#65289;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;FastText&#27169;&#22411;&#30340;&#34920;&#24449;&#33021;&#21147;&#19981;&#36275;&#65292;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#19968;&#20123;&#20302;&#39057;&#26597;&#35810;&#21644;&#23614;&#37096;&#31867;&#21035;&#19978;&#12290;&#20351;&#29992;&#26356;&#28145;&#20837;&#19988;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#22312;&#32447;&#25512;&#26029;&#24310;&#36831;&#21644;&#26356;&#26114;&#36149;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#22312;&#25512;&#26029;&#25928;&#29575;&#21644;&#20998;&#31867;&#24615;&#33021;&#20043;&#38388;&#25240;&#34935;&#26174;&#28982;&#20855;&#26377;&#37325;&#22823;&#23454;&#38469;&#24847;&#20041;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30693;&#35782;&#33976;&#39311;&#65288;KC&#65289;&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#20302;&#24310;&#36831;&#32422;&#26463;&#19979;&#25552;&#21319;&#22312;&#32447;FastText&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#19968;&#20010;&#31163;&#32447;&#27169;&#22411;&#65292;&#36890;&#36807;&#33976;&#39311;&#30693;&#35782;&#26469;&#25913;&#21892;&#22312;&#32447;&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query classification, as an effective way to understand user intents, is of great importance in real-world online ads systems. To ensure a lower latency, a shallow model (e.g. FastText) is widely used for efficient online inference. However, the representation ability of the FastText model is insufficient, resulting in poor classification performance, especially on some low-frequency queries and tailed categories. Using a deeper and more complex model (e.g. BERT) is an effective solution, but it will cause a higher online inference latency and more expensive computing costs. Thus, how to juggle both inference efficiency and classification performance is obviously of great practical importance. To overcome this challenge, in this paper, we propose knowledge condensation (KC), a simple yet effective knowledge distillation framework to boost the classification performance of the online FastText model under strict low latency constraints. Specifically, we propose to train an offline
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#20102;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#30340;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;PAPM&#65292;&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#12290;</title><link>http://arxiv.org/abs/2308.00530</link><description>&lt;p&gt;
&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;: &#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Point Annotation Probability Map: Towards Dense Object Counting by Tolerating Annotation Noise. (arXiv:2308.00530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00530
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#23454;&#29616;&#20102;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#30340;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;PAPM&#65292;&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#22330;&#26223;&#20013;&#35745;&#25968;&#23545;&#35937;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39640;&#26031;&#23494;&#24230;&#22238;&#24402;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#31181;&#26292;&#21147;&#22238;&#24402;&#26041;&#27861;&#25928;&#26524;&#19981;&#38169;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#24456;&#22909;&#22320;&#32771;&#34385;&#21040;&#30001;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#24341;&#36215;&#30340;&#27880;&#37322;&#22122;&#22768;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#22312;&#23494;&#38598;&#23545;&#35937;&#35745;&#25968;&#20219;&#21153;&#20013;&#32771;&#34385;&#27880;&#37322;&#22122;&#22768;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#23545;&#27880;&#37322;&#22122;&#22768;&#30340;&#24378;&#40065;&#26834;&#24615;&#65292;&#21033;&#29992;&#20855;&#26377;&#21487;&#35843;&#24102;&#23485;&#21644;&#24418;&#29366;&#21442;&#25968;&#30340;&#24191;&#20041;&#39640;&#26031;&#20998;&#24067;&#65288;GGD&#65289;&#20989;&#25968;&#26469;&#24418;&#25104;&#23398;&#20064;&#30446;&#26631;&#28857;&#27880;&#37322;&#27010;&#29575;&#22270;&#65288;PAPM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#26041;&#27861;&#65288;HD-PAPM&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;GGD&#30340;&#20989;&#25968;&#26469;&#23481;&#24525;&#27880;&#37322;&#22122;&#22768;&#12290;&#23545;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#22522;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;PAPM&#21487;&#33021;&#23545;&#20110;&#29305;&#23450;&#30340;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;PAPM&#26041;&#27861;&#65288;AL-PAPM&#65289;&#65292;
&lt;/p&gt;
&lt;p&gt;
Counting objects in crowded scenes remains a challenge to computer vision. The current deep learning based approach often formulate it as a Gaussian density regression problem. Such a brute-force regression, though effective, may not consider the annotation noise properly which arises from the human annotation process and may lead to different distributions. We conjecture that it would be beneficial to consider the annotation noise in the dense object counting task. To obtain strong robustness against annotation noise, generalized Gaussian distribution (GGD) function with a tunable bandwidth and shape parameter is exploited to form the learning target point annotation probability map, PAPM. Specifically, we first present a hand-designed PAPM method (HD-PAPM), in which we design a function based on GGD to tolerate the annotation noise. For end-to-end training, the hand-designed PAPM may not be optimal for the particular network and dataset. An adaptively learned PAPM method (AL-PAPM) is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.00158</link><description>&lt;p&gt;
&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#36755;&#20986;&#20013;&#30340;&#23436;&#32654;&#36136;&#37327;&#27573;&#33853;&#65306;&#26159;&#21542;&#21487;&#20197;&#20174;&#21382;&#21490;&#25968;&#25454;&#20013;&#25429;&#25417;&#32534;&#36753;&#36317;&#31163;&#27169;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Fine-Tuned&#30340;OpenAI LLM&#36827;&#34892;&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#30340;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;Fine-Tuned&#30340;ChatGPT&#26469;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#36136;&#37327;&#20272;&#35745;&#65288;TQE&#65289;&#26159;&#23558;&#36755;&#20986;&#32763;&#35793;&#37096;&#32626;&#21040;&#20351;&#29992;&#20013;&#20043;&#21069;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290; TQE&#23545;&#20110;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#30340;&#36136;&#37327;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#26597;&#30475;&#21442;&#32771;&#32763;&#35793;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20026;TQE&#20219;&#21153;&#21644;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;Fine-Tune&#12290;&#25105;&#20204;&#20197;ChatGPT&#20026;&#20363;&#65292;&#23558;TQE&#35270;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#20351;&#29992;&#33521;&#24847;&#21644;&#33521;&#24503;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;ChatGPT&#30340;API Fine-Tuned&#21487;&#20197;&#22312;&#39044;&#27979;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#33719;&#24471;&#30456;&#23545;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#21363;&#26159;&#21542;&#38656;&#35201;&#32534;&#36753;&#32763;&#35793;&#65292;&#20294;&#32943;&#23450;&#26377;&#25913;&#36827;&#20934;&#30830;&#24615;&#30340;&#31354;&#38388;&#12290;&#33521;&#24847;&#21452;&#35821;&#25688;&#35201;&#21487;&#22312;&#35770;&#25991;&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translation Quality Estimation (TQE) is an important step before deploying the output translation into usage. TQE is also critical in assessing machine translation (MT) and human translation (HT) quality without seeing the reference translations. In this work, we examine if the state-of-the-art large language models (LLMs) can be fine-tuned for the TQE task and their capability. We take ChatGPT as one example and approach TQE as a binary classification task. Using English-Italian and English-German training corpus, our experimental results show that fine-tuned ChatGPT via its API can achieve a relatively high score on predicting translation quality, i.e. if the translation needs to be edited, but there is definitely space to improve the accuracy. English-Italiano bilingual Abstract is available in the paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.00113</link><description>&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#20010;&#26041;&#27861;&#24041;&#22266;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Three Bricks to Consolidate Watermarks for Large Language Models. (arXiv:2308.00113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#30340;&#26041;&#27861;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#25552;&#20379;&#31283;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#21644;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#26029;&#29983;&#25104;&#25991;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#27700;&#21360;&#25216;&#26415;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23558;&#29983;&#25104;&#25991;&#26412;&#24402;&#23646;&#20110;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#23427;&#25913;&#21464;&#20102;&#37319;&#26679;&#29983;&#25104;&#36807;&#31243;&#65292;&#30041;&#19979;&#20102;&#26080;&#24418;&#30340;&#30165;&#36857;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#65292;&#20197;&#20415;&#20110;&#21518;&#32493;&#30340;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#19977;&#20010;&#29702;&#35770;&#21644;&#23454;&#35777;&#32771;&#34385;&#65292;&#24041;&#22266;&#20102;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21363;&#20351;&#22312;&#20302;&#38169;&#35823;&#38451;&#24615;&#29575;&#19979;&#65288;&#23567;&#20110;10^(-6)&#65289;&#65292;&#36825;&#20123;&#20445;&#35777;&#20381;&#28982;&#26377;&#25928;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20351;&#29992;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#23545;&#27604;&#20102;&#27700;&#21360;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#34892;&#24615;&#30340;&#35265;&#35299;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#21487;&#20197;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#26223;&#20197;&#21450;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.14953</link><description>&lt;p&gt;
&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36827;&#34892;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;MSDA&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22495;&#34920;&#31034;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#26469;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#26681;&#25454;&#35813;&#23383;&#20856;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#21644;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22810;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#20174;&#22810;&#20010;&#26631;&#35760;&#30340;&#28304;&#22495;&#36716;&#31227;&#30693;&#35782;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#26102;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#20856;&#23398;&#20064;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#22411;MSDA&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;MSDA&#20013;&#30340;&#27599;&#20010;&#22495;&#35299;&#37322;&#20026;&#32463;&#39564;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#22495;&#34920;&#36798;&#20026;&#23383;&#20856;&#21407;&#23376;&#30340;Wasserstein&#37325;&#24515;&#65292;&#36825;&#20123;&#21407;&#23376;&#26159;&#32463;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#23567;&#25209;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;DaDiL&#65306;&#65288;i&#65289;&#21407;&#23376;&#20998;&#24067;&#65307;&#65288;ii&#65289;&#37325;&#24515;&#22352;&#26631;&#30697;&#38453;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23383;&#20856;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;MSDA&#26041;&#27861;&#65306;DaDiL-R&#65292;&#22522;&#20110;&#30446;&#26631;&#22495;&#26631;&#35760;&#26679;&#26412;&#30340;&#37325;&#26500;&#65307;DaDiL-E&#65292;&#22522;&#20110;&#22312;&#21407;&#23376;&#20998;&#24067;&#19978;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;3&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;Caltech-Office&#12289;Office 31&#21644;CRWU&#65292;&#22312;&#20998;&#31867;&#19978;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;3.15&#65285;&#12289;2.29&#65285;&#21644;7.71&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09933</link><description>&lt;p&gt;
Spuriosity&#24182;&#27809;&#26377;&#23548;&#33268;&#20998;&#31867;&#22120;&#22833;&#36133;&#65306;&#21033;&#29992;&#19981;&#21464;&#30340;&#39044;&#27979;&#26469;&#21033;&#29992;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#25552;&#21462;&#20855;&#26377;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#31283;&#23450;&#25110;&#19981;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#33293;&#24323;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#20851;&#31995;&#21464;&#21270;&#30340;"&#34394;&#20551;"&#25110;&#19981;&#31283;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#19981;&#31283;&#23450;&#29305;&#24449;&#24120;&#24120;&#25658;&#24102;&#20851;&#20110;&#26631;&#31614;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#22495;&#20013;&#27491;&#30830;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22914;&#20309;&#22312;&#27979;&#35797;&#22495;&#20013;&#20351;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#21487;&#33021;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#31283;&#23450;&#29305;&#24449;&#30340;&#20266;&#26631;&#31614;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21069;&#25552;&#26159;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#31283;&#23450;&#29305;&#24449;&#21644;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#29305;&#24449;&#22686;&#24378;&#65288;SFB&#65289;&#31639;&#27861;&#65306;(i)&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20998;&#31163;&#31283;&#23450;&#29305;&#24449;&#21644;&#26465;&#20214;&#29420;&#31435;&#19981;&#31283;&#23450;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#65307;(ii)&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#39044;&#27979;&#26469;&#36866;&#24212;&#27979;&#35797;&#22495;
&lt;/p&gt;
&lt;p&gt;
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09342</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#23427;&#20204;&#32534;&#30721;&#20026;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#30340;&#23454;&#20363;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#32422;&#26463;&#31867;&#22411;&#22312;&#25991;&#29486;&#20013;&#20063;&#26377;&#24456;&#22810;&#32534;&#30721;&#26041;&#24335;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#32534;&#30721;&#26041;&#24335;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#23454;&#20363;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#32422;&#26463;&#38382;&#39064;&#30340;&#29305;&#24449;&#38598;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#19968;&#32452;&#26032;&#29305;&#24449;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36873;&#25321;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#31867;&#21035;&#30340;&#32534;&#30721;&#26041;&#24335;&#26102;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;AutoFolio&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#20363;&#29305;&#24449;&#23545;&#20110;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#20219;&#21153;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03718</link><description>&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#65306;&#31649;&#29702;&#23545;&#20844;&#20849;&#23433;&#20840;&#30340;&#26032;&#20852;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Frontier AI Regulation: Managing Emerging Risks to Public Safety. (arXiv:2307.03718v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03718
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#26631;&#20934;&#21046;&#23450;&#12289;&#27880;&#20876;&#25253;&#21578;&#21644;&#23433;&#20840;&#21512;&#35268;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20026;&#20154;&#31867;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#65292;&#20294;&#31038;&#20250;&#38656;&#35201;&#20027;&#21160;&#31649;&#29702;&#30456;&#20851;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#20851;&#27880;&#25105;&#20204;&#25152;&#31216;&#30340;&#8220;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#8221;&#27169;&#22411;&#65306;&#39640;&#24230;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#33021;&#20855;&#22791;&#36275;&#20197;&#23545;&#20844;&#20849;&#23433;&#20840;&#36896;&#25104;&#20005;&#37325;&#39118;&#38505;&#30340;&#21361;&#38505;&#33021;&#21147;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#30417;&#31649;&#25361;&#25112;&#65306;&#21361;&#38505;&#33021;&#21147;&#21487;&#33021;&#20986;&#20046;&#24847;&#26009;&#65307;&#24456;&#38590;&#26377;&#25928;&#38450;&#27490;&#37096;&#32626;&#27169;&#22411;&#34987;&#28389;&#29992;&#65307;&#24182;&#19988;&#24456;&#38590;&#38459;&#27490;&#27169;&#22411;&#30340;&#33021;&#21147;&#24191;&#27867;&#25193;&#25955;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36793;&#32536;&#27169;&#22411;&#30340;&#30417;&#31649;&#38656;&#35201;&#33267;&#23569;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65306;(1) &#35774;&#23450;&#26631;&#20934;&#30340;&#36807;&#31243;&#65292;&#20197;&#30830;&#23450;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#32773;&#30340;&#36866;&#24403;&#35201;&#27714;&#65307;(2) &#27880;&#20876;&#21644;&#25253;&#21578;&#35201;&#27714;&#65292;&#20026;&#30417;&#31649;&#26426;&#26500;&#25552;&#20379;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#30340;&#21487;&#35265;&#24615;&#65307;(3) &#20445;&#35777;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#23433;&#20840;&#26631;&#20934;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced AI models hold the promise of tremendous benefits for humanity, but society needs to proactively manage the accompanying risks. In this paper, we focus on what we term "frontier AI" models: highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety. Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly. To address these challenges, at least three building blocks for the regulation of frontier models are needed: (1) standard-setting processes to identify appropriate requirements for frontier AI developers, (2) registration and reporting requirements to provide regulators with visibility into frontier AI development processes, and (3) mechanisms to ensure compliance with safety standards for the development and deplo
&lt;/p&gt;</description></item><item><title>ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.16750</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#21450;&#20854;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Eigensubspace of Temporal-Difference Dynamics and How It Improves Value Approximation in Reinforcement Learning. (arXiv:2306.16750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16750
&lt;/p&gt;
&lt;p&gt;
ERC&#26159;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#26102;&#38388;&#24046;&#20998;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#21363;&#29305;&#24449;&#23376;&#31354;&#38388;&#35268;&#33539;&#21270;&#25209;&#35780;&#23478;&#65288;ERC&#65289;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290; ERC&#21463;&#21040;&#20102;&#23545;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#26041;&#27861;&#20013;Q&#20540;&#20272;&#35745;&#35823;&#24046;&#21160;&#21147;&#23398;&#30340;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#30001;&#19982;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30456;&#20851;&#30340;&#36716;&#31227;&#26680;&#20851;&#32852;&#30340;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#23450;&#20041;&#30340;&#36335;&#24452;&#12290;&#23427;&#25581;&#31034;&#20102;TD&#23398;&#20064;&#30340;&#19968;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#22312;&#20808;&#21069;&#30340;&#28145;&#24230;RL&#26041;&#27861;&#20013;&#26410;&#34987;&#20351;&#29992;&#12290;&#22312;ERC&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#25351;&#23548;&#36817;&#20284;&#35823;&#24046;&#36235;&#21521;&#20110;1-&#29305;&#24449;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#39640;&#25928;&#31283;&#23450;&#30340;&#20540;&#20272;&#35745;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ERC&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;ERC&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20540;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#22312;DMControl&#22522;&#20934;&#27979;&#35797;&#30340;26&#20010;&#20219;&#21153;&#20013;&#65292;ERC&#20248;&#20110;20&#20010;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;Q&#20540;&#20272;&#35745;&#26041;&#38754;&#20063;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel value approximation method, namely Eigensubspace Regularized Critic (ERC) for deep reinforcement learning (RL). ERC is motivated by an analysis of the dynamics of Q-value approximation error in the Temporal-Difference (TD) method, which follows a path defined by the 1-eigensubspace of the transition kernel associated with the Markov Decision Process (MDP). It reveals a fundamental property of TD learning that has remained unused in previous deep RL approaches. In ERC, we propose a regularizer that guides the approximation error tending towards the 1-eigensubspace, resulting in a more efficient and stable path of value approximation. Moreover, we theoretically prove the convergence of the ERC method. Besides, theoretical analysis and experiments demonstrate that ERC effectively reduces the variance of value functions. Among 26 tasks in the DMControl benchmark, ERC outperforms state-of-the-art methods for 20. Besides, it shows significant advantages in Q-value approxim
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#21644;&#22686;&#37327;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04031</link><description>&lt;p&gt;
&#24102;&#26377;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21512;&#35268;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Certified Reasoning with Language Models. (arXiv:2306.04031v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#23427;&#20351;&#29992;&#29366;&#24577;&#21644;&#22686;&#37327;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36890;&#36807;&#36880;&#27493;&#25512;&#29702;&#23454;&#29616;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#21487;&#20197;&#26159;&#19981;&#23436;&#22791;&#30340;&#12289;&#19981;&#19968;&#33268;&#30340;&#25110;&#32773;&#20381;&#36182;&#20110;&#19981;&#33391;&#30340;&#20248;&#20808;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25351;&#21335;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#31867;&#65292;&#21033;&#29992;&#29366;&#24577;&#21644;&#36882;&#22686;&#32422;&#26463;&#26469;&#25351;&#23548;&#29983;&#25104;&#12290;&#27169;&#22411;&#21487;&#20197;&#35843;&#29992;&#25351;&#21335;&#65292;&#23558;&#20854;&#33258;&#24049;&#30340;&#29983;&#25104;&#38480;&#21046;&#22312;&#19968;&#32452;&#24037;&#20855;&#32473;&#20986;&#30340;&#26377;&#25928;&#38472;&#36848;&#20043;&#20869;&#12290;&#21453;&#36807;&#26469;&#65292;&#27169;&#22411;&#30340;&#36873;&#25321;&#21487;&#20197;&#25913;&#21464;&#25351;&#21335;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31995;&#32479;&#26469;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#21487;&#20197;&#34987;&#29992;&#20316;&#25351;&#21335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LogicGuide&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#27169;&#22411;&#21487;&#20197;&#20026;LogicGuide&#24418;&#24335;&#21270;&#23427;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20445;&#35777;&#20854;&#25512;&#29702;&#27493;&#39588;&#26159;&#23436;&#22791;&#30340;&#12290;&#22312;PrOntoQA&#21644;ProofWriter&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;LogicGuide&#26174;&#33879;&#25552;&#39640;&#20102;GPT-3&#12289;GPT-3.5 Turbo&#21644;LLaMA&#30340;&#24615;&#33021;&#65288;&#31934;&#24230;&#25552;&#39640;&#20102;35%&#65289;&#12290;LogicGuide&#20063;&#22823;&#22823;&#20943;&#23569;&#20102;&#20869;&#23481;&#25928;&#26524;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often achieve higher accuracy when reasoning step-by-step in complex tasks. However, their reasoning can be unsound, inconsistent, or rely on undesirable prior assumptions. To tackle these issues, we introduce a class of tools for language models called guides that use state and incremental constraints to guide generation. A guide can be invoked by the model to constrain its own generation to a set of valid statements given by the tool. In turn, the model's choices can change the guide's state. We show how a general system for logical reasoning can be used as a guide, which we call LogicGuide. Given a reasoning problem in natural language, a model can formalize its assumptions for LogicGuide and then guarantee that its reasoning steps are sound. In experiments with the PrOntoQA and ProofWriter reasoning datasets, LogicGuide significantly improves the performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35%). LogicGuide also drastically reduces content eff
&lt;/p&gt;</description></item><item><title>&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03286</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29983;&#23384;&#26412;&#33021;
&lt;/p&gt;
&lt;p&gt;
Survival Instinct in Offline Reinforcement Learning. (arXiv:2306.03286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03286
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21363;&#20351;&#20351;&#29992;&#38169;&#35823;&#30340;&#22870;&#21169;&#26631;&#31614;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#65292;&#36825;&#31181;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#30001;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24754;&#35266;&#20027;&#20041;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#30340;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#34892;&#20026;&#30340;&#26032;&#35266;&#23519;&#65306;&#22312;&#35768;&#22810;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21363;&#20351;&#20351;&#29992;&#8220;&#38169;&#35823;&#8221;&#30340;&#22870;&#21169;&#26631;&#31614;&#65288;&#20363;&#22914;&#22312;&#25152;&#26377;&#22320;&#26041;&#37117;&#20026;&#38646;&#25110;&#26159;&#30495;&#23454;&#22870;&#21169;&#30340;&#36127;&#25968;&#65289;&#65292;&#20063;&#33021;&#20135;&#29983;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#23433;&#20840;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#29616;&#35937;&#19981;&#33021;&#20165;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#26469;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#23427;&#36171;&#20104;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#22312;&#20854;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#24212;&#29289;&#20013;&#26159;&#19981;&#20856;&#22411;&#30340;&#65292;&#22240;&#20026;&#21518;&#32773;&#23545;&#22870;&#21169;&#35774;&#35745;&#25935;&#24863;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#24778;&#20154;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#24754;&#35266;&#20027;&#20041;&#27010;&#24565;&#21644;&#24120;&#35265;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#20013;&#26576;&#31181;&#20559;&#35265;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#12290;&#24754;&#35266;&#20027;&#20041;&#36171;&#20104;&#20102;&#20195;&#29702;&#29983;&#23384;&#26412;&#33021;&#65292;&#21363;&#38271;&#26399;&#20869;&#30041;&#22312;&#25968;&#25454;&#25903;&#25345;&#20013;&#30340;&#28608;&#21169;&#65292;&#32780;&#26377;&#38480;&#19988;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#35206;&#30422;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#29983;&#23384;&#34892;&#20026;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel observation about the behavior of offline reinforcement learning (RL) algorithms: on many benchmark datasets, offline RL can produce well-performing and safe policies even when trained with "wrong" reward labels, such as those that are zero everywhere or are negatives of the true rewards. This phenomenon cannot be easily explained by offline RL's return maximization objective. Moreover, it gives offline RL a degree of robustness that is uncharacteristic of its online RL counterparts, which are known to be sensitive to reward design. We demonstrate that this surprising robustness property is attributable to an interplay between the notion of pessimism in offline RL algorithms and a certain bias implicit in common data collection practices. As we prove in this work, pessimism endows the agent with a "survival instinct", i.e., an incentive to stay within the data support in the long term, while the limited and biased data coverage further constrains the set of survival 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14979</link><description>&lt;p&gt;
&#23610;&#24230;&#24456;&#37325;&#35201;&#65306;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#35299;&#37322;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scale Matters: Attribution Meets the Wavelet Domain to Explain Model Sensitivity to Image Corruptions. (arXiv:2305.14979v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;WCAM&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#65292;&#30830;&#23450;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#38416;&#26126;&#32553;&#25918;&#22914;&#20309;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#30001;&#20110;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#23646;&#24615;&#26041;&#27861;&#23545;&#20110;&#35299;&#37322;&#23545;&#22270;&#20687;&#25439;&#22351;&#30340;&#25935;&#24863;&#24615;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#24378;&#20581;&#24615;&#39046;&#22495;&#30340;&#25991;&#29486;&#20165;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#20687;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#23457;&#26597;&#27169;&#22411;&#30340;&#34892;&#20026;&#33021;&#21147;&#23545;&#20110;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Wavelet sCale Attribution Method (WCAM)&#65292;&#23427;&#26159;&#20174;&#20687;&#32032;&#22495;&#21040;&#31354;&#38388;&#23610;&#24230;&#22495;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#27010;&#25324;&#12290;&#22312;&#31354;&#38388;&#23610;&#24230;&#22495;&#20013;&#36827;&#34892;&#23646;&#24615;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#27880;&#28857;&#21644;&#23610;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;WCAM&#35299;&#37322;&#20102;&#27169;&#22411;&#22312;&#22270;&#20687;&#30772;&#22351;&#19979;&#30340;&#22833;&#25928;&#65292;&#30830;&#23450;&#20102;&#39044;&#27979;&#30340;&#36275;&#22815;&#20449;&#24687;&#65292;&#24182;&#35299;&#37322;&#20102;&#22914;&#20309;&#36890;&#36807;&#32553;&#25918;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have shown remarkable performance in computer vision, but their deployment in real-world scenarios is challenging due to their sensitivity to image corruptions. Existing attribution methods are uninformative for explaining the sensitivity to image corruptions, while the literature on robustness only provides model-based explanations. However, the ability to scrutinize models' behavior under image corruptions is crucial to increase the user's trust. Towards this end, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain. Attribution in the space-scale domain reveals where and on what scales the model focuses. We show that the WCAM explains models' failures under image corruptions, identifies sufficient information for prediction, and explains how zoom-in increases accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11414</link><description>&lt;p&gt;
&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65306;&#29992;&#20110;&#22823;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;BERT&#12289;GPT&#12289;ViT&#21644;CLIP&#65292;&#20294;&#20854;&#20248;&#21270;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11243</link><description>&lt;p&gt;
&#27604;&#36739;&#26426;&#22120;&#21644;&#20799;&#31461;&#65306;&#20351;&#29992;&#21457;&#23637;&#24515;&#29702;&#23398;&#23454;&#39564;&#35780;&#20272;LaMDA&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11243
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#33457;&#36153;&#20102;&#20960;&#21313;&#24180;&#30340;&#26102;&#38388;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#23156;&#20799;&#21644;&#20799;&#31461;&#30340;&#26234;&#21147;&#21644;&#30693;&#35782;&#65292;&#36861;&#28335;&#37325;&#35201;&#27010;&#24565;&#21644;&#33021;&#21147;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#30340;&#32463;&#20856;&#23454;&#39564;&#26159;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;LLM&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#20043;&#19968;&#12290;&#20854;&#27425;&#65292;&#23558;LLM&#19982;&#20799;&#31461;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#26377;&#20154;&#31867;&#29305;&#28857;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#23884;&#20837;&#21040;&#38656;&#35201;&#19982;&#20154;&#20132;&#20114;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on othe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10782</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#20540;&#22823;&#23567;&#27604;&#36739;&#25928;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numeric Magnitude Comparison Effects in Large Language Models. (arXiv:2305.10782v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23383;&#22823;&#23567;&#27604;&#36739;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#36798;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24182;&#27809;&#26377;&#21306;&#20998;&#20986;&#25991;&#23383;&#20013;&#30340;&#25968;&#23383;&#65292;&#32780;&#25968;&#23383;&#22312;&#25991;&#26412;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#23545;&#25968;&#23383;&#21644;&#21333;&#35789;&#26377;&#30528;&#19981;&#21516;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;&#34892;&#20026;&#35282;&#24230;&#25506;&#31350;&#27969;&#34892;&#30340;LLMs&#33021;&#22815;&#22810;&#22909;&#22320;&#25429;&#25417;&#25968;&#23383;&#30340;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;$4&lt;5$&#65289;&#12290;&#20197;&#24448;&#23545;LLMs&#34920;&#24449;&#33021;&#21147;&#30340;&#30740;&#31350;&#21697;&#35780;&#20182;&#20204;&#26159;&#21542;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#27604;&#22914;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#25972;&#20307;&#20934;&#30830;&#29575;&#36739;&#39640;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#19982;&#35748;&#30693;&#31185;&#23398;&#30456;&#20851;&#30340;&#19981;&#21516;&#38382;&#39064;&#65306;LLMs&#25968;&#23383;&#34920;&#24449;&#19982;&#20154;&#31867;&#35821;&#35328;&#29992;&#25143;&#30340;&#34920;&#29616;&#26377;&#22810;&#25509;&#36817;&#65292;&#20182;&#20204;&#36890;&#24120;&#34920;&#29616;&#20986;&#36317;&#31163;&#12289;&#22823;&#23567;&#21644;&#27604;&#20363;&#25928;&#24212;? &#25105;&#20204;&#20381;&#38752;&#19968;&#20010;&#36830;&#25509;&#20551;&#35774;&#23558;&#25968;&#23383;&#21333;&#35789;&#21644;&#25968;&#23383;&#30340;&#27169;&#22411;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26144;&#23556;&#21040;&#20154;&#31867;&#21453;&#24212;&#26102;&#38388;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#32570;&#20047;&#25968;&#23383;&#34920;&#31034;&#65292;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#37117;&#20855;&#26377;&#24778;&#20154;&#30340;&#31867;&#20154;&#34920;&#24449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that $4 &lt; 5$) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07143</link><description>&lt;p&gt;
&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Longitudinal Car-Following Model. (arXiv:2304.07143v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36880;&#36710;&#36319;&#39536;&#27169;&#22411;&#30340;&#19981;&#21516;&#21407;&#21017;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36319;&#36710;&#27169;&#22411;&#26159;&#20132;&#36890;&#20223;&#30495;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#24050;&#32463;&#20869;&#32622;&#20110;&#35768;&#22810;&#37197;&#22791;ADAS&#30340;&#27773;&#36710;&#20013;&#12290;&#23545;&#36710;&#36319;&#36710;&#34892;&#20026;&#30340;&#30740;&#31350;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#30001;&#22522;&#26412;&#30340;&#36710;&#36742;&#20132;&#20114;&#36807;&#31243;&#24341;&#36215;&#30340;&#19981;&#21516;&#23439;&#35266;&#29616;&#35937;&#30340;&#26681;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#23613;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#21508;&#31181;&#36710;&#36319;&#36710;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#21035;&#12289;&#20114;&#34917;&#24615;&#21644;&#37325;&#21472;&#20043;&#22788;&#12290;&#35813;&#23457;&#26597;&#23558;&#22312;&#19981;&#21516;&#21407;&#21017;&#20013;&#27010;&#24565;&#21270;&#30340;&#36710;&#36319;&#36710;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The car-following (CF) model is the core component for traffic simulations and has been built-in in many production vehicles with Advanced Driving Assistance Systems (ADAS). Research of CF behavior allows us to identify the sources of different macro phenomena induced by the basic process of pairwise vehicle interaction. The CF behavior and control model encompasses various fields, such as traffic engineering, physics, cognitive science, machine learning, and reinforcement learning. This paper provides a comprehensive survey highlighting differences, complementarities, and overlaps among various CF models according to their underlying logic and principles. We reviewed representative algorithms, ranging from the theory-based kinematic models, stimulus-response models, and cruise control models to data-driven Behavior Cloning (BC) and Imitation Learning (IL) and outlined their strengths and limitations. This review categorizes CF models that are conceptualized in varying principles and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.07074</link><description>&lt;p&gt;
&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#31243;&#24207;&#30340;&#22768;&#38899;&#21644;&#30456;&#23545;&#23436;&#22791;&#30340;&#20449;&#24565; Hoare &#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Sound and Relatively Complete Belief Hoare Logic for Statistical Hypothesis Testing Programs. (arXiv:2208.07074v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07074
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27491;&#24335;&#25551;&#36848;&#32479;&#35745;&#25512;&#26029;&#30340;&#35201;&#27714;&#65292;&#24182;&#26816;&#26597;&#31243;&#24207;&#26159;&#21542;&#36866;&#24403;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20449;&#24565; Hoare &#36923;&#36753; (BHL) &#20197;&#35268;&#33539;&#21644;&#25512;&#29702;&#32463;&#30001;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#30340;&#32479;&#35745;&#20449;&#24565;&#12290;&#35813;&#31243;&#24207;&#36923;&#36753;&#22312;&#20551;&#35774;&#27979;&#35797;&#30340; Kripke &#27169;&#22411;&#20013;&#26159;&#21487;&#38752;&#30340;&#21644;&#30456;&#23545;&#23436;&#22791;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#28436;&#31034;&#20102; BHL &#29992;&#20110;&#25512;&#29702;&#20551;&#35774;&#26816;&#39564;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#33719;&#24471;&#32479;&#35745;&#20449;&#24565;&#20013;&#20808;&#39564;&#20449;&#24565;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#31243;&#24207;&#36923;&#36753;&#20869;&#22806;&#32479;&#35745;&#25512;&#26029;&#30340;&#25972;&#20010;&#22270;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to formally describing the requirement for statistical inference and checking whether a program uses the statistical method appropriately. Specifically, we define belief Hoare logic (BHL) for formalizing and reasoning about the statistical beliefs acquired via hypothesis testing. This program logic is sound and relatively complete with respect to a Kripke model for hypothesis tests. We demonstrate by examples that BHL is useful for reasoning about practical issues in hypothesis testing. In our framework, we clarify the importance of prior beliefs in acquiring statistical beliefs through hypothesis testing, and discuss the whole picture of the justification of statistical inference inside and outside the program logic.
&lt;/p&gt;</description></item></channel></rss>