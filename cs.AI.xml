<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.03134</link><description>&lt;p&gt;
&#22797;&#26434;&#20013;&#30340;&#31616;&#21333;
&lt;/p&gt;
&lt;p&gt;
Simplicity in Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#65292;&#19982;&#20043;&#21069;&#22797;&#26434;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#33021;&#27867;&#21270;&#65292;&#21448;&#33021;&#20026;&#29702;&#35770;&#29702;&#35299;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21050;&#28608;&#30340;&#22797;&#26434;&#24615;&#22312;&#35768;&#22810;&#35748;&#30693;&#29616;&#35937;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#27880;&#24847;&#21147;&#12289;&#21442;&#19982;&#24230;&#12289;&#26131;&#35760;&#24615;&#12289;&#26102;&#38388;&#24863;&#30693;&#21644;&#32654;&#23398;&#35780;&#20215;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#22797;&#26434;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#65292;&#35773;&#21050;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#22270;&#20687;&#22797;&#26434;&#24615;&#27169;&#22411;&#30456;&#24403;&#22797;&#26434;&#12290;&#26089;&#20808;&#30340;&#27169;&#22411;&#35797;&#22270;&#23547;&#25214;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#26159;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#27867;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22797;&#26434;&#24615;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#19981;&#25351;&#23548;&#23545;&#38382;&#39064;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#21306;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#24314;&#27169;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;SAM&#21644;FC-CLIP&#65292;&#26469;&#37327;&#21270;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#31890;&#24230;&#30340;&#21306;&#27573;&#25968;&#37327;&#65292;&#20197;&#21450;&#22270;&#20687;&#20013;&#30340;&#31867;&#21035;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03134v1 Announce Type: cross  Abstract: The complexity of visual stimuli plays an important role in many cognitive phenomena, including attention, engagement, memorability, time perception and aesthetic evaluation. Despite its importance, complexity is poorly understood and ironically, previous models of image complexity have been quite \textit{complex}. There have been many attempts to find handcrafted features that explain complexity, but these features are usually dataset specific, and hence fail to generalise. On the other hand, more recent work has employed deep neural networks to predict complexity, but these models remain difficult to interpret, and do not guide a theoretical understanding of the problem. Here we propose to model complexity using segment-based representations of images. We use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the number of segments at multiple granularities, and the number of classes in an image respectively. We find 
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11319</link><description>&lt;p&gt;
&#20351;&#29992;RGBD&#24863;&#30693;&#21644;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#23545;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30340;&#28382;&#21518;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11319
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#22240;&#33021;&#22815;&#36890;&#36807;&#38750;&#32447;&#24615;&#36335;&#24452;&#36827;&#20837;&#29421;&#31364;&#31354;&#38388;&#32780;&#34987;&#37325;&#35270;&#20110;&#24494;&#21019;&#25163;&#26415;&#12290;&#20294;&#26159;&#21463;&#21040;&#30005;&#32518;&#25928;&#24212;&#65288;&#22914;&#25705;&#25830;&#12289;&#20280;&#38271;&#21644;&#32806;&#21512;&#65289;&#24341;&#36215;&#30340;&#28382;&#21518;&#25928;&#24212;&#23548;&#33268;&#30005;&#32518;&#39537;&#21160;&#26426;&#26500;&#38754;&#20020;&#25511;&#21046;&#22256;&#38590;&#12290;&#36825;&#20123;&#25928;&#24212;&#30001;&#20110;&#38750;&#32447;&#24615;&#32780;&#24456;&#38590;&#24314;&#27169;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#19988;&#22810;&#33410;&#27573;&#26426;&#26800;&#33218;&#26102;&#36825;&#20123;&#22256;&#38590;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#30005;&#32518;&#39537;&#21160;&#30340;&#36825;&#31181;&#38750;&#32447;&#24615;&#21644;&#20197;&#24448;&#29366;&#24577;&#20381;&#36182;&#29305;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#23450;&#21046;&#30340;&#22522;&#20934;&#26631;&#35760;&#26469;&#25910;&#38598;&#29289;&#29702;&#20851;&#33410;&#37197;&#32622;&#20316;&#20026;&#25968;&#25454;&#38598;&#12290;&#23545;&#22235;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#21033;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;TCN&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25511;&#21046;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11319v1 Announce Type: cross  Abstract: Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.03328</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;AI&#27169;&#22411;&#32570;&#20047;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large-scale Generative AI Models Lack Visual Number Sense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;AI&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21629;&#21517;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#25968;&#37327;&#29289;&#21697;&#30340;&#22270;&#20687;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#30340;&#26041;&#24335;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#36731;&#26494;&#21028;&#26029;&#29289;&#20307;&#30340;&#25968;&#37327;&#65292;&#21363;&#20351;&#19981;&#36827;&#34892;&#35745;&#25968;&#65292;&#32780;&#19988;&#36825;&#31181;&#25216;&#33021;&#22312;&#21508;&#31181;&#21160;&#29289;&#29289;&#31181;&#21644;&#35821;&#35328;&#21457;&#23637;&#21644;&#27491;&#24335;&#23398;&#26657;&#25945;&#32946;&#20043;&#21069;&#30340;&#23156;&#20799;&#20013;&#37117;&#26377;&#35760;&#24405;&#12290;&#23545;&#20110;&#23567;&#30340;&#29289;&#20307;&#38598;&#65292;&#25968;&#23383;&#21028;&#26029;&#26159;&#26080;&#35823;&#30340;&#65292;&#32780;&#23545;&#20110;&#26356;&#22823;&#30340;&#38598;&#21512;&#65292;&#22238;&#24212;&#21464;&#24471;&#36817;&#20284;&#65292;&#24182;&#19988;&#21464;&#24322;&#24615;&#19982;&#30446;&#26631;&#25968;&#23383;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#23613;&#31649;&#29289;&#20307;&#29305;&#24449;&#65288;&#22914;&#39068;&#33394;&#25110;&#24418;&#29366;&#65289;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#31181;&#22238;&#24212;&#27169;&#24335;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#29289;&#20307;&#19978;&#35266;&#23519;&#21040;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#35270;&#35273;&#25968;&#23383;&#24863;&#30693;&#20381;&#36182;&#20110;&#25968;&#23383;&#25968;&#37327;&#30340;&#25277;&#35937;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21487;&#38752;&#22320;&#21629;&#21517;&#31616;&#21333;&#35270;&#35273;&#21050;&#28608;&#20013;&#30340;&#29289;&#20307;&#25968;&#37327;&#25110;&#29983;&#25104;&#21253;&#21547;&#30446;&#26631;&#29289;&#21697;&#25968;&#37327;&#30340;&#22270;&#20687;&#65288;1-10&#33539;&#22260;&#20869;&#65289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#32771;&#34385;&#30340;&#25152;&#26377;&#22522;&#30784;&#27169;&#22411;&#37117;&#27809;&#26377;&#20197;&#31867;&#20284;&#20154;&#31867;&#19968;&#26679;&#30340;&#26041;&#24335;&#34920;&#29616;&#20986;&#26469;&#65306;&#21363;&#20351;&#26159;&#20855;&#26377;&#36739;&#23567;&#25968;&#37327;&#30340;&#29289;&#20307;&#20063;&#20250;&#29359;&#19979;&#26174;&#33879;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can readily judge the number of objects in a visual scene, even without counting, and such a skill has been documented in a variety of animal species and in babies prior to language development and formal schooling. Numerical judgments are error-free for small sets, while for larger collections responses become approximate, with variability increasing proportionally to the target number. This response pattern is observed for items of all kinds, despite variation in object features (such as color or shape), suggesting that our visual number sense relies on abstract representations of numerosity. Here, we investigated whether generative Artificial Intelligence (AI) models based on large-scale transformer architectures can reliably name the number of objects in simple visual stimuli or generate images containing a target number of items in the 1-10 range. Surprisingly, none of the foundation models considered performed in a human-like way: They all made striking errors even with sm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36890;&#36807;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;20&#21517;&#27861;&#24459;&#19987;&#23478;&#30340;&#35752;&#35770;&#20013;&#25552;&#21462;&#20102;&#22235;&#20010;&#24433;&#21709;&#22240;&#32032;&#65306;&#29992;&#25143;&#23646;&#24615;&#12289;&#26597;&#35810;&#29305;&#24449;&#12289;AI&#33021;&#21147;&#21644;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01864</link><description>&lt;p&gt;
(A)&#25105;&#19981;&#26159;&#24459;&#24072;, &#20294;&#26159;...: &#21521;&#24459;&#24072;&#19987;&#23478;&#21046;&#23450;&#36127;&#36131;&#20219;&#30340;&#27861;&#24459;&#21672;&#35810;&#30340;LLM&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01864
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#35843;&#26597;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#20379;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#36890;&#36807;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20174;20&#21517;&#27861;&#24459;&#19987;&#23478;&#30340;&#35752;&#35770;&#20013;&#25552;&#21462;&#20102;&#22235;&#20010;&#24433;&#21709;&#22240;&#32032;&#65306;&#29992;&#25143;&#23646;&#24615;&#12289;&#26597;&#35810;&#29305;&#24449;&#12289;AI&#33021;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36805;&#36895;&#25193;&#25955;, &#24102;&#26469;&#20102;&#25193;&#22823;&#20844;&#20247;&#33719;&#24471;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#19987;&#19994;&#25351;&#23548;&#30340;&#24076;&#26395;, &#21516;&#26102;&#24341;&#21457;&#20102;&#20844;&#20247;&#23545;LLM&#22312;&#37325;&#22823;&#20107;&#20214;&#20013;&#30340;&#20381;&#36182;&#30340;&#25285;&#24551;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#29468;&#27979;&#20102;&#39640;&#23618;&#27425;&#30340;&#20262;&#29702;&#32771;&#34385;, &#20294;&#32570;&#20047;&#20855;&#20307;&#30340;&#26631;&#20934;&#26469;&#30830;&#23450;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#24212;&#35813;&#25110;&#19981;&#24212;&#35813;&#25552;&#20379;&#19987;&#19994;&#24110;&#21161;&#12290;&#36890;&#36807;&#30740;&#31350;&#27861;&#24459;&#39046;&#22495;, &#25105;&#20204;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#20998;&#26512;, &#20197;&#21457;&#29616;&#20851;&#20110;&#20351;&#29992;LLM&#36827;&#34892;&#19987;&#19994;&#21672;&#35810;&#30340;&#25919;&#31574;&#32771;&#34385;&#30340;&#32454;&#24494;&#24046;&#21035;, &#24182;&#37319;&#29992;&#26696;&#20363;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19982;20&#21517;&#27861;&#24459;&#19987;&#23478;&#21484;&#24320;&#20102;&#30740;&#35752;&#20250;, &#24182;&#20174;&#26679;&#26412;&#29992;&#25143;&#26597;&#35810;("&#26696;&#20363;")&#20013;&#25552;&#21462;&#20986;&#36866;&#24403;&#30340;AI&#36741;&#21161;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#23558;&#19987;&#19994;&#32500;&#24230;&#20998;&#20026;: (1)&#29992;&#25143;&#23646;&#24615;, (2)&#26597;&#35810;&#29305;&#24449;, (3)AI&#33021;&#21147;, &#21644; (4)&#24433;&#21709;&#12290;&#38500;&#20102;&#24050;&#30693;&#30340;&#38382;&#39064;, &#22914;&#24187;&#35273;, &#19987;&#23478;&#20204;&#36824;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of large language models (LLMs) as general purpose chatbots available to the public raises hopes around expanding access to professional guidance in law, medicine, and finance, while triggering concerns about public reliance on LLMs for high-stakes circumstances. Prior research has speculated on high-level ethical considerations but lacks concrete criteria determining when and why LLM chatbots should or should not provide professional assistance. Through examining the legal domain, we contribute a structured expert analysis to uncover nuanced policy considerations around using LLMs for professional advice, using methods inspired by case-based reasoning. We convened workshops with 20 legal experts and elicited dimensions on appropriate AI assistance for sample user queries (``cases''). We categorized our expert dimensions into: (1) user attributes, (2) query characteristics, (3) AI capabilities, and (4) impacts. Beyond known issues like hallucinations, experts re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2311.16834</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#20351;&#29992;&#27880;&#24847;&#21147;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#27668;&#35937;&#23398;&#21644;&#29983;&#21629;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#34987;&#25209;&#35780;&#20026;&#8220;&#40657;&#30418;&#8221;&#25110;&#26080;&#27861;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#26500;&#36896;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#25233;&#21046;&#22312;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#24615;&#20013;&#20351;&#29992;&#30340;&#20887;&#20313;&#29305;&#24449;&#12290;&#20174;&#36873;&#25321;&#30340;&#29305;&#24449;&#29420;&#31435;&#35757;&#32451;&#27169;&#22359;&#21270;&#28145;&#24230;&#32593;&#32476;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.10329</link><description>&lt;p&gt;
&#39640;&#20445;&#30495;&#24230;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Person-centric Subject-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Face-diffuser&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#35299;&#20915;&#20027;&#20307;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20197;&#20027;&#20307;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#29983;&#25104;&#20013;&#36935;&#21040;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#21407;&#22240;&#22312;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#36890;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#26469;&#23398;&#20064;&#35821;&#20041;&#22330;&#26223;&#21644;&#20154;&#29289;&#29983;&#25104;&#65292;&#36825;&#28041;&#21450;&#21040;&#19968;&#31181;&#26080;&#27861;&#35843;&#21644;&#30340;&#35757;&#32451;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Face-diffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#21327;&#20316;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#28040;&#38500;&#19978;&#36848;&#35757;&#32451;&#19981;&#24179;&#34913;&#21644;&#36136;&#37327;&#22949;&#21327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10329v3 Announce Type: replace-cross  Abstract: Current subject-driven image generation methods encounter significant challenges in person-centric image generation. The reason is that they learn the semantic scene and person generation by fine-tuning a common pre-trained diffusion, which involves an irreconcilable training imbalance. Precisely, to generate realistic persons, they need to sufficiently tune the pre-trained model, which inevitably causes the model to forget the rich semantic scene prior and makes scene generation over-fit to the training data. Moreover, even with sufficient fine-tuning, these methods can still not generate high-fidelity persons since joint learning of the scene and person generation also lead to quality compromise. In this paper, we propose Face-diffuser, an effective collaborative generation pipeline to eliminate the above training imbalance and quality compromise. Specifically, we first develop two specialized pre-trained diffusion models, i.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2109.07319</link><description>&lt;p&gt;
InceptionXML&#65306;&#19968;&#31181;&#24102;&#26377;&#21516;&#27493;&#36127;&#37319;&#26679;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#25968;&#25454;&#23545;&#22823;&#37327;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#34987;&#31216;&#20026;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#30456;&#20851;&#25628;&#32034;&#39044;&#27979;&#21644;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#26550;&#26500;InceptionXML&#65292;&#20854;&#36731;&#37327;&#20294;&#21151;&#33021;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#25628;&#32034;&#21644;&#25512;&#33616;&#20219;&#21153;&#20013;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#32570;&#20047;&#21333;&#35789;&#39034;&#24207;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21367;&#31215;&#30340;&#25805;&#20316;&#27839;&#30528;&#23884;&#20837;&#32500;&#24230;&#37325;&#26032;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;CNNs&#19968;&#26679;&#27839;&#30528;&#21333;&#35789;&#32500;&#24230;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#24212;&#29992;&#21367;&#31215;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#26631;&#31614;&#31579;&#36873;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
&lt;/p&gt;</description></item><item><title>LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.10314</link><description>&lt;p&gt;
LangProp: &#19968;&#31181;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10314
&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30417;&#30563;/&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36845;&#20195;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;LLM&#33021;&#22815;&#38646;-shot&#22320;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#21021;&#22987;&#20195;&#30721;&#21487;&#33021;&#22312;&#26576;&#20123;&#36793;&#32536;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;LangProp&#33258;&#21160;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#20195;&#30721;&#24615;&#33021;&#65292;&#24182;&#25429;&#25417;&#20219;&#20309;&#24322;&#24120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20351;LLM&#21487;&#20197;&#36845;&#20195;&#25913;&#36827;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24230;&#37327;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#36827;&#34892;&#20195;&#30721;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20511;&#37492;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;DAgger&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#30340;&#31532;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;LangProp&#21487;&#20197;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#39550;&#39542;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09441</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;-&#33021;&#32791;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#23427;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#21019;&#26032;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SFL&#20013;&#27169;&#22411;&#22312;&#29305;&#23450;&#23618;&#65288;&#31216;&#20026;&#20999;&#21106;&#23618;&#65289;&#19978;&#34987;&#20998;&#21106;&#20026;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#65292;&#36873;&#25321;&#20999;&#21106;&#23618;&#21487;&#33021;&#23545;&#23458;&#25143;&#31471;&#30340;&#33021;&#32791;&#21644;&#38544;&#31169;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#20102;&#35757;&#32451;&#36127;&#25285;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20999;&#21106;&#23618;&#30340;&#35774;&#35745;&#25361;&#25112;&#38750;&#24120;&#22797;&#26434;&#65292;&#20027;&#35201;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#32593;&#32476;&#33021;&#21147;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;SFL&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#33021;&#32791;&#21644;&#38544;&#31169;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.11884</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#28608;&#27963;&#21040;&#27010;&#24565;: &#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#27010;&#24565;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#33258;&#28982;&#26725;&#26753;&#65306;&#19968;&#26086;&#30830;&#23450;&#20102;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#23601;&#21487;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#25512;&#29702;&#31995;&#32479;&#25972;&#21512;&#65292;&#29992;&#20110;&#25512;&#29702;&#25110;&#20351;&#29992;&#25512;&#29702;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#25110;&#22686;&#24378;&#20197;&#25913;&#21892;&#23398;&#20064;&#31995;&#32479;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#20165;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#23558;&#27010;&#24565;&#30693;&#35782;&#25554;&#20837;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#30001;&#20110;&#25972;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#26159;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#25152;&#20197;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#33719;&#24471;&#30340;&#35265;&#35299;&#21487;&#20197;&#25104;&#20026;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
&lt;/p&gt;</description></item><item><title>RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17176</link><description>&lt;p&gt;
RLAdapter&#65306;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
RLAdapter: Bridging Large Language Models to Reinforcement Learning in Open Worlds. (arXiv:2309.17176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17176
&lt;/p&gt;
&lt;p&gt;
RLAdapter&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#36890;&#36807;&#35299;&#20915;LLM&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#36890;&#36807;&#36991;&#20813;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#24494;&#35843;LLM&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20915;&#31574;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#20132;&#20114;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20026;&#20195;&#29702;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#22686;&#24378;RL&#31639;&#27861;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#29702;&#35299;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26368;&#20248;&#22320;&#24110;&#21161;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25968;&#25454;&#26469;&#24494;&#35843;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#29992;&#30340;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#22256;&#38590;&#65292;&#27604;&#22914;&#26080;&#27861;&#35775;&#38382;&#30340;&#27169;&#22411;&#26435;&#37325;&#25110;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#20351;&#20854;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RLAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;RL&#31639;&#27861;&#21644;LLM&#20043;&#38388;&#24314;&#31435;&#26356;&#22909;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16263</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#21160;&#21147;&#23398;&#65306;&#25506;&#32034;&#20855;&#26377;&#22343;&#20540;&#22330;&#22343;&#34913;&#30340;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;
&lt;/p&gt;
&lt;p&gt;
Cooperation Dynamics in Multi-Agent Systems: Exploring Game-Theoretic Scenarios with Mean-Field Equilibria. (arXiv:2309.16263v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#21644;&#24341;&#20837;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#30340;&#20462;&#25913;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#20010;&#20307;&#25910;&#30410;&#21644;&#38598;&#20307;&#22238;&#25253;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#21338;&#24328;&#29702;&#35770;&#24773;&#26223;&#20013;&#28608;&#21457;&#21512;&#20316;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;&#36845;&#20195;&#22234;&#24466;&#22256;&#22659;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#24517;&#39035;&#20248;&#21270;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#32467;&#26524;&#12290;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#21512;&#20316;&#31574;&#30053;&#23545;&#20110;&#20419;&#36827;&#37325;&#22797;&#21338;&#24328;&#20013;&#22242;&#38431;&#23548;&#21521;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#65292;&#21363;&#40723;&#21169;&#22242;&#38431;&#22238;&#25253;&#20063;&#23558;&#23548;&#33268;&#26356;&#39640;&#30340;&#20010;&#20307;&#25910;&#30410;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#29616;&#23454;&#22256;&#22659;&#12290;&#30740;&#31350;&#36824;&#25193;&#23637;&#21040;&#26234;&#33021;&#20307;&#20154;&#21475;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#26223;&#65288;$N \longrightarrow +\infty$&#65289;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#35745;&#31639;&#21644;&#24179;&#34913;&#30830;&#23450;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21033;&#29992;&#22343;&#20540;&#22330;&#21338;&#24328;&#29702;&#35770;&#65292;&#24314;&#31435;&#20102;&#26080;&#38480;&#22823;&#26234;&#33021;&#20307;&#38598;&#21512;&#20013;&#30340;&#24179;&#34913;&#35299;&#21644;&#22870;&#21169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperation is fundamental in Multi-Agent Systems (MAS) and Multi-Agent Reinforcement Learning (MARL), often requiring agents to balance individual gains with collective rewards. In this regard, this paper aims to investigate strategies to invoke cooperation in game-theoretic scenarios, namely the Iterated Prisoner's Dilemma, where agents must optimize both individual and group outcomes. Existing cooperative strategies are analyzed for their effectiveness in promoting group-oriented behavior in repeated games. Modifications are proposed where encouraging group rewards will also result in a higher individual gain, addressing real-world dilemmas seen in distributed systems. The study extends to scenarios with exponentially growing agent populations ($N \longrightarrow +\infty$), where traditional computation and equilibrium determination are challenging. Leveraging mean-field game theory, equilibrium solutions and reward structures are established for infinitely large agent sets in repea
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2309.05030</link><description>&lt;p&gt;
&#21435;&#27542;&#27665;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#65306;&#23041;&#33394;&#36798;&#23572;&#29595;&#12289;&#35770;&#35777;&#21644;&#33402;&#26415;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Decolonial AI Alignment: Vi\'{s}esadharma, Argument, and Artistic Expression. (arXiv:2309.05030v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#27542;&#27665;&#21270;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19977;&#20010;&#24314;&#35758;&#65306;&#25913;&#21464;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#20801;&#35768;&#22810;&#20803;&#20027;&#20041;&#30340;&#35770;&#35777;&#20256;&#32479;&#23384;&#22312;&#20110;&#23545;&#40784;&#25216;&#26415;&#20013;&#65292;&#20197;&#21450;&#23558;&#20215;&#20540;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#38416;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#27542;&#27665;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#23545;&#40784;&#65306;&#21363;&#22522;&#20110;&#32454;&#33268;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34892;&#20026;&#19982;&#26399;&#26395;&#20540;&#19968;&#33268;&#12290;&#38500;&#20102;&#20854;&#20182;&#23454;&#36341;&#65292;&#27542;&#27665;&#20027;&#20041;&#36824;&#26377;&#19968;&#37096;&#20998;&#26159;&#25913;&#21464;&#34987;&#27542;&#27665;&#27665;&#26063;&#30340;&#20449;&#20208;&#21644;&#20215;&#20540;&#35266;&#30340;&#21382;&#21490;&#65307;&#32780;&#24403;&#21069;&#30340;LLM&#23545;&#40784;&#23454;&#36341;&#27491;&#26159;&#36825;&#19968;&#21382;&#21490;&#30340;&#22797;&#21046;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#19977;&#20010;&#25552;&#35758;&#23545;AI&#23545;&#40784;&#36827;&#34892;&#21435;&#27542;&#27665;&#21270;&#65306;&#65288;a&#65289;&#23558;&#22522;&#26412;&#36947;&#24503;&#21746;&#23398;&#20174;&#35199;&#26041;&#21746;&#23398;&#36716;&#21464;&#20026;&#36798;&#23572;&#29595;&#21746;&#23398;&#65292;&#65288;b&#65289;&#22312;&#23545;&#40784;&#25216;&#26415;&#20013;&#20801;&#35768;&#35770;&#35777;&#21644;&#22810;&#20803;&#20027;&#20041;&#30340;&#20256;&#32479;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#20215;&#20540;&#30340;&#35748;&#35782;&#35770;&#25193;&#23637;&#21040;&#36229;&#36234;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#20196;&#25110;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment. One process that that work has not engaged with much is alignment: the tuning of large language model (LLM) behavior to be in line with desired values based on fine-grained human feedback. In addition to other practices, colonialism has a history of altering the beliefs and values of colonized peoples; this history is recapitulated in current LLM alignment practices. We suggest that AI alignment be decolonialized using three proposals: (a) changing the base moral philosophy from Western philosophy to dharma, (b) permitting traditions of argument and pluralism in alignment technologies, and (c) expanding the epistemology of values beyond instructions or commandments given in natural language.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.04504</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#27010;&#24565;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#36890;&#36807;&#25968;&#30334;&#19975;&#20010;&#22238;&#21512;&#30340;&#35757;&#32451;&#25165;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#19982;&#25351;&#20196;&#30456;&#20851;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#25351;&#20196;&#32452;&#21512;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20799;&#31461;&#21487;&#20197;&#20998;&#35299;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#24182;&#23548;&#33322;&#21040;&#25351;&#23450;&#30340;&#29289;&#20307;&#65292;&#21363;&#20351;&#20182;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#26597;&#35810;&#30340;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;3D&#29615;&#22659;&#65292;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#23398;&#20064;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#12289;BERT&#65289;&#22312;&#26356;&#23569;&#30340;&#22238;&#21512;&#20013;&#23398;&#20064;&#21333;&#35789;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#20195;&#29702;&#22312;&#24418;&#29366;&#25110;&#39068;&#33394;&#27010;&#24565;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#25351;&#20196;&#32452;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02511</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27004;&#23618;&#24179;&#38754;&#31034;&#20363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#29983;&#25104;&#22120;&#22240;&#20854;&#33021;&#22815;&#26681;&#25454;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#20294;&#26159;&#65292;&#22312;&#22303;&#26408;&#24037;&#31243;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#21019;&#24314;&#29305;&#23450;&#30340;&#24314;&#31569;&#35774;&#35745;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#27004;&#23618;&#24179;&#38754;&#20316;&#20026;&#31034;&#20363;&#65292;&#25506;&#32034;&#22522;&#20110;&#25193;&#25955;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30446;&#21069;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#20174;6%&#25552;&#39640;&#21040;90%&#65292;&#24182;&#25913;&#36827;&#20102;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#20026;&#22303;&#26408;&#24037;&#31243;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#35299;&#20915;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13723</link><description>&lt;p&gt;
&#31038;&#20132;AI&#19982;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social AI and the Challenges of the Human-AI Ecosystem. (arXiv:2306.13723v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#35299;&#20915;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30340;&#23835;&#36215;&#65288;&#21253;&#25324;&#21161;&#25163;&#21644;&#25512;&#33616;&#31995;&#32479;&#65289;&#22686;&#21152;&#20102;&#20986;&#29616;&#38598;&#20307;&#29616;&#35937;&#21644;&#20020;&#30028;&#28857;&#30340;&#26426;&#20250;&#65292;&#24182;&#21487;&#33021;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#12289;&#21487;&#33021;&#26159;&#24847;&#22806;&#30340;&#21518;&#26524;&#12290;&#20363;&#22914;&#65292;&#23548;&#33322;&#31995;&#32479;&#30340;&#24314;&#35758;&#21487;&#33021;&#20250;&#22312;&#22826;&#22810;&#30340;&#21496;&#26426;&#34987;&#24341;&#23548;&#21040;&#21516;&#19968;&#36335;&#32447;&#26102;&#36896;&#25104;&#28151;&#20081;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#21487;&#33021;&#20250;&#25918;&#22823;&#26497;&#21270;&#12289;&#36807;&#28388;&#27668;&#27873;&#21644;&#28608;&#36827;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#22521;&#32946;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#21644;&#38598;&#20307;&#34892;&#21160;&#25928;&#24212;&#65292;&#20197;&#24212;&#23545;&#31038;&#20250;&#21644;&#29615;&#22659;&#25361;&#25112;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#23545;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#30340;&#24433;&#21709;&#24182;&#35774;&#35745;&#19982;&#20154;&#31867;&#21512;&#20316;&#20197;&#24110;&#21161;&#20811;&#26381;&#31038;&#20250;&#38382;&#39064;&#30340;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#22797;&#26434;&#31995;&#32479;&#12289;&#32593;&#32476;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#28857;&#19978;&#24314;&#31435;&#31038;&#20132;AI&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#24320;&#22987;&#35752;&#35770;&#31038;&#20132;AI&#25552;&#39640;&#38598;&#20307;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#31038;&#20132;AI&#21487;&#20197;&#20026;&#31038;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#21033;&#30410;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#26032;&#30340;&#25216;&#26415;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the "wisdom of crowds" and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective pape
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07465</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#21306;&#21035;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#21363;&#20351;&#24453;&#27979;&#35797;&#30340;&#24046;&#36317;&#24456;&#23567;&#65292;&#27979;&#35797;&#19968;&#20010;&#22343;&#34913;&#20063;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#38745;&#24577;&#28216;&#25103;&#20013;&#23384;&#22312;&#22810;&#20010;&#26368;&#20248;&#35299;&#65288;&#22343;&#34913;&#65289;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#22914;&#19968;&#33324;&#21644;&#21338;&#24328;&#12289;&#28508;&#22312;&#21338;&#24328;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21482;&#35201;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#37197;&#22791;&#36866;&#24403;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#31070;&#35861;&#12290;&#24403;&#38750;&#24179;&#31283;&#31243;&#24230;&#65288;&#36890;&#36807;&#24635;&#21464;&#21270;&#37327; $\Delta$ &#27979;&#37327;&#65289;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ &#30340;&#36951;&#25022;&#65292;&#24403; $\Delta$ &#26410;&#30693;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#22788;&#29702;&#28041;&#21450;&#26102;&#38388;&#32422;&#26463;&#30340;&#21160;&#24577;&#31995;&#32479;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#36890;&#36807;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#19968;&#19968;&#38454;&#20844;&#24335;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14778</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#36712;&#36857;&#30340;&#24230;&#37327;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Metric Temporal Equilibrium Logic over Timed Traces. (arXiv:2304.14778v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#22788;&#29702;&#28041;&#21450;&#26102;&#38388;&#32422;&#26463;&#30340;&#21160;&#24577;&#31995;&#32479;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#36890;&#36807;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#19968;&#19968;&#38454;&#20844;&#24335;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#32447;&#24615;&#26102;&#38388;&#30340;Answer Set Programming (ASP)&#30340;&#26102;&#38388;&#25193;&#23637;&#20013;&#65292;&#21160;&#24577;&#31995;&#32479;&#30340;&#34892;&#20026;&#36890;&#36807;&#29366;&#24577;&#24207;&#21015;&#26469;&#25429;&#33719;&#12290;&#34429;&#28982;&#27492;&#34920;&#31034;&#21453;&#26144;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#39034;&#24207;&#65292;&#20294;&#23427;&#25277;&#35937;&#25481;&#20102;&#19982;&#27599;&#20010;&#29366;&#24577;&#20851;&#32852;&#30340;&#20855;&#20307;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#32422;&#26463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#37325;&#35201;&#30340;&#65292;&#27604;&#22914;&#24403;&#35745;&#21010;&#21644;&#35843;&#24230;&#30456;&#20114;&#37197;&#21512;&#26102;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24230;&#37327;&#30340;&#32447;&#24615;&#26102;&#38388;&#24179;&#34913;&#36923;&#36753;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#36816;&#31639;&#31526;&#21463;&#33258;&#28982;&#25968;&#21306;&#38388;&#30340;&#32422;&#26463;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#24230;&#37327;&#24179;&#34913;&#36923;&#36753;&#20026;&#25351;&#23450;&#23450;&#24615;&#21644;&#23450;&#37327;&#21160;&#24577;&#32422;&#26463;&#30340;&#22522;&#20110;ASP&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24230;&#37327;&#20844;&#24335;&#36716;&#21270;&#20026;&#21333;&#35843;&#19968;&#38454;&#20844;&#24335;&#65292;&#24182;&#20998;&#21035;&#32473;&#20986;&#22312;Metric Equilibrium Logic&#21644;Monadic Quantified Equilibrium Logic&#20013;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32763;&#35793;&#25552;&#20379;&#20102;Metric Equilibrium Logic&#30340;&#23454;&#29616;&#27169;&#22411;&#26816;&#26597;&#30340;&#34013;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In temporal extensions of Answer Set Programming (ASP) based on linear-time, the behavior of dynamic systems is captured by sequences of states. While this representation reflects their relative order, it abstracts away the specific times associated with each state. However, timing constraints are important in many applications like, for instance, when planning and scheduling go hand in hand. We address this by developing a metric extension of linear-time temporal equilibrium logic, in which temporal operators are constrained by intervals over natural numbers. The resulting Metric Equilibrium Logic provides the foundation of an ASP-based approach for specifying qualitative and quantitative dynamic constraints. To this end, we define a translation of metric formulas into monadic first-order formulas and give a correspondence between their models in Metric Equilibrium Logic and Monadic Quantified Equilibrium Logic, respectively. Interestingly, our translation provides a blue print for im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;&#25340;&#25509;&#24182;&#23450;&#20301;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.14682</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32422;&#26463;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks. (arXiv:2207.14682v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14682
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#20934;&#30830;&#26816;&#27979;&#20986;&#25340;&#25509;&#24182;&#23450;&#20301;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23481;&#26131;&#33719;&#21462;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#38899;&#39057;&#32534;&#36753;&#24037;&#20855;&#20351;&#24471;&#38899;&#39057;&#25340;&#25509;&#21464;&#24471;&#31616;&#21333;&#12290;&#21487;&#36890;&#36807;&#32452;&#21512;&#21516;&#19968;&#20154;&#30340;&#21508;&#31181;&#35821;&#38899;&#26679;&#26412;&#26469;&#21019;&#24314;&#20196;&#20154;&#20449;&#26381;&#30340;&#20266;&#36896;&#21697;&#12290;&#26816;&#27979;&#27492;&#31867;&#25340;&#25509;&#22312;&#20844;&#20849;&#39046;&#22495;&#20013;&#32771;&#34385;&#21040;&#34394;&#20551;&#20449;&#24687;&#29978;&#33267;&#22312;&#27861;&#24459;&#32972;&#26223;&#19979;&#26680;&#23454;&#35777;&#25454;&#30340;&#23436;&#25972;&#24615;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#31639;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#29305;&#24449;&#24182;&#20570;&#20986;&#20102;&#29305;&#23450;&#30340;&#20551;&#35774;&#12290;&#20294;&#26159;&#65292;&#21009;&#20107;&#35843;&#26597;&#20154;&#21592;&#36890;&#24120;&#38754;&#20020;&#26469;&#33258;&#26410;&#30693;&#26469;&#28304;&#20855;&#26377;&#26410;&#30693;&#29305;&#24449;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#26356;&#26222;&#36866;&#30340;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#26397;&#30528;&#26080;&#32422;&#26463;&#38899;&#39057;&#25340;&#25509;&#26816;&#27979;&#26041;&#21521;&#36808;&#20986;&#31532;&#19968;&#27493;&#20197;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#21518;&#22788;&#29702;&#25805;&#20316;&#26469;&#27169;&#25311;&#19981;&#21516;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#21487;&#33021;&#25513;&#30422;&#25340;&#25509;&#23384;&#22312;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Transformer &#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#32593;&#32476;&#29992;&#20110;&#25340;&#25509;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21644;&#23450;&#20301;&#29978;&#33267;&#23384;&#22312;&#24378;&#28872;&#22833;&#30495;&#21644;&#32972;&#26223;&#22122;&#22768;&#30340;&#25340;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Freely available and easy-to-use audio editing tools make it straightforward to perform audio splicing. Convincing forgeries can be created by combining various speech samples from the same person. Detection of such splices is important both in the public sector when considering misinformation, and in a legal context to verify the integrity of evidence. Unfortunately, most existing detection algorithms for audio splicing use handcrafted features and make specific assumptions. However, criminal investigators are often faced with audio samples from unconstrained sources with unknown characteristics, which raises the need for more generally applicable methods.  With this work, we aim to take a first step towards unconstrained audio splicing detection to address this need. We simulate various attack scenarios in the form of post-processing operations that may disguise splicing. We propose a Transformer sequence-to-sequence (seq2seq) network for splicing detection and localization. Our exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#34385;&#20449;&#24515;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#20445;&#35777;&#26696;&#20363;&#65292;&#20854;&#20013;&#20027;&#35201;&#30340;&#25514;&#26045;&#26159;&#23558;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#30340;&#23436;&#22791;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.04522</link><description>&lt;p&gt;
&#35780;&#20272;&#8220;Assurance 2.0&#8221;&#20013;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Assessing Confidence with Assurance 2.0. (arXiv:2205.04522v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#34385;&#20449;&#24515;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#20445;&#35777;&#26696;&#20363;&#65292;&#20854;&#20013;&#20027;&#35201;&#30340;&#25514;&#26045;&#26159;&#23558;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#30340;&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#26696;&#20363;&#26088;&#22312;&#23545;&#20854;&#39030;&#32423;&#22768;&#26126;&#65288;&#36890;&#24120;&#28041;&#21450;&#23433;&#20840;&#24615;&#25110;&#23433;&#20840;&#65289;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20449;&#24515;&#12290;&#37027;&#20040;&#38382;&#39064;&#26469;&#20102;&#65292;&#8220;&#36825;&#31181;&#20449;&#24515;&#8221;&#26377;&#22810;&#23569;&#65311;&#25105;&#20204;&#35748;&#20026;&#20449;&#24515;&#19981;&#33021;&#24402;&#32467;&#20026;&#21333;&#19968;&#23646;&#24615;&#25110;&#27979;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#23427;&#24212;&#35813;&#22522;&#20110;&#19977;&#20010;&#19981;&#21516;&#35270;&#35282;&#30340;&#23646;&#24615;&#65306;&#31215;&#26497;&#65292;&#28040;&#26497;&#21644;&#21097;&#20313;&#30097;&#34385;&#12290;&#31215;&#26497;&#35270;&#35282;&#32771;&#34385;&#35777;&#25454;&#21644;&#25972;&#20010;&#26696;&#20363;&#30340;&#31243;&#24230;&#65292;&#23558;&#31215;&#26497;&#22768;&#26126;&#21512;&#29702;&#21270;&#20026;&#25903;&#25345;&#20854;&#20027;&#24352;&#30340;&#20449;&#20208;&#12290;&#25105;&#20204;&#23545;&#35777;&#26126;&#35774;&#31435;&#20102;&#39640;&#38376;&#27099;&#65292;&#35201;&#27714;&#35777;&#26126;&#26159;&#19981;&#21487;&#21542;&#35748;&#30340;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#27491;&#38754;&#25514;&#26045;&#26159;&#23436;&#22791;&#24615;&#65292;&#23558;&#35813;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#12290;&#35777;&#25454;&#30340;&#20449;&#24515;&#21487;&#20197;&#20197;&#27010;&#29575;&#26041;&#24335;&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;&#30830;&#35748;&#25514;&#26045;&#30830;&#20445;&#35777;&#25454;&#30340;"&#37325;&#37327;"&#36234;&#36807;&#26576;&#20010;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#21487;&#20197;&#20174;&#35777;&#25454;&#20013;&#32858;&#21512;&#65292;&#36890;&#36807;&#23558;&#35777;&#25454;&#35270;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#33410;&#28857;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An assurance case is intended to provide justifiable confidence in the truth of its top claim, which typically concerns safety or security. A natural question is then "how much" confidence does the case provide? We argue that confidence cannot be reduced to a single attribute or measurement. Instead, we suggest it should be based on attributes that draw on three different perspectives: positive, negative, and residual doubts.  Positive Perspectives consider the extent to which the evidence and overall argument of the case combine to make a positive statement justifying belief in its claims. We set a high bar for justification, requiring it to be indefeasible. The primary positive measure for this is soundness, which interprets the argument as a logical proof. Confidence in evidence can be expressed probabilistically and we use confirmation measures to ensure that the "weight" of evidence crosses some threshold. In addition, probabilities can be aggregated from evidence through the step
&lt;/p&gt;</description></item></channel></rss>