<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#35814;&#32454;&#29305;&#24449;&#65292;&#24182;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.01004</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning. (arXiv:2311.01004v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#30340;Sam&#24341;&#23548;&#22686;&#24378;&#32454;&#31890;&#24230;&#32534;&#30721;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#35814;&#32454;&#29305;&#24449;&#65292;&#24182;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#20855;&#26377;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35786;&#26029;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36890;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25551;&#36848;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#22797;&#26434;&#32454;&#33410;&#26102;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#24341;&#23548;&#30340;&#26032;&#22411;&#21307;&#23398;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#19968;&#33324;&#21644;&#35814;&#32454;&#29305;&#24449;&#30340;&#22686;&#24378;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#29420;&#29305;&#30340;&#28151;&#21512;&#35821;&#20041;&#23398;&#20064;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#25429;&#25417;&#21307;&#23398;&#22270;&#20687;&#30340;&#25972;&#20307;&#20449;&#24687;&#21644;&#26356;&#32454;&#33410;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#22312;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#25551;&#36848;&#30340;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#39044;&#35757;&#32451;&#30340;BLIP2&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of multimodality and large language models, the deep learning-based technique for medical image captioning holds the potential to offer valuable diagnostic recommendations. However, current generic text and image pre-trained models do not yield satisfactory results when it comes to describing intricate details within medical images. In this paper, we present a novel medical image captioning method guided by the segment anything model (SAM) to enable enhanced encoding with both general and detailed feature extraction. In addition, our approach employs a distinctive pre-training strategy with mixed semantic learning to simultaneously capture both the overall information and finer details within medical images. We demonstrate the effectiveness of this approach, as it outperforms the pre-trained BLIP2 model on various evaluation metrics for generating descriptions of medical images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00983</link><description>&lt;p&gt;
&#20248;&#21270;&#24211;&#23384;&#37197;&#36865;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Inventory Routing: A Decision-Focused Learning Approach using Neural Networks. (arXiv:2311.00983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65292;&#36890;&#36807;&#30452;&#25509;&#38598;&#25104;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23384;&#37197;&#36865;&#38382;&#39064;&#65288;IRP&#65289;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#22312;&#32771;&#34385;&#24211;&#23384;&#38656;&#27714;&#35268;&#21010;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#26377;&#25928;&#30340;&#36335;&#24452;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;IRP&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#38656;&#27714;&#65292;&#28982;&#21518;&#20351;&#29992;&#20248;&#21270;&#31639;&#27861;&#26469;&#26368;&#23567;&#21270;&#37197;&#36865;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#23436;&#32654;&#20934;&#30830;&#24230;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#24211;&#23384;&#27700;&#24179;&#21463;&#21160;&#24577;&#19994;&#21153;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#19968;&#38454;&#27573;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;IRP&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#20102;&#24211;&#23384;&#39044;&#27979;&#21644;&#36335;&#24452;&#20248;&#21270;&#65292;&#21487;&#33021;&#30830;&#20445;&#19968;&#20010;&#24378;&#22823;&#30340;&#20379;&#24212;&#38142;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inventory Routing Problem (IRP) is a crucial challenge in supply chain management as it involves optimizing efficient route selection while considering the uncertainty of inventory demand planning. To solve IRPs, usually a two-stage approach is employed, where demand is predicted using machine learning techniques first, and then an optimization algorithm is used to minimize routing costs. Our experiment shows machine learning models fall short of achieving perfect accuracy because inventory levels are influenced by the dynamic business environment, which, in turn, affects the optimization problem in the next stage, resulting in sub-optimal decisions. In this paper, we formulate and propose a decision-focused learning-based approach to solving real-world IRPs. This approach directly integrates inventory prediction and routing optimization within an end-to-end system potentially ensuring a robust supply chain strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.20199</link><description>&lt;p&gt;
&#36861;&#23547;&#22833;&#33853;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
In Search of Lost Online Test-time Adaptation: A Survey. (arXiv:2310.20199v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#35843;&#30740;&#32467;&#26524;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#35299;&#20915;&#27169;&#31946;&#35774;&#32622;&#12289;&#36807;&#26102;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#35843;&#30740;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;OTTA&#65289;&#30340;&#32508;&#21512;&#27010;&#20917;&#65292;&#35813;&#33539;&#24335;&#19987;&#27880;&#20110;&#22312;&#25209;&#37327;&#21040;&#36798;&#26102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35843;&#25972;&#21040;&#26032;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#23613;&#31649;&#26368;&#36817;OTTA&#26041;&#27861;&#30340;&#22686;&#21152;&#65292;&#20294;&#35813;&#39046;&#22495;&#23384;&#22312;&#27169;&#31946;&#30340;&#35774;&#32622;&#12289;&#36807;&#26102;&#30340;&#39592;&#24178;&#32467;&#26500;&#21644;&#19981;&#19968;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#30495;&#27491;&#30340;&#25361;&#25112;&#21464;&#24471;&#38590;&#20197;&#22797;&#29616;&#12290;&#20026;&#20102;&#28165;&#26224;&#21644;&#20005;&#26684;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#23558;OTTA&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#24182;&#20351;&#29992;&#21151;&#33021;&#24378;&#22823;&#30340;Vision Transformer&#65288;ViT&#65289;&#39592;&#24178;&#26550;&#26500;&#23545;&#23427;&#20204;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#21457;&#29616;&#30495;&#27491;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19981;&#20165;&#28085;&#30422;&#20256;&#32479;&#30340;&#21463;&#25439;&#25968;&#25454;&#38598;&#65292;&#22914;CIFAR-10/100-C&#21644;ImageNet-C&#65292;&#36824;&#21253;&#25324;&#20307;&#29616;&#22312;CIFAR-10.1&#21644;CIFAR-10-Warehouse&#20013;&#30340;&#29616;&#23454;&#19990;&#30028;&#36716;&#21464;&#65292;&#28085;&#30422;&#20102;&#25628;&#32034;&#24341;&#25806;&#30340;&#21464;&#21270;&#21644;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#34913;&#37327;&#22312;&#32447;&#22330;&#26223;&#20013;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a comprehensive survey on online test-time adaptation (OTTA), a paradigm focused on adapting machine learning models to novel data distributions upon batch arrival. Despite the proliferation of OTTA methods recently, the field is mired in issues like ambiguous settings, antiquated backbones, and inconsistent hyperparameter tuning, obfuscating the real challenges and making reproducibility elusive. For clarity and a rigorous comparison, we classify OTTA techniques into three primary categories and subject them to benchmarks using the potent Vision Transformer (ViT) backbone to discover genuinely effective strategies. Our benchmarks span not only conventional corrupted datasets such as CIFAR-10/100-C and ImageNet-C but also real-world shifts embodied in CIFAR-10.1 and CIFAR-10-Warehouse, encapsulating variations across search engines and synthesized data by diffusion models. To gauge efficiency in online scenarios, we introduce novel evaluation metrics, inclusiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.09130</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65306;&#25286;&#20998;&#19982;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Split-and-Denoise: Protect large language model inference with local differential privacy. (arXiv:2310.09130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SnD&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38454;&#27573;&#30340;&#38544;&#31169;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#21644;&#24341;&#20837;&#22122;&#22768;&#26469;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#25429;&#25417;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#38544;&#34255;&#35821;&#20041;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36825;&#19968;&#36807;&#31243;&#20016;&#23500;&#20102;&#25991;&#26412;&#23884;&#20837;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#20316;&#20026;&#26381;&#21153;&#65288;EaaS&#65289;&#30340;&#23884;&#20837;&#27169;&#22411;&#21830;&#19994;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#25991;&#26412;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#38754;&#20020;&#30528;&#36739;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#24471;&#21040;&#26377;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Split-N-Denoise&#65288;SnD&#65289;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#19978;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#25191;&#34892;&#20196;&#29260;&#23884;&#20837;&#23618;&#26469;&#25286;&#20998;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#23558;&#23884;&#20837;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#20043;&#21069;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#38543;&#21518;&#25509;&#25910;&#21644;&#21435;&#22122;&#21518;&#30340;&#25200;&#21160;&#36755;&#20986;&#23884;&#20837;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;LLMs&#30340;&#25512;&#26029;&#38454;&#27573;&#35774;&#35745;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SnD&#22312;&#21508;&#31181;LLM&#20013;&#20248;&#21270;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03940</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#38590;&#35270;&#22270;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Hard View Selection for Contrastive Learning. (arXiv:2310.03940v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Easy&#12289;&#26080;&#38656;&#23398;&#20064;&#20294;&#24378;&#22823;&#30340;Hard View Selection&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#23545;&#22270;&#20687;&#36755;&#20837;&#30340;&#19981;&#21516;&#8220;&#35270;&#22270;&#8221;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#32780;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#22686;&#24378;&#27969;&#31243;&#23545;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23545;&#22270;&#20687;&#22686;&#24378;&#27969;&#31243;&#20013;&#30340;&#25805;&#20316;&#36827;&#34892;&#38543;&#26426;&#25277;&#26679;&#65292;&#22914;&#38543;&#26426;&#35009;&#21098;&#25110;&#39068;&#33394;&#25197;&#26354;&#25805;&#20316;&#12290;&#26412;&#25991;&#35748;&#20026;&#35270;&#22270;&#29983;&#25104;&#21450;&#20854;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#22312;&#30446;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#23454;&#26045;&#20294;&#24378;&#22823;&#30340;&#8220;&#38590;&#35270;&#22270;&#36873;&#25321;&#8221;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#35270;&#22270;&#29983;&#25104;&#25193;&#23637;&#21040;&#26356;&#38590;&#30340;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31574;&#30053;&#21253;&#25324;&#20197;&#19979;&#36845;&#20195;&#27493;&#39588;&#65306;1&#65289;&#38543;&#26426;&#36873;&#25321;&#22810;&#20010;&#35270;&#22270;&#24182;&#21019;&#24314;&#20004;&#20010;&#35270;&#22270;&#30340;&#37197;&#23545;&#65292;2&#65289;&#36827;&#34892;&#21521;&#21069;&#20256;&#36882;...
&lt;/p&gt;
&lt;p&gt;
Many Contrastive Learning (CL) methods train their models to be invariant to different "views" of an image input for which a good data augmentation pipeline is crucial. While considerable efforts were directed towards improving pre-text tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax centering), the majority of these methods remain strongly reliant on the random sampling of operations within the image augmentation pipeline, such as the random resized crop or color distortion operation. In this paper, we argue that the role of the view generation and its effect on performance has so far received insufficient attention. To address this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS) strategy designed to extend the random view generation to expose the pretrained model to harder samples during CL training. It encompasses the following iterative steps: 1) randomly sample multiple views and create pairs of two views, 2) run forward pa
&lt;/p&gt;</description></item><item><title>Q-Bench&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14181</link><description>&lt;p&gt;
Q-Bench: &#19968;&#31181;&#29992;&#20110;&#20302;&#32423;&#21035;&#35270;&#35273;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision. (arXiv:2309.14181v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14181
&lt;/p&gt;
&lt;p&gt;
Q-Bench&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20174;&#19987;&#38376;&#27169;&#22411;&#21521;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLMs&#22312;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Q-Bench&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;MLLMs&#22312;&#19977;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#33021;&#21147;&#65306;&#20302;&#32423;&#21035;&#35270;&#35273;&#24863;&#30693;&#12289;&#20302;&#32423;&#21035;&#35270;&#35273;&#25551;&#36848;&#21644;&#25972;&#20307;&#35270;&#35273;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions o
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08883</link><description>&lt;p&gt;
&#35299;&#20915;&#31526;&#21495;&#21644;&#32479;&#35745;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Satisfiability Modulo Counting for Symbolic and Statistical AI Integration With Provable Guarantees. (arXiv:2309.08883v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08883
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#28085;&#30422;&#20102;&#38656;&#35201;&#31526;&#21495;&#20915;&#31574;&#21644;&#32479;&#35745;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#25429;&#25417;&#20102;&#31526;&#21495;&#21644;&#32479;&#35745;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#12290;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#23547;&#25214;&#31574;&#30053;&#24178;&#39044;&#20197;&#25511;&#21046;&#27010;&#29575;&#24615;&#32467;&#26524;&#12290;&#35299;&#20915;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#20855;&#26377;&#39640;&#24230;&#38590;&#35299;&#30340;&#29305;&#24615;&#65288;$\text{NP}^{\text{PP}}$-complete&#65289;&#65292;&#34701;&#21512;&#20102;&#32479;&#35745;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#12290;&#20808;&#21069;&#23545;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#30340;&#30740;&#31350;&#32570;&#20047;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#21644;/&#25110;&#22312;&#32452;&#21512;&#32422;&#26463;&#23384;&#22312;&#26102;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#32463;&#39564;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;XOR-SMC&#65292;&#19968;&#20010;&#20855;&#26377;&#35775;&#38382;NP-oracles&#26435;&#38480;&#30340;&#22810;&#39033;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#39640;&#24230;&#38590;&#35299;&#30340;&#21487;&#28385;&#36275;&#24615;&#27169;&#25968;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24658;&#23450;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;XOR-SMC&#36890;&#36807;&#29992;&#38543;&#26426;&#30340;XOR&#32422;&#26463;&#26367;&#25442;SMC&#20013;&#30340;&#27169;&#22411;&#35745;&#25968;&#65292;&#23558;&#39640;&#24230;&#38590;&#35299;&#30340;SMC&#36716;&#21270;&#20026;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Satisfiability Modulo Counting (SMC) encompasses problems that require both symbolic decision-making and statistical reasoning. Its general formulation captures many real-world problems at the intersection of symbolic and statistical Artificial Intelligence. SMC searches for policy interventions to control probabilistic outcomes. Solving SMC is challenging because of its highly intractable nature($\text{NP}^{\text{PP}}$-complete), incorporating statistical inference and symbolic reasoning. Previous research on SMC solving lacks provable guarantees and/or suffers from sub-optimal empirical performance, especially when combinatorial constraints are present. We propose XOR-SMC, a polynomial algorithm with access to NP-oracles, to solve highly intractable SMC problems with constant approximation guarantees. XOR-SMC transforms the highly intractable SMC into satisfiability problems, by replacing the model counting in SMC with SAT formulae subject to randomized XOR constraints. Experiments o
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02332</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#22788;&#29702;&#65306;&#25968;&#25454;&#21644;&#25805;&#20316;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations. (arXiv:2309.02332v1 [q-bio.NC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02332
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#22312;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#20013;&#20351;&#29992;&#25968;&#23398;&#32467;&#26500;&#31934;&#30830;&#22320;&#34920;&#31034;&#21644;&#25805;&#20316;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#31561;&#22810;&#31181;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21754;&#20083;&#21160;&#29289;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30340;&#22797;&#26434;&#32467;&#26500;&#20013;&#65292;&#31070;&#32463;&#20803;&#24418;&#25104;&#32676;&#20307;&#12290;&#36724;&#32034;&#26463;&#36890;&#36807;&#33033;&#20914;&#21015;&#20316;&#20026;&#23186;&#20171;&#22312;&#36825;&#20123;&#32676;&#38598;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#30340;&#31934;&#30830;&#32534;&#30721;&#21644;&#25805;&#20316;&#36824;&#26377;&#24453;&#21457;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#20986;&#21457;&#28857;&#26159;&#19968;&#20010;&#20855;&#26377;&#21487;&#22609;&#24615;&#30340;&#36890;&#29992;&#31070;&#32463;&#20803;&#30340;&#20808;&#36827;&#30340;&#26426;&#26800;&#27169;&#22411;&#12290;&#20174;&#36825;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#28145;&#21051;&#30340;&#25968;&#23398;&#26500;&#36896;&#65306;&#36890;&#36807;&#26377;&#38480;&#20984;&#38181;&#30340;&#20195;&#25968;&#21487;&#20197;&#20934;&#30830;&#22320;&#25551;&#36848;&#20449;&#24687;&#30340;&#34920;&#31034;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31070;&#32463;&#32676;&#20307;&#19981;&#20165;&#20165;&#26159;&#34987;&#21160;&#20256;&#36755;&#32773;&#12290;&#23427;&#20204;&#22312;&#36825;&#20010;&#20195;&#25968;&#32467;&#26500;&#20013;&#25198;&#28436;&#30528;&#36816;&#31639;&#31526;&#30340;&#35282;&#33394;&#65292;&#21453;&#26144;&#20102;&#20302;&#32423;&#32534;&#31243;&#35821;&#35328;&#30340;&#21151;&#33021;&#12290;&#24403;&#36825;&#20123;&#32676;&#20307;&#20114;&#36830;&#26102;&#65292;&#23427;&#20204;&#20855;&#26377;&#31616;&#27905;&#32780;&#24378;&#22823;&#30340;&#20195;&#25968;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#32593;&#32476;&#20351;&#23427;&#20204;&#33021;&#22815;&#23454;&#29616;&#35768;&#22810;&#25805;&#20316;&#65292;&#22914;&#29305;&#21270;&#12289;&#27867;&#21270;&#12289;&#26032;&#22855;&#26816;&#27979;&#12289;&#32500;&#24230;&#38477;&#20302;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensiona
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.15821</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Federated Two Stage Decoupling With Adaptive Personalization Layers. (arXiv:2308.15821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#23618;&#30340;&#32852;&#37030;&#21270;&#20004;&#38454;&#27573;&#35299;&#32806;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#26041;&#24335;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32858;&#31867;&#26102;&#38388;&#36873;&#25321;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#35774;&#22791;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20445;&#25345;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#23398;&#20064;&#38477;&#32423;&#21644;&#24930;&#25910;&#25947;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#22320;&#37319;&#29992;&#23558;&#21516;&#36136;&#23458;&#25143;&#31471;&#32858;&#31867;&#21040;&#21516;&#19968;&#32452;&#30340;&#27010;&#24565;&#65292;&#21482;&#20801;&#35768;&#22312;&#27599;&#20010;&#32452;&#20869;&#32858;&#21512;&#27169;&#22411;&#26435;&#37325;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#27169;&#22411;&#26799;&#24230;&#25110;&#25512;&#29702;&#36755;&#20986;&#20316;&#20026;&#23458;&#25143;&#31471;&#20998;&#21306;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#30446;&#30340;&#26159;&#23558;&#30456;&#20284;&#35774;&#22791;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20294;&#27599;&#20010;&#32858;&#31867;&#20869;&#37096;&#20173;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30740;&#31350;&#25506;&#32034;&#30830;&#23450;&#32858;&#31867;&#30340;&#36866;&#24403;&#26102;&#38388;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#23548;&#33268;&#24120;&#35265;&#20570;&#27861;&#26159;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21040;&#20854;&#33258;&#24049;&#30340;&#29420;&#31435;&#32858;&#31867;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#38750;ind&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained significant attention due to its groundbreaking ability to enable distributed learning while maintaining privacy constraints. However, as a consequence of data heterogeneity among decentralized devices, it inherently experiences significant learning degradation and slow convergence speed. Therefore, it is natural to employ the concept of clustering homogeneous clients into the same group, allowing only the model weights within each group to be aggregated. While most existing clustered federated learning methods employ either model gradients or inference outputs as metrics for client partitioning, with the goal of grouping similar devices together, may still have heterogeneity within each cluster. Moreover, there is a scarcity of research exploring the underlying reasons for determining the appropriate timing for clustering, resulting in the common practice of assigning each client to its own individual cluster, particularly in the context of highly non ind
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07931</link><description>&lt;p&gt;
&#31934;&#31616;&#29305;&#24449;&#22330;&#20351;&#24471;&#35821;&#35328;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#35821;&#35328;&#30417;&#30563;&#30340;&#22270;&#20687;&#27169;&#22411;&#21253;&#21547;&#20102;&#19990;&#30028;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#23545;&#20110;&#27867;&#21270;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#38656;&#35201;&#23545; 3D &#20960;&#20309;&#30340;&#35814;&#32454;&#29702;&#35299;&#65292;&#36825;&#22312; 2D &#22270;&#20687;&#29305;&#24449;&#20013;&#24448;&#24448;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340; 3D &#20960;&#20309;&#19982; 2D &#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#26469;&#24357;&#21512;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340; 2D &#21040; 3D &#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545; 6 &#33258;&#30001;&#24230;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#36890;&#36807;&#20174;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; CLIP &#20013;&#31934;&#31616;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#26032;&#39062;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#35265;&#36807;&#30340;&#34920;&#36798;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#29289;&#20307;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.05836</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20174;&#30456;&#20851;&#24615;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;?
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Infer Causation from Correlation?. (arXiv:2306.05836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65288;Corr2Cause&#65289;&#65292;&#29992;&#20110;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#24456;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20154;&#31867;&#26234;&#24935;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#34429;&#28982;CausalNLP&#39046;&#22495;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;NLP&#20013;&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#32463;&#39564;&#30693;&#35782;&#65288;&#20363;&#22914;&#24120;&#35782;&#30693;&#35782;&#65289;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32431;&#22240;&#26524;&#25512;&#26029;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;Corr2Cause&#65292;&#23427;&#37319;&#29992;&#19968;&#32452;&#30456;&#20851;&#35821;&#21477;&#24182;&#30830;&#23450;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;400K&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;17&#20010;&#29616;&#26377;&#30340;LLMs&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;LLMs&#22312;&#22240;&#26524;&#25512;&#26029;&#25216;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20960;&#20046;&#25509;&#36817;&#38543;&#26426;&#12290;&#24403;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#24494;&#35843;&#23558;LLMs&#37325;&#26032;&#29992;&#20110;&#36825;&#31181;&#25216;&#33021;&#26102;&#65292;&#36825;&#31181;&#32570;&#38519;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24471;&#21040;&#20102;&#32531;&#35299;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#22833;&#36133;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 400K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.08842</link><description>&lt;p&gt;
UDTIRI:&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#36947;&#36335;&#22353;&#27934;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214;UDTIRI&#65292;&#21253;&#21547;&#20102;&#26631;&#35760;&#40784;&#20840;&#30340;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#21487;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30475;&#21040;&#22312;&#22478;&#24066;&#25968;&#23383;&#23402;&#29983;&#39046;&#22495;&#20013;&#21033;&#29992;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22312;&#26234;&#33021;&#36947;&#36335;&#26816;&#26597;&#39046;&#22495;&#65292;&#30446;&#21069;&#30740;&#31350;&#21644;&#25968;&#25454;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;Urban Digital Twins Intelligent Road Inspection (UDTIRI)&#25968;&#25454;&#38598;&#30340;&#26631;&#35760;&#40784;&#20840;&#30340;&#36947;&#36335;&#22353;&#27934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#35753;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#22478;&#24066;&#36947;&#36335;&#26816;&#26597;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#35753;&#31639;&#27861;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#22330;&#26223;&#24182;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;1000&#24352;&#36947;&#36335;&#22353;&#27934;&#22270;&#20687;&#65292;&#25293;&#25668;&#20110;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#29031;&#21644;&#28287;&#24230;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24847;&#22270;&#26159;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#33457;&#36153;&#20102;&#22823;&#37327;&#31934;&#21147;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#23545;UDTIRI&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05673</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;
&lt;/p&gt;
&lt;p&gt;
Precise localization of corneal reflections in eye images using deep learning trained on synthetic data. (arXiv:2304.05673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#30524;&#37096;&#22270;&#20687;&#20013;&#20934;&#30830;&#23450;&#20301;&#35282;&#33180;&#21453;&#23556;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#20165;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#19988;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20934;&#30830;&#22320;&#23450;&#20301;&#21333;&#20010;&#30524;&#37096;&#22270;&#20687;&#20013;&#35282;&#33180;&#21453;&#23556;&#30340;&#20013;&#24515;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#32431;&#31929;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#20351;&#29992;&#21482;&#26377;&#27169;&#25311;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#23436;&#20840;&#36991;&#24320;&#20102;&#38656;&#35201;&#23545;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#30340;&#32321;&#29712;&#27880;&#37322;&#36807;&#31243;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#25918;&#32622;&#22312;&#19981;&#21516;&#32972;&#26223;&#20013;&#21644;&#23884;&#20837;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20174;&#30495;&#23454;&#30524;&#30555;&#20013;&#25293;&#25668;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30524;&#37096;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#31934;&#24230;&#26041;&#38754;&#38477;&#20302;&#20102;35&#65285;&#65292;&#24182;&#22312;&#27169;&#25311;&#22270;&#20687;&#26041;&#38754;&#20197;&#31354;&#38388;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#35282;&#33180;&#21453;&#23556;&#20013;&#24515;&#23450;&#20301;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning method for accurately localizing the center of a single corneal reflection (CR) in an eye image. Unlike previous approaches, we use a convolutional neural network (CNN) that was trained solely using simulated data. Using only simulated data has the benefit of completely sidestepping the time-consuming process of manual annotation that is required for supervised training on real eye images. To systematically evaluate the accuracy of our method, we first tested it on images with simulated CRs placed on different backgrounds and embedded in varying levels of noise. Second, we tested the method on high-quality videos captured from real eyes. Our method outperformed state-of-the-art algorithmic methods on real eye images with a 35% reduction in terms of spatial precision, and performed on par with state-of-the-art on simulated images in terms of spatial accuracy.We conclude that our method provides a precise method for CR center localization and provides a solutio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2304.03365</link><description>&lt;p&gt;
&#22870;&#21169;&#36716;&#31227;&#30340;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Decision-Focused Learning for Reward Transfer. (arXiv:2304.03365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#35782;&#21035;&#24615;&#30340;DF&#35299;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20915;&#31574;&#37325;&#28857;&#65288;Decision-focused&#65292;DF&#65289;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#34987;&#20171;&#32461;&#20026;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#19987;&#27880;&#20110;&#23398;&#20064;&#26368;&#26377;&#21033;&#20110;&#33719;&#24471;&#39640;&#25253;&#37228;&#30340;MDP&#21160;&#24577;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19987;&#27880;&#20110;&#30452;&#25509;&#20248;&#21270;&#25253;&#37228;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#65292;&#20294;&#20174;MLE&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#19981;&#22815;&#20934;&#30830;&#65292;&#22240;&#27492;&#21487;&#33021;&#23545;&#22870;&#21169;&#20989;&#25968;&#30340;&#21464;&#21270;&#24456;&#33030;&#24369;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31283;&#20581;&#20915;&#31574;&#37325;&#28857;&#65288;RDF&#65289;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;DF&#35299;&#30340;&#38750;&#35782;&#21035;&#24615;&#65292;&#23398;&#20064;&#21516;&#26102;&#26368;&#22823;&#21270;&#26399;&#26395;&#22238;&#25253;&#21644;&#25269;&#24481;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29609;&#20855;&#31034;&#20363;&#21644;&#21307;&#30103;&#27169;&#25311;&#22120;&#19978;&#23637;&#31034;&#20102;RDF&#26174;&#30528;&#22686;&#21152;&#20102;DF&#23545;&#22870;&#21169;&#20989;&#25968;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#26234;&#33021;&#20307;&#30340;&#24635;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused (DF) model-based reinforcement learning has recently been introduced as a powerful algorithm which can focus on learning the MDP dynamics which are most relevant for obtaining high rewards. While this approach increases the performance of agents by focusing the learning towards optimizing for the reward directly, it does so by learning less accurate dynamics (from a MLE standpoint), and may thus be brittle to changes in the reward function. In this work, we develop the robust decision-focused (RDF) algorithm which leverages the non-identifiability of DF solutions to learn models which maximize expected returns while simultaneously learning models which are robust to changes in the reward function. We demonstrate on a variety of toy example and healthcare simulators that RDF significantly increases the robustness of DF to changes in the reward function, without decreasing the overall return the agent obtains.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.02478</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#29983;&#20889;&#20316;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#65306;AI&#33021;&#36215;&#21040;&#20160;&#20040;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02478
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#30340;&#23398;&#29983;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#25552;&#39640;&#20182;&#20204;&#30340;&#20889;&#20316;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#23398;&#29983;&#30340;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#20889;&#20316;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;23&#21517;&#39321;&#28207;&#20013;&#23398;&#29983;&#25776;&#20889;&#25925;&#20107;&#65288;&#21253;&#21547;&#33258;&#24049;&#30340;&#25991;&#23383;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#30340;&#23581;&#35797;&#12290;&#20154;&#31867;&#19987;&#23478;&#23545;&#36825;&#20123;&#25925;&#20107;&#36827;&#34892;&#20102;&#20869;&#23481;&#12289;&#35821;&#35328;&#21644;&#32452;&#32455;&#26041;&#38754;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25925;&#20107;&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#32455;&#32467;&#26500;&#21644;&#21477;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#25191;&#34892;&#20102;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#21644;&#32858;&#31867;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#35789;&#35821;&#30340;&#25968;&#37327;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35789;&#35821;&#30340;&#25968;&#37327;&#23545;&#20998;&#25968;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#19982;&#21516;&#40836;&#20154;&#30456;&#27604;&#65292;&#23398;&#29983;&#30340;&#20889;&#20316;&#21487;&#20197;&#20998;&#20026;&#25797;&#38271;&#21644;&#19981;&#25797;&#38271;&#20351;&#29992;&#26356;&#22810;&#25110;&#26356;&#23569;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#20004;&#32452;&#12290;&#32858;&#31867;&#27604;&#36739;&#26174;&#31034;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#36807;&#24230;&#20381;&#36182;&#36825;&#31181;&#24037;&#20855;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16212</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65306;&#39640;&#25928;&#24555;&#36895;
&lt;/p&gt;
&lt;p&gt;
An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#21487;&#20197;&#24179;&#34913;&#32593;&#32476;&#30340;&#21098;&#26525;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#31181;&#32676;&#30340;&#29305;&#24615;&#65292;&#23427;&#32463;&#24120;&#21463;&#21040;&#22797;&#26434;&#30340;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#21644;&#39640;&#24230;&#36164;&#28304;&#28040;&#32791;&#30340;&#21098;&#26525;&#32467;&#26500;&#39564;&#35777;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65288;EMO-PMS&#65289;&#65292;&#20197;&#20943;&#23569;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#36825;&#31181;&#20998;&#35299;&#20943;&#23569;&#20102;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#24182;&#38477;&#20302;&#20102;&#20248;&#21270;&#38590;&#24230;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36739;&#23567;&#30340;&#32593;&#32476;&#32467;&#26500;&#25910;&#25947;&#26356;&#24555;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#38544;&#31169;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.04030</link><description>&lt;p&gt;
&#25581;&#31034;&#38544;&#31169;&#19982;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#27602;&#21270;&#25915;&#20987;&#20013;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#21644;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#38544;&#31169;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#21033;&#29992;&#20998;&#24067;&#24335;&#29992;&#25143;&#30340;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21487;&#20449;&#20219;&#30340;&#19981;&#21516;&#29992;&#25143;&#65292;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;FL&#23481;&#26131;&#21463;&#21040;&#27602;&#21270;&#25915;&#20987;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#20445;&#25252;&#26412;&#22320;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;FL&#36890;&#24120;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65288;DPFL&#65289;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#24046;&#20998;&#38544;&#31169;&#21644;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#35748;&#35777;&#31283;&#23450;&#24615;&#20043;&#38388;&#23384;&#22312;&#20309;&#31181;&#32852;&#31995;&#65311;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;DPFL&#30340;&#38544;&#31169;&#23646;&#24615;&#20026;FL&#25552;&#20379;&#35748;&#35777;&#31283;&#23450;&#24615;&#65311;&#25105;&#20204;&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#36827;FL&#30340;&#38544;&#31169;&#20197;&#25552;&#39640;&#36825;&#31181;&#31283;&#23450;&#24615;&#35748;&#35777;&#65311;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;FL&#30340;&#29992;&#25143;&#32423;&#21644;&#23454;&#20363;&#32423;&#38544;&#31169;&#65292;&#24182;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20998;&#26512;&#20197;&#23454;&#29616;&#25552;&#39640;&#23454;&#20363;&#32423;&#38544;&#31169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31283;&#23450;&#24615;&#35748;&#35777;&#25351;&#26631;&#65306;&#35748;&#35777;&#39044;&#27979;&#21644;&#35748;&#35777;&#25915;&#20987;&#26080;&#25928;&#24615;&#65292;&#29992;&#20110;&#29992;&#25143;&#21644;&#23454;&#20363;&#32423;DPFL&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) provides an efficient paradigm to jointly train a global model leveraging data from distributed users. As local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the privacy of local users, FL is usually trained in a differentially private way (DPFL). Thus, in this paper, we ask: What are the underlying connections between differential privacy and certified robustness in FL against poisoning attacks? Can we leverage the innate privacy property of DPFL to provide certified robustness for FL? Can we further improve the privacy of FL to improve such robustness certification? We first investigate both user-level and instance-level privacy of FL and provide formal privacy analysis to achieve improved instance-level privacy. We then provide two robustness certification criteria: certified prediction and certified attack inefficacy for DPFL on both user and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.09894</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#20154;&#20063;&#33021;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#24515;&#21270;&#21098;&#35009;&#30340;&#34928;&#33853;
&lt;/p&gt;
&lt;p&gt;
Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26694;&#26550;&#30001;&#20110;&#22312;&#24191;&#27867;&#30340;&#21327;&#20316;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#26576;&#20123;&#23433;&#20840;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#39118;&#38505;&#26159;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#24694;&#24847;&#23458;&#25143;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;FL &#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#26159;&#28040;&#38500; Byzantine attacks &#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#30830;&#20445;&#26368;&#32456;&#27169;&#22411;&#26159;&#21487;&#20449;&#30340;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;/&#26356;&#26032;&#20043;&#38388;&#30340;&#26041;&#24046;&#36234;&#22823;&#65292;&#38544;&#34255; Byzantine attacks &#30340;&#31354;&#38388;&#23601;&#36234;&#22823;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#26041;&#24046;&#65292;&#21487;&#20197;&#21066;&#24369;&#24050;&#30693; Byzantine attacks &#30340;&#21147;&#37327;&#12290;&#20013;&#24515;&#21270;&#21098;&#35009; (CC) &#26694;&#26550;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19978;&#19968;&#27425;&#30340;&#21160;&#37327;&#39033;&#38500;&#20102;&#20943;&#23569;&#26041;&#24046;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21442;&#32771;&#28857;&#26356;&#22909;&#22320;&#28040;&#38500; Byzantine attacks&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#30340;&#24694;&#24847;&#20195;&#29702;&#26377;&#19981;&#21516;&#30446;&#26631;&#26102; CC &#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21098;&#35009;&#31639;&#27861;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC)&#65292;&#20197;&#20811;&#26381;&#36825;&#31181;&#33030;&#24369;&#24615;&#12290;MRPC &#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26469;&#28040;&#38500;&#19987;&#38376;&#35774;&#35745;&#20197;&#32469;&#36807; CC &#26041;&#27861;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
&lt;/p&gt;</description></item><item><title>SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2203.07436</link><description>&lt;p&gt;
&#36229;&#32423;&#21160;&#29289;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#21160;&#29289;&#34892;&#20026;&#30340;&#21363;&#25554;&#21363;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SuperAnimal models pretrained for plug-and-play analysis of animal behavior. (arXiv:2203.07436v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07436
&lt;/p&gt;
&lt;p&gt;
SuperAnimal&#26159;&#19968;&#31181;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#29992;&#20110;&#36229;&#36807;45&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#24494;&#35843;&#27169;&#22411;&#25928;&#29575;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#37327;&#21270;&#22312;&#31070;&#32463;&#31185;&#23398;&#12289;&#20861;&#21307;&#21644;&#21160;&#29289;&#20445;&#25252;&#31561;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34892;&#20026;&#20998;&#26512;&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#39318;&#20808;&#25552;&#21462;&#19982;&#21160;&#29289;&#30456;&#20851;&#30340;&#20851;&#38190;&#28857;&#65292;&#21363;&#23039;&#21183;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#30340;&#23039;&#21183;&#25512;&#26029;&#30446;&#21069;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#21644;&#25163;&#21160;&#26631;&#27880;&#26469;&#26500;&#24314;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#21019;&#26032;&#65292;&#20351;&#19968;&#31181;&#21517;&#20026;SuperAnimal&#30340;&#26032;&#26041;&#27861;&#33021;&#22815;&#24320;&#21457;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#26631;&#27880;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;SuperAnimal&#20801;&#35768;&#23545;45&#22810;&#20010;&#29289;&#31181;&#36827;&#34892;&#35270;&#39057;&#25512;&#26029;&#65292;&#21516;&#26102;&#21482;&#20351;&#29992;&#20004;&#31181;&#20840;&#23616;&#21160;&#29289;&#23039;&#21183;&#27169;&#22411;&#12290;&#22914;&#26524;&#38656;&#35201;&#24494;&#35843;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SuperAnimal&#27169;&#22411;&#20855;&#26377;10&#20493;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#19988;&#32988;&#36807;&#20808;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#33258;&#36866;&#24212;&#26041;&#27861;&#26469;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#35270;&#39057;&#32454;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification of behavior is critical in applications ranging from neuroscience, veterinary medicine and animal conservation efforts. A common key step for behavioral analysis is first extracting relevant keypoints on animals, known as pose estimation. However, reliable inference of poses currently requires domain knowledge and manual labeling effort to build supervised models. We present a series of technical innovations that enable a new method, collectively called SuperAnimal, to develop and deploy deep learning models that require zero additional human labels and model training. SuperAnimal allows video inference on over 45 species with only two global classes of animal pose models. If the models need fine-tuning, we show SuperAnimal models are 10$\times$ more data efficient and outperform prior transfer learning approaches. Moreover, we provide a new video-adaptation method to perform unsupervised refinement of videos, and we illustrate the utility of our model in behavioral clas
&lt;/p&gt;</description></item></channel></rss>