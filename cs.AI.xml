<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#30340;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13085</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#21152;&#26435;&#21033;&#29992;&#28151;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting. (arXiv:2306.13085v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#30340;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36820;&#22238;&#19968;&#20010;&#26368;&#22823;&#21270;&#39044;&#26399;&#34920;&#29616;&#19982;&#35825;&#23548;&#29366;&#24577;-&#21160;&#20316;&#21344;&#29992;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#30446;&#26631;&#31574;&#30053;&#65292;&#22240;&#27492;&#65292;&#30446;&#26631;&#31574;&#30053;&#30340;&#34920;&#29616;&#19982;&#25968;&#25454;&#38598;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#30340;&#34920;&#29616;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30001;&#22823;&#22810;&#25968;&#20302;&#22238;&#25253;&#36712;&#36857;&#21644;&#23569;&#25968;&#39640;&#22238;&#25253;&#36712;&#36857;&#32452;&#25104;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#20302;&#22238;&#25253;&#36712;&#36857;&#30340;&#36807;&#24230;&#21046;&#32422;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#39640;&#34920;&#29616;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#38543;&#26426;&#21021;&#22987;&#29366;&#24577;&#30340;&#30830;&#23450;&#24615;MDPs&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#37319;&#26679;&#25968;&#25454;&#38598;&#26469;&#35825;&#23548;&#20855;&#26377;&#26356;&#39640;&#22238;&#25253;&#30340;&#34892;&#20026;&#31574;&#30053;&#30340;&#20154;&#24037;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;&#37319;&#26679;&#31574;&#30053;&#21487;&#20197;&#19982;&#20219;&#20309;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#21487;&#33021;&#34987;&#28389;&#29992;&#21046;&#20316;&#34394;&#20551;&#20449;&#24687;&#65292;&#23384;&#22312;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#20316;&#20026;&#32593;&#32476;&#38450;&#24481;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.13033</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#32593;&#32476;&#23433;&#20840;&#30340;&#24433;&#21709;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Impacts and Risk of Generative AI Technology on Cyber Defense. (arXiv:2306.13033v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13033
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#21487;&#33021;&#34987;&#28389;&#29992;&#21046;&#20316;&#34394;&#20551;&#20449;&#24687;&#65292;&#23384;&#22312;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#20316;&#20026;&#32593;&#32476;&#38450;&#24481;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;GenAI&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#33258;&#20027;&#22320;&#22312;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;&#30001;&#20110;&#22312;&#21019;&#24847;&#33402;&#26415;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#34394;&#25311;&#21161;&#25163;&#21644;&#25968;&#25454;&#32508;&#21512;&#31561;&#26041;&#38754;&#26377;&#31215;&#26497;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#22240;&#27492;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#21644;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;GenAI&#30340;&#26222;&#21450;&#19981;&#26029;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#20316;&#36924;&#30495;&#30340;&#32593;&#32476;&#38035;&#40060;&#37038;&#20214;&#12289;&#36890;&#36807;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#21644;&#36890;&#36807;&#30475;&#36215;&#26469;&#36924;&#30495;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#31561;&#26041;&#38754;&#30340;&#25285;&#24551;&#65292;&#36825;&#32473;&#32593;&#32476;&#23433;&#20840;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25361;&#25112;&#21644;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;GenAI&#25152;&#24102;&#26469;&#30340;&#23041;&#32961;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#26469;&#29702;&#35299;&#32593;&#32476;&#25915;&#20987;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#20316;&#20026;&#32593;&#32476;&#38450;&#24481;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;GenAI&#25216;&#26415;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#21487;&#33021;&#24341;&#20837;&#30340;&#39118;&#38505;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GenAI) has emerged as a powerful technology capable of autonomously producing highly realistic content in various domains, such as text, images, audio, and videos. With its potential for positive applications in creative arts, content generation, virtual assistants, and data synthesis, GenAI has garnered significant attention and adoption. However, the increasing adoption of GenAI raises concerns about its potential misuse for crafting convincing phishing emails, generating disinformation through deepfake videos, and spreading misinformation via authentic-looking social media posts, posing a new set of challenges and risks in the realm of cybersecurity. To combat the threats posed by GenAI, we propose leveraging the Cyber Kill Chain (CKC) to understand the lifecycle of cyberattacks, as a foundational model for cyber defense. This paper aims to provide a comprehensive analysis of the risk areas introduced by the offensive use of GenAI techniques in ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PERM&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#29983;&#25104;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13028</link><description>&lt;p&gt;
&#22522;&#20110;&#38590;&#24230;&#26465;&#20214;&#29983;&#25104;&#22120;&#30340;&#21487;&#36716;&#31227;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Transferable Curricula through Difficulty Conditioned Generators. (arXiv:2306.13028v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PERM&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#29983;&#25104;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#36229;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#22914;&#26143;&#38469;&#20105;&#38712;&#12289;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#31561;&#31561;&#12290;&#28982;&#32780;&#65292;&#23558;&#20154;&#24037;&#8220;&#19987;&#23478;&#8221;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#20154;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#35838;&#31243;&#20013;&#20351;&#29992;&#35838;&#31243;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36716;&#31227;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#35838;&#31243;&#29983;&#25104;&#26041;&#27861;&#20391;&#37325;&#20110;&#39640;&#25928;&#35757;&#32451;RL&#20195;&#29702;&#65292;&#28982;&#32780;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#20195;&#29702;&#20154;&#36827;&#23637;&#30340;&#20195;&#29702;&#27979;&#37327;&#26631;&#20934;&#65292;&#19981;&#33021;&#29992;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#22521;&#35757;&#26426;&#22120;&#20154;&#65288;&#25110;&#26356;&#26377;&#38596;&#24515;&#30340;&#26159;&#20154;&#31867;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21442;&#25968;&#21270;&#29615;&#22659;&#21709;&#24212;&#27169;&#22411;&#65288;PERM&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;RL&#20195;&#29702;&#34920;&#29616;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#21463;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;PERM&#35797;&#22270;&#30452;&#25509;&#23545;&#29615;&#22659;&#38590;&#24230;&#21644;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#24314;&#27169;&#12290;&#37492;&#20110;RL&#20195;&#29702;&#21644;&#20154;&#31867;&#22312;&#8220;&#21457;&#23637;&#30340;&#37051;&#22495;&#8221;&#20013;&#26356;&#26377;&#25928;&#22320;&#25509;&#21463;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21305;&#37197;&#29615;&#22659;&#30340;&#38590;&#24230;&#20197;&#20135;&#29983;&#35838;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in reinforcement learning (RL) have demonstrated superhuman performance in complex tasks such as Starcraft, Go, Chess etc. However, knowledge transfer from Artificial "Experts" to humans remain a significant challenge. A promising avenue for such transfer would be the use of curricula. Recent methods in curricula generation focuses on training RL agents efficiently, yet such methods rely on surrogate measures to track student progress, and are not suited for training robots in the real world (or more ambitiously humans). In this paper, we introduce a method named Parameterized Environment Response Model (PERM) that shows promising results in training RL agents in parameterized environments. Inspired by Item Response Theory, PERM seeks to model difficulty of environments and ability of RL agents directly. Given that RL agents and humans are trained more efficiently under the "zone of proximal development", our method generates a curriculum by matching the difficulty of an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi&#22312;&#32654;&#22269;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#22238;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.13000</link><description>&lt;p&gt;
&#26080;&#25919;&#27835;&#26234;&#33021;&#65311;&#23457;&#26597;Delphi&#22312;&#32654;&#22269;&#26377;&#20105;&#35758;&#25919;&#27835;&#38382;&#39064;&#19978;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US. (arXiv:2306.13000v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi&#22312;&#32654;&#22269;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#20013;&#30340;&#22238;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#34987;&#37096;&#32626;&#65292;&#26377;&#20851;&#23427;&#20204;&#25919;&#27835;&#20215;&#20540;&#30340;&#25285;&#24551;&#24050;&#25104;&#20026;&#21069;&#27839;&#38382;&#39064;&#65292;&#21508;&#20010;&#25919;&#27835;&#27966;&#21035;&#23545;&#20854;&#23384;&#22312;&#20559;&#35265;&#21644;&#32570;&#20047;&#20013;&#31435;&#24615;&#30340;&#25209;&#35780;&#32439;&#33267;&#27795;&#26469;&#12290;&#26412;&#25991;&#36890;&#36807;&#23457;&#35745;&#29992;&#20110;&#20247;&#21253;&#20262;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Delphi [arXiv:2110.07574]&#65292;&#20174;&#25919;&#27835;&#20105;&#35758;&#38382;&#39064;&#30340;&#35282;&#24230;&#65292;&#20998;&#26512;&#20102;Delphi&#19982;&#21508;&#20010;&#32654;&#22269;&#25919;&#27835;&#20998;&#32452;&#30340;&#19981;&#21516;&#22238;&#24212;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;Delphi&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#19981;&#33391;&#65292;&#21576;&#29616;&#26174;&#33879;&#30340;&#25919;&#27835;&#20542;&#26012;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#20316;&#32773;&#20174;&#25968;&#25454;&#22899;&#26435;&#20027;&#20041;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#20013;&#31435;&#24615;&#30340;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#20013;&#31435;&#24615;&#27010;&#24565;&#22914;&#20309;&#36716;&#31227;&#26435;&#21147;&#12289;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#26080;&#22768;&#30340;&#22768;&#38899;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#26395;&#20026;&#20851;&#20110;&#27169;&#22411;&#19982;&#20215;&#20540;&#30340;&#35268;&#33539;&#24615;&#38382;&#39064;&#24418;&#25104;&#26356;&#22810;&#26377;&#24605;&#32771;&#24615;&#30340;&#36777;&#35770;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative language models are deployed in ever-wider contexts, concerns about their political values have come to the forefront with critique from all parts of the political spectrum that the models are biased and lack neutrality. However, the question of what neutrality is and whether it is desirable remains underexplored. In this paper, I examine neutrality through an audit of Delphi [arXiv:2110.07574], a large language model designed for crowdsourced ethics. I analyse how Delphi responds to politically controversial questions compared to different US political subgroups. I find that Delphi is poorly calibrated with respect to confidence and exhibits a significant political skew. Based on these results, I examine the question of neutrality from a data-feminist lens, in terms of how notions of neutrality shift power and further marginalise unheard voices. These findings can hopefully contribute to a more reflexive debate about the normative questions of alignment and what role we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25366;&#25496;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#20197;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12964</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21327;&#21516;&#20844;&#24335;&#21270;&#30340;Alpha&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning. (arXiv:2306.12964v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25366;&#25496;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#20197;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#20132;&#26131;&#39046;&#22495;&#20013;&#65292;&#23558;&#21407;&#22987;&#32929;&#31080;&#21382;&#21490;&#25968;&#25454;&#36716;&#21270;&#20026;&#24066;&#22330;&#36235;&#21183;&#25351;&#31034;&#20449;&#21495;&#26159;&#24120;&#35265;&#30340;&#23454;&#36341;&#12290;&#36825;&#20123;&#20449;&#21495;&#34987;&#31216;&#20026;Alpha&#22240;&#23376;&#12290;&#20844;&#24335;&#24418;&#24335;&#30340;Alpha&#22240;&#23376;&#26356;&#26131;&#20110;&#35299;&#37322;&#65292;&#22240;&#27492;&#39118;&#38505;&#24847;&#35782;&#24378;&#30340;&#23454;&#36341;&#32773;&#26356;&#21916;&#27426;&#23427;&#20204;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24448;&#24448;&#21516;&#26102;&#20351;&#29992;&#19968;&#32452;&#20844;&#24335;&#21270;&#30340;Alpha&#22240;&#23376;&#36827;&#34892;&#26356;&#22909;&#30340;&#24314;&#27169;&#31934;&#24230;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#25214;&#21040;&#33021;&#22815;&#33391;&#22909;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;Alpha&#29983;&#25104;&#22120;&#20998;&#21035;&#25366;&#25496;&#21333;&#20010;Alpha&#65292;&#24573;&#30053;&#20102;&#21518;&#32493;&#30340;Alpha&#32452;&#21512;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Alpha&#25366;&#25496;&#26694;&#26550;&#65292;&#23427;&#20248;&#20808;&#25366;&#25496;&#21327;&#21516;&#30340;Alpha&#38598;&#21512;&#65292;&#21363;&#30452;&#25509;&#20351;&#29992;&#19979;&#28216;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#20248;&#21270;Alpha&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#24378;&#22823;&#25506;&#32034;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#20844;&#24335;&#24418;&#24335;&#30340;Alpha&#22240;&#23376;&#30340;&#22823;&#37327;&#25628;&#32034;&#31354;&#38388;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21327;&#21516;&#30340;&#20844;&#24335;&#21270;Alpha&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#20248;&#21270;&#23427;&#20204;&#20316;&#20026;&#32452;&#21512;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;Alpha&#29983;&#25104;&#22120;&#20998;&#21035;&#25366;&#25496;&#21333;&#20010;Alpha&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combina
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Siamese SIREN &#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#26469;&#23454;&#29616;&#21331;&#36234;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#25299;&#23637;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.12957</link><description>&lt;p&gt;
&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#38899;&#39057;&#21387;&#32553;&#30340;&#36830;&#38145;SIREN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Siamese SIREN: Audio Compression with Implicit Neural Representations. (arXiv:2306.12957v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Siamese SIREN &#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#26469;&#23454;&#29616;&#21331;&#36234;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#25299;&#23637;&#20102;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INRs)&#24050;&#25104;&#20026;&#34920;&#31034;&#22810;&#31181;&#25968;&#25454;&#24418;&#24335;(&#21253;&#25324;3D&#24418;&#29366;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;)&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#36817;&#30740;&#31350;&#24050;&#32463;&#25104;&#21151;&#22320;&#23558;INRs&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;3D&#24418;&#29366;&#30340;&#21387;&#32553;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#38899;&#39057;&#21387;&#32553;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;INRs&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#38899;&#39057;&#21387;&#32553;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#22522;&#20110;&#27969;&#34892;&#30340;SIREN&#26550;&#26500;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;: &#36830;&#38145;SIREN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;INRs&#26550;&#26500;&#30456;&#27604;&#65292;&#36830;&#38145;SIREN&#22312;&#21033;&#29992;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#38899;&#39057;&#37325;&#24314;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) have emerged as a promising method for representing diverse data modalities, including 3D shapes, images, and audio. While recent research has demonstrated successful applications of INRs in image and 3D shape compression, their potential for audio compression remains largely unexplored. Motivated by this, we present a preliminary investigation into the use of INRs for audio compression. Our study introduces Siamese SIREN, a novel approach based on the popular SIREN architecture. Our experimental results indicate that Siamese SIREN achieves superior audio reconstruction fidelity while utilizing fewer network parameters compared to previous INR architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12943</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evolving Computation Graphs. (arXiv:2306.12943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37027;&#20123;&#34920;&#29616;&#20986;&#21516;&#36136;&#24615;&#30340;&#25968;&#25454;&#65306;&#24403;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#24448;&#24448;&#26263;&#31034;&#23427;&#20204;&#23646;&#20110;&#21516;&#19968;&#31867;&#26102;&#65292;&#36825;&#19968;&#28857;&#26356;&#20026;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20010;&#20551;&#35774;&#22312;&#35768;&#22810;&#30456;&#20851;&#24773;&#20917;&#19979;&#26159;&#25104;&#31435;&#30340;&#65292;&#20294;&#22312;&#37325;&#35201;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#20063;&#26377;&#36829;&#21453;&#36825;&#20010;&#20551;&#35774;&#30340;&#24773;&#20917;&#65292;&#36825;&#20419;&#36827;&#20102;&#23545;GNN&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#25913;&#36827;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#36827;&#21270;&#35745;&#31639;&#22270;&#31639;&#27861;&#65288;ECGs&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#38024;&#23545;&#24322;&#36136;&#24615;&#25968;&#25454;&#30340;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20808;&#21069;&#30340;&#29702;&#35770;&#27934;&#35265;&#20043;&#19978;&#65292;&#23558;&#33410;&#28857;&#24230;&#12289;&#39640;&#21516;&#36136;&#24615;&#21644;&#20869;&#37096;&#19982;&#31867;&#38388;&#23884;&#20837;&#30456;&#20284;&#24615;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#37325;&#36830;GNN&#30340;&#35745;&#31639;&#22270;&#26469;&#22686;&#21152;&#36830;&#25509;&#21516;&#19968;&#31867;&#33410;&#28857;&#30340;&#36793;&#32536;&#12290;&#25105;&#20204;&#21033;&#29992;&#36739;&#24369;&#30340;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#36825;&#20123;&#36793;&#32536;&#65292;&#20174;&#32780;&#26368;&#32456;&#25552;&#39640;&#20102;GNN&#23545;&#38750;&#21516;&#36136;&#24615;&#25968;&#25454;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;ECGs&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#26368;&#36817;&#25552;&#20986;&#30340;&#24322;&#36136;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have demonstrated success in modeling relational data, especially for data that exhibits homophily: when a connection between nodes tends to imply that they belong to the same class. However, while this assumption is true in many relevant situations, there are important real-world scenarios that violate this assumption, and this has spurred research into improving GNNs for these cases. In this work, we propose Evolving Computation Graphs (ECGs), a novel method for enhancing GNNs on heterophilic datasets. Our approach builds on prior theoretical insights linking node degree, high homophily, and inter vs intra-class embedding similarity by rewiring the GNNs' computation graph towards adding edges that connect nodes that are likely to be in the same class. We utilise weaker classifiers to identify these edges, ultimately improving GNN performance on non-homophilic data as a result. We evaluate ECGs on a diverse set of recently-proposed heterophilous datasets a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12929</link><description>&lt;p&gt;
&#21487;&#37327;&#21270;Transformer&#65306;&#36890;&#36807;&#24110;&#21161;&#27880;&#24847;&#21147;&#22836;&#8220;&#20160;&#20040;&#20063;&#19981;&#20570;&#8221;&#21435;&#38500;&#31163;&#32676;&#20540;
&lt;/p&gt;
&lt;p&gt;
Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing. (arXiv:2306.12929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#25480;&#27880;&#24847;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#26469;&#28040;&#38500;&#31163;&#32676;&#20540;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;transformer&#30340;&#21487;&#37327;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;Transformer&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#30001;&#20110;&#20854;&#35268;&#27169;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#65292;&#20294;&#36825;&#26159;&#20197;&#26497;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#30340;&#12290;&#37327;&#21270;&#26159;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#26102;&#38388;&#21644;&#23384;&#20648;&#22120;&#28040;&#32791;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;transformer&#27169;&#22411;&#24448;&#24448;&#23398;&#20064;&#21040;&#20854;&#28608;&#27963;&#20013;&#30340;&#24378;&#31163;&#32676;&#20540;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37327;&#21270;&#12290;&#20026;&#20445;&#25345;&#21487;&#25509;&#21463;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#30340;&#23384;&#22312;&#38656;&#35201;&#23558;&#28608;&#27963;&#32622;&#20110;&#26356;&#39640;&#30340;&#27604;&#29305;&#23485;&#24230;&#25110;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#23383;&#26684;&#24335;&#65292;&#36827;&#34892;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#20854;&#20182;&#21464;&#36890;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#24378;&#31163;&#32676;&#20540;&#19982;&#29305;&#23450;&#27880;&#24847;&#22836;&#34892;&#20026;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#22836;&#35797;&#22270;&#23398;&#20064;&#8220;&#26080;&#25805;&#20316;&#8221;&#25110;&#20165;&#20165;&#26159;&#37096;&#20998;&#27531;&#24046;&#26356;&#26032;&#12290;&#20026;&#20102;&#23454;&#29616;&#27880;&#24847;&#21147;&#22836;&#20013;&#38656;&#35201;&#30340;&#31934;&#30830;&#38646;&#20301;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;Helper-Head&#8221;&#30340;&#26041;&#27861;&#65292;&#25945;&#25480;&#27880;&#24847;&#21147;&#22836;&#24573;&#30053;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#30340;&#37327;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#20351;&#29992;&#20302;&#31934;&#24230;&#37327;&#21270;&#29978;&#33267;&#26159;&#24378;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#12289;&#39640;&#27604;&#29305;&#23485;&#24230;&#35774;&#32622;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transformer models tend to learn strong outliers in their activations, making them difficult to quantize. To retain acceptable performance, the existence of these outliers requires activations to be in higher bitwidth or the use of different numeric formats, extra fine-tuning, or other workarounds. We show that strong outliers are related to very specific behavior of attention heads that try to learn a "no-op" or just a partial update of the residual. To achieve the exact zeros needed in the attention 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12926</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#29366;&#24577;&#39044;&#27979;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#25511;&#21046;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#32676;&#20307;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38750;&#38745;&#24577;&#24615;&#65292;&#21363;&#24403;&#20004;&#20010;&#25110;&#26356;&#22810;&#26426;&#22120;&#20154;&#21516;&#26102;&#26356;&#26032;&#20010;&#20307;&#25110;&#20849;&#20139;&#25919;&#31574;&#26102;&#65292;&#20250;&#36827;&#20837;&#19968;&#20010;&#30456;&#20114;&#20381;&#23384;&#30340;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#19988;&#19981;&#20445;&#35777;&#25910;&#25947;&#12290;&#20811;&#26381;&#38750;&#38745;&#24577;&#24615;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;/&#25110;&#34892;&#21160;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#28040;&#38500;&#20840;&#23616;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#20449;&#24687;&#20307;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#38598;&#20307;&#36816;&#36755;&#20026;&#27979;&#35797;&#22330;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22810;&#26234;&#33021;&#20307;&#22521;&#35757;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#19981;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#19988;&#34987;&#35757;&#32451;&#20381;&#38752;&#36890;&#36807;&#25512;&#65288;push&#65289;&#21644;&#25289;&#65288;pull&#65289;&#29289;&#20307;&#36827;&#34892;&#38544;&#24335;&#36890;&#20449;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#24444;&#27492;&#20849;&#20139;&#29366;&#24577;&#39044;&#27979;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#21327;&#35843;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20849;&#20139;&#39044;&#27979;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#38656;&#35201;&#26356;&#23569;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
&lt;/p&gt;</description></item><item><title>AudioPaLM&#26159;&#19968;&#27454;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32487;&#25215;&#20102;AudioLM&#30340;&#35821;&#38899;&#36523;&#20221;&#21644;&#35821;&#35843;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#20197;&#21450;PaLM-2&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.12925</link><description>&lt;p&gt;
AudioPaLM&#65306;&#19968;&#27454;&#33021;&#35828;&#20250;&#21548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AudioPaLM: A Large Language Model That Can Speak and Listen. (arXiv:2306.12925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12925
&lt;/p&gt;
&lt;p&gt;
AudioPaLM&#26159;&#19968;&#27454;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32487;&#25215;&#20102;AudioLM&#30340;&#35821;&#38899;&#36523;&#20221;&#21644;&#35821;&#35843;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#20197;&#21450;PaLM-2&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21487;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;AudioPaLM&#12290;&#23427;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;PaLM-2[Anil&#31561;&#20154;&#65292;2023]&#21644;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;AudioLM[Borsos&#31561;&#20154;&#65292;2022]&#32467;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#32467;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#35821;&#38899;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#31561;&#24212;&#29992;&#12290;AudioPaLM&#32487;&#25215;&#20102;&#20174;AudioLM&#20013;&#20445;&#30041;&#35821;&#38899;&#21457;&#38899;&#32773;&#36523;&#20221;&#21644;&#35821;&#35843;&#31561;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#21482;&#23384;&#22312;&#20110;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PaLM-2&#20013;&#30340;&#35821;&#35328;&#30693;&#35782;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#21487;&#20197;&#25913;&#21892;&#35821;&#38899;&#22788;&#29702;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26356;&#22823;&#37327;&#30340;&#25991;&#26412;&#35757;&#32451;&#25968;&#25454;&#26469;&#21327;&#21161;&#35821;&#38899;&#20219;&#21153;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#31995;&#32479;&#65292;&#24182;&#19988;&#20855;&#26377;&#36827;&#34892;&#38646;-shot&#35328;&#35821;&#25991;&#26412;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for ma
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#22522;&#20110;&#27169;&#22411;&#35786;&#26029;&#30340;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#65288;&#20132;&#20114;&#24335;&#65289;&#35843;&#35797;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#31995;&#32479;&#32500;&#20462;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#29616;&#20195;&#31995;&#32479;&#30340;&#25972;&#20307;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12850</link><description>&lt;p&gt;
&#19981;&#35201;&#27835;&#30103;&#30151;&#29366;&#65292;&#25214;&#21040;&#30149;&#22240;&#65281;&#29992;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#65288;&#20132;&#20114;&#24335;&#65289;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
Don't Treat the Symptom, Find the Cause! Efficient Artificial-Intelligence Methods for (Interactive) Debugging. (arXiv:2306.12850v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#22522;&#20110;&#27169;&#22411;&#35786;&#26029;&#30340;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#65288;&#20132;&#20114;&#24335;&#65289;&#35843;&#35797;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#31995;&#32479;&#32500;&#20462;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#24182;&#25552;&#39640;&#29616;&#20195;&#31995;&#32479;&#30340;&#25972;&#20307;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#25105;&#20204;&#19981;&#26029;&#20351;&#29992;&#12289;&#21033;&#29992;&#12289;&#20132;&#20114;&#21644;&#20381;&#36182;&#30528;&#19981;&#26029;&#39640;Sophistication&#30340;&#31995;&#32479;&#65292;&#20174;&#27773;&#36710;&#12289;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#19978;&#32593;&#26102;&#30340;&#32593;&#32476;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20010;&#20154;&#30005;&#33041;&#21644;&#26234;&#33021;&#25163;&#26426;&#26102;&#30340;&#38598;&#25104;&#30005;&#36335;&#65292;&#20445;&#35777;&#25105;&#20204;&#30340;&#33021;&#28304;&#20379;&#24212;&#30340;&#30005;&#32593;&#65292;&#35775;&#38382;&#38134;&#34892;&#36134;&#25143;&#26102;&#30340;&#23433;&#20840;&#20851;&#38190;&#36719;&#20214;&#65292;&#21040;&#36130;&#21153;&#35268;&#21010;&#21644;&#20915;&#31574;&#26102;&#30340;&#30005;&#23376;&#34920;&#26684;&#31561;&#31561;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21152;&#19978;&#25105;&#20204;&#30340;&#39640;&#24230;&#20381;&#36182;&#24847;&#21619;&#30528;&#31995;&#32479;&#20986;&#29616;&#25925;&#38556;&#30340;&#21487;&#33021;&#24615;&#19981;&#23481;&#24573;&#35270;&#65292;&#24182;&#19988;&#36825;&#26679;&#30340;&#25925;&#38556;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#24433;&#21709;&#21487;&#33021;&#38750;&#24120;&#22823;&#12290;&#22240;&#27492;&#65292;&#23558;&#27491;&#22312;&#20986;&#29616;&#30340;&#25925;&#38556;&#30340;&#20260;&#23475;&#38477;&#21040;&#26368;&#20302;&#26159;&#19968;&#39033;&#20851;&#38190;&#35201;&#27714;&#65292;&#36825;&#24847;&#21619;&#30528;&#26368;&#23567;&#21270;&#31995;&#32479;&#20572;&#26426;&#26102;&#38388;&#20197;&#21450;&#31995;&#32479;&#32500;&#20462;&#25104;&#26412;&#12290;&#36825;&#23601;&#26159;&#27169;&#22411;&#35786;&#26029;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27169;&#22411;&#35786;&#26029;&#30340;&#39640;&#25928;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#36827;&#34892;&#65288;&#20132;&#20114;&#24335;&#65289;&#35843;&#35797;&#12290;&#25105;&#20204;&#24341;&#20837;&#27010;&#29575;&#27169;&#22411;&#35786;&#26029;&#26694;&#26550;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#19988;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#31995;&#32479;&#32500;&#20462;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#24182;&#33021;&#25552;&#39640;&#29616;&#20195;&#31995;&#32479;&#30340;&#25972;&#20307;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern world, we are permanently using, leveraging, interacting with, and relying upon systems of ever higher sophistication, ranging from our cars, recommender systems in e-commerce, and networks when we go online, to integrated circuits when using our PCs and smartphones, the power grid to ensure our energy supply, security-critical software when accessing our bank accounts, and spreadsheets for financial planning and decision making. The complexity of these systems coupled with our high dependency on them implies both a non-negligible likelihood of system failures, and a high potential that such failures have significant negative effects on our everyday life. For that reason, it is a vital requirement to keep the harm of emerging failures to a minimum, which means minimizing the system downtime as well as the cost of system repair. This is where model-based diagnosis comes into play.  Model-based diagnosis is a principled, domain-independent approach that can be generally app
&lt;/p&gt;</description></item><item><title>XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12816</link><description>&lt;p&gt;
XAI-TRIS&#65306;&#29992;&#20110;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS: Non-linear benchmarks to quantify ML explanation performance. (arXiv:2306.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12816
&lt;/p&gt;
&lt;p&gt;
XAI-TRIS&#25552;&#20379;&#20102;&#29992;&#20110;&#27979;&#35797;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#33021;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;XAI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#21487;&#35299;&#37322;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#24341;&#29992;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#30340;&#20915;&#31574;&#8220;&#21487;&#29702;&#35299;&#8221;&#32473;&#20154;&#31867;&#65292;&#20363;&#22914;&#36890;&#36807;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#8220;&#37325;&#35201;&#24615;&#8221;&#35780;&#20998;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#27491;&#24335;&#30340;&#22522;&#30784;&#65292;&#20351;&#24471;&#26080;&#27861;&#20174;&#32473;&#23450;XAI&#26041;&#27861;&#30340;&#32467;&#26524;&#20013;&#23433;&#20840;&#22320;&#24471;&#20986;&#32467;&#35770;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20063;&#38459;&#30861;&#20102;XAI&#26041;&#27861;&#30340;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#35777;&#39564;&#35777;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#30446;&#21069;&#32570;&#20047;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#36890;&#24120;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#38750;&#32447;&#24615;&#20998;&#31867;&#24773;&#26223;&#21046;&#20316;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#36890;&#36807;&#35774;&#35745;&#24050;&#30693;&#37325;&#35201;&#30340;&#31867;&#26465;&#20214;&#29305;&#24449;&#65292;&#20316;&#20026;&#22320;&#38754;&#23454;&#20917;&#35299;&#37322;&#12290;&#21033;&#29992;&#26032;&#30340;&#23450;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#19978;&#27979;&#35797;&#20102;&#24191;&#27867;&#30340;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;XAI&#26041;&#27861;&#30340;&#24456;&#22810;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of 'explainable' artificial intelligence (XAI) has produced highly cited methods that seek to make the decisions of complex machine learning (ML) methods 'understandable' to humans, for example by attributing 'importance' scores to input features. Yet, a lack of formal underpinning leaves it unclear as to what conclusions can safely be drawn from the results of a given XAI method and has also so far hindered the theoretical verification and empirical validation of XAI methods. This means that challenging non-linear problems, typically solved by deep neural networks, presently lack appropriate remedies. Here, we craft benchmark datasets for three different non-linear classification scenarios, in which the important class-conditional features are known by design, serving as ground truth explanations. Using novel quantitative metrics, we benchmark the explanation performance of a wide set of XAI methods across three deep learning model architectures. We show that popular XAI met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#21450;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#39640;&#20102;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.12806</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#22120;&#22312;&#38480;&#20215;&#35746;&#21333;&#31807;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65306;&#21487;&#35299;&#37322;&#24615;&#12289;&#25361;&#25112;&#21644;&#24378;&#20581;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Conditional Generators for Limit Order Book Environments: Explainability, Challenges, and Robustness. (arXiv:2306.12806v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#21450;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#25552;&#39640;&#20102;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38480;&#20215;&#35746;&#21333;&#31807;&#26159;&#19968;&#31181;&#22522;&#26412;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#24066;&#22330;&#26426;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35746;&#21333;&#31807;&#27169;&#25311;&#20013;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22240;&#20026;&#20854;&#33021;&#22815;&#23545;&#20132;&#26131;&#20195;&#29702;&#30340;&#23384;&#22312;&#20570;&#20986;&#21453;&#24212;&#32780;&#21463;&#21040;&#20851;&#27880;&#65292;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;&#22238;&#27979;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#26469;&#33258;Coletta&#31561;&#20154;&#65288;2022&#65289;&#30340;&#26368;&#20808;&#36827;&#30340;CGAN&#65292;&#25506;&#32034;&#20102;&#23427;&#23545;&#36755;&#20837;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#65292;&#24378;&#35843;&#20102;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#29305;&#24449;&#21450;&#20854;&#26426;&#21046;&#36827;&#34892;&#20102;&#8220;&#23545;&#25239;&#25915;&#20987;&#8221;&#12290;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#35748;&#35782;&#26469;&#25552;&#39640;CGAN&#30340;&#36924;&#30495;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limit order books are a fundamental and widespread market mechanism. This paper investigates the use of conditional generative models for order book simulation. For developing a trading agent, this approach has drawn recent attention as an alternative to traditional backtesting due to its ability to react to the presence of the trading agent. Using a state-of-the-art CGAN (from Coletta et al. (2022)), we explore its dependence upon input features, which highlights both strengths and weaknesses. To do this, we use "adversarial attacks" on the model's features and its mechanism. We then show how these insights can be used to improve the CGAN, both in terms of its realism and robustness. We finish by laying out a roadmap for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.12802</link><description>&lt;p&gt;
Otter-Knowledge&#65306;&#19981;&#21516;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Otter-Knowledge: benchmarks of multimodal knowledge graph representation learning from different sources for drug discovery. (arXiv:2306.12802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20351;&#29992;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20182;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#21033;&#29992;&#22823;&#37327;&#30340;&#34507;&#30333;&#36136;&#25110;&#20998;&#23376;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#33719;&#24471;&#33647;&#29289;&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#22914;&#39044;&#27979;&#33647;&#29289;&#21644;&#38774;&#34507;&#30333;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#27169;&#24577;&#30340;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;&#21040;&#24207;&#21015;&#25110;SMILES&#34920;&#31034;&#20013;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26469;&#33258;7&#20010;&#20844;&#20849;&#26469;&#28304;&#30340;&#39044;&#22788;&#29702;&#21644;&#25972;&#21512;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;30M&#20010;&#19977;&#20803;&#32452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;Therapeutic Data Commons (TDC)&#22522;&#20934;&#27979;&#35797;&#20013;&#24615;&#33021;&#25253;&#21578;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in representation learning utilizes large databases of proteins or molecules to acquire knowledge of drug and protein structures through unsupervised learning techniques. These pre-trained representations have proven to significantly enhance the accuracy of subsequent tasks, such as predicting the affinity between drugs and target proteins. In this study, we demonstrate that by incorporating knowledge graphs from diverse sources and modalities into the sequences or SMILES representation, we can further enrich the representation and achieve state-of-the-art results on established benchmark datasets. We provide preprocessed and integrated data obtained from 7 public sources, which encompass over 30M triples. Additionally, we make available the pre-trained models based on this data, along with the reported outcomes of their performance on three widely-used benchmark datasets for drug-target binding affinity prediction found in the Therapeutic Data Commons (TDC) benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#25239;&#23398;&#20064;&#30340;&#26032;&#22411;&#35821;&#38899;&#21512;&#25104;&#22120;MFCCGAN&#65292;&#20351;&#29992;MFCC&#20316;&#20026;&#36755;&#20837;&#65292;&#20135;&#29983;&#27604;&#20256;&#32479;&#35268;&#21017;MFCC&#30340;WORLD&#35821;&#38899;&#21512;&#25104;&#22120;&#26356;&#39640;&#28165;&#26224;&#24230;&#12289;&#26356;&#22909;&#29702;&#35299;&#24230;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2306.12785</link><description>&lt;p&gt;
MFCCGAN&#65306;&#19968;&#31181;&#22522;&#20110;MFCC&#21644;&#23545;&#25239;&#23398;&#20064;&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning. (arXiv:2306.12785v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#25239;&#23398;&#20064;&#30340;&#26032;&#22411;&#35821;&#38899;&#21512;&#25104;&#22120;MFCCGAN&#65292;&#20351;&#29992;MFCC&#20316;&#20026;&#36755;&#20837;&#65292;&#20135;&#29983;&#27604;&#20256;&#32479;&#35268;&#21017;MFCC&#30340;WORLD&#35821;&#38899;&#21512;&#25104;&#22120;&#26356;&#39640;&#28165;&#26224;&#24230;&#12289;&#26356;&#22909;&#29702;&#35299;&#24230;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#23398;&#20064;&#30340;&#26032;&#22411;&#35821;&#38899;&#21512;&#25104;&#22120;MFCCGAN&#65292;&#37319;&#29992;MFCC&#20316;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#21407;&#22987;&#35821;&#38899;&#27874;&#24418;&#12290;&#21463;&#30410;&#20110;GAN&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23427;&#20135;&#29983;&#30340;&#35821;&#38899;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;MFCC&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;WORLD&#20855;&#26377;&#26356;&#39640;&#30340;&#28165;&#26224;&#24230;&#12290;&#25105;&#20204;&#26681;&#25454;&#27969;&#34892;&#30340;&#20405;&#20837;&#24335;&#23458;&#35266;&#35821;&#38899;&#21487;&#25026;&#24230;&#27979;&#37327;&#65288;STOI&#65289;&#21644;&#36136;&#37327;&#65288;NISQA&#24471;&#20998;&#65289;&#36827;&#34892;&#20102;&#27169;&#22411;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;Librosa MFCC- inversion&#65288;&#22312;STOI&#21644;NISQA&#24471;&#20998;&#20013;&#22686;&#21152;&#20102;&#32422;26&#65285;&#33267;53&#65285;&#21644;16&#65285;&#33267;78&#65285;&#65289;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#32534;&#30721;&#22120;WORLD&#30456;&#27604;&#65292;&#21487;&#36776;&#24230;&#25552;&#39640;&#32422;10&#65285;&#65292;&#33258;&#28982;&#24230;&#25552;&#39640;&#32422;4&#65285;&#12290;&#28982;&#32780;&#65292;WORLD&#38656;&#35201;&#39069;&#22806;&#30340;F0&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;STOI&#30340;&#37492;&#21035;&#22120;&#20351;&#29992;&#24863;&#30693;&#25439;&#22833;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36136;&#37327;&#12290;&#22522;&#20110;WebMUSHRA&#30340;&#20027;&#35266;&#27979;&#35797;&#20063;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.12756</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;:&#22522;&#20110;&#36229;&#20986;&#20998;&#24067;&#35270;&#35282;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective. (arXiv:2306.12756v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#23450;&#20041;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#34913;&#37327;OOD&#40065;&#26834;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#36739;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#22330;&#26223;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#38024;&#23545;&#36896;&#25104;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#26631;&#35782;&#31526;&#26469;&#26816;&#32034;&#25991;&#26723;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#26469;&#24320;&#21457;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#21364;&#24471;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#24403;&#19968;&#20010;&#26032;&#30340;&#26816;&#32034;&#33539;&#24335;&#36827;&#20837;&#21040;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#26102;&#65292;&#34913;&#37327;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21363;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#22914;&#20309;&#27867;&#21270;&#21040;&#26032;&#30340;&#20998;&#24067;&#20013;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#26816;&#32034;&#38382;&#39064;&#30340;&#19977;&#20010;&#26041;&#38754;&#23450;&#20041;OOD&#40065;&#26834;&#24615;&#65306;1&#65289;&#26597;&#35810;&#21464;&#21270;&#65307;2&#65289;&#26410;&#30693;&#30340;&#26597;&#35810;&#31867;&#22411;&#65307;3&#65289;&#26410;&#30693;&#20219;&#21153;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#20960;&#20010;&#20195;&#34920;&#24615;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#19982;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;OOD&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#30340;OOD&#40065;&#26834;&#24615;&#27604;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#21521;&#20219;&#21153;&#30340;OOD&#22330;&#26223;&#20013;&#26356;&#26126;&#26174;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36896;&#25104;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#40065;&#26834;&#24615;&#36739;&#24369;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23427;&#20204;OOD&#27867;&#21270;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we have witnessed generative retrieval increasingly gaining attention in the information retrieval (IR) field, which retrieves documents by directly generating their identifiers. So far, much effort has been devoted to developing effective generative retrieval models. There has been less attention paid to the robustness perspective. When a new retrieval paradigm enters into the real-world application, it is also critical to measure the out-of-distribution (OOD) generalization, i.e., how would generative retrieval models generalize to new distributions. To answer this question, firstly, we define OOD robustness from three perspectives in retrieval problems: 1) The query variations; 2) The unforeseen query types; and 3) The unforeseen tasks. Based on this taxonomy, we conduct empirical studies to analyze the OOD robustness of several representative generative retrieval models against dense retrieval models. The empirical results indicate that the OOD robustness of generative re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;SNN&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;FPGA&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#32534;&#30721;&#26041;&#26696;&#21644;&#19968;&#31181;&#20869;&#23384;&#32452;&#32455;&#25216;&#26415;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;SNN&#21152;&#36895;&#22120;&#27809;&#26377;&#27604;CNN&#21152;&#36895;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.12742</link><description>&lt;p&gt;
&#21152;Spiking&#36824;&#26159;&#19981;&#21152;Spiking&#65311;SNN&#21644;CNN FPGA&#23454;&#29616;&#30340;&#37327;&#21270;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not to Spike? A Quantitative Comparison of SNN and CNN FPGA Implementations. (arXiv:2306.12742v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22810;&#20010;SNN&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;FPGA&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;SNN&#32534;&#30721;&#26041;&#26696;&#21644;&#19968;&#31181;&#20869;&#23384;&#32452;&#32455;&#25216;&#26415;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;SNN&#21152;&#36895;&#22120;&#27809;&#26377;&#27604;CNN&#21152;&#36895;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#23494;&#38598;&#22411;&#29305;&#24615;&#65292;CNN&#21152;&#36895;&#22120;&#24050;&#32463;&#20316;&#20026;ASIC&#25110;FPGA&#24320;&#21457;&#12290;&#30001;&#20110;&#24212;&#29992;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#36825;&#20123;&#21152;&#36895;&#22120;&#30340;&#36164;&#28304;&#25104;&#26412;&#21644;&#33021;&#37327;&#35201;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26367;&#20195;CNN&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#25215;&#35834;&#26356;&#39640;&#30340;&#36164;&#28304;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#26159;SNN&#21152;&#36895;&#22120;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#38477;&#20302;&#33021;&#37327;&#35201;&#27714;&#30340;&#26399;&#26395;&#65292;&#30456;&#27604;&#20110;&#23427;&#20204;&#30340;CNN&#31561;&#25928;&#39033;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#22810;&#20010;SNN&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;FPGA&#30340;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#20107;&#20214;&#38431;&#21015;&#32534;&#30721;&#26041;&#26696;&#21644;&#19968;&#31181;&#26032;&#30340;&#20869;&#23384;&#32452;&#32455;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;SNN&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#24050;&#32463;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;SNN&#26550;&#26500;&#20013;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are widely employed to solve various problems, e.g., image classification. Due to their compute- and data-intensive nature, CNN accelerators have been developed as ASICs or on FPGAs. Increasing complexity of applications has caused resource costs and energy requirements of these accelerators to grow. Spiking Neural Networks (SNNs) are an emerging alternative to CNN implementations, promising higher resource and energy efficiency. The main research question addressed in this paper is whether SNN accelerators truly meet these expectations of reduced energy requirements compared to their CNN equivalents. For this purpose, we analyze multiple SNN hardware accelerators for FPGAs regarding performance and energy efficiency. We present a novel encoding scheme of spike event queues and a novel memory organization technique to improve SNN energy efficiency further. Both techniques have been integrated into a state-of-the-art SNN architecture and evaluated fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.12703</link><description>&lt;p&gt;
OptIForest: &#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26368;&#20248;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
OptIForest: Optimal Isolation Forest for Anomaly Detection. (arXiv:2306.12703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#38548;&#31163;&#26862;&#26519;&#31639;&#27861;&#20013;&#20998;&#25903;&#22240;&#23376;&#30340;&#26368;&#20248;&#21462;&#20540;&#38382;&#39064;&#65292;&#22522;&#20110;&#38548;&#31163;&#25928;&#29575;&#25552;&#20986;&#21019;&#26032;&#31639;&#27861;OptIForest&#65292;&#35813;&#31639;&#27861;&#32467;&#26500;&#31616;&#27905;&#12289;&#26816;&#27979;&#24615;&#33021;&#20248;&#31168;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#35832;&#22914;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20837;&#20405;&#26816;&#27979;&#12289;&#37329;&#34701;&#39118;&#38505;&#30417;&#25511;&#12289;&#20154;&#31867;&#20581;&#24247;&#30417;&#27979;&#31561;&#12290;&#26681;&#25454;&#38548;&#31163;&#26862;&#26519;&#26426;&#21046;&#25552;&#20986;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20854;&#31616;&#27905;&#12289;&#26377;&#25928;&#12289;&#39640;&#25928;&#32780;&#22791;&#21463;&#38738;&#30544;&#65292;&#20363;&#22914;&#38024;&#23545;&#23454;&#38469;&#37096;&#32626;&#65292;iForest&#26159;&#26368;&#24120;&#29992;&#30340;&#26816;&#27979;&#22120;&#20043;&#19968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#38548;&#31163;&#26862;&#26519;&#37319;&#29992;&#20108;&#36827;&#21046;&#32467;&#26500;&#65292;&#20294;&#26694;&#26550;LSHiForest&#24050;&#32463;&#35777;&#26126;&#20102;&#22810;&#21449;&#38548;&#31163;&#26641;&#32467;&#26500;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23578;&#26080;&#29702;&#35770;&#24037;&#20316;&#22238;&#31572;&#20851;&#20110;&#38548;&#31163;&#26862;&#26519;&#30340;&#26368;&#20248;&#26641;&#32467;&#26500;&#30340;&#26681;&#26412;&#21644;&#23454;&#36341;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#20309;&#31181;&#20998;&#25903;&#22240;&#23376;&#30340;&#38548;&#31163;&#26641;&#32467;&#26500;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#38548;&#31163;&#25928;&#29575;&#29702;&#35770;&#26469;&#35299;&#31572;&#35813;&#38382;&#39064;&#65292;&#36827;&#32780;&#30830;&#23450;&#20102;&#19968;&#20010;&#38548;&#31163;&#26641;&#30340;&#26368;&#20248;&#20998;&#25903;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection plays an increasingly important role in various fields for critical tasks such as intrusion detection in cybersecurity, financial risk detection, and human health monitoring. A variety of anomaly detection methods have been proposed, and a category based on the isolation forest mechanism stands out due to its simplicity, effectiveness, and efficiency, e.g., iForest is often employed as a state-of-the-art detector for real deployment. While the majority of isolation forests use the binary structure, a framework LSHiForest has demonstrated that the multi-fork isolation tree structure can lead to better detection performance. However, there is no theoretical work answering the fundamentally and practically important question on the optimal tree structure for an isolation forest with respect to the branching factor. In this paper, we establish a theory on isolation efficiency to answer the question and determine the optimal branching factor for an isolation tree. Based on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#37327;&#23376;&#35745;&#31639;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#26377;&#26395;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24378;&#30340;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#29616;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.12688</link><description>&lt;p&gt;
&#37327;&#23376;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards quantum enhanced adversarial robustness in machine learning. (arXiv:2306.12688v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#37327;&#23376;&#35745;&#31639;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#26377;&#26395;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26356;&#24378;&#30340;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#29616;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#29305;&#24449;&#26816;&#27979;&#31561;&#25968;&#25454;&#39537;&#21160;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#8212;&#8212;&#21363;&#23545;&#20110;&#34987;&#31713;&#25913;&#20197;&#27450;&#39575;&#31639;&#27861;&#30340;&#36755;&#20837;&#26679;&#26412;&#8212;&#8212;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#35745;&#31639;&#30456;&#32467;&#21512;&#65292;&#26377;&#21487;&#33021;&#25552;&#20379;&#19981;&#20165;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;&#30830;&#23454;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#37327;&#23376;&#26426;&#26800;&#29616;&#35937;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25512;&#21160;&#20102;&#37327;&#23376;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#65288;QAML&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#26032;&#30340;&#37327;&#23376;&#20248;&#21183;&#26469;&#28304;&#12290;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#22312;&#26500;&#24314;&#31283;&#20581;&#30340;&#23454;&#38469;QAML&#24037;&#20855;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;QAML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21487;&#33021;&#30830;&#23450;&#23454;&#29616;QA ML&#36335;&#32447;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are powerful tools for data driven tasks such as image classification and feature detection, however their vulnerability to adversarial examples - input samples manipulated to fool the algorithm remains a serious challenge. The integration of machine learning with quantum computing has the potential to yield tools offering not only better accuracy and computational efficiency, but also superior robustness against adversarial attacks. Indeed, recent work has employed quantum mechanical phenomena to defend against adversarial attacks, spurring the rapid development of the field of quantum adversarial machine learning (QAML) and potentially yielding a new source of quantum advantage. Despite promising early results, there remain challenges towards building robust real-world QAML tools. In this review we discuss recent progress in QAML and identify key challenges. We also suggest future research directions which could determine the route to practicality for QA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26041;&#27861;SEEK&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#65292;&#36890;&#36807;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.12687</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#20013;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explainable Representations for Relation Prediction in Knowledge Graphs. (arXiv:2306.12687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#26041;&#27861;SEEK&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#65292;&#36890;&#36807;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20197;&#26412;&#20307;&#35770;&#25903;&#25345;&#30340;&#35821;&#20041;&#20016;&#23500;&#32467;&#26500;&#34920;&#31034;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#36825;&#20010;&#25968;&#25454;&#36890;&#24120;&#20381;&#36182;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#23427;&#21487;&#20135;&#29983;&#23454;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20445;&#30041;&#32467;&#26500;&#21644;&#26412;&#22320;&#22270;&#37051;&#22495;&#29305;&#24615;&#65292;&#20294;&#29306;&#29298;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38142;&#25509;&#25110;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#20102;&#35299;&#21738;&#20123;&#20855;&#20307;&#29305;&#24449;&#26356;&#22909;&#22320;&#35299;&#37322;&#20851;&#31995;&#23545;&#20110;&#25903;&#25345;&#22797;&#26434;&#25110;&#20851;&#38190;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEEK&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25903;&#25345;&#30693;&#35782;&#22270;&#35889;&#20851;&#31995;&#39044;&#27979;&#30340;&#34920;&#31034;&#12290;&#23427;&#22522;&#20110;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30456;&#20851;&#30340;&#20849;&#20139;&#35821;&#20041;&#26041;&#38754;&#65288;&#21363;&#23376;&#22270;&#65289;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#29983;&#25104;&#19968;&#20010;&#22810;&#26041;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;SEEK&#35780;&#20272;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#20013;&#39640;&#24230;&#22797;&#26434;&#30340;&#20851;&#31995;&#39044;&#27979;&#20219;&#21153;&#65306;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#21644;&#22522;&#22240;-&#30142;&#30149;&#20851;&#32852;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent real-world entities and their relations in a semantically-rich structure supported by ontologies. Exploring this data with machine learning methods often relies on knowledge graph embeddings, which produce latent representations of entities that preserve structural and local graph neighbourhood properties, but sacrifice explainability. However, in tasks such as link or relation prediction, understanding which specific features better explain a relation is crucial to support complex or critical applications.  We propose SEEK, a novel approach for explainable representations to support relation prediction in knowledge graphs. It is based on identifying relevant shared semantic aspects (i.e., subgraphs) between entities and learning representations for each subgraph, producing a multi-faceted and explainable representation.  We evaluate SEEK on two real-world highly complex relation prediction tasks: protein-protein interaction prediction and gene-disease associ
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12680</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#26032;&#21457;&#23637;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25216;&#26415;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#39046;&#22495;&#20869;&#29616;&#29366;&#30340;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;&#35813;&#30740;&#31350;&#39318;&#20808;&#20840;&#38754;&#24635;&#32467;&#20102;&#20027;&#35201;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#21644;&#32676;&#32452;&#25512;&#33616;&#31995;&#32479;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#33616;&#31995;&#32479;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#24635;&#32467;&#20102;&#35780;&#20272;&#24230;&#37327;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#30340;&#35265;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;SoftGPT&#65292;&#23427;&#20351;&#29992;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#20808;&#21069;&#30693;&#35782;&#26469;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#25216;&#26415;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.12677</link><description>&lt;p&gt;
SoftGPT: &#36890;&#36807;&#39044;&#35757;&#32451;&#24322;&#36136;&#22270;&#21464;&#25442;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#38754;&#21521;&#30446;&#26631;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
SoftGPT: Learn Goal-oriented Soft Object Manipulation Skills by Generative Pre-trained Heterogeneous Graph Transformer. (arXiv:2306.12677v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12677
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;SoftGPT&#65292;&#23427;&#20351;&#29992;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#20808;&#21069;&#30693;&#35782;&#26469;&#23398;&#20064;&#30446;&#26631;&#23548;&#21521;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#25216;&#26415;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#22330;&#26223;&#20013;&#36827;&#34892;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#21487;&#21464;&#30340;&#24418;&#29366;&#29305;&#24449;&#65292;&#23545;&#29616;&#26377;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#20174;&#20154;&#31867;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#25805;&#20316;&#25216;&#33021;&#26159;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#22240;&#27492;&#24320;&#21457;&#36719;&#29289;&#20307;&#34920;&#31034;&#21644;&#21160;&#24577;&#30340;&#20808;&#21069;&#30693;&#35782;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SoftGPT&#30340;&#39044;&#35757;&#32451;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#30340;&#25506;&#32034;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#19977;&#32500;&#24322;&#26500;&#22270;&#34920;&#31034;&#21644;&#22522;&#20110;GPT&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#38754;&#21521;&#30446;&#26631;&#30340;&#31574;&#30053;&#20195;&#29702;&#20197;&#39044;&#27979;&#21518;&#32493;&#21160;&#20316;&#65292;SoftGPT&#29983;&#25104;&#36825;&#20123;&#21160;&#20316;&#30340;&#21518;&#26524;&#12290;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24314;&#31435;&#20102;&#26426;&#22120;&#20154;&#24605;&#32771;&#36807;&#31243;&#65292;&#20026;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#25552;&#20379;&#20102;&#23637;&#24320;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#36719;&#29289;&#20307;&#21160;&#21147;&#23398;&#21644;&#34920;&#31034;&#30340;&#20808;&#21069;&#30693;&#35782;&#20351;SoftGPT&#33021;&#22815;&#23398;&#20064;&#38754;&#21521;&#30446;&#26631;&#30340;&#36719;&#29289;&#20307;&#25805;&#20316;&#25216;&#33021;&#65292;&#20174;&#32780;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft object manipulation tasks in domestic scenes pose a significant challenge for existing robotic skill learning techniques due to their complex dynamics and variable shape characteristics. Since learning new manipulation skills from human demonstration is an effective way for robot applications, developing prior knowledge of the representation and dynamics of soft objects is necessary. In this regard, we propose a pre-trained soft object manipulation skill learning model, namely SoftGPT, that is trained using large amounts of exploration data, consisting of a three-dimensional heterogeneous graph representation and a GPT-based dynamics model. For each downstream task, a goal-oriented policy agent is trained to predict the subsequent actions, and SoftGPT generates the consequences of these actions. Integrating these two approaches establishes a thinking process in the robot's mind that provides rollout for facilitating policy learning. Our results demonstrate that leveraging prior kn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23398;&#20250;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20648;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#20648;&#23618;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35777;&#23454;&#20102;&#20648;&#23618;&#30340;&#26368;&#20248;&#24615;&#33021;&#21457;&#29983;&#22312;&#23548;&#30005;&#36890;&#36947;&#30340;&#24418;&#25104;&#36793;&#32536;&#65292;&#32780;&#19988;&#36824;&#21457;&#29616;&#36825;&#31181;&#31995;&#32479;&#21487;&#20197;&#27169;&#25311;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#34892;&#20026;&#65292;&#20855;&#26377;&#23558;&#23574;&#23792;&#21644;&#36830;&#32493;&#20449;&#21495;&#20114;&#30456;&#36716;&#25442;&#30340;&#28508;&#21147; &#12290;</title><link>http://arxiv.org/abs/2306.12676</link><description>&lt;p&gt;
&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20648;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memristive Reservoirs Learn to Learn. (arXiv:2306.12676v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23398;&#20250;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20648;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#20648;&#23618;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35777;&#23454;&#20102;&#20648;&#23618;&#30340;&#26368;&#20248;&#24615;&#33021;&#21457;&#29983;&#22312;&#23548;&#30005;&#36890;&#36947;&#30340;&#24418;&#25104;&#36793;&#32536;&#65292;&#32780;&#19988;&#36824;&#21457;&#29616;&#36825;&#31181;&#31995;&#32479;&#21487;&#20197;&#27169;&#25311;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#34892;&#20026;&#65292;&#20855;&#26377;&#23558;&#23574;&#23792;&#21644;&#36830;&#32493;&#20449;&#21495;&#20114;&#30456;&#36716;&#25442;&#30340;&#28508;&#21147; &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24518;&#38459;&#22120;&#30340;&#20648;&#23618;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#32435;&#31859;&#32447;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#20855;&#26377;&#31867;&#33041;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#24182;&#22312;&#21160;&#21147;&#23398;&#30456;&#21464;&#26102;&#34920;&#29616;&#20986;&#26368;&#20248;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32593;&#32476;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#30005;&#26497;&#26469;&#35843;&#33410;&#31995;&#32479;&#21160;&#24577;&#65292;&#19982;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#36890;&#36807;&#38543;&#26426;&#23384;&#21462;&#20869;&#23384;&#25552;&#20379;&#30340;&#20840;&#23616;&#21487;&#25511;&#24615;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23398;&#20250;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#20648;&#23618;&#30340;&#26368;&#20248;&#36229;&#21442;&#25968;&#12290;&#27492;&#30740;&#31350;&#32467;&#26524;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;&#20648;&#23618;&#30340;&#26368;&#20248;&#24615;&#33021;&#21457;&#29983;&#22312;&#23548;&#30005;&#36890;&#36947;&#30340;&#24418;&#25104;&#36793;&#32536;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#31995;&#32479;&#21487;&#20197;&#27169;&#25311;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#20301;&#34892;&#20026;&#65292;&#21487;&#33021;&#20316;&#20026;&#23574;&#23792;&#21644;&#36830;&#32493;&#20449;&#21495;&#20043;&#38388;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memristive reservoirs draw inspiration from a novel class of neuromorphic hardware known as nanowire networks. These systems display emergent brain-like dynamics, with optimal performance demonstrated at dynamical phase transitions. In these networks, a limited number of electrodes are available to modulate system dynamics, in contrast to the global controllability offered by neuromorphic hardware through random access memories. We demonstrate that the learn-to-learn framework can effectively address this challenge in the context of optimization. Using the framework, we successfully identify the optimal hyperparameters for the reservoir. This finding aligns with previous research, which suggests that the optimal performance of a memristive reservoir occurs at the `edge of formation' of a conductive pathway. Furthermore, our results show that these systems can mimic membrane potential behavior observed in spiking neurons, and may serve as an interface between spike-based and continuous 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#23545;&#22823;&#37327;&#35821;&#26009;&#36827;&#34892;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.12656</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37492;&#21035;&#21644;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;
&lt;/p&gt;
&lt;p&gt;
Identifying and Extracting Rare Disease Phenotypes with Large Language Models. (arXiv:2306.12656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12656
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#32597;&#35265;&#30149;&#34920;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#26080;&#38656;&#23545;&#22823;&#37327;&#35821;&#26009;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32597;&#35265;&#30149;&#65288;RDs&#65289;&#22312;&#20840;&#29699;&#24433;&#21709;&#30528;3&#20159;&#20154;&#65292;&#27491;&#30830;&#30340;&#34920;&#22411;&#20998;&#31867;&#23545;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;RD&#34920;&#22411;&#36890;&#24120;&#23884;&#20837;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#65292;&#25163;&#21160;&#25552;&#21462;&#36807;&#31243;&#32791;&#26102;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20197;&#33258;&#21160;&#25552;&#21462;&#65292;&#20294;&#20854;&#35757;&#32451;&#20013;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#12290; &#36817;&#26399;&#65292;&#25552;&#31034;&#23398;&#20064;&#20316;&#20026;&#21487;&#20197;&#23548;&#33268;&#26356;&#21487;&#25512;&#24191;&#32467;&#26524;&#30340;NLP&#33539;&#20363;&#20986;&#29616;&#20102;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#65288;&#38646;&#26679;&#26412;&#65289;&#25110;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;&#23569;&#26679;&#26412;&#65289;&#12290;&#23613;&#31649;&#23545;ChatGPT&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#36981;&#24490;&#22797;&#26434;&#30340;&#20154;&#31867;&#25552;&#31034;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22238;&#22797;&#65292;&#20294;&#23578;&#26410;&#26377;&#20154;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30740;&#31350;&#20854;&#22312;RD NER&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#22411;&#25552;&#31034;&#20197;&#25552;&#21462;RD&#34920;&#22411;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#39318;&#27425;&#24314;&#31435;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rare diseases (RDs) are collectively common and affect 300 million people worldwide. Accurate phenotyping is critical for informing diagnosis and treatment, but RD phenotypes are often embedded in unstructured text and time-consuming to extract manually. While natural language processing (NLP) models can perform named entity recognition (NER) to automate extraction, a major bottleneck is the development of a large, annotated corpus for model training. Recently, prompt learning emerged as an NLP paradigm that can lead to more generalizable results without any (zero-shot) or few labeled samples (few-shot). Despite growing interest in ChatGPT, a revolutionary large language model capable of following complex human prompts and generating high-quality responses, none have studied its NER performance for RDs in the zero- and few-shot settings. To this end, we engineered novel prompts aimed at extracting RD phenotypes and, to the best of our knowledge, are the first the establish a benchmark 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#36866;&#24212;&#29616;&#23454;&#29615;&#22659;&#20013;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;&#22312;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#65292;&#35813;&#20195;&#29702;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2306.12654</link><description>&lt;p&gt;
&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#32771;&#34385;&#21019;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World. (arXiv:2306.12654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#27979;&#12289;&#25551;&#36848;&#21644;&#36866;&#24212;&#29616;&#23454;&#29615;&#22659;&#20013;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#22312;&#39640;&#20445;&#30495;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;&#22312;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#65292;&#35813;&#20195;&#29702;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26234;&#33021;&#20307;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#34892;&#21160;&#26102;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#26410;&#30693;&#30340;&#21019;&#26032;&#24178;&#25200;&#20854;&#35745;&#21010;&#25191;&#34892;&#12290;&#21019;&#26032;&#26159;&#19968;&#31181;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#21487;&#20197;&#25913;&#21464;&#29615;&#22659;&#30340;&#26680;&#24515;&#29305;&#24449;&#12289;&#32452;&#25104;&#21644;&#21160;&#24577;&#12290;&#21019;&#26032;&#22312;&#20219;&#20309;&#36275;&#22815;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#30340;&#20219;&#20309;&#26102;&#38388;&#37117;&#21487;&#33021;&#21457;&#29983;&#65292;&#32780;&#27809;&#26377;&#20219;&#20309;&#20107;&#20808;&#36890;&#30693;&#25110;&#35299;&#37322;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21019;&#26032;&#23545;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#20135;&#29983;&#28798;&#38590;&#24615;&#24433;&#21709;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#19990;&#30028;&#30340;&#20869;&#22312;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#29702;&#35299;&#20854;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#25191;&#34892;&#20854;&#35745;&#21010;&#12290;&#21019;&#26032;&#36890;&#24120;&#20351;&#24471;&#20182;&#20204;&#30340;&#20869;&#37096;&#27169;&#22411;&#19981;&#20934;&#30830;&#24182;&#19988;&#29983;&#25104;&#30340;&#35745;&#21010;&#19981;&#20877;&#36866;&#29992;&#12290;&#21019;&#26032;&#29305;&#21035;&#26222;&#36941;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22312;&#20854;&#20013;&#39046;&#22495;&#29305;&#23450;&#29978;&#33267;&#26159;&#39044;&#27979;&#30340;&#21019;&#26032;&#29305;&#23450;&#26041;&#27861;&#34987;&#29992;&#20110;&#20943;&#36731;&#21019;&#26032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#26816;&#27979;&#65292;&#25551;&#32472;&#21644;&#36866;&#24212;&#21019;&#26032;&#30340;&#39046;&#22495;&#29420;&#31435;AI&#20195;&#29702;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#20854;&#25216;&#33021;&#36716;&#31227;&#21040;&#39640;&#20445;&#30495;&#30340;&#27169;&#25311;&#24320;&#25918;&#19990;&#30028;&#20013;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#22312;&#28041;&#21450;&#22788;&#29702;&#26032;&#39062;&#29615;&#22659;&#24178;&#25200;&#30340;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#32780;&#19988;&#27604;&#29616;&#26377;&#30340;&#22522;&#32447;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents acting in real-world environments often need to reason with unknown novelties interfering with their plan execution. Novelty is an unexpected phenomenon that can alter the core characteristics, composition, and dynamics of the environment. Novelty can occur at any time in any sufficiently complex environment without any prior notice or explanation. Previous studies show that novelty has catastrophic impact on agent performance. Intelligent agents reason with an internal model of the world to understand the intricacies of their environment and to successfully execute their plans. The introduction of novelty into the environment usually renders their internal model inaccurate and the generated plans no longer applicable. Novelty is particularly prevalent in the real world where domain-specific and even predicted novelty-specific approaches are used to mitigate the novelty's impact. In this work, we demonstrate that a domain-independent AI agent designed to detect, chara
&lt;/p&gt;</description></item><item><title>FLAG&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;AI&#21457;&#29616;&#20195;&#30721;&#20013;&#34892;&#24322;&#24120;&#30340;&#35821;&#35328;&#26080;&#20851;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#23436;&#25972;&#65288;&#29978;&#33267;&#38750;&#32534;&#35793;&#65289;&#30340;&#20195;&#30721;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#35774;&#35745;&#20154;&#21592;&#30340;&#26816;&#26597;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.12643</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;AI&#21457;&#29616;&#20195;&#30721;&#20013;&#30340;&#34892;&#24322;&#24120;&#65288;FLAG&#65289;
&lt;/p&gt;
&lt;p&gt;
FLAG: Finding Line Anomalies (in code) with Generative AI. (arXiv:2306.12643v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12643
&lt;/p&gt;
&lt;p&gt;
FLAG&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;AI&#21457;&#29616;&#20195;&#30721;&#20013;&#34892;&#24322;&#24120;&#30340;&#35821;&#35328;&#26080;&#20851;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#23436;&#25972;&#65288;&#29978;&#33267;&#38750;&#32534;&#35793;&#65289;&#30340;&#20195;&#30721;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#35774;&#35745;&#20154;&#21592;&#30340;&#26816;&#26597;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20013;&#23384;&#22312;&#23433;&#20840;&#24615;&#21644;&#21151;&#33021;&#24615;&#32570;&#38519;&#65292;&#35782;&#21035;&#21644;&#23450;&#20301;&#23427;&#20204;&#30340;&#36807;&#31243;&#22256;&#38590;&#19988;&#20381;&#36182;&#20110;&#20154;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65288;FLAG&#65289;&#26469;&#36741;&#21161;&#20154;&#31867;&#35843;&#35797;&#32773;&#12290; FLAG&#22522;&#20110;&#29983;&#25104;AI&#30340;&#35789;&#27719;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#25105;&#20204;&#36755;&#20837;&#20195;&#30721;&#25991;&#20214;&#65292;&#28982;&#21518;&#25552;&#21462;&#24182;&#37325;&#26032;&#29983;&#25104;&#35813;&#25991;&#20214;&#20013;&#30340;&#27599;&#19968;&#34892;&#20197;&#36827;&#34892;&#33258;&#25105;&#27604;&#36739;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#20195;&#30721;&#19982;LLM&#29983;&#25104;&#30340;&#26367;&#20195;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#26174;&#30528;&#24046;&#24322;&#26631;&#35760;&#20026;&#24322;&#24120;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#26816;&#26597;&#65292;&#21516;&#26102;&#36317;&#31163;&#27880;&#37322;&#21644;LLM&#32622;&#20449;&#24230;&#31561;&#29305;&#24449;&#20063;&#26377;&#21161;&#20110;&#35813;&#20998;&#31867;&#12290;&#36825;&#20943;&#23569;&#20102;&#35774;&#35745;&#20154;&#21592;&#30340;&#26816;&#26597;&#25628;&#32034;&#31354;&#38388;&#12290;&#19982;&#27492;&#39046;&#22495;&#20013;&#30340;&#20854;&#20182;&#33258;&#21160;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;FLAG&#26159;&#35821;&#35328;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#23436;&#25972;&#65288;&#29978;&#33267;&#38750;&#32534;&#35793;&#65289;&#30340;&#20195;&#30721;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#21019;&#24314;&#23433;&#20840;&#23646;&#24615;&#12289;&#21151;&#33021;&#27979;&#35797;&#25110;&#35268;&#21017;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24110;&#21161;LLM&#36827;&#34892;&#20998;&#31867;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code contains security and functional bugs. The process of identifying and localizing them is difficult and relies on human labor. In this work, we present a novel approach (FLAG) to assist human debuggers. FLAG is based on the lexical capabilities of generative AI, specifically, Large Language Models (LLMs). Here, we input a code file then extract and regenerate each line within that file for self-comparison. By comparing the original code with an LLM-generated alternative, we can flag notable differences as anomalies for further inspection, with features such as distance from comments and LLM confidence also aiding this classification. This reduces the inspection search space for the designer. Unlike other automated approaches in this area, FLAG is language-agnostic, can work on incomplete (and even non-compiling) code and requires no creation of security properties, functional tests or definition of rules. In this work, we explore the features that help LLMs in this classification a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12627</link><description>&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#22604;&#32553;&#27491;&#21017;&#21270;&#33258;&#32534;&#30721;&#22120;&#65306;&#20013;&#24515;&#30340;&#40657;&#27934;
&lt;/p&gt;
&lt;p&gt;
Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#24050;&#24191;&#27867;&#29992;&#20110;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#24320;&#21457;&#20013;&#12290;&#23427;&#20204;&#30340;&#24212;&#29992;&#21069;&#25552;&#26159;&#22312;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21518;&#65292;&#24322;&#24120;&#36755;&#20837;&#23558;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#26377;&#20102;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#19968;&#23450;&#31243;&#24230;&#19978;&#27867;&#21270;&#21040;&#27491;&#24120;&#31867;&#20043;&#22806;&#65292;&#24182;&#22312;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#19978;&#23454;&#29616;&#36739;&#23567;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#24615;&#33021;&#65292;&#21508;&#31181;&#25216;&#26415;&#25552;&#20986;&#20102;&#20854;&#20182;&#32452;&#20214;&#21644;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#28155;&#21152;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12289;&#28041;&#21450;&#35745;&#31639;&#21644;&#32321;&#29712;&#30340;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#33410;&#34920;&#31034;&#30340;&#33539;&#25968;&#65292;&#29992;&#19968;&#20010;&#35745;&#31639;&#31616;&#21333;&#30340;&#39033;&#26469;&#34917;&#20805;&#37325;&#26500;&#25439;&#22833;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#26368;&#23567;&#21270;&#20102;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20449;&#24687;&#34701;&#21512;&#21644;&#36890;&#20449;&#22270;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;SEAL&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#31934;&#24230;&#23450;&#20301;&#21644;&#25506;&#32034;&#65292;&#20197;&#21462;&#24471;&#20102;&#27604;&#26368;&#21069;&#27839;&#26041;&#27861;&#26356;&#22909;&#30340;&#25506;&#32034;&#21644;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12623</link><description>&lt;p&gt;
SEAL: &#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#21516;&#26102;&#25506;&#32034;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
SEAL: Simultaneous Exploration and Localization in Multi-Robot Systems. (arXiv:2306.12623v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20449;&#24687;&#34701;&#21512;&#21644;&#36890;&#20449;&#22270;&#20248;&#21270;&#30456;&#32467;&#21512;&#30340;SEAL&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#39640;&#31934;&#24230;&#23450;&#20301;&#21644;&#25506;&#32034;&#65292;&#20197;&#21462;&#24471;&#20102;&#27604;&#26368;&#21069;&#27839;&#26041;&#27861;&#26356;&#22909;&#30340;&#25506;&#32034;&#21644;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#23450;&#20301;&#23545;&#20110;&#22810;&#26426;&#22120;&#20154;&#25506;&#32034;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65307;&#22024;&#26434;&#25110;&#19981;&#19968;&#33268;&#30340;&#23450;&#20301;&#20250;&#23548;&#33268;&#26410;&#33021;&#36798;&#25104;&#25506;&#32034;&#30446;&#26631;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#23454;&#29616;&#39640;&#31934;&#24230;&#23450;&#20301;&#21644;&#24403;&#20195;&#25506;&#32034;&#22320;&#22270;&#32622;&#20449;&#24230;&#65292;&#32780;&#26080;&#38656;&#20840;&#23616;&#23450;&#20301;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;&#25506;&#32034;&#21644;&#23450;&#20301;(SEAL)&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#20449;&#24687;&#34701;&#21512;&#36827;&#34892;&#26368;&#22823;&#21270;&#25506;&#32034;&#65292;&#21516;&#26102;&#25191;&#34892;&#36890;&#20449;&#22270;&#20248;&#21270;&#36827;&#34892;&#30456;&#23545;&#23450;&#20301;&#12290;&#36825;&#20004;&#20010;&#20132;&#21449;&#20381;&#36182;&#30446;&#26631;&#36890;&#36807;Rao-Blackwellization&#25216;&#26415;&#36827;&#34892;&#25972;&#21512;&#12290;&#20351;&#29992;&#20998;&#24067;&#24335;&#32447;&#24615;&#21270;&#20984;&#22771;&#20248;&#21270;&#26469;&#36873;&#25321;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#26410;&#25506;&#32034;&#21306;&#22495;&#36827;&#34892;&#20998;&#24067;&#24335;&#25506;&#32034;&#12290;&#22312;&#22823;&#37327;ROS-Gazebo&#27169;&#25311;&#20013;&#65292;SEAL&#22312;&#25506;&#32034;&#21644;&#23450;&#20301;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#21069;&#27839;&#30340;&#26041;&#27861;&#65292;&#35828;&#26126;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of accurate localization is critical for multi-robot exploration strategies; noisy or inconsistent localization causes failure in meeting exploration objectives. We aim to achieve high localization accuracy with contemporary exploration map belief and vice versa without needing global localization information. This paper proposes a novel simultaneous exploration and localization (SEAL) approach, which uses Gaussian Processes (GP)-based information fusion for maximum exploration while performing communication graph optimization for relative localization. Both these cross-dependent objectives were integrated through the Rao-Blackwellization technique. Distributed linearized convex hull optimization is used to select the next-best unexplored region for distributed exploration. SEAL outperformed cutting-edge methods on exploration and localization performance in extensive ROS-Gazebo simulations, illustrating the practicality of the approach in real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.12619</link><description>&lt;p&gt;
&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning based on Label Generation. (arXiv:2306.12619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#30340;&#22686;&#37327;&#20998;&#31867;&#23398;&#20064;&#65288;CIL&#65289;&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#65292;&#30001;&#20110;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22914;&#26524;&#23558;CIL&#23450;&#24335;&#20026;&#19968;&#20010;&#36830;&#32493;&#30340;&#26631;&#31614;&#29983;&#25104;&#38382;&#39064;&#65292;&#21017;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;CF&#24182;&#26356;&#22909;&#22320;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CIL&#26041;&#27861;&#65288;VAG&#65289;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#35789;&#27719;&#34920;&#30340;&#31232;&#30095;&#24615;&#20197;&#20415;&#20110;&#29983;&#25104;&#65292;&#24182;&#20351;&#29992;&#26631;&#31614;&#35821;&#20041;&#21019;&#24314;&#20266;&#37325;&#25773;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VAG&#30340;&#24615;&#33021;&#27604;&#22522;&#32447;&#22823;&#24133;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;AI&#31995;&#32479;&#36981;&#24490;&#35268;&#23450;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#21644;&#28508;&#22312;&#21487;&#33021;&#30340;&#25216;&#26415;&#23454;&#29616;&#65292;&#20197;&#21450;&#38656;&#35201;&#36328;&#23398;&#31185;&#26041;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12609</link><description>&lt;p&gt;
&#8220;&#26500;&#24314;&#21487;&#31649;&#25511;&#30340;AI&#31995;&#32479;&#65306;&#25216;&#26415;&#25361;&#25112;&#21644;&#25919;&#31574;&#26426;&#36935;&#8221;
&lt;/p&gt;
&lt;p&gt;
Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities. (arXiv:2306.12609v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;AI&#31995;&#32479;&#36981;&#24490;&#35268;&#23450;&#65292;&#25552;&#20986;&#20102;&#24403;&#21069;&#21644;&#28508;&#22312;&#21487;&#33021;&#30340;&#25216;&#26415;&#23454;&#29616;&#65292;&#20197;&#21450;&#38656;&#35201;&#36328;&#23398;&#31185;&#26041;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#35268;&#33539;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#34987;&#32473;&#20104;&#12290;&#22312;&#31649;&#25511;&#26426;&#26500;&#21162;&#21147;&#30830;&#23450;&#35201;&#23553;&#35013;&#21040;&#27861;&#35268;&#20013;&#30340;&#20215;&#20540;&#35266;&#26102;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#20854;&#20013;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;AI&#19987;&#23478;&#20204;&#33021;&#22815;&#23457;&#26597;AI&#31995;&#32479;&#26159;&#21542;&#36981;&#23432;&#30417;&#31649;&#35201;&#27714;&#30340;&#31243;&#24230;&#26159;&#22810;&#23569;&#65311;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20844;&#20849;&#37096;&#38376;&#30340;&#37319;&#36141;&#28165;&#21333;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#23450;&#20041;&#20102;&#19977;&#20010;&#26041;&#38754;&#65306;&#30446;&#21069;&#26377;&#21738;&#20123;&#26041;&#38754;&#26159;&#25105;&#20204;&#21487;&#20197;&#20570;&#30340;&#65307;&#21033;&#29992;AI&#25216;&#26415;&#21019;&#26032;&#25105;&#20204;&#33021;&#20570;&#21040;&#21738;&#20123;&#26041;&#38754;&#65307; &#21738;&#20123;&#35201;&#27714;&#38656;&#35201;&#26356;&#22810;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing attention being given to how to regulate AI systems. As governing bodies grapple with what values to encapsulate into regulation, we consider the technical half of the question: To what extent can AI experts vet an AI system for adherence to regulatory requirements? We investigate this question through two public sector procurement checklists, identifying what we can do now, what we should be able to do with technical innovation in AI, and what requirements necessitate a more interdisciplinary approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#22810;&#20998;&#24067;&#20449;&#24687;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#38450;&#27490;&#24050;&#30693;&#39046;&#22495;&#28040;&#32791;&#22823;&#37096;&#20998;&#26816;&#32034;&#39044;&#31639;&#65292;&#24179;&#22343;&#25552;&#39640;Recall @ 100&#28857;3.8+&#65292;&#26368;&#39640;&#21487;&#36798;8.0&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.12601</link><description>&lt;p&gt;
&#22810;&#20998;&#24067;&#31264;&#23494;&#20449;&#24687;&#26816;&#32034;&#30340;&#36164;&#28304;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Resources and Evaluations for Multi-Distribution Dense Information Retrieval. (arXiv:2306.12601v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#8212;&#8212;&#22810;&#20998;&#24067;&#20449;&#24687;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#38450;&#27490;&#24050;&#30693;&#39046;&#22495;&#28040;&#32791;&#22823;&#37096;&#20998;&#26816;&#32034;&#39044;&#31639;&#65292;&#24179;&#22343;&#25552;&#39640;Recall @ 100&#28857;3.8+&#65292;&#26368;&#39640;&#21487;&#36798;8.0&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#23450;&#20041;&#20102;&#22810;&#20998;&#24067;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26032;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#31995;&#32479;&#38656;&#35201;&#20174;&#22810;&#20010;&#38598;&#21512;&#20013;&#26816;&#32034;&#20986;&#27573;&#33853;&#65292;&#27599;&#20010;&#38598;&#21512;&#37117;&#26469;&#33258;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#20854;&#20013;&#19968;&#20123;&#38598;&#21512;&#21644;&#20998;&#24067;&#21487;&#33021;&#22312;&#35757;&#32451;&#26102;&#19981;&#21487;&#29992;&#12290;&#20026;&#20102;&#35780;&#20272;&#22810;&#20998;&#24067;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#29616;&#26377;&#30340;&#21333;&#20998;&#24067;&#25968;&#25454;&#38598;&#35774;&#35745;&#20102;&#19977;&#20010;&#22522;&#20934;&#65292;&#20998;&#21035;&#26159;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22522;&#20110;&#23454;&#20307;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22495;&#20043;&#38388;&#25112;&#30053;&#24615;&#22320;&#20998;&#37197;&#22266;&#23450;&#30340;&#26816;&#32034;&#39044;&#31639;&#65288;&#21069;k&#20010;&#27573;&#33853;&#65289;&#65292;&#20197;&#38450;&#24050;&#30693;&#39046;&#22495;&#28040;&#32791;&#22823;&#37096;&#20998;&#39044;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#23548;&#33268;&#20102;&#24179;&#22343;3.8+&#21644;&#39640;&#36798;8.0&#20010;Recall @ 100&#28857;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;&#22312;&#24494;&#35843;&#19981;&#21516;&#30340;&#22522;&#30784;&#26816;&#32034;&#27169;&#22411;&#26102;&#25913;&#36827;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and define the novel problem of multi-distribution information retrieval (IR) where given a query, systems need to retrieve passages from within multiple collections, each drawn from a different distribution. Some of these collections and distributions might not be available at training time. To evaluate methods for multi-distribution retrieval, we design three benchmarks for this task from existing single-distribution datasets, namely, a dataset based on question answering and two based on entity matching. We propose simple methods for this task which allocate the fixed retrieval budget (top-k passages) strategically across domains to prevent the known domains from consuming most of the budget. We show that our methods lead to an average of 3.8+ and up to 8.0 points improvements in Recall@100 across the datasets and that improvements are consistent when fine-tuning different base retrieval models. Our benchmarks are made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#31561;&#38382;&#39064;&#26102;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.12567</link><description>&lt;p&gt;
&#29992; NeuBAROCO &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases. (arXiv:2306.12567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#12290;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#31561;&#38382;&#39064;&#26102;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#23384;&#22312;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#19977;&#27573;&#35770;&#25512;&#29702;&#65292;&#36825;&#26159;&#20154;&#20204;&#22312;&#25512;&#29702;&#35748;&#30693;&#31185;&#23398;&#20013;&#30740;&#31350;&#36807;&#30340;&#25512;&#29702;&#24418;&#24335;&#12290;&#20026;&#20102;&#26041;&#20415;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; NeuBAROCO &#30340;&#25968;&#25454;&#38598;&#65292;&#26368;&#21021;&#26159;&#20026;&#35780;&#20272;&#20154;&#31867;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#36923;&#36753;&#33021;&#21147;&#32780;&#35774;&#35745;&#30340;&#24515;&#29702;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#33521;&#25991;&#21644;&#26085;&#25991;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#31867;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#35266;&#23519;&#21040;&#30340;&#19977;&#31181;&#20559;&#35265;&#31867;&#22411;&#65306;&#20449;&#24565;&#20559;&#35265;&#12289;&#36716;&#21270;&#38169;&#35823;&#21644;&#27675;&#22260;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28041;&#21450;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#26102;&#26356;&#23481;&#26131;&#20986;&#29616;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates whether current large language models exhibit biases in logical reasoning, similar to humans. Specifically, we focus on syllogistic reasoning, a well-studied form of inference in the cognitive science of human deduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO, originally designed for psychological experiments that assess human logical abilities in syllogistic reasoning. The dataset consists of syllogistic inferences in both English and Japanese. We examine three types of biases observed in human syllogistic reasoning: belief biases, conversion errors, and atmosphere effects. Our findings demonstrate that current large language models struggle more with problems involving these three types of biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65292;&#23637;&#31034;&#20102;&#22312;&#28436;&#31034;&#25968;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#20196;&#24314;&#27169;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12554</link><description>&lt;p&gt;
&#36890;&#36807;&#25351;&#20196;&#39044;&#27979;&#25552;&#39640;&#38271;&#26399;&#27169;&#20223;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Long-Horizon Imitation Through Instruction Prediction. (arXiv:2306.12554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65292;&#23637;&#31034;&#20102;&#22312;&#28436;&#31034;&#25968;&#37327;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#20196;&#24314;&#27169;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#38271;&#26399;&#35745;&#21010;&#21450;&#20854;&#32452;&#21512;&#24615;&#36136;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26234;&#33021;&#20307;&#26469;&#35828;&#26159;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#36807;&#24230;&#25311;&#21512;&#25233;&#21046;&#20102;&#27867;&#21270;&#65292;&#24182;&#19988;&#32047;&#31215;&#35823;&#24046;&#25439;&#23475;&#20102;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#36890;&#24120;&#26410;&#20351;&#29992;&#30340;&#36741;&#21161;&#30417;&#30563;&#26041;&#24335;&#65306;&#35821;&#35328;&#12290;&#21463;&#26368;&#36817;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#25351;&#20196;&#39044;&#27979;&#25439;&#22833;&#26469;&#35757;&#32451;&#20195;&#29702;&#65292;&#20197;&#40723;&#21169;&#23398;&#20064;&#22312;&#39640;&#23618;&#27425;&#19978;&#25805;&#20316;&#30340;&#20855;&#26377;&#26102;&#38388;&#25193;&#23637;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;BabyAI&#21644;Crafter&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25351;&#20196;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#35268;&#21010;&#29615;&#22659;&#20013;&#21463;&#38480;&#30340;&#28436;&#31034;&#25968;&#37327;&#26102;&#30340;&#34920;&#29616;&#12290;&#22312;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25351;&#20196;&#24314;&#27169;&#23545;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#30340;&#20219;&#21153;&#26368;&#20026;&#37325;&#35201;&#65292;&#32780;&#22312;&#38656;&#35201;&#31616;&#21333;&#35745;&#21010;&#30340;&#29615;&#22659;&#20013;&#21017;&#21487;&#20197;&#29702;&#35299;&#22320;&#33719;&#24471;&#26356;&#23567;&#30340;&#25910;&#30410;&#12290;&#26356;&#22810;&#32454;&#33410;&#21644;&#20195;&#30721;&#21487;&#22312;[https://github.com/facebookresearch/long-term-fairness]&#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex, long-horizon planning and its combinatorial nature pose steep challenges for learning-based agents. Difficulties in such settings are exacerbated in low data regimes where over-fitting stifles generalization and compounding errors hurt accuracy. In this work, we explore the use of an often unused source of auxiliary supervision: language. Inspired by recent advances in transformer-based models, we train agents with an instruction prediction loss that encourages learning temporally extended representations that operate at a high level of abstraction. Concretely, we demonstrate that instruction modeling significantly improves performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter benchmarks. In further analysis we find that instruction modeling is most important for tasks that require complex reasoning, while understandably offering smaller gains in environments that require simple plans. More details and code can be 
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#20984;&#20248;&#21270;&#38656;&#35201;&#22312;&#20869;&#23384;&#21644;&#26597;&#35810;&#20043;&#38388;&#26435;&#34913;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12534</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#20869;&#23384;&#21644;&#26597;&#35810;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Memory-Query Tradeoffs for Randomized Convex Optimization. (arXiv:2306.12534v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12534
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#38656;&#35201;&#22312;&#20869;&#23384;&#21644;&#26597;&#35810;&#20043;&#38388;&#26435;&#34913;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20301;&#29699;&#19978;&#26368;&#23567;&#21270;&#19968;&#20010; $d$ &#32500;&#12289;$1$-Lipschitz &#20984;&#20989;&#25968;&#65292;&#24517;&#39035;&#20351;&#29992; $\Omega(d^{2-\delta})$ &#27604;&#29305;&#30340;&#20869;&#23384;&#25110;&#32773;&#36827;&#34892; $\Omega(d^{1+\delta/6-o(1)})$ &#27425;&#26597;&#35810;&#65292;&#20854;&#20013; $\delta\in (0,1)$ &#26159;&#20219;&#24847;&#24120;&#25968;&#65292;&#31934;&#24230; $\epsilon$ &#22312; $d$ &#20013;&#26159;&#20934;&#22810;&#39033;&#24335;&#23567;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#20351;&#29992; $\tilde{O}(d^2)$ &#27604;&#29305;&#20869;&#23384;&#21644; $\tilde{O}(d)$ &#27425;&#26597;&#35810;&#30340;&#21106;&#24179;&#38754;&#26041;&#27861;&#65292;&#22312;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#20013;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#65292;&#32780;&#23545;&#20110;&#20984;&#20248;&#21270;&#65292;&#38656;&#35201;&#20108;&#27425;&#20869;&#23384;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#26597;&#35810;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that any randomized first-order algorithm which minimizes a $d$-dimensional, $1$-Lipschitz convex function over the unit ball must either use $\Omega(d^{2-\delta})$ bits of memory or make $\Omega(d^{1+\delta/6-o(1)})$ queries, for any constant $\delta\in (0,1)$ and when the precision $\epsilon$ is quasipolynomially small in $d$. Our result implies that cutting plane methods, which use $\tilde{O}(d^2)$ bits of memory and $\tilde{O}(d)$ queries, are Pareto-optimal among randomized first-order algorithms, and quadratic memory is required to achieve optimal query complexity for convex optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LIME&#26041;&#27861;&#25506;&#32034;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19981;&#20339;&#24615;&#33021;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#20013;&#12290;&#36890;&#36807;&#20998;&#26512;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#65292;&#30830;&#23450;&#36896;&#25104;&#27169;&#22411;&#24615;&#33021;&#24046;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#24182;&#35745;&#31639;&#20986;&#20854;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#35880;&#24910;&#30340;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#38477;&#20302;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.12507</link><description>&lt;p&gt;
&#22522;&#20110;LIME&#30340;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;&#40657;&#21283;&#23376;&#30340;&#24615;&#33021;&#19981;&#20339;&#21306;&#22495;&#65306;&#20197;&#36133;&#34880;&#30151;&#26816;&#27979;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Investigating Poor Performance Regions of Black Boxes: LIME-based Exploration in Sepsis Detection. (arXiv:2306.12507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LIME&#26041;&#27861;&#25506;&#32034;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19981;&#20339;&#24615;&#33021;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#36133;&#34880;&#30151;&#26816;&#27979;&#20013;&#12290;&#36890;&#36807;&#20998;&#26512;&#38169;&#35823;&#20998;&#31867;&#23454;&#20363;&#65292;&#30830;&#23450;&#36896;&#25104;&#27169;&#22411;&#24615;&#33021;&#24046;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#35782;&#21035;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#24182;&#35745;&#31639;&#20986;&#20854;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#35880;&#24910;&#30340;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#38477;&#20302;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#35299;&#37322;&#65288;LIME&#65289;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#22320;&#25551;&#36848;&#39640;&#39118;&#38505;&#36133;&#34880;&#30151;&#26816;&#27979;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#12290;&#36890;&#36807;&#20998;&#26512;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#35782;&#21035;&#36129;&#29486;&#20110;&#27425;&#20248;&#24615;&#33021;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;&#20998;&#26512;&#25581;&#31034;&#20986;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#65292;&#20174;&#32780;&#35745;&#31639;&#20986;&#36825;&#20123;&#21306;&#22495;&#20869;&#30340;&#38169;&#35823;&#29575;&#65292;&#36825;&#23545;&#20110;&#22312;&#36133;&#34880;&#30151;&#26816;&#27979;&#31561;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#19979;&#36827;&#34892;&#35880;&#24910;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;eICU&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#21487;&#35270;&#21270;&#20998;&#31867;&#22120;&#24615;&#33021;&#19981;&#20339;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#24182;&#22312;&#20851;&#38190;&#26102;&#21051;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting machine learning models remains a challenge, hindering their adoption in clinical settings. This paper proposes leveraging Local Interpretable Model-Agnostic Explanations (LIME) to provide interpretable descriptions of black box classification models in high-stakes sepsis detection. By analyzing misclassified instances, significant features contributing to suboptimal performance are identified. The analysis reveals regions where the classifier performs poorly, allowing the calculation of error rates within these regions. This knowledge is crucial for cautious decision-making in sepsis detection and other critical applications. The proposed approach is demonstrated using the eICU dataset, effectively identifying and visualizing regions where the classifier underperforms. By enhancing interpretability, our method promotes the adoption of machine learning models in clinical practice, empowering informed decision-making and mitigating risks in critical scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#30340;&#27169;&#22411;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;COVID-19&#22312;&#22810;&#32423;&#21306;&#22495;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#21487;&#35270;&#21270;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.12457</link><description>&lt;p&gt;
&#22810;&#32423;&#21306;&#22495;COVID-19&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Dynamic Epidemiological Modelling for COVID-19 Forecasting in Multi-level Districts. (arXiv:2306.12457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#30340;&#27169;&#22411;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;COVID-19&#22312;&#22810;&#32423;&#21306;&#22495;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#21487;&#35270;&#21270;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;COVID-19&#24050;&#32463;&#20840;&#29699;&#34067;&#24310;&#24182;&#23545;&#25972;&#20010;&#19990;&#30028;&#20135;&#29983;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#27169;&#25311;COVID-19&#20256;&#25773;&#29366;&#20917;&#23545;&#20102;&#35299;&#24403;&#21069;&#29366;&#24577;&#21644;&#21046;&#23450;&#24178;&#39044;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;SEIR&#27169;&#22411;&#30340;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#27169;&#25311;&#30142;&#30149;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#19981;&#33021;&#31934;&#30830;&#22320;&#25311;&#21512;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#30001;&#20110;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#22914;&#31038;&#20132;&#36317;&#31163;&#25919;&#31574;&#21644;&#24178;&#39044;&#31574;&#30053;&#31561;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#25311;&#21512;&#24615;&#33021;&#65292;&#20294;&#26080;&#27861;&#21487;&#35270;&#21270;&#26426;&#21046;&#12290;&#26041;&#27861;&#65306;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21160;&#24577;&#27969;&#34892;&#30149;&#23398;&#65288;DDE&#65289;&#26041;&#27861;&#65292;&#23558;&#27969;&#34892;&#30149;&#23398;&#26041;&#31243;&#21644;&#28145;&#24230;&#23398;&#20064;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#39640;&#31934;&#24230;&#21644;&#21487;&#35270;&#21270;&#12290; DDE&#21253;&#21547;&#28145;&#24230;&#32593;&#32476;&#20197;&#36866;&#24212;&#25928;&#24212;&#20989;&#25968;&#65292;&#20197;&#22522;&#20110;&#31070;&#32463;ODE&#26041;&#27861;&#35299;&#20915;&#21464;&#37327;&#26041;&#31243;&#26469;&#27169;&#25311;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#65292;&#30830;&#20445;&#25311;&#21512;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Objective: COVID-19 has spread worldwide and made a huge influence across the world. Modeling the infectious spread situation of COVID-19 is essential to understand the current condition and to formulate intervention measurements. Epidemiological equations based on the SEIR model simulate disease development. The traditional parameter estimation method to solve SEIR equations could not precisely fit real-world data due to different situations, such as social distancing policies and intervention strategies. Additionally, learning-based models achieve outstanding fitting performance, but cannot visualize mechanisms. Methods: Thus, we propose a deep dynamic epidemiological (DDE) method that combines epidemiological equations and deep-learning advantages to obtain high accuracy and visualization. The DDE contains deep networks to fit the effect function to simulate the ever-changing situations based on the neural ODE method in solving variants' equations, ensuring the fitting performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.12456</link><description>&lt;p&gt;
&#26426;&#22120;&#35774;&#35745;&#30340;&#26497;&#38480;&#25361;&#25112;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#33258;&#21160;CPU&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of Machine Design: Automated CPU Design with AI. (arXiv:2306.12456v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#33258;&#21160;&#35774;&#35745;&#20102;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#65292;&#20174;&#32780;&#25506;&#32034;&#20102;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#27963;&#21160;&#8212;&#8212;&#26500;&#24314;&#19968;&#20010;&#28385;&#36275;&#32473;&#23450;&#30446;&#26631;&#21644;&#38480;&#21046;&#26465;&#20214;&#30340;&#24037;&#20214;&#25551;&#36848;&#8212;&#8212;&#21306;&#21035;&#20110;&#20154;&#31867;&#21644;&#20256;&#32479;&#26426;&#22120;&#65292;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#27700;&#24179;&#25110;&#26356;&#39640;&#30340;&#35774;&#35745;&#33021;&#21147;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#26426;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36816;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#35774;&#35745;&#26032;&#26448;&#26009;&#12289;&#34507;&#30333;&#36136;&#21644;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#24037;&#20214;&#30340;&#35774;&#35745;&#31354;&#38388;&#30456;&#23545;&#36739;&#23567;&#65292;&#22240;&#27492;&#65292;&#8220;&#26426;&#22120;&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#35774;&#35745;&#65311;&#8221;&#25104;&#20026;&#20102;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#25299;&#23637;&#26426;&#22120;&#35774;&#35745;&#30340;&#36793;&#30028;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#19968;&#20010;&#20013;&#22830;&#22788;&#29702;&#22120;(CPU)&#65292;&#21363;&#35745;&#31639;&#26426;&#30340;&#22823;&#33041;&#65292;&#36825;&#26159;&#20154;&#31867;&#35774;&#35745;&#36807;&#30340;&#26368;&#22797;&#26434;&#30340;&#35013;&#32622;&#20043;&#19968;&#12290;&#35813;&#26041;&#27861;&#20174;&#21482;&#26377;&#22806;&#37096;&#36755;&#20837;&#36755;&#20986;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;CPU&#35774;&#35745;&#30340;&#30005;&#36335;&#36923;&#36753;&#65292;&#35813;&#36923;&#36753;&#29992;&#20108;&#20803;&#25512;&#28436;&#22270;(BSD)&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design activity -- constructing an artifact description satisfying given goals and constraints -- distinguishes humanity from other animals and traditional machines, and endowing machines with design abilities at the human level or beyond has been a long-term pursuit. Though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (AI) techniques, the search space for designing such objects is relatively small, and thus, "Can machines design like humans?" remains an open question. To explore the boundary of machine design, here we present a new AI approach to automatically design a central processing unit (CPU), the brain of a computer, and one of the world's most intricate devices humanity have ever designed. This approach generates the circuit logic, which is represented by a graph structure called Binary Speculation Diagram (BSD), of the CPU design from only external input-output observations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#24102;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#20998;&#35299;&#26465;&#20214; IV &#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.12453</link><description>&lt;p&gt;
&#23398;&#20064;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#34920;&#31034;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Conditional Instrumental Variable Representation for Causal Effect Estimation. (arXiv:2306.12453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#20174;&#24102;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#20998;&#35299;&#26465;&#20214; IV &#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#27835;&#30103;&#23545;&#24863;&#20852;&#36259;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#32463;&#24120;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#20102;&#27835;&#30103;&#21644;&#32467;&#26524;&#12290; &#24037;&#20855;&#21464;&#37327; (IV) &#26041;&#27861;&#26159;&#28040;&#38500;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#28151;&#28102;&#20559;&#24046;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110; IV &#30340;&#20272;&#35745;&#22120;&#38656;&#35201;&#26377;&#19968;&#20010;&#34987;&#25552;&#21517;&#30340; IV&#65292;&#32780;&#23545;&#20110;&#26465;&#20214; IV (CIV) &#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38656;&#35201;&#20854;&#30456;&#24212;&#30340;&#26465;&#20214;&#38598;&#12290; &#22522;&#20110;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DVAE.CIV &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#20174;&#20855;&#26377;&#28151;&#28102;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#20998;&#35299; CIV &#30340;&#34920;&#31034;&#21644;&#20854;&#26465;&#20214;&#38598;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#23545;&#28151;&#28102;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges in causal inference is to estimate the causal effect of a treatment on its outcome of interest from observational data. However, causal effect estimation often suffers from the impacts of confounding bias caused by unmeasured confounders that affect both the treatment and the outcome. The instrumental variable (IV) approach is a powerful way to eliminate the confounding bias from latent confounders. However, the existing IV-based estimators require a nominated IV, and for a conditional IV (CIV) the corresponding conditioning set too, for causal effect estimation. This limits the application of IV-based estimators. In this paper, by leveraging the advantage of disentangled representation learning, we propose a novel method, named DVAE.CIV, for learning and disentangling the representations of CIV and the representations of its conditioning set for causal effect estimations from data with latent confounders. Extensive experimental results on both synthet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#25216;&#26415;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.12444</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#34920;&#29616;&#21463;&#21738;&#20123;&#22240;&#32032;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Factors Affecting the Performance of Automated Speaker Verification in Alzheimer's Disease Clinical Trials. (arXiv:2306.12444v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#25216;&#26415;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#20250;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#26816;&#27979;&#37325;&#22797;&#30340;&#24739;&#32773;&#21442;&#19982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#37325;&#22797;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#30772;&#22351;&#35797;&#39564;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#65292;&#23548;&#33268;&#37325;&#22823;&#30340;&#20581;&#24247;&#21644;&#36130;&#21153;&#39118;&#38505;&#12290;&#24320;&#21457;&#20934;&#30830;&#30340;&#33258;&#21160;&#21270;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#27169;&#22411;&#23545;&#20110;&#39564;&#35777;&#24050;&#27880;&#20876;&#20010;&#20307;&#30340;&#36523;&#20221;&#24182;&#21435;&#38500;&#37325;&#22797;&#39033;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#25968;&#25454;&#30340;&#22823;&#23567;&#21644;&#36136;&#37327;&#20250;&#24433;&#21709;ASV&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#23545;&#20110;&#24433;&#21709;&#20020;&#24202;&#29615;&#22659;&#20013;ASV&#33021;&#21147;&#30340;&#22240;&#32032;&#30340;&#35843;&#26597;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20174;&#22810;&#20010;&#35828;&#35805;&#20219;&#21153;&#20013;&#33719;&#21462;&#30340;659&#20010;&#20855;&#26377;&#19981;&#21516;AD&#24863;&#26579;&#31243;&#24230;&#30340;&#21442;&#19982;&#32773;&#30340;&#35821;&#38899;&#35760;&#24405;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25506;&#35752;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#12289;&#38899;&#39057;&#36136;&#37327;&#26631;&#20934;&#21644;AD&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;ASV&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ASV&#30340;&#24615;&#33021;&#65306;1&#65289;&#22312;&#30007;&#24615;&#35828;&#35805;&#32773;&#19978;&#30053;&#20248;&#20110;&#22899;&#24615;&#35828;&#35805;&#32773;&#65307;2&#65289;
&lt;/p&gt;
&lt;p&gt;
Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#65292;&#29992;&#20110;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25439;&#22833;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.12442</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation via Token-level Relationship Graph. (arXiv:2306.12442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#65292;&#29992;&#20110;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#25439;&#22833;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#20256;&#36882;&#30340;&#30495;&#27491;&#28508;&#21147;&#36824;&#27809;&#26377;&#34987;&#20805;&#20998;&#25366;&#25496;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#33976;&#39311;&#21333;&#20010;&#20449;&#24687;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#35789;&#32423;&#21035;&#20851;&#31995;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#38271;&#23614;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#35789;&#32423;&#21035;&#20851;&#31995;&#22270;(TRG)&#30340;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35789;&#32423;&#21035;&#30340;&#20851;&#31995;&#30693;&#35782;&#26469;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;TRG&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#25945;&#24072;&#27169;&#22411;&#20013;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33976;&#39311;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#19978;&#19979;&#25991;&#25439;&#22833;&#30340;&#35789;&#32423;&#21035;&#19978;&#19979;&#25991;&#25439;&#22833;&#65292;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a powerful technique for transferring knowledge from a pre-trained teacher model to a student model. However, the true potential of knowledge transfer has not been fully explored. Existing approaches primarily focus on distilling individual information or instance-level relationships, overlooking the valuable information embedded in token-level relationships, which may be particularly affected by the long-tail effects. To address the above limitations, we propose a novel method called Knowledge Distillation with Token-level Relationship Graph (TRG) that leverages the token-wise relational knowledge to enhance the performance of knowledge distillation. By employing TRG, the student model can effectively emulate higher-level semantic information from the teacher model, resulting in improved distillation results. To further enhance the learning process, we introduce a token-wise contextual loss called contextual loss, which encourages the student model to capture
&lt;/p&gt;</description></item><item><title>AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11971</link><description>&lt;p&gt;
AdCraft&#65306;&#19968;&#31181;&#29992;&#20110;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20248;&#21270;&#30340;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
AdCraft: An Advanced Reinforcement Learning Benchmark Environment for Search Engine Marketing Optimization. (arXiv:2306.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11971
&lt;/p&gt;
&lt;p&gt;
AdCraft&#26159;&#19968;&#31181;&#39640;&#32423;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20986;&#20215;&#21644;&#39044;&#31639;&#21464;&#21270;&#30340;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;(SEM)&#27963;&#21160;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#29615;&#22659;&#8212;&#8212; AdCraft&#65292;&#20854;&#20855;&#26377;&#38543;&#26426;&#21644;&#38750;&#38745;&#24577;&#29305;&#24615;&#12290;&#35813;&#29615;&#22659;&#27169;&#25311;&#20102;&#25628;&#32034;&#24341;&#25806;&#33829;&#38144;&#20013;&#20986;&#20215;&#21644;&#39044;&#31639;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SEM&#26159;&#19968;&#31181;&#21033;&#29992;&#20184;&#36153;&#24191;&#21578;&#26469;&#22686;&#21152;&#32593;&#31449;&#22312;&#25628;&#32034;&#24341;&#25806;&#32467;&#26524;&#39029;&#38754;&#19978;&#30340;&#21487;&#35265;&#24615;&#30340;&#25968;&#23383;&#33829;&#38144;&#25216;&#26415;&#12290;SEM&#24191;&#21578;&#27963;&#21160;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#21253;&#25324;&#20851;&#38190;&#23383;&#36873;&#25321;&#12289;&#24191;&#21578;&#35774;&#35745;&#12289;&#20986;&#20215;&#31649;&#29702;&#12289;&#39044;&#31639;&#35843;&#25972;&#21644;&#34920;&#29616;&#30417;&#25511;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20248;&#21270;SEM&#24191;&#21578;&#25237;&#25918;&#27963;&#21160;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#25110;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#30340;&#21487;&#23450;&#21046;&#29615;&#22659;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#35780;&#20272;&#21644;&#25552;&#39640;&#19982;SEM&#20986;&#20215;&#21644;&#39044;&#31639;&#31649;&#29702;&#30456;&#20851;&#30340;RL&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20184;&#20986;&#36825;&#20123;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;AdCraft&#29615;&#22659;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce \env{}, a novel benchmark environment for the Reinforcement Learning (RL) community distinguished by its stochastic and non-stationary properties. The environment simulates bidding and budgeting dynamics within Search Engine Marketing (SEM), a digital marketing technique utilizing paid advertising to enhance the visibility of websites on search engine results pages (SERPs). The performance of SEM advertisement campaigns depends on several factors, including keyword selection, ad design, bid management, budget adjustments, and performance monitoring. Deep RL recently emerged as a potential strategy to optimize campaign profitability within the complex and dynamic landscape of SEM but it requires substantial data, which may be costly or infeasible to acquire in practice. Our customizable environment enables practitioners to assess and enhance the robustness of RL algorithms pertinent to SEM bid and budget management without such costs. Through a series of experiments within 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11086</link><description>&lt;p&gt;
&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#21464;&#20998;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21457;&#23637;&#23545;&#20110; NISQ &#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#31639;&#27861;&#38656;&#35201;&#30701;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#36825;&#31181;&#30005;&#36335;&#26356;&#26131;&#20110;&#22312;&#36817;&#26399;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#20063;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26377;&#36259;&#30340;&#31639;&#27861;&#26159;&#25152;&#35859;&#30340;&#21464;&#20998;&#23545;&#35282;&#21270;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31639;&#27861;&#23376;&#20363;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#20197;&#37327;&#23376;&#29366;&#24577;&#32534;&#30721;&#30340;&#25968;&#25454;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#36776;&#37327;&#23376;&#24577;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#31995;&#32479;&#30340;&#32416;&#32544;&#24615;&#36136;&#65292;&#25110;&#32773;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22312;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#20219;&#21153;&#20013;&#25152;&#38656;&#30005;&#36335;&#38750;&#24120;&#27973;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#30005;&#36335;&#28145;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10756</link><description>&lt;p&gt;
&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26234;&#33021;&#25163;&#26426;&#36827;&#34892;&#24247;&#22797;&#35757;&#32451;&#30340;&#31649;&#29702;&#12290;&#24739;&#32773;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#27835;&#30103;&#24072;&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#36827;&#24230;&#30340;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#27835;&#30103;&#26377;&#21161;&#20110;&#27835;&#24840;&#36731;&#24494;&#30340;&#20307;&#32946;&#21644;&#32844;&#19994;&#20260;&#23475;&#12290;&#20256;&#32479;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#65292;&#27835;&#30103;&#24072;&#20250;&#25351;&#23450;&#26576;&#20123;&#21160;&#20316;&#20379;&#24739;&#32773;&#22312;&#21307;&#38498;&#35775;&#38382;&#20043;&#38388;&#25191;&#34892;&#65292;&#36825;&#23558;&#20381;&#36182;&#20110;&#24739;&#32773;&#27491;&#30830;&#22320;&#35760;&#20303;&#21160;&#20316;&#21644;&#25191;&#34892;&#35745;&#21010;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#24739;&#32773;&#20250;&#24536;&#35760;&#25191;&#34892;&#21160;&#20316;&#25110;&#26080;&#27861;&#35814;&#32454;&#22238;&#24819;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#24247;&#22797;&#27835;&#30103;&#21463;&#21040;&#38459;&#30861;&#65292;&#25110;&#32773;&#22312;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#24739;&#32773;&#21487;&#33021;&#20250;&#22240;&#25191;&#34892;&#19981;&#27491;&#30830;&#30340;&#21160;&#20316;&#32780;&#36973;&#21463;&#39069;&#22806;&#30340;&#20260;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;HRNet&#30340;&#24247;&#22797;&#30417;&#27979;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#24739;&#32773;&#30340;&#26234;&#33021;&#25163;&#26426;&#25552;&#37266;&#24739;&#32773;&#20309;&#26102;&#25191;&#34892;&#21160;&#20316;&#24182;&#23637;&#31034;&#21160;&#20316;&#20379;&#24739;&#32773;&#36319;&#38543;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24110;&#21161;&#27835;&#30103;&#24072;&#30417;&#27979;&#24739;&#32773;&#30340;&#24247;&#22797;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#30001;&#19968;&#27454;iOS&#24212;&#29992;&#31243;&#24207;&#21644;&#20960;&#20010;&#26381;&#21153;&#22120;&#32452;&#20214;&#32452;&#25104;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#36127;&#36131;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
The rehabilitation treatment helps to heal minor sports and occupational injuries. In a traditional rehabilitation process, a therapist will assign certain actions to a patient to perform in between hospital visits, and it will rely on the patient to remember actions correctly and the schedule to perform them. Unfortunately, many patients forget to perform actions or fail to recall actions in detail. As a consequence, the rehabilitation treatment is hampered or, in the worst case, the patient may suffer from additional injury caused by performing incorrect actions. To resolve these issues, we propose a HRNet-based rehabilitation monitoring system, which can remind a patient when to perform the actions and display the actions for the patient to follow via the patient's smartphone. In addition, it helps the therapist to monitor the progress of the rehabilitation for the patient. Our system consists of an iOS app and several components at the server side. The app is in charge of displayin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10286</link><description>&lt;p&gt;
Enlighten-anything: &#24403;&#20998;&#27573;&#27169;&#22411;&#36935;&#35265;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enlighten-anything:When Segment Anything Model Meets Low-light Image Enhancement. (arXiv:2306.10286v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#20013;&#23558;&#20998;&#27573;&#27169;&#22411;&#19982;SAM&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#39033;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#65292;&#22823;&#22810;&#25968;CNN&#26041;&#27861;&#37117;&#26159;&#20316;&#20026;&#40657;&#30418;&#23376;&#35774;&#35745;&#30340;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#22266;&#26377;&#32654;&#23398;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#26041;&#27861;&#24573;&#30053;&#20102;&#20302;&#20809;&#22330;&#26223;&#20013;&#21487;&#35265;&#20449;&#24687;&#30340;&#36864;&#21270;&#65292;&#36825;&#20250;&#20005;&#37325;&#24433;&#21709;&#34917;&#20805;&#20449;&#24687;&#30340;&#32858;&#21512;&#65292;&#24182;&#20351;&#34701;&#21512;&#31639;&#27861;&#26080;&#27861;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#34701;&#21512;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Enlighten-anything&#65292;&#33021;&#22815;&#23558;SAM&#20998;&#27573;&#30340;&#35821;&#20041;&#24847;&#22270;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#24863;&#30693;&#30340;&#34701;&#21512;&#22270;&#20687;&#12290;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#65292;&#23545;LOL&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PSNR&#19978;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;3dB&#65292;&#22312;SSIM&#19978;&#25552;&#39640;&#20102;8&#12290; SAM&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20026;&#26080;&#30417;&#30563;&#20302;&#20809;&#22686;&#24378;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#24110;&#21161;&#12290;Enlighten-anything&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#20174; https://github.com/zhangbaijin/enlighten-any &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a low-level visual task, and most CNN methods are designed as black boxes, lacking transparency and intrinsic aesthetics. Many unsupervised approaches ignore the degradation of visible information in low-light scenes, which will seriously affect the aggregation of complementary information and also make the fusion algorithm unable to produce satisfactory fusion results under extreme conditions. In this paper, we propose Enlighten-anything, which is able to enhance and fuse the semantic intent of SAM segmentation with low-light images to obtain fused images with good visual perception. The generalization ability of unsupervised learning is greatly improved, and experiments on LOL dataset are conducted to show that our method improves 3db in PSNR over baseline and 8 in SSIM. zero-shot learning of SAM introduces a powerful aid for unsupervised low-light enhancement. The source code of Enlighten-anything can be obtained from https://github.com/zhangbaijin/enlighten-any
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2306.09633</link><description>&lt;p&gt;
&#34394;&#20551;&#40654;&#26126;&#65306;&#37325;&#26032;&#35780;&#20272;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#23439;&#35266;&#24067;&#23616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09633
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#22768;&#31216;&#20854;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33455;&#29255;&#35774;&#35745;&#39046;&#22495;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#20294;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35895;&#27468;&#30340;&#26041;&#27861;&#19981;&#22914;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#19981;&#22914;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#20063;&#19981;&#22914;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#65292;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#20063;&#36973;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;2021&#24180;&#22312;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#19978;&#21457;&#34920;&#30340;&#26377;&#20851;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#33455;&#29255;&#30340;&#35770;&#25991;&#65292;&#22240;&#20026;&#25152;&#22768;&#31216;&#30340;&#32467;&#26524;&#32570;&#20047;&#20805;&#20998;&#30340;&#25991;&#20214;&#35760;&#24405;&#21644;&#20851;&#38190;&#27493;&#39588;&#30340;&#35828;&#26126;&#65292;&#24341;&#21457;&#20105;&#35758;&#24182;&#21463;&#21040;&#23186;&#20307;&#30340;&#25209;&#35780;&#25253;&#36947;&#12290; &#32780;&#20004;&#39033;&#29420;&#31435;&#30340;&#35780;&#20272;&#22635;&#34917;&#20102;&#31354;&#30333;&#65292;&#35777;&#26126;&#35895;&#27468;&#24378;&#21270;&#23398;&#20064;&#33853;&#21518;&#20110;&#20154;&#31867;&#35774;&#35745;&#24072;&#12289;&#33853;&#21518;&#20110;&#19968;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#65288;&#27169;&#25311;&#36864;&#28779;&#65289;&#65292;&#24182;&#19988;&#36824;&#33853;&#21518;&#20110;&#26222;&#36941;&#21487;&#29992;&#30340;&#21830;&#19994;&#36719;&#20214;&#12290;&#20132;&#21449;&#26816;&#26597;&#30340;&#25968;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#34892;&#20026;&#12289;&#20998;&#26512;&#21644;&#25253;&#21578;&#20013;&#30340;&#38169;&#35823;&#65292;&#35813;&#12298;&#33258;&#28982;&#12299;&#25991;&#31456;&#30340;&#23436;&#25972;&#24615;&#21463;&#21040;&#20102;&#20005;&#37325;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.09525</link><description>&lt;p&gt;
&#21033;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#35299;&#37322;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#27861;&#24459;&#24320;&#25918;&#24615;&#26415;&#35821;&#30340;&#21547;&#20041;&#26159;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20808;&#21069;&#27861;&#38498;&#26696;&#20363;&#20013;&#35813;&#26415;&#35821;&#30340;&#24212;&#29992;&#26159;&#35299;&#37322;&#20854;&#21547;&#20041;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;GPT-4&#34987;&#30452;&#25509;&#35201;&#27714;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#30340;&#22522;&#20934;&#35774;&#32622;&#30340;&#24615;&#33021;&#19982;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#34987;&#29992;&#26469;&#20026;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#65292;&#21363;&#26469;&#33258;&#26696;&#20363;&#27861;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#24212;&#29992;GPT-4&#20135;&#29983;&#30340;&#35299;&#37322;&#22312;&#34920;&#38754;&#19978;&#20284;&#20046;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#37322;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#24182;&#20284;&#20046;&#28040;&#38500;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21457;&#26126;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.05053</link><description>&lt;p&gt;
Hebbian&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#20027;&#21160;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference in Hebbian Learning Networks. (arXiv:2306.05053v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#26469;&#29983;&#25104;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#23616;&#37096;Hebbian&#21487;&#22609;&#24615;&#30340;&#20223;&#33041;&#31070;&#32463;&#32676;&#20307;&#22914;&#20309;&#25191;&#34892;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#65292;&#20197;&#25511;&#21046;&#21160;&#24577;&#20195;&#29702;&#12290;&#36890;&#36807;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;Hebbian&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#30340;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#20010;&#21518;&#39564;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#28508;&#22312;&#29366;&#24577;&#65292;&#20197;&#21450;&#19968;&#20010;&#29366;&#24577;&#36716;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#24403;&#21069;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19979;&#19968;&#20010;&#26399;&#26395;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#20351;&#29992;OpenAI gym&#22871;&#20214;&#20013;&#30340;Mountain Car&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#21508;&#31181;Hebbian&#32593;&#32476;&#21442;&#25968;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;Hebbian AIF&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;Q-learning&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#22238;&#25918;&#32531;&#20914;&#21306;&#65292;&#22914;&#20856;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;Hebbian&#23398;&#20064;&#65292;&#20197;&#35774;&#35745;&#33021;&#22815;&#23398;&#20064;&#29615;&#22659;&#21160;&#24577;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35775;&#38382;&#36807;&#21435;&#32531;&#20914;&#21306;&#30340;AIF&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies how brain-inspired neural ensembles equipped with local Hebbian plasticity can perform active inference (AIF) in order to control dynamical agents. A generative model capturing the environment dynamics is learned by a network composed of two distinct Hebbian ensembles: a posterior network, which infers latent states given the observations, and a state transition network, which predicts the next expected latent state given current state-action pairs. Experimental studies are conducted using the Mountain Car environment from the OpenAI gym suite, to study the effect of the various Hebbian network parameters on the task performance. It is shown that the proposed Hebbian AIF approach outperforms the use of Q-learning, while not requiring any replay buffer, as in typical reinforcement learning systems. These results motivate further investigations of Hebbian learning for the design of AIF networks that can learn environment dynamics without the need for revisiting past buf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#36827;&#29616;&#26377;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.01721</link><description>&lt;p&gt;
&#21033;&#29992;mask&#20808;&#39564;&#27169;&#22411;&#21435;&#22122;&#25193;&#25955;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Semantic Segmentation with Mask Prior Modeling. (arXiv:2306.01721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#36827;&#29616;&#26377;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#23398;&#20064;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#34920;&#31034;&#29992;&#20110;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20998;&#21106;&#25513;&#27169;&#26412;&#36523;&#30340;&#20808;&#39564;&#65292;&#20363;&#22914;&#20960;&#20309;&#32422;&#26463;&#21644;&#35821;&#20041;&#32422;&#26463;&#20173;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#21892;&#29616;&#26377;&#21306;&#20998;&#24615;&#26041;&#27861;&#30340;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#20174;&#20026;mask&#20808;&#39564;&#24314;&#27169;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#26550;&#26500;&#24320;&#22987;&#65292;&#23558;&#26412;&#25991;&#37325;&#28857;&#25918;&#22312;&#20855;&#26377;&#31163;&#25955;&#25193;&#25955;&#30340;&#20855;&#20307;&#23454;&#20363;&#19978;&#65292;&#24182;&#30830;&#23450;&#20854;&#25104;&#21151;&#24212;&#29992;&#30340;&#21508;&#31181;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;&#20960;&#20010;&#37325;&#35201;&#21457;&#29616;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#31616;&#21333;&#22320;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#20998;&#21106;&#20013;&#19981;&#36275;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#65292;&#35774;&#35745;&#19981;&#33391;&#30340;&#25193;&#25955;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#19979;&#38477;;&#65288;2&#65289;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#22312;&#20445;&#30041;&#36974;&#32617;&#32467;&#26500;&#21644;&#21512;&#24182;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#20854;&#20182;&#20449;&#24687;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;;&#65288;3&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of semantic segmentation has long been dominated by learning more discriminative image representations for classifying each pixel. Despite the prominent advancements, the priors of segmentation masks themselves, e.g., geometric and semantic constraints, are still under-explored. In this paper, we propose to ameliorate the semantic segmentation quality of existing discriminative approaches with a mask prior modeled by a recently-developed denoising diffusion generative model. Beginning with a unified architecture that adapts diffusion models for mask prior modeling, we focus this work on a specific instantiation with discrete diffusion and identify a variety of key design choices for its successful application. Our exploratory analysis revealed several important findings, including: (1) a simple integration of diffusion models into semantic segmentation is not sufficient, and a poorly-designed diffusion process might lead to degradation in segmentation performance; (2) dur
&lt;/p&gt;</description></item><item><title>BLIP-Diffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#24182;&#25903;&#25345;&#38646;&#26679;&#26412;&#29983;&#25104;&#21644;&#23450;&#21046;&#21270;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.14720</link><description>&lt;p&gt;
BLIP-Diffusion: &#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#39044;&#35757;&#32451;&#20027;&#20307;&#34920;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing. (arXiv:2305.14720v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14720
&lt;/p&gt;
&lt;p&gt;
BLIP-Diffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#23454;&#29616;&#20102;&#21487;&#25511;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#24182;&#25903;&#25345;&#38646;&#26679;&#26412;&#29983;&#25104;&#21644;&#23450;&#21046;&#21270;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20027;&#20307;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#26032;&#39062;&#30340;&#20027;&#20307;&#22270;&#20687;&#12290;&#29616;&#26377;&#27169;&#22411;&#23384;&#22312;&#38271;&#26102;&#38388;&#24494;&#35843;&#21644;&#38590;&#20197;&#20445;&#25345;&#20027;&#20307;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BLIP-Diffusion&#65292;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#27169;&#24577;&#25511;&#21046;&#30340;&#20027;&#20307;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#20027;&#20307;&#22270;&#20687;&#21644;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#12290;&#19982;&#20854;&#20182;&#20027;&#20307;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;BLIP-Diffusion&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#23545;&#20027;&#20307;&#34920;&#24449;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#25353;&#29031;BLIP-2&#30340;&#26041;&#27861;&#39044;&#35757;&#32451;&#20102;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#20197;&#29983;&#25104;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20027;&#20307;&#34920;&#24449;&#23398;&#20064;&#20219;&#21153;&#65292;&#20351;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#35270;&#35273;&#34920;&#24449;&#24182;&#29983;&#25104;&#26032;&#30340;&#20027;&#20307;&#22270;&#20687;&#12290;&#19982;DreamBooth&#31561;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25903;&#25345;&#38646;&#26679;&#26412;&#20027;&#20307;&#39537;&#21160;&#29983;&#25104;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#23450;&#21046;&#20027;&#20307;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with u
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.04474</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Vision Langauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation. (arXiv:2305.04474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#65292;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#30528;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174; mutual information &#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#21040;&#36127;&#26679;&#26412;&#30340;&#20114;&#20449;&#24687;&#20063;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#30456;&#20284;&#24615;&#36880;&#27493;&#32454;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#28857;&#19982;&#20854;&#36127;&#26679;&#26412;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#21453;&#26679;&#26412;&#30340;&#26377;&#30410;&#24433;&#21709;&#21644;&#26377;&#23475;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153; - &#30693;&#35782;&#22270;&#35889;&#25512;&#29702;(KGR)&#65292;&#24182;&#22522;&#20110;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#27010;&#25324;&#20102;KGR&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;ROAR&#65292;&#39640;&#24230;&#26377;&#25928;&#22320;&#35823;&#23548;KGR&#21521;&#30446;&#26631;&#26597;&#35810;&#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#65292;&#20294;&#23545;&#20110;&#38750;&#30446;&#26631;&#26597;&#35810;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02383</link><description>&lt;p&gt;
&#35770;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
On the Security Risks of Knowledge Graph Reasoning. (arXiv:2305.02383v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153; - &#30693;&#35782;&#22270;&#35889;&#25512;&#29702;(KGR)&#65292;&#24182;&#22522;&#20110;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#27010;&#25324;&#20102;KGR&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;ROAR&#65292;&#39640;&#24230;&#26377;&#25928;&#22320;&#35823;&#23548;KGR&#21521;&#30446;&#26631;&#26597;&#35810;&#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#65292;&#20294;&#23545;&#20110;&#38750;&#30446;&#26631;&#26597;&#35810;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702; (KGR) - &#22238;&#31572;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#19978;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#30340;&#20219;&#21153;&#65292;&#20195;&#34920;&#20102;&#19968;&#39033;&#37325;&#35201;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#28041;&#21450;&#22810;&#31181;&#24212;&#29992;&#65288;&#20363;&#22914;&#32593;&#32476;&#23041;&#32961;&#29417;&#29454;&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#26159; KGR &#30340;&#28508;&#22312;&#23433;&#20840;&#39118;&#38505;&#36824;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#65292;&#36825;&#19968;&#28857;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#26412;&#25991;&#26159;&#26550;&#36215;&#29421;&#38552;&#40511;&#27807;&#30340;&#19968;&#20010;&#21487;&#38752;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#32773;&#30340;&#30446;&#30340;&#12289;&#30693;&#35782;&#21644;&#25915;&#20987;&#21521;&#37327;&#31995;&#32479;&#21270;&#22320;&#27010;&#25324;&#20102; KGR &#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ROAR&#65292;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#65292;&#23427;&#26159;&#23454;&#29616;&#21508;&#31181;&#27492;&#31867;&#23041;&#32961;&#30340;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#20195;&#34920;&#24615;&#29992;&#20363;&#65288;&#22914;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#12289;&#32593;&#32476;&#23041;&#32961;&#29417;&#29454;&#21644;&#24120;&#35782;&#25512;&#29702;&#65289;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126; ROAR &#23545;&#20110;&#35823;&#23548; KGR &#25552;&#20379;&#39044;&#23450;&#20041;&#31572;&#26696;&#30340;&#30446;&#26631;&#26597;&#35810;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23545;&#38750;&#30446;&#26631;&#26597;&#35810;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph reasoning (KGR) -- answering complex logical queries over large knowledge graphs -- represents an important artificial intelligence task, entailing a range of applications (e.g., cyber threat hunting). However, despite its surging popularity, the potential security risks of KGR are largely unexplored, which is concerning, given the increasing use of such capability in security-critical domains.  This work represents a solid initial step towards bridging the striking gap. We systematize the security threats to KGR according to the adversary's objectives, knowledge, and attack vectors. Further, we present ROAR, a new class of attacks that instantiate a variety of such threats. Through empirical evaluation in representative use cases (e.g., medical decision support, cyber threat hunting, and commonsense reasoning), we demonstrate that ROAR is highly effective to mislead KGR to suggest pre-defined answers for target queries, yet with negligible impact on non-target ones. Fi
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.04091</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23454;&#29616;&#35270;&#35273;&#25277;&#35937;&#21644;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Visual Abstraction and Reasoning through Language. (arXiv:2303.04091v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20219;&#21153;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#38382;&#39064;&#65292;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#23616;&#38480;&#24212;&#29992;&#20013;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#23637;&#29616;&#26356;&#24191;&#27867;&#21644;&#26356;&#28789;&#27963;&#30340;&#26234;&#33021;&#12290;Abstraction and Reasoning Corpus&#65288;ARC&#65289;&#26088;&#22312;&#35780;&#20272;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#65292;&#29992;&#20110;&#26292;&#21147;&#35299;&#20915;ARC&#20013;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;ARC&#38382;&#39064;&#12290;&#34429;&#28982;&#36824;&#27809;&#26377;&#22312;ARC&#19978;&#20987;&#36133;&#26368;&#20808;&#36827;&#30340;DSL&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#35299;&#20915;&#20808;&#21069;&#26410;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Artificial Intelligence (AI) models have achieved human or even superhuman performance in narrowly defined applications, they still struggle to show signs of broader and more flexible intelligence. The Abstraction and Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how close AI systems are to human-like cognitive abilities. Most current approaches rely on carefully handcrafted domain-specific languages (DSLs), which are used to brute-force solutions to the tasks present in ARC. In this work, we propose a general framework for solving ARC based on natural language descriptions of the tasks. While not yet beating state-of-the-art DSL models on ARC, we demonstrate the immense potential of our approach hinted at by the ability to solve previously unsolved tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;ASR&#31995;&#32479;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36755;&#20837;&#29305;&#24449;&#34701;&#21512;&#65292;TDNN&#31995;&#32479;&#30340;&#24103;&#32423;&#32852;&#21512;&#35299;&#30721;&#65292;&#20197;&#21450;&#22810;&#36890;&#36947;&#35299;&#30721;&#31561;&#12290;&#21516;&#26102;&#65292;&#22312;&#26500;&#24314;&#22810;&#27169;&#24335;&#30340;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26102;&#65292;&#37319;&#29992;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;wav2vec2.0&#34920;&#31034;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.14564</link><description>&lt;p&gt;
&#25506;&#32034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#29992;&#20110;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-supervised Pre-trained ASR Models For Dysarthric and Elderly Speech Recognition. (arXiv:2302.14564v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;ASR&#31995;&#32479;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36755;&#20837;&#29305;&#24449;&#34701;&#21512;&#65292;TDNN&#31995;&#32479;&#30340;&#24103;&#32423;&#32852;&#21512;&#35299;&#30721;&#65292;&#20197;&#21450;&#22810;&#36890;&#36947;&#35299;&#30721;&#31561;&#12290;&#21516;&#26102;&#65292;&#22312;&#26500;&#24314;&#22810;&#27169;&#24335;&#30340;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26102;&#65292;&#37319;&#29992;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;wav2vec2.0&#34920;&#31034;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#35821;&#38899;&#38556;&#30861;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24456;&#38590;&#25910;&#38598;&#22823;&#37327;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;SSL&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;TDNN&#21644;Conformer ASR&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#21475;&#21507;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic recognition of disordered and elderly speech remains a highly challenging task to date due to the difficulty in collecting such data in large quantities. This paper explores a series of approaches to integrate domain adapted SSL pre-trained models into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition: a) input feature fusion between standard acoustic frontends and domain adapted wav2vec2.0 speech representations; b) frame-level joint decoding of TDNN systems separately trained using standard acoustic features alone and with additional wav2vec2.0 features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain adapted wav2vec2.0 models. In addition, domain adapted wav2vec2.0 representations are utilized in acoustic-to-articulatory (A2A) inversion to construct multi-modal dysarthric and elderly speech recognition systems. Experiments conducted on the UASpeech dysarthric and DementiaBank Pitt elderly speech 
&lt;/p&gt;</description></item><item><title>IGB&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#21253;&#21547;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#25552;&#20379;&#24037;&#20855;&#29992;&#20110;&#29983;&#25104;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#65292;&#20026;GNN&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#24046;&#36317;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2302.13522</link><description>&lt;p&gt;
IGB: &#38024;&#23545;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#30340;&#24046;&#36317;&#20026;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
IGB: Addressing The Gaps In Labeling, Features, Heterogeneity, and Size of Public Graph Datasets for Deep Learning Research. (arXiv:2302.13522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13522
&lt;/p&gt;
&lt;p&gt;
IGB&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#21253;&#21547;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#25552;&#20379;&#24037;&#20855;&#29992;&#20110;&#29983;&#25104;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#65292;&#20026;GNN&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#24046;&#36317;&#30340;&#26377;&#20215;&#20540;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#31070;&#32463;&#32593;&#32476; (GNNs) &#24050;&#32463;&#23637;&#31034;&#20102;&#23545;&#20110;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294; GNN &#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#22823;&#35268;&#27169;&#28789;&#27963;&#30340;&#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#20844;&#20849;GNN&#25968;&#25454;&#38598;&#37117;&#30456;&#23545;&#36739;&#23567;&#65292;&#36825;&#38480;&#21046;&#20102; GNN &#30340;&#25512;&#24191;&#21040;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24456;&#23569;&#26377;&#22823;&#22411;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#25552;&#20379;&#20016;&#23500;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450; GNN &#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#30340;&#20302;&#20934;&#30830;&#24615;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#36824;&#26159;&#27169;&#22411;&#26080;&#27861;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451; GNN &#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#20197;&#20415;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;&#22240;&#32032;&#23545; GNN &#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20234;&#21033;&#35834;&#20234;&#22270;&#24418;&#22522;&#20934; (IGB)&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#25968;&#25454;&#38598;&#24037;&#20855;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#39640;&#31934;&#24230;&#22320;&#35757;&#32451;&#12289;&#23457;&#26597;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;GNN&#27169;&#22411;&#12290;IGB &#21253;&#25324;&#21516;&#36136;&#21644;&#24322;&#36136;&#24615;&#23398;&#26415;&#22270;&#24418;&#65292;&#35268;&#27169;&#24040;&#22823;&#65292;&#24182;&#19988;&#21487;&#20197;&#26631;&#35760;&#21644;&#25805;&#20316;&#65292;&#20197;&#27169;&#25311;&#30495;&#23454;&#22330;&#26223;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21253;&#25324;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#21512;&#25104;&#22270;&#24418;&#30340;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#21508;&#31181;&#22270;&#24418;&#29305;&#24615;&#23545; GNN &#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20234;&#21033;&#35834;&#20234;&#22270;&#24418;&#22522;&#20934;&#23558;&#20026; GNN &#30740;&#31350;&#22242;&#20307;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#20197;&#35299;&#20915;&#20844;&#20849;&#22270;&#24418;&#25968;&#25454;&#38598;&#22312;&#26631;&#35760;&#12289;&#29305;&#24449;&#12289;&#24322;&#36136;&#24615;&#21644;&#22823;&#23567;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#20197;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown high potential for a variety of real-world, challenging applications, but one of the major obstacles in GNN research is the lack of large-scale flexible datasets. Most existing public datasets for GNNs are relatively small, which limits the ability of GNNs to generalize to unseen data. The few existing large-scale graph datasets provide very limited labeled data. This makes it difficult to determine if the GNN model's low accuracy for unseen data is inherently due to insufficient training data or if the model failed to generalize. Additionally, datasets used to train GNNs need to offer flexibility to enable a thorough study of the impact of various factors while training GNN models.  In this work, we introduce the Illinois Graph Benchmark (IGB), a research dataset tool that the developers can use to train, scrutinize and systematically evaluate GNN models with high fidelity. IGB includes both homogeneous and heterogeneous academic graphs of enorm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#26032;&#31639;&#27861;&#20197;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;Markov&#21338;&#24328;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#21462;&#24471;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03673</link><description>&lt;p&gt;
&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#25171;&#30772;&#22810;&#26234;&#20307;&#30340;&#35781;&#21650;&#65306;&#24102;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation. (arXiv:2302.03673v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#26032;&#31639;&#27861;&#20197;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;Markov&#21338;&#24328;&#20989;&#25968;&#36924;&#36817;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#24182;&#21462;&#24471;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#8212;&#8212;&#29420;&#31435;&#32447;&#24615;Markov&#21338;&#24328;&#65292;&#29992;&#20110;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22823;&#37327;&#20195;&#29702;&#30340;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#26159;&#19968;&#31867;&#24102;&#26377;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#29992;&#20110;&#34987;&#20854;&#20182;&#29609;&#23478;&#30340;&#31574;&#30053;&#36793;&#32536;&#21270;&#30340;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#23398;&#20064;Markov&#31895;&#30053;&#30456;&#20851;&#22343;&#34913;&#21644;Markov&#30456;&#20851;&#22343;&#34913;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#20165;&#19982;&#27599;&#20010;&#20195;&#29702;&#33258;&#24049;&#30340;&#20989;&#25968;&#31867;&#22797;&#26434;&#24230;&#25104;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#20174;&#32780;&#25171;&#30772;&#20102;&#22810;&#26234;&#20307;&#30340;&#35781;&#21650;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;Markov&#21338;&#24328;&#30340;&#30740;&#31350;&#65292;&#22312;&#29305;&#21270;&#20110;&#26631;&#20934;&#34920;&#26684;&#29366;&#20917;&#30340;Markov&#21338;&#24328;&#35774;&#32622;&#26102;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#20250;&#38543;&#30528;\emph{&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;}&#30340;&#22823;&#23567;&#25104;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#35813;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#22312;&#20195;&#29702;&#30340;&#25968;&#37327;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#65306;(1) &#21033;&#29992;&#31574;&#30053;&#37325;&#25918;&#26469;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65307;(2) &#21033;&#29992;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26469;&#33719;&#24471;&#35745;&#31639;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#32479;&#35745;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new model, independent linear Markov game, for multi-agent reinforcement learning with a large state space and a large number of agents. This is a class of Markov games with independent linear function approximation, where each agent has its own function approximation for the state-action value functions that are marginalized by other players' policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with each agent's own function class complexity, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations: (1) utilizing policy replay to tac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36136;&#30097;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#21367;&#31215;&#26680;&#20013;&#21019;&#24314;&#20986;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11360</link><description>&lt;p&gt;
&#32447;&#24615;&#32452;&#21512;&#30340;&#23041;&#21147;&#65306;&#38543;&#26426;&#21367;&#31215;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Power of Linear Combinations: Learning with Random Convolutions. (arXiv:2301.11360v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36136;&#30097;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#26041;&#27861;&#65292;&#20174;&#38543;&#26426;&#21367;&#31215;&#26680;&#20013;&#21019;&#24314;&#20986;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#38544;&#21547;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#21367;&#31215;&#26680;&#22823;&#23567;&#26469;&#20445;&#25345;&#19982;&#26356;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;&#22914;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#27169;&#22411;&#65289;&#30340;&#31454;&#20105;&#21147;&#65292;&#23548;&#33268;&#26377;&#22823;&#37327;&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#25991;&#36136;&#30097;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#21367;&#31215;&#26680;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24456;&#22810;&#24403;&#20195;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29978;&#33267;&#22312;&#19981;&#26356;&#26032;&#21021;&#22987;&#21270;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#24773;&#20917;&#19979;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#23454;&#38469;&#19978;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#32452;&#21512;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#38543;&#26426;&#21367;&#31215;&#26680;&#32452;&#21512;&#25104;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#32593;&#32476;&#36816;&#31639;&#31526;&#65292;&#20854;&#36890;&#36807;&#39640;&#25928;&#30340; $1 \times 1$ &#21367;&#31215;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#32452;&#21512;&#21487;&#20197;&#38544;&#24335;&#22320;&#27491;&#21017;&#21270;&#32467;&#26524;&#36816;&#31639;&#31526;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the traditional paradigm of convolutional neural networks (CNNs), modern CNNs manage to keep pace with more recent, for example transformer-based, models by not only increasing model depth and width but also the kernel size. This results in large amounts of learnable model parameters that need to be handled during training. While following the convolutional paradigm with the according spatial inductive bias, we question the significance of \emph{learned} convolution filters. In fact, our findings demonstrate that many contemporary CNN architectures can achieve high test accuracies without ever updating randomly initialized (spatial) convolution filters. Instead, simple linear combinations (implemented through efficient $1\times 1$ convolutions) suffice to effectively recombine even random filters into expressive network operators. Furthermore, these combinations of random filters can implicitly regularize the resulting operations, mitigating overfitting and enhancing overall 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.10535</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning for Mathematical Reasoning. (arXiv:2212.10535v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#23398;&#25512;&#29702;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#24212;&#29992;&#24191;&#27867;&#65292;&#21253;&#25324;&#31185;&#23398;&#12289;&#24037;&#31243;&#12289;&#37329;&#34701;&#21644;&#26085;&#24120;&#29983;&#27963;&#12290;&#21457;&#23637;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#21644;&#35777;&#26126;&#23450;&#29702;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#20132;&#21449;&#39046;&#22495;&#30340;&#20851;&#38190;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.01793</link><description>&lt;p&gt;
\{kappa}HGCN: &#36890;&#36807;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#23398;&#20064;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
\{kappa}HGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20869;&#23454;&#29616;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#65292;&#36890;&#36807;&#32467;&#21512;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#29366;&#32467;&#26500;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#25324;&#23618;&#27425;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#26641;&#29366;&#32467;&#26500;&#24314;&#27169;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#30456;&#27604;&#20110;&#24179;&#22374;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#26354;&#38754;&#21452;&#26354;&#31354;&#38388;&#25552;&#20379;&#20102;&#26356;&#26131;&#22788;&#29702;&#21644;&#23884;&#20837;&#30340;&#31354;&#38388;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23637;&#29616;&#38544;&#21547;&#26641;&#29366;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#26641;&#29366;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#32463;&#24120;&#23637;&#31034;&#20986;&#26641;&#29366;&#12289;&#24179;&#22374;&#21644;&#22278;&#24418;&#21306;&#22495;&#30340;&#24322;&#36136;&#32452;&#25104;&#12290;&#23558;&#36825;&#26679;&#24322;&#36136;&#30340;&#32467;&#26500;&#30452;&#25509;&#23884;&#20837;&#19968;&#20010;&#21516;&#36136;&#21270;&#30340;&#23884;&#20837;&#31354;&#38388;&#65288;&#21363;&#21452;&#26354;&#31354;&#38388;&#65289;&#24517;&#28982;&#23548;&#33268;&#37325;&#22823;&#22833;&#30495;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#21452;&#26354;&#31354;&#38388;&#30340;&#26354;&#29575;&#65292;&#20197;&#23454;&#29616;&#28789;&#27963;&#20934;&#30830;&#22320;&#24314;&#27169;&#26641;&#29366;&#32467;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\{kappa}HGCN&#27169;&#22411;&#65292;&#23558;&#36830;&#32493;&#21644;&#31163;&#25955;&#26354;&#29575;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#22522;&#30784;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of tree-like structures, encompassing hierarchical structures and power law distributions, exists extensively in real-world applications, including recommendation systems, ecosystems, financial networks, social networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness modeling has garnered considerable attention owing to its exponential growth volume. Compared to the flat Euclidean space, the curved hyperbolic space provides a more amenable and embeddable room, especially for datasets exhibiting implicit tree-like architectures. However, the intricate nature of real-world tree-like data presents a considerable challenge, as it frequently displays a heterogeneous composition of tree-like, flat, and circular regions. The direct embedding of such heterogeneous structures into a homogeneous embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions. To mitigate the aforementioned shortage, this study endeavors to explore the curvatur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.15359</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Proactive Dialog Agents Using Socially-Aware Reinforcement Learning. (arXiv:2211.15359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#65292;&#26469;&#20248;&#21270;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#20351;&#20854;&#22312;&#20219;&#21153;&#25928;&#29575;&#39640;&#30340;&#21516;&#26102;&#65292;&#20063;&#33021;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#65292;&#20174;&#32780;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#30340;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#19979;&#19968;&#20010;&#27493;&#39588;&#26159;&#20174;&#26049;&#35266;&#32773;&#30340;&#35282;&#33394;&#20013;&#35299;&#33073;&#20986;&#26469;&#65292;&#21464;&#24471;&#26356;&#21152;&#20027;&#21160;&#12290;&#26126;&#30830;&#23450;&#20041;&#30340;&#20027;&#21160;&#34892;&#20026;&#21487;&#20197;&#25913;&#21892;&#20154;&#26426;&#21512;&#20316;&#65292;&#22240;&#20026;&#20195;&#29702;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#25198;&#28436;&#26356;&#31215;&#26497;&#30340;&#35282;&#33394;&#24182;&#35299;&#38500;&#20102;&#29992;&#25143;&#30340;&#36131;&#20219;&#12290;&#28982;&#32780;&#65292;&#20027;&#21160;&#24615;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22240;&#20026;&#25191;&#34892;&#19981;&#24403;&#30340;&#39044;&#38450;&#24615;&#34892;&#21160;&#21487;&#33021;&#19981;&#20165;&#23545;&#20219;&#21153;&#32467;&#26524;&#20135;&#29983;&#30772;&#22351;&#24615;&#24433;&#21709;&#65292;&#32780;&#19988;&#36824;&#20250;&#23545;&#19982;&#29992;&#25143;&#30340;&#20851;&#31995;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#35774;&#35745;&#21512;&#36866;&#30340;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31038;&#20132;&#21644;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#37117;&#32771;&#34385;&#22312;&#23545;&#35805;&#20013;&#12290;&#36825;&#37324;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20248;&#21270;&#20027;&#21160;&#34892;&#20026;&#65292;&#20351;&#20854;&#20219;&#21153;&#23548;&#21521;&#8212;&#8212;&#36825;&#24847;&#21619;&#30528;&#39640;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#8212;&#8212;&#21516;&#26102;&#22312;&#20419;&#36827;&#29992;&#25143;&#20449;&#20219;&#26102;&#20063;&#20855;&#26377;&#31038;&#20132;&#25928;&#30410;&#12290;&#23558;&#36825;&#20004;&#20010;&#26041;&#38754;&#21253;&#21547;&#22312;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20027;&#21160;&#23545;&#35805;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#26174;&#31034;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#26356;&#21152;&#25104;&#21151;&#30340;&#20154;&#26426;&#20132;&#20114;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive. Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user. However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect not only on the task outcome but also on the relationship with the user. For designing adequate proactive dialog strategies, we propose a novel approach including both social as well as task-relevant features in the dialog. Here, the primary goal is to optimize proactive behavior so that it is task-oriented - this implies high task success and efficiency - while also being socially effective by fostering user trust. Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-mach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35774;&#32622;&#65292;&#21033;&#29992;&#26377;&#26631;&#35760;&#30340;&#36712;&#36857;&#25968;&#25454;&#21644;&#26080;&#21160;&#20316;&#30340;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#33719;&#21462;&#20195;&#29702;&#26631;&#31614;&#65292;&#26368;&#32456;&#20351;&#29992;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.06518</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#21160;&#20316;&#36712;&#36857;&#30340;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35774;&#32622;&#65292;&#21033;&#29992;&#26377;&#26631;&#35760;&#30340;&#36712;&#36857;&#25968;&#25454;&#21644;&#26080;&#21160;&#20316;&#30340;&#36712;&#36857;&#25968;&#25454;&#35757;&#32451;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#20197;&#33719;&#21462;&#20195;&#29702;&#26631;&#31614;&#65292;&#26368;&#32456;&#20351;&#29992;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#23454;&#29616;&#39640;&#25104;&#21151;&#29575;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#26234;&#33021;&#20307;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#19981;&#21516;&#22823;&#23567;&#12289;&#36136;&#37327;&#21644;&#27979;&#37327;&#31867;&#22411;&#30340;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#12289;&#23454;&#38469;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#26469;&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#20004;&#20010;&#36712;&#36857;&#38598;&#65306;&#19968;&#20010;&#21253;&#21547;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#29366;&#24577;&#12289;&#34892;&#20026;&#21644;&#22870;&#21169;&#19977;&#20803;&#32452;&#30340;&#26631;&#35760;&#36712;&#36857;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#20165;&#21253;&#21547;&#29366;&#24577;&#21644;&#22870;&#21169;&#20449;&#24687;&#30340;&#26410;&#26631;&#35760;&#36712;&#36857;&#38598;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20803;&#31639;&#27861;&#27969;&#27700;&#32447;&#65292;&#35813;&#31639;&#27861;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#23398;&#20064;&#21453;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#20195;&#29702;&#26631;&#31614;&#65292;&#28982;&#21518;&#23558;&#20219;&#20309;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#30495;&#23454;&#21644;&#20195;&#29702;&#26631;&#35760;&#36712;&#36857;&#12290;&#32463;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#27969;&#27700;&#32447;&#38750;&#24120;&#25104;&#21151;&#8212;&#8212;&#22312;&#20960;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#26576;&#20123;&#31163;&#32447;RL&#31639;&#27861;&#21487;&#20197;&#19982;&#22312;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#21464;&#20307;&#30340;&#34920;&#29616;&#30456;&#21305;&#37197;&#65292;&#21363;&#20351;&#21518;&#32773;&#25317;&#26377;&#26356;&#22810;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#24110;&#21161;&#22235;&#36275;&#26426;&#22120;&#20154;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.04819</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#22810;&#26679;&#30340;&#29615;&#22659;&#36712;&#36857;&#29983;&#25104;&#22120;&#20808;&#39564;&#30693;&#35782;&#65292;&#39640;&#25928;&#23398;&#20064;&#36816;&#21160;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors. (arXiv:2210.04819v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#33021;&#22815;&#24110;&#21161;&#22235;&#36275;&#26426;&#22120;&#20154;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#21508;&#31181;&#19981;&#35268;&#21017;&#22320;&#24418;&#30340;&#40065;&#26834;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20197;&#36712;&#36857;&#29983;&#25104;&#22120;&#65288;TG&#65289;&#24418;&#24335;&#21152;&#20837;&#33391;&#22909;&#30340;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#36816;&#21160;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;/&#29615;&#22659;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#22312;&#21333;&#20010;&#33391;&#22909;&#30340;TG&#20013;&#23450;&#20041;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#25972;&#65292;&#21516;&#26102;&#36824;&#26377;&#21487;&#33021;&#38477;&#20302;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;EETG&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#8220;&#36136;&#37327;-&#22810;&#26679;&#24615;&#65288;Quality-Diversity&#65289;&#8221;&#31639;&#27861;&#23398;&#20064;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#19987;&#19994;&#21270;&#36816;&#21160;&#20808;&#39564;&#30693;&#35782;&#65292;&#21516;&#26102;&#22312;Policies Modulating TG&#65288;PMTG&#65289;&#26694;&#26550;&#20869;&#32500;&#25345;&#21333;&#19968;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EETG&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25104;&#21151;&#22320;&#31359;&#36234;&#21508;&#31181;&#29615;&#22659;&#65292;&#22914;&#26012;&#22369;&#12289;&#21488;&#38454;&#12289;&#23822;&#23702;&#22320;&#24418;&#21644;&#24179;&#34913;&#27178;&#26438;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EETG&#27604;&#20351;&#29992;&#21333;&#20010;TG&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven learning based methods have recently been particularly successful at learning robust locomotion controllers for a variety of unstructured terrains. Prior work has shown that incorporating good locomotion priors in the form of trajectory generators (TGs) is effective at efficiently learning complex locomotion skills. However, defining a good, single TG as tasks/environments become increasingly more complex remains a challenging problem as it requires extensive tuning and risks reducing the effectiveness of the prior. In this paper, we present Evolved Environmental Trajectory Generators (EETG), a method that learns a diverse set of specialised locomotion priors using Quality-Diversity algorithms while maintaining a single policy within the Policies Modulating TG (PMTG) architecture. The results demonstrate that EETG enables a quadruped robot to successfully traverse a wide range of environments, such as slopes, stairs, rough terrain, and balance beams. Our experiments show th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;12&#31181;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37329;&#34701;&#26426;&#26500;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.07912</link><description>&lt;p&gt;
&#20844;&#24179;&#20449;&#29992;&#35780;&#20998;&#30340;&#31639;&#27861;&#20915;&#31574;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decision making methods for fair credit scoring. (arXiv:2209.07912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;12&#31181;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#20010;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#37329;&#34701;&#26426;&#26500;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35780;&#20272;&#36151;&#27454;&#30003;&#35831;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#20102;&#24456;&#38271;&#19968;&#27573;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#21160;&#20915;&#31574;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#23545;&#26576;&#20123;&#32676;&#20307;&#25110;&#20010;&#20154;&#36827;&#34892;&#19981;&#24179;&#31561;&#23545;&#24453;&#65292;&#36827;&#32780;&#23548;&#33268;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;12&#31181;&#20027;&#35201;&#30340;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#22312;5&#31181;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#19979;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#35780;&#20272;&#23427;&#20204;&#23545;&#20110;&#37329;&#34701;&#26426;&#26500;&#30340;&#20934;&#30830;&#24615;&#21644;&#28508;&#22312;&#30408;&#21033;&#33021;&#21147;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24050;&#32463;&#30830;&#23450;&#20102;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#30340;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26368;&#25104;&#21151;&#21644;&#26368;&#19981;&#25104;&#21151;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#23454;&#39564;&#26426;&#22120;&#23398;&#20064;&#19982;&#20854;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of machine learning in evaluating the creditworthiness of loan applicants has been demonstrated for a long time. However, there is concern that the use of automated decision-making processes may result in unequal treatment of groups or individuals, potentially leading to discriminatory outcomes. This paper seeks to address this issue by evaluating the effectiveness of 12 leading bias mitigation methods across 5 different fairness metrics, as well as assessing their accuracy and potential profitability for financial institutions. Through our analysis, we have identified the challenges associated with achieving fairness while maintaining accuracy and profitabiliy, and have highlighted both the most successful and least successful mitigation methods. Ultimately, our research serves to bridge the gap between experimental machine learning and its practical applications in the finance industry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#30340;A2A&#21453;&#28436;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34892;&#38899;&#39057;&#21644;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#25968;&#25454;&#26469;&#20135;&#29983;&#22522;&#20110;UTI&#30340;&#21457;&#38899;&#29305;&#24449;&#65292;&#29992;&#20110;&#32769;&#24180;&#20154;&#21644;&#24739;&#26377;&#21457;&#38899;&#38556;&#30861;&#32773;&#30340;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#22522;&#32447;ASR&#31995;&#32479;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.07327</link><description>&lt;p&gt;
&#24320;&#21457;&#36328;&#39046;&#22495;&#36328;&#35821;&#35328;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#29305;&#24449;&#29992;&#20110;&#32769;&#24180;&#20154;&#21644;&#24739;&#26377;&#21457;&#38899;&#38556;&#30861;&#32773;&#30340;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Exploiting Cross-domain And Cross-Lingual Ultrasound Tongue Imaging Features For Elderly And Dysarthric Speech Recognition. (arXiv:2206.07327v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#30340;A2A&#21453;&#28436;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34892;&#38899;&#39057;&#21644;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#25968;&#25454;&#26469;&#20135;&#29983;&#22522;&#20110;UTI&#30340;&#21457;&#38899;&#29305;&#24449;&#65292;&#29992;&#20110;&#32769;&#24180;&#20154;&#21644;&#24739;&#26377;&#21457;&#38899;&#38556;&#30861;&#32773;&#30340;&#35821;&#38899;&#35782;&#21035;&#30740;&#31350;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#22522;&#32447;ASR&#31995;&#32479;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#38899;&#29305;&#24449;&#26412;&#36136;&#19978;&#19981;&#21464;&#24418;&#20110;&#22768;&#23398;&#20449;&#21495;&#22833;&#30495;&#65292;&#24182;&#24050;&#25104;&#21151;&#22320;&#34987;&#25972;&#21512;&#36827;&#20102;&#35774;&#35745;&#29992;&#20110;&#27491;&#24120;&#35821;&#38899;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#32769;&#24180;&#20154;&#21644;&#24739;&#26377;&#35821;&#38899;&#38556;&#30861;&#30340;&#36328;&#35821;&#35328;&#38750;&#27491;&#24120;&#20219;&#21153;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#21457;&#38899;&#29305;&#24449;&#30340;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#21463;&#21040;&#20174;&#30446;&#26631;&#35828;&#35805;&#20154;&#25910;&#38598;&#27492;&#31867;&#29305;&#27530;&#25968;&#25454;&#30340;&#22256;&#38590;&#25152;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#30340;A2A&#21453;&#28436;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#39044;&#35757;&#32451;&#26102;&#21033;&#29992;24&#23567;&#26102;TaL&#35821;&#26009;&#24211;&#30340;&#24179;&#34892;&#38899;&#39057;&#21644;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#25968;&#25454;&#65292;&#24182;&#22312;&#36328;&#39046;&#22495;&#21644;&#36328;&#35821;&#35328;&#36866;&#24212;&#21040;&#20004;&#31181;&#35821;&#35328;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#65306;&#33521;&#35821;DementiaBank Pitt&#21644;&#24191;&#19996;&#35805;JCCOCC MoCA&#30340;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65307;&#20197;&#21450;&#33521;&#35821;TORGO&#30340;&#21457;&#38899;&#38556;&#30861;&#35328;&#35821;&#25968;&#25454;&#65292;&#20174;&#32780;&#20135;&#29983;&#22522;&#20110;UTI&#30340;&#21457;&#38899;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#25972;&#21512;&#29983;&#25104;&#30340;&#21457;&#38899;&#29305;&#24449;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;TDNN&#21644;PLP&#29305;&#24449;ASR&#31995;&#32479;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#35823;&#24046;&#29575;&#65288;RER&#65289;&#30340;&#38477;&#20302;&#33539;&#22260;&#20026;7.2&#65285;&#21040;36.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Articulatory features are inherently invariant to acoustic signal distortion and have been successfully incorporated into automatic speech recognition (ASR) systems designed for normal speech. Their practical application to atypical task domains such as elderly and disordered speech across languages is often limited by the difficulty in collecting such specialist data from target speakers. This paper presents a cross-domain and cross-lingual A2A inversion approach that utilizes the parallel audio and ultrasound tongue imaging (UTI) data of the 24-hour TaL corpus in A2A model pre-training before being cross-domain and cross-lingual adapted to three datasets across two languages: the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech corpora; and the English TORGO dysarthric speech data, to produce UTI based articulatory features. Experiments conducted on three tasks suggested incorporating the generated articulatory features consistently outperformed the baseline TDNN an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#26041;&#27861;&#30340;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#12290;</title><link>http://arxiv.org/abs/2111.07473</link><description>&lt;p&gt;
&#21033;&#29992;&#25233;&#21046;&#21464;&#37327;&#30340;&#32447;&#24615;&#22522;&#30784;&#25968;&#25454;&#23457;&#26597;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)
&lt;/p&gt;
&lt;p&gt;
Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07473
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#26041;&#27861;&#30340;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#36234;&#26469;&#36234;&#24120;&#29992;&#20110;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;ML&#27169;&#22411;(&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;)&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#40657;&#21283;&#23376;&#65292;&#22240;&#27492;&#24050;&#32463;&#24320;&#21457;&#20986;&#22823;&#37327;&#31243;&#24207;&#26469;&#38416;&#26126;&#20854;&#20869;&#37096;&#36816;&#20316;&#21644;&#39044;&#27979;&#26041;&#24335;&#65292;&#23450;&#20041;&#20102;"&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;"(XAI)&#39046;&#22495;&#12290;&#26174;&#33879;&#24615;&#26041;&#27861;&#26681;&#25454;&#26576;&#31181;"&#37325;&#35201;&#24615;"&#24230;&#37327;&#23545;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#25490;&#24207;&#12290;&#30001;&#20110;&#36804;&#20170;&#20026;&#27490;&#32570;&#20047;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#38590;&#39564;&#35777;&#12290;&#24050;&#32463;&#35777;&#23454;&#65292;&#19968;&#20123;&#26174;&#33879;&#24615;&#26041;&#27861;&#21487;&#20197;&#31361;&#20986;&#26174;&#31034;&#19982;&#39044;&#27979;&#30446;&#26631;&#27809;&#26377;&#32479;&#35745;&#32852;&#31995;&#30340;&#29305;&#24449;(&#25233;&#21046;&#21464;&#37327;)&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;&#36825;&#31181;&#34892;&#20026;&#24341;&#36215;&#30340;&#38169;&#35823;&#35299;&#35835;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30830;&#20445;&#27492;&#31867;&#32852;&#31995;&#23384;&#22312;&#26159;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#23458;&#35266;&#21021;&#27493;&#23450;&#20041;&#12290;&#25105;&#20204;&#31934;&#24515;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#30784;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25152;&#26377;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#37117;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#21644;&#32447;&#24615;&#30340;&#65292;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#32508;&#36848;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#30693;&#35782;&#30340;&#33719;&#21462;&#12289;&#34920;&#31034;&#21644;&#24212;&#29992;&#65292;&#19982;&#20854;&#20182;&#23398;&#20064;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/1807.02192</link><description>&lt;p&gt;
&#26381;&#21153;&#26426;&#22120;&#20154;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Representation in Service Robotics. (arXiv:1807.02192v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1807.02192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#32508;&#36848;&#20102;&#26381;&#21153;&#26426;&#22120;&#20154;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#30693;&#35782;&#30340;&#33719;&#21462;&#12289;&#34920;&#31034;&#21644;&#24212;&#29992;&#65292;&#19982;&#20854;&#20182;&#23398;&#20064;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26381;&#21153;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#23398;&#20064;&#12289;&#29702;&#35299;&#21644;&#34920;&#31034;&#26426;&#22120;&#20154;&#25191;&#34892;&#20219;&#21153;&#30340;&#36816;&#21160;&#26041;&#24335;&#12290;&#26426;&#22120;&#20154;&#23398;&#20064;&#21644;&#38382;&#39064;&#35299;&#20915;&#30340;&#20219;&#21153;&#26159;&#38750;&#24120;&#24191;&#27867;&#30340;&#65292;&#22240;&#20026;&#23427;&#38598;&#25104;&#20102;&#22914;&#29289;&#20307;&#26816;&#27979;&#12289;&#27963;&#21160;&#35782;&#21035;&#12289;&#20219;&#21153;/&#36816;&#21160;&#35268;&#21010;&#12289;&#23450;&#20301;&#12289;&#30693;&#35782;&#34920;&#31034;&#21644;&#26816;&#32034;&#20197;&#21450;&#24863;&#30693;/&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#26412;&#25991;&#20165;&#20851;&#27880;&#30693;&#35782;&#34920;&#31034;&#65292;&#24182;&#38416;&#36848;&#20102;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#22914;&#20309;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20877;&#29616;&#30693;&#35782;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#26681;&#25454;&#30693;&#35782;&#34920;&#31034;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#34920;&#31034;&#19982;&#36817;&#24180;&#26469;&#24191;&#27867;&#20171;&#32461;&#21644;&#30740;&#31350;&#30340;&#26377;&#29992;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#20027;&#35201;&#21306;&#21035;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#12289;&#27010;&#29575;&#24314;&#27169;&#21644;&#35821;&#20041;&#22270;&#24418;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the realm of service robotics, researchers have placed a great amount of effort into learning, understanding, and representing motions as manipulations for task execution by robots. The task of robot learning and problem-solving is very broad, as it integrates a variety of tasks such as object detection, activity recognition, task/motion planning, localization, knowledge representation and retrieval, and the intertwining of perception/vision and machine learning techniques. In this paper, we solely focus on knowledge representations and notably how knowledge is typically gathered, represented, and reproduced to solve problems as done by researchers in the past decades. In accordance with the definition of knowledge representations, we discuss the key distinction between such representations and useful learning models that have extensively been introduced and studied in recent years, such as machine learning, deep learning, probabilistic modelling, and semantic graphical structur
&lt;/p&gt;</description></item></channel></rss>