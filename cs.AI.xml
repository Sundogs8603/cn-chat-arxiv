<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#36127;&#38754;&#30340;&#28508;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#23637;&#29616;&#20102;&#26041;&#35328;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00742</link><description>&lt;p&gt;
&#26041;&#35328;&#20559;&#35265;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#23545;&#20154;&#29289;&#24615;&#26684;&#12289;&#23601;&#19994;&#33021;&#21147;&#21644;&#29359;&#32618;&#20542;&#21521;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Dialect prejudice predicts AI decisions about people's character, employability, and criminality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00742
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#36127;&#38754;&#30340;&#28508;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#23637;&#29616;&#20102;&#26041;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20159;&#20154;&#29616;&#22312;&#19982;&#35821;&#35328;&#27169;&#22411;&#20114;&#21160;&#65292;&#20854;&#29992;&#36884;&#20174;&#20316;&#20026;&#20889;&#20316;&#36741;&#21161;&#21040;&#24433;&#21709;&#25307;&#32856;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#20250;&#20256;&#25773;&#31995;&#32479;&#24615;&#31181;&#26063;&#20559;&#35265;&#65292;&#20351;&#23427;&#20204;&#23545;&#20687;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#36825;&#26679;&#30340;&#32676;&#20307;&#20570;&#20986;&#26377;&#38382;&#39064;&#30340;&#20559;&#35265;&#21028;&#26029;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20307;&#29616;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#31181;&#26063;&#25991;&#21270;&#20559;&#35265;&#65292;&#21363;&#26041;&#35328;&#20559;&#35265;&#65306;&#25105;&#20204;&#25193;&#23637;&#20102;&#20851;&#20110;&#32654;&#22269;&#20154;&#23545;&#38750;&#35028;&#32654;&#22269;&#33521;&#35821;&#35828;&#35805;&#32773;&#25345;&#26377;&#31181;&#26063;&#35821;&#35328;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#20063;&#26377;&#21516;&#26679;&#30340;&#20559;&#35265;&#65292;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#31181;&#21051;&#26495;&#21360;&#35937;&#27604;&#23454;&#39564;&#20013;&#35760;&#24405;&#30340;&#20219;&#20309;&#20154;&#31867;&#20851;&#20110;&#38750;&#35028;&#32654;&#22269;&#20154;&#30340;&#21051;&#26495;&#21360;&#35937;&#37117;&#26356;&#20026;&#36127;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00742v1 Announce Type: cross  Abstract: Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorde
&lt;/p&gt;</description></item><item><title>&#19987;&#23478;&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#20854;&#39046;&#22495;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#20197;&#24110;&#21161;&#22312;&#21516;&#19968;&#39046;&#22495;&#20869;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.00694</link><description>&lt;p&gt;
&#23450;&#20041;&#19987;&#19994;&#30693;&#35782;&#65306;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Defining Expertise: Applications to Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00694
&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#20915;&#31574;&#32773;&#30340;&#34892;&#21160;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#20854;&#39046;&#22495;&#30693;&#35782;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#20197;&#24110;&#21161;&#22312;&#21516;&#19968;&#39046;&#22495;&#20869;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20013;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#36890;&#24120;&#26159;&#20854;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#24182;&#22522;&#20110;&#20854;&#39046;&#22495;&#30693;&#35782;&#37319;&#21462;&#34892;&#21160;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#39046;&#22495;&#20013;&#19987;&#19994;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#19987;&#19994;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00694v1 Announce Type: cross  Abstract: Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and "expertise" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise - particularly the type of expertise the decision-makers of a domain are likely to have - can be informative in designin
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#20013;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;&#20013;&#30340;&#20840;&#29699;&#21355;&#26143;&#36890;&#20449;&#23433;&#25490;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.00692</link><description>&lt;p&gt;
&#22522;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00692
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#20013;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#33258;&#20027;&#21512;&#20316;&#20013;&#30340;&#20840;&#29699;&#21355;&#26143;&#36890;&#20449;&#23433;&#25490;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00692v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#26410;&#26469;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#30340;&#26684;&#23616;&#23558;&#30001;&#35201;&#27714;&#28385;&#36275;&#20005;&#26684;&#20219;&#21153;&#38656;&#27714;&#65292;&#22914;&#37325;&#35775;&#26102;&#38388;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#32593;&#32476;&#24322;&#26500;&#32435;&#31859;&#21355;&#26143;&#26143;&#24231;&#25152;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#21019;&#24314;&#20840;&#29699;&#21355;&#26143;&#25509;&#35302;&#35745;&#21010;&#65288;CP&#65289;&#26469;&#23433;&#25490;&#21355;&#26143;&#36890;&#20449;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#35201;&#27714;&#22320;&#38754;&#21327;&#35843;&#25110;&#21463;&#21040;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26143;&#24231;&#21644;CP&#24314;&#27169;&#20026;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35780;&#20272;&#32473;&#23450;CP&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#26356;&#26032;&#23427;&#12290;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20197;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;3.6&#20998;&#38047;&#26469;&#39044;&#27979;&#32593;&#32476;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00692v1 Announce Type: cross  Abstract: The upcoming landscape of Earth Observation missions will defined by networked heterogeneous nanosatellite constellations required to meet strict mission requirements, such as revisit times and spatial resolution. However, scheduling satellite communications in these satellite networks through efficiently creating a global satellite Contact Plan (CP) is a complex task, with current solutions requiring ground-based coordination or being limited by onboard computational resources. The paper proposes a novel approach to overcome these challenges by modeling the constellations and CP as dynamic networks and employing graph-based techniques. The proposed method utilizes a state-of-the-art dynamic graph neural network to evaluate the performance of a given CP and update it using a heuristic algorithm based on simulated annealing. The trained neural network can predict the network delay with a mean absolute error of 3.6 minutes. Simulation re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#20316;&#20026;&#39069;&#22806;&#27169;&#24577;&#65292;&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;LAVIMO&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#25991;&#26412;&#21644;&#36816;&#21160;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#27880;&#24847;&#26426;&#21046;&#20419;&#36827;&#20102;&#22686;&#24378;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.00691</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#19977;&#27169;&#24577;&#36816;&#21160;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Tri-Modal Motion Retrieval by Learning a Joint Embedding Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00691
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#20316;&#20026;&#39069;&#22806;&#27169;&#24577;&#65292;&#35813;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;LAVIMO&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#25991;&#26412;&#21644;&#36816;&#21160;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#21033;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#27880;&#24847;&#26426;&#21046;&#20419;&#36827;&#20102;&#22686;&#24378;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#39640;&#36136;&#37327;&#20154;&#20307;&#36816;&#21160;&#25968;&#25454;&#30340;&#24040;&#22823;&#38656;&#27714;&#23588;&#20854;&#22312;&#22312;&#32447;&#33719;&#21462;&#26041;&#38754;&#23548;&#33268;&#20102;&#20154;&#20307;&#36816;&#21160;&#30740;&#31350;&#24037;&#20316;&#30340;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21452;&#27169;&#24577;&#23398;&#20064;&#19978;&#65292;&#22914;&#25991;&#26412;&#21644;&#36816;&#21160;&#20219;&#21153;&#65292;&#20294;&#24456;&#23569;&#25506;&#32034;&#19977;&#27169;&#24577;&#23398;&#20064;&#12290;&#30452;&#35273;&#19978;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#24577;&#21487;&#20197;&#20016;&#23500;&#27169;&#22411;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23545;&#39069;&#22806;&#27169;&#24577;&#30340;&#20805;&#20998;&#36873;&#25321;&#36824;&#21487;&#20197;&#20316;&#20026;&#20013;&#20171;&#24182;&#22686;&#24378;&#20854;&#20182;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LAVIMO&#65288;LAnguage-VIdeo-MOtion&#23545;&#40784;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19977;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35270;&#39057;&#20316;&#20026;&#39069;&#22806;&#30340;&#27169;&#24577;&#38598;&#25104;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24357;&#21512;&#25991;&#26412;&#21644;&#36816;&#21160;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#20419;&#36827;&#22686;&#24378;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00691v1 Announce Type: cross  Abstract: Information retrieval is an ever-evolving and crucial research domain. The substantial demand for high-quality human motion data especially in online acquirement has led to a surge in human motion research works. Prior works have mainly concentrated on dual-modality learning, such as text and motion tasks, but three-modality learning has been rarely explored. Intuitively, an extra introduced modality can enrich a model's application scenario, and more importantly, an adequate choice of the extra modality can also act as an intermediary and enhance the alignment between the other two disparate modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion alignment), a novel framework for three-modality learning integrating human-centric videos as an additional modality, thereby effectively bridging the gap between text and motion. Moreover, our approach leverages a specially designed attention mechanism to foster enhanced alignme
&lt;/p&gt;</description></item><item><title>NetHack&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;NetPlay&#20316;&#20026;&#31532;&#19968;&#20010;&#24212;&#29992;&#20110;NetHack&#30340;LLM&#38646;-shot&#20195;&#29702;&#65292;&#20811;&#26381;&#20102;&#31616;&#21333;&#29615;&#22659;&#21644;&#22797;&#26434;&#20114;&#21160;&#30340;&#38480;&#21046;&#65292;&#20026;&#21160;&#24577;&#26426;&#22120;&#20154;&#29615;&#22659;&#30340;&#39640;&#32423;&#35268;&#21010;&#32773;&#24102;&#26469;&#28508;&#21147;&#21644;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.00690</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29609;NetHack&#65306;&#20316;&#20026;&#38646;-shot&#20195;&#29702;&#30340;&#28508;&#21147;&#19982;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Playing NetHack with LLMs: Potential &amp; Limitations as Zero-Shot Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00690
&lt;/p&gt;
&lt;p&gt;
NetHack&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28216;&#25103;&#29615;&#22659;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;NetPlay&#20316;&#20026;&#31532;&#19968;&#20010;&#24212;&#29992;&#20110;NetHack&#30340;LLM&#38646;-shot&#20195;&#29702;&#65292;&#20811;&#26381;&#20102;&#31616;&#21333;&#29615;&#22659;&#21644;&#22797;&#26434;&#20114;&#21160;&#30340;&#38480;&#21046;&#65292;&#20026;&#21160;&#24577;&#26426;&#22120;&#20154;&#29615;&#22659;&#30340;&#39640;&#32423;&#35268;&#21010;&#32773;&#24102;&#26469;&#28508;&#21147;&#21644;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#39640;&#32423;&#35268;&#21010;&#32773;&#22312;&#38646;-shot&#28216;&#25103;&#20195;&#29702;&#20013;&#34920;&#29616;&#20986;&#24456;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20195;&#29702;&#20027;&#35201;&#22312;Minecraft&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#37027;&#37324;&#30340;&#38271;&#26399;&#35268;&#21010;&#30456;&#23545;&#31616;&#21333;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21160;&#24577;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#27979;&#35797;&#30340;&#20195;&#29702;&#38754;&#20020;&#38480;&#21046;&#65292;&#22240;&#20026;&#29615;&#22659;&#31616;&#21333;&#65292;&#21482;&#26377;&#23569;&#37327;&#29289;&#20307;&#21644;&#20114;&#21160;&#12290;&#20026;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;NetPlay&#65292;&#36825;&#26159;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;roguelike&#28216;&#25103;NetHack&#30340;&#31532;&#19968;&#20010;LLM&#21160;&#24341;&#25806;&#38646;-shot&#20195;&#29702;&#12290;NetHack&#30001;&#20110;&#20854;&#22810;&#26679;&#30340;&#29289;&#21697;&#21644;&#24618;&#29289;&#12289;&#22797;&#26434;&#30340;&#20114;&#21160;&#20197;&#21450;&#27515;&#20129;&#30340;&#22810;&#31181;&#26041;&#24335;&#32780;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;NetPlay&#20351;&#29992;&#20026;&#21160;&#24577;&#26426;&#22120;&#20154;&#29615;&#22659;&#32780;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#32463;&#36807;&#20462;&#25913;&#20197;&#36866;&#24212;NetHack&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#31867;&#20284;&#65292;&#23427;&#25552;&#31034;LLM&#20174;&#39044;&#23450;&#20041;&#30340;&#25216;&#33021;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#24182;&#36319;&#36394;&#36807;&#21435;&#30340;&#20114;&#21160;&#20197;&#22686;&#24378;&#20915;&#31574;&#33021;&#21147;&#12290;&#37492;&#20110;NetHack&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;NetPlay&#26816;&#27979;&#21040;&#37325;&#35201;&#30340;&#28216;&#25103;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00690v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown great success as high-level planners for zero-shot game-playing agents. However, these agents are primarily evaluated on Minecraft, where long-term planning is relatively straightforward. In contrast, agents tested in dynamic robot environments face limitations due to simplistic environments with only a few objects and interactions. To fill this gap in the literature, we present NetPlay, the first LLM-powered zero-shot agent for the challenging roguelike NetHack. NetHack is a particularly challenging environment due to its diverse set of items and monsters, complex interactions, and many ways to die.   NetPlay uses an architecture designed for dynamic robot environments, modified for NetHack. Like previous approaches, it prompts the LLM to choose from predefined skills and tracks past interactions to enhance decision-making. Given NetHack's unpredictable nature, NetPlay detects important game event
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#65292;&#24212;&#29992;&#26694;&#26550;&#27604;&#36739;&#20102;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.00685</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#24322;&#24120;&#65306;&#36208;&#21521;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#24322;&#24120;&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
Know your exceptions: Towards an Ontology of Exceptions in Knowledge Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#65292;&#24212;&#29992;&#26694;&#26550;&#27604;&#36739;&#20102;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36777;&#39539;&#25512;&#29702;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65292;&#20854;&#20013;&#19968;&#20123;&#27010;&#25324;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#25928;&#65292;&#20063;&#23601;&#26159;&#36890;&#24120;&#24773;&#20917;&#19979;&#30340;&#19968;&#33324;&#24615;&#32467;&#35770;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#24418;&#24335;&#21270;&#20307;&#31995;&#26469;&#27169;&#25311;&#36825;&#31181;&#25512;&#29702;&#65292;&#36825;&#26159;&#26222;&#36890;&#24120;&#35782;&#32972;&#26223;&#19979;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20010;&#27169;&#22411;&#32773;&#26469;&#35828;&#65292;&#20174;&#26412;&#20307;&#35770;&#30340;&#35282;&#24230;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#39046;&#22495;&#30340;&#20307;&#31995;&#24182;&#19981;&#23481;&#26131;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24322;&#24120;&#24615;&#21644;&#21487;&#36777;&#35782;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#20415;&#27604;&#36739;&#24418;&#24335;&#21270;&#20307;&#31995;&#24182;&#25581;&#31034;&#20854;&#26412;&#20307;&#35770;&#25215;&#35834;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#35813;&#26694;&#26550;&#26469;&#27604;&#36739;&#22235;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20174;&#26412;&#20307;&#35770;&#35282;&#24230;&#21487;&#33021;&#21457;&#29983;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00685v1 Announce Type: new  Abstract: Defeasible reasoning is a kind of reasoning where some generalisations may not be valid in all circumstances, that is general conclusions may fail in some cases. Various formalisms have been developed to model this kind of reasoning, which is characteristic of common-sense contexts. However, it is not easy for a modeller to choose among these systems the one that better fits its domain from an ontological point of view. In this paper we first propose a framework based on the notions of exceptionality and defeasibility in order to be able to compare formalisms and reveal their ontological commitments. Then, we apply this framework to compare four systems, showing the differences that may occur from an ontological perspective.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00642</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking The Uniformity Metric in Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#21248;&#24615;&#22312;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20043;&#21069;&#30340;&#19968;&#39033;&#24320;&#21019;&#24615;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#23450;&#37327;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#23849;&#28291;&#31243;&#24230;&#12290;&#30452;&#25509;&#20248;&#21270;&#36825;&#19968;&#24230;&#37327;&#19982;&#23545;&#40784;&#19968;&#36215;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#19981;&#26029;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#36825;&#19968;&#24230;&#37327;&#32570;&#20047;&#23545;&#32500;&#24230;&#23849;&#28291;&#30340;&#25935;&#24863;&#24615;&#65292;&#20984;&#26174;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#24182;&#35774;&#35745;&#19968;&#20010;&#26356;&#26377;&#25928;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#28385;&#36275;&#20854;&#20013;&#30340;&#19968;&#20123;&#12290;&#25105;&#20204;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#65292;&#24182;&#19988;&#23545;&#32500;&#24230;&#23849;&#28291;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#21253;&#21547;&#26126;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#21512;&#25104;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;SR4G&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;9&#20998;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.00587</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26174;&#24335;&#31354;&#38388;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#21253;&#21547;&#26126;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#21512;&#25104;&#26631;&#39064;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;SR4G&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;9&#20998;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#26410;&#33021;&#20934;&#30830;&#21453;&#26144;&#29289;&#20307;&#20043;&#38388;&#30340;&#26126;&#30830;&#31354;&#38388;&#20851;&#31995;&#65292;&#27604;&#22914;&#8220;&#22312;&#24038;&#20391;&#8221;&#25110;&#8220;&#22312;&#19979;&#26041;&#8221;&#12290; &#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#22270;&#20687;&#26631;&#39064;&#20013;&#24456;&#23569;&#20986;&#29616;&#26126;&#30830;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#32473;&#23450;&#29616;&#26377;&#22270;&#20687;&#29983;&#25104;&#21253;&#21547;14&#20010;&#26126;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#21512;&#25104;&#26631;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#29983;&#25104;&#31354;&#38388;&#20851;&#31995;&#8221;(SR4G)&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;990&#19975;&#20010;&#29992;&#20110;&#35757;&#32451;&#30340;&#22270;&#20687;&#26631;&#39064;&#23545;&#21644;&#36229;&#36807;6&#19975;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26631;&#39064;&#12290;&#20026;&#20102;&#27979;&#35797;&#27867;&#21270;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#8220;&#26410;&#35265;&#36807;&#8221;&#30340;&#20999;&#20998;&#65292;&#20854;&#20013;&#35757;&#32451;&#21644;&#27979;&#35797;&#26631;&#39064;&#20013;&#30340;&#23545;&#35937;&#38598;&#26159;&#19981;&#30456;&#20132;&#30340;&#12290; SR4G&#26159;&#21487;&#20197;&#29992;&#26469;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#20013;&#36827;&#34892;&#31354;&#38388;&#24494;&#35843;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24494;&#35843;&#20004;&#20010;&#19981;&#21516;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;&#26631;&#35760;&#20026;SD$_{SR4G}$&#65289;&#21487;&#20197;&#33719;&#24471;&#22810;&#36798;9&#20010;&#20998;&#25968;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00587v1 Announce Type: cross  Abstract: Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as 'left of' or 'below'. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an 'unseen' split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00570</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#32858;&#31867;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking cluster-conditioned diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#26368;&#26032;&#30340;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#26368;&#20339;&#32858;&#31867;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;FID&#24182;&#20855;&#26377;&#36739;&#24378;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#30340;&#32858;&#31867;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#20351;&#29992;&#32858;&#31867;&#20998;&#37197;&#30340;&#22270;&#29255;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#20851;&#20110;&#22270;&#29255;&#32858;&#31867;&#30340;&#20010;&#21035;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#29255;&#21512;&#25104;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#29255;&#32858;&#31867;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#32771;&#34385;&#21040;&#22270;&#29255;&#21512;&#25104;&#65288;&#35270;&#35273;&#32452;&#65289;&#30340;&#26368;&#20339;&#31751;&#31890;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#32858;&#31867;&#26465;&#20214;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;FID&#65288;&#21363;&#22312;CIFAR10&#21644;CIFAR100&#19978;&#20998;&#21035;&#20026;1.67&#21644;2.17&#65289;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#24378;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#22522;&#20110;&#29305;&#24449;&#30340;&#32858;&#31867;&#26469;&#25512;&#23548;&#20943;&#23569;&#35270;&#35273;&#32452;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#38480;&#31751;&#36793;&#30028;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#32858;&#31867;&#19982;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#29255;&#29983;&#25104;&#20043;&#38388;&#27809;&#26377;&#26174;&#33879;&#32852;&#31995;&#12290;&#20195;&#30721;&#21644;&#32858;&#31867;&#20998;&#37197;&#23558;&#20250;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00570v1 Announce Type: cross  Abstract: We present a comprehensive experimental study on image-level conditioning for diffusion models using cluster assignments. We elucidate how individual components regarding image clustering impact image synthesis across three datasets. By combining recent advancements from image clustering and diffusion models, we show that, given the optimal cluster granularity with respect to image synthesis (visual groups), cluster-conditioning can achieve state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively), while attaining a strong training sample efficiency. Finally, we propose a novel method to derive an upper cluster bound that reduces the search space of the visual groups using solely feature-based clustering. Unlike existing approaches, we find no significant connection between clustering and cluster-conditional image generation. The code and cluster assignments will be released.
&lt;/p&gt;</description></item><item><title>&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#35266;&#23519;&#21040;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#65292;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#21644;&#20419;&#36827;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.00567</link><description>&lt;p&gt;
&#25674;&#24179;&#38271;&#31243;&#20002;&#22833;&#26223;&#35266;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00567
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#35266;&#23519;&#21040;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#65292;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#21644;&#20419;&#36827;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;CDFSL&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#30340;&#20016;&#23500;&#35757;&#32451;&#26679;&#26412;&#36716;&#31227;&#20808;&#21069;&#30693;&#35782;&#65292;&#20197;&#20174;&#30446;&#26631;&#22495;&#30340;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#12290;&#26412;&#25991;&#38024;&#23545;CDFSL&#22312;&#36328;&#19981;&#21516;&#39046;&#22495;&#20256;&#36755;&#30693;&#35782;&#21644;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#24494;&#35843;&#27169;&#22411;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#21516;&#26102;&#35299;&#37322;CDFSL&#27169;&#22411;&#30340;&#20256;&#36755;&#21644;&#24494;&#35843;&#22256;&#38590;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#34920;&#31034;&#31354;&#38388;&#25439;&#22833;&#26223;&#35266;&#20013;&#30340;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#24179;&#22374;&#24615;&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#30701;&#31243;&#24179;&#22374;&#24615;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#24182;&#20419;&#36827;&#24494;&#35843;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00567v1 Announce Type: cross  Abstract: Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#24182;&#20248;&#21270;&#20102;&#26102;&#38388;&#25139;&#37319;&#26679;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#20462;&#22797;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00565</link><description>&lt;p&gt;
&#39044;&#27979;&#26080;&#20154;&#26426;&#31867;&#22411;&#65306;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#36827;&#34892;&#37319;&#26679;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Predicting UAV Type: An Exploration of Sampling and Data Augmentation for Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#26080;&#20154;&#26426;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#24182;&#20248;&#21270;&#20102;&#26102;&#38388;&#25139;&#37319;&#26679;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#24182;&#19988;&#20855;&#26377;&#35768;&#22810;&#29983;&#20135;&#29992;&#36884;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22686;&#21152;&#24341;&#21457;&#20102;&#23433;&#20840;&#25285;&#24551;--&#25105;&#20204;&#22914;&#20309;&#20445;&#25252;&#21463;&#38480;&#21046;&#30340;&#31354;&#22495;&#65311;&#20102;&#35299;&#26080;&#20154;&#26426;&#30340;&#31867;&#22411;&#21487;&#20197;&#24456;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#30830;&#23450;&#20854;&#28508;&#22312;&#39118;&#38505;&#12290;&#20363;&#22914;&#65292;&#22266;&#23450;&#32764;&#39134;&#34892;&#22120;&#21487;&#20197;&#22312;&#36739;&#38271;&#36317;&#31163;&#19978;&#25658;&#24102;&#26356;&#22810;&#37325;&#37327;&#65292;&#22240;&#27492;&#21487;&#33021;&#26500;&#25104;&#26356;&#22823;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23558;&#26080;&#20154;&#26426;&#20998;&#31867;&#20026;&#22235;&#26059;&#32764;&#12289;&#20845;&#26059;&#32764;&#25110;&#22266;&#23450;&#32764;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#24212;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#27979;&#35797;&#26356;&#25913;&#26102;&#38388;&#25139;&#37319;&#26679;&#26041;&#27861;&#21644;&#35299;&#20915;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#37319;&#26679;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#20462;&#22797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00565v1 Announce Type: cross  Abstract: Unmanned aerial vehicles are becoming common and have many productive uses. However, their increased prevalence raises safety concerns -- how can we protect restricted airspace? Knowing the type of unmanned aerial vehicle can go a long way in determining any potential risks it carries. For instance, fixed-wing craft can carry more weight over longer distances, thus potentially posing a more significant threat. This paper presents a machine learning model for classifying unmanned aerial vehicles as quadrotor, hexarotor, or fixed-wing. Our approach effectively applies a Long-Short Term Memory (LSTM) neural network for the purpose of time series classification. We performed experiments to test the effects of changing the timestamp sampling method and addressing the imbalance in the class distribution. Through these experiments, we identified the top-performing sampling and class imbalance fixing methods. Averaging the macro f-scores acros
&lt;/p&gt;</description></item><item><title>EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.00564</link><description>&lt;p&gt;
&#39640;&#25928;Zero V2&#65306;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25484;&#25569;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00564
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EfficientZero V2&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;RL&#31639;&#27861;&#35774;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;EfficientZero&#30340;&#24615;&#33021;&#25193;&#23637;&#21040;&#22810;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#36830;&#32493;&#21644;&#31163;&#25955;&#34892;&#21160;&#65292;&#20197;&#21450;&#35270;&#35273;&#21644;&#20302;&#32500;&#36755;&#20837;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#65292;EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;EfficientZero V2&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36827;&#27493;&#65292;&#27604;&#22914;Atari 100k&#65292;Proprio Control&#31561;&#20013;&#65292;&#22312;66&#20010;&#35780;&#20272;&#20219;&#21153;&#20013;&#26377;50&#20010;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00564v1 Announce Type: cross  Abstract: Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#30828;&#21442;&#25968;&#20849;&#20139;&#35299;&#20915;&#20102;&#24322;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#23545;&#22810;&#20010;&#20154;&#33080;&#23646;&#24615;&#30340;&#26368;&#20339;&#20272;&#35745;&#25928;&#26524;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.00561</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#36827;&#34892;&#24322;&#26500;&#20154;&#33080;&#23646;&#24615;&#20272;&#35745;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous Face Attribute Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#30828;&#21442;&#25968;&#20849;&#20139;&#35299;&#20915;&#20102;&#24322;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#23545;&#22810;&#20010;&#20154;&#33080;&#23646;&#24615;&#30340;&#26368;&#20339;&#20272;&#35745;&#25928;&#26524;&#65292;&#24182;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#22270;&#20687;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#20849;&#20139;&#30340;&#32852;&#21512;&#20272;&#35745;&#26377;&#24207;&#21644;&#21517;&#20041;&#23646;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27973;&#23618;&#29305;&#24449;&#36827;&#34892;&#30828;&#21442;&#25968;&#20849;&#20139;&#26469;&#35299;&#20915;&#24322;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#23646;&#24615;&#20272;&#35745;&#20219;&#21153;&#30340;&#21516;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#26469;&#26435;&#34913;&#22810;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#20154;&#33080;&#30340;&#22810;&#20010;&#23646;&#24615;&#36827;&#34892;&#26368;&#20339;&#20272;&#35745;&#65292;&#24182;&#20943;&#23569;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#20855;&#26377;&#22810;&#20010;&#20154;&#33080;&#23646;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#20154;&#33080;&#23646;&#24615;&#20272;&#35745;&#20013;&#24341;&#36215;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#22312;&#36793;&#32536;&#31995;&#32479;&#19978;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00561v1 Announce Type: cross  Abstract: Face images contain a wide variety of attribute information. In this paper, we propose a generalized framework for joint estimation of ordinal and nominal attributes based on information sharing. We tackle the correlation problem between heterogeneous attributes using hard parameter sharing of shallow features, and trade-off multiple loss functions by considering homoskedastic uncertainty for each attribute estimation task. This leads to optimal estimation of multiple attributes of the face and reduces the training cost of multitask learning. Experimental results on benchmarks with multiple face attributes show that the proposed approach has superior performance compared to state of the art. Finally, we discuss the bias issues arising from the proposed approach in face attribute estimation and validate its feasibility on edge systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25919;&#31574;&#30340;&#32452;&#32455;&#12289;&#29616;&#25104;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#30340;&#25552;&#20379;&#20197;&#21450;&#24120;&#35265;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#20998;&#20139;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#21644;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00550</link><description>&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65306;&#21019;&#24314;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#26234;&#33021;&#20307;&#21644;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning Datasets: A Toolkit For Creating Datasets, Training Agents and Benchmarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25919;&#31574;&#30340;&#32452;&#32455;&#12289;&#29616;&#25104;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#30340;&#25552;&#20379;&#20197;&#21450;&#24120;&#35265;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#20998;&#20139;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#21644;&#35780;&#20272;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#19987;&#23478;&#25968;&#25454;&#26469;&#35757;&#32451;&#26234;&#33021;&#20307;&#23436;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#22240;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#32780;&#22791;&#21463;&#22256;&#25200;&#65292;&#23548;&#33268;&#25216;&#26415;&#27979;&#35797;&#37117;&#26159;&#22522;&#20110;&#33258;&#26377;&#25968;&#25454;&#12290;&#21019;&#24314;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#19987;&#23478;&#26234;&#33021;&#20307;&#12289;&#35760;&#24405;&#23427;&#20204;&#30340;&#20132;&#20114;&#24182;&#29992;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#27979;&#35797;&#27599;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#27599;&#31181;&#26032;&#25216;&#26415;&#21019;&#24314;&#26032;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#35780;&#20272;&#36807;&#31243;&#32570;&#20047;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#20998;&#24067;&#19978;&#37117;&#21487;&#33021;&#22823;&#19981;&#30456;&#21516;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#27169;&#20223;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#65306;(i) &#25552;&#20379;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#19987;&#23478;&#31574;&#30053;&#65292;&#24182;&#25903;&#25345;&#22810;&#32447;&#31243;&#20197;&#21152;&#24555;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#65307;(ii) &#25552;&#20379;&#29616;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#20197;&#21450;&#31934;&#30830;&#30340;&#27979;&#37327;&#65307;&#20197;&#21450;(iii) &#20998;&#20139;&#24120;&#29992;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00550v1 Announce Type: cross  Abstract: Imitation learning field requires expert data to train agents in a task. Most often, this learning approach suffers from the absence of available data, which results in techniques being tested on its dataset. Creating datasets is a cumbersome process requiring researchers to train expert agents from scratch, record their interactions and test each benchmark method with newly created data. Moreover, creating new datasets for each new technique results in a lack of consistency in the evaluation process since each dataset can drastically vary in state and action distribution. In response, this work aims to address these issues by creating Imitation Learning Datasets, a toolkit that allows for: (i) curated expert policies with multithreaded support for faster dataset creation; (ii) readily available datasets and techniques with precise measurements; and (iii) sharing implementations of common imitation learning techniques. Demonstration li
&lt;/p&gt;</description></item><item><title>ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.00510</link><description>&lt;p&gt;
ROME: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35760;&#24518;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00510
&lt;/p&gt;
&lt;p&gt;
ROME&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21644;&#38750;&#35760;&#24518;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#21270;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20102;&#35299;&#27169;&#22411;&#35760;&#24518;&#30340;&#27934;&#23519;&#21644;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#21270;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#29992;&#20110;&#37327;&#21270;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#22797;&#21046;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25552;&#31034;&#38271;&#24230;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#35760;&#24518;&#21270;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#35821;&#26009;&#24211;&#35268;&#27169;&#24040;&#22823;&#19988;&#20854;&#39044;&#22788;&#29702;&#32791;&#26102;&#12290;&#20026;&#20102;&#22312;&#19981;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#35760;&#24518;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25506;&#32034;&#35760;&#24518;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27169;&#22411;&#39318;&#20808;&#23558;&#36873;&#23450;&#30340;&#26679;&#26412;&#20998;&#20026;&#35760;&#24518;&#21270;&#21644;&#38750;&#35760;&#24518;&#21270;&#32452;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#12289;&#27010;&#29575;&#21644;&#38544;&#34255;&#29366;&#24577;&#30340;&#35265;&#35299;&#27604;&#36739;&#36825;&#20004;&#32452;&#20013;&#30340;&#28436;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21253;&#25324;&#35789;&#38271;&#12289;&#35789;&#24615;&#12289;&#35789;&#39057;&#12289;&#22343;&#20540;&#21644;&#26041;&#24046;&#22312;&#20869;&#30340;&#22240;&#32032;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00510v1 Announce Type: cross  Abstract: Probing the memorization of large language models holds significant importance. Previous works have established metrics for quantifying memorization, explored various influencing factors, such as data duplication, model size, and prompt length, and evaluated memorization by comparing model outputs with training corpora. However, the training corpora are of enormous scale and its pre-processing is time-consuming. To explore memorization without accessing training data, we propose a novel approach, named ROME, wherein memorization is explored by comparing disparities across memorized and non-memorized. Specifically, models firstly categorize the selected samples into memorized and non-memorized groups, and then comparing the demonstrations in the two groups from the insights of text, probability, and hidden state. Experimental findings show the disparities in factors including word length, part-of-speech, word frequency, mean and varianc
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#26500;&#24314;&#65292;&#37319;&#29992;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00509</link><description>&lt;p&gt;
&#35843;&#26597;&#27515;&#20129;&#24605;&#32500;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#23545;&#20110;&#21476;&#20856;&#20013;&#25991;
&lt;/p&gt;
&lt;p&gt;
Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00509
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#26500;&#24314;&#34920;&#31034;&#65288;CCR&#65289;&#30340;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#26500;&#24314;&#65292;&#37319;&#29992;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#21476;&#20856;&#20013;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#21382;&#21490;&#24515;&#29702;&#25991;&#26412;&#20998;&#26512;&#27969;&#31243;&#12290;&#25968;&#21315;&#24180;&#26469;&#65292;&#20154;&#31867;&#29992;&#21508;&#31181;&#35821;&#35328;&#21019;&#20316;&#25991;&#26412;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35745;&#31639;&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#24403;&#20195;&#35821;&#35328;&#21644;&#35821;&#26009;&#24211;&#19978;&#12290;&#21382;&#21490;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20381;&#36182;&#35745;&#31639;&#25216;&#26415;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24320;&#21457;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#21382;&#21490;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#21517;&#20026;&#19978;&#19979;&#25991;&#21270;&#32467;&#26500;&#34920;&#24449;&#65288;CCR&#65289;&#30340;&#24403;&#21069;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#65288;&#21363;&#24515;&#29702;&#35843;&#26597;&#65289;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36890;&#36807;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20197;&#27979;&#37327;&#21476;&#20856;&#20013;&#25991;&#35821;&#26009;&#24211;&#20013;&#30340;&#20256;&#32479;&#20027;&#20041;&#12289;&#35268;&#33539;&#21147;&#37327;&#21644;&#38598;&#20307;&#20027;&#20041;&#31561;&#24515;&#29702;&#26500;&#24314;&#12290;&#37492;&#20110;&#21487;&#29992;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38388;&#25509;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#20013;&#25991;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00509v1 Announce Type: cross  Abstract: In this work, we develop a pipeline for historical-psychological text analysis in classical Chinese. Humans have produced texts in various languages for thousands of years; however, most of the computational literature is focused on contemporary languages and corpora. The emerging field of historical psychology relies on computational techniques to extract aspects of psychology from historical corpora using new methods developed in natural language processing (NLP). The present pipeline, called Contextualized Construct Representations (CCR), combines expert knowledge in psychometrics (i.e., psychological surveys) with text representations generated via transformer-based language models to measure psychological constructs such as traditionalism, norm strength, and collectivism in classical Chinese corpora. Considering the scarcity of available data, we propose an indirect supervised contrastive learning approach and build the first Chin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#20197;&#21450;&#24341;&#20837;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#65292;&#24182;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#32456;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.00504</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#21033;&#29992;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning and Leveraging World Models in Visual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00504
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#20197;&#21450;&#24341;&#20837;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#65292;&#24182;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#26368;&#32456;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;JEPA&#39044;&#27979;&#20219;&#21153;&#27010;&#25324;&#20026;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#25439;&#22351;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#20687;&#19990;&#30028;&#27169;&#22411;&#65288;IWM&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#65292;&#23398;&#20250;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#20840;&#23616;&#20809;&#24230;&#21464;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#33391;&#22909;&#30340;IWM&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#26465;&#20214;&#12289;&#39044;&#27979;&#22256;&#38590;&#24230;&#21644;&#23481;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#24494;&#35843;&#21487;&#20197;&#23558;IWM&#23398;&#21040;&#30340;&#39044;&#27979;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65307;&#32463;&#36807;&#24494;&#35843;&#30340;IWM&#19990;&#30028;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#29978;&#33267;&#36229;&#36807;&#20197;&#24448;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;IWM&#23398;&#20064;&#21487;&#20197;&#25511;&#21046;&#25152;&#23398;&#20064;&#34920;&#31034;&#30340;&#25277;&#35937;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00504v1 Announce Type: cross  Abstract: Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising self-supervised approach that learns by leveraging a world model. While previously limited to predicting missing parts of an input, we explore how to generalize the JEPA prediction task to a broader set of corruptions. We introduce Image World Models, an approach that goes beyond masked image modeling and learns to predict the effect of global photometric transformations in latent space. We study the recipe of learning performant IWMs and show that it relies on three key aspects: conditioning, prediction difficulty, and capacity. Additionally, we show that the predictive world model learned by IWM can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM world model matches or surpasses the performance of previous self-supervised methods. Finally, we show that learning with an IWM allows one to control the abstraction level of the learned represe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38745;&#40664;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#29305;&#23450;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26356;&#22823;&#26356;&#28789;&#27963;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#20102;&#24182;&#34892;&#21270;&#30340;&#21463;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.00450</link><description>&lt;p&gt;
&#24182;&#34892;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Hyperparameter Optimization Of Spiking Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#38745;&#40664;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#29305;&#23450;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#26356;&#22823;&#26356;&#28789;&#27963;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#36229;&#21442;&#25968;&#30340;&#20248;&#21270;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#24212;&#29992;&#20102;&#24182;&#34892;&#21270;&#30340;&#21463;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#12290;SNN&#22522;&#20110;&#27604;&#36890;&#24120;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26356;&#20855;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#28857;&#26159;&#31070;&#32463;&#20803;&#21644;&#33033;&#20914;&#20043;&#38388;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#36229;&#21442;&#25968;&#38750;&#24120;&#25935;&#24863;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#20248;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;SNN&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;SNN&#30340;&#20449;&#21495;&#25439;&#22833;&#38382;&#39064;&#25193;&#23637;&#21040;&#25105;&#20204;&#31216;&#20043;&#20026;&#38745;&#40664;&#32593;&#32476;&#12290;&#36825;&#20123;&#32593;&#32476;&#30001;&#20110;&#36229;&#21442;&#25968;&#25110;&#26550;&#26500;&#35774;&#32622;&#19981;&#24403;&#32780;&#26080;&#27861;&#22312;&#36755;&#20986;&#31471;&#21457;&#20986;&#36275;&#22815;&#30340;&#33033;&#20914;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25628;&#32034;&#31354;&#38388;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#65292;&#26377;&#26102;&#29978;&#33267;&#26159;&#31163;&#25955;&#21270;&#30340;&#65292;&#20197;&#38450;&#27490;&#23545;&#36825;&#26679;&#30340;&#32593;&#32476;&#36827;&#34892;&#37319;&#26679;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26089;&#20572;&#20934;&#21017;&#26469;&#26816;&#27979;&#38745;&#40664;&#32593;&#32476;&#65292;&#24182;&#35774;&#35745;&#29305;&#23450;&#30340;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#20363;&#21270;&#26356;&#22823;&#12289;&#26356;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#21463;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#23558;&#20854;&#24322;&#27493;&#24182;&#34892;&#21270;&#65292;&#22240;&#20026;&#19968;&#20010;&#35780;&#20272;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00450v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired approach than usual artificial neural networks. Such models are characterized by complex dynamics between neurons and spikes. These are very sensitive to the hyperparameters, making their optimization challenging. To tackle hyperparameter optimization of SNNs, we initially extended the signal loss issue of SNNs to what we call silent networks. These networks fail to emit enough spikes at their outputs due to mistuned hyperparameters or architecture. Generally, search spaces are heavily restrained, sometimes even discretized, to prevent the sampling of such networks. By defining an early stopping criterion detecting silent networks and by designing specific constraints, we were able to instantiate larger and more flexible search spaces. We applied a constrained Bayesian optimization technique, which was asynchronously parallelized, as the evaluation time of a 
&lt;/p&gt;</description></item><item><title>&#20316;&#32773;&#20204;&#36890;&#36807;&#23545;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;&#21019;&#24847;&#35821;&#35328;&#33402;&#26415;&#36827;&#34892;&#20102;&#20215;&#20540;&#35266;&#21644;&#24577;&#24230;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;AI&#22914;&#20309;&#24433;&#21709;&#20316;&#32773;&#12289;&#20316;&#21697;&#21644;&#35266;&#20247;&#20043;&#38388;&#30340;&#21033;&#30410;&#20851;&#31995;&#65292;&#20197;&#21450;&#20316;&#32773;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.00439</link><description>&lt;p&gt;
&#20316;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;&#21487;&#20280;&#32553;&#20010;&#24615;&#21270;&#21019;&#24847;&#35821;&#35328;&#33402;&#26415;&#30340;&#20215;&#20540;&#35266;&#21644;&#24577;&#24230;
&lt;/p&gt;
&lt;p&gt;
Authors' Values and Attitudes Towards AI-bridged Scalable Personalization of Creative Language Arts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00439
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#20204;&#36890;&#36807;&#23545;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;&#21019;&#24847;&#35821;&#35328;&#33402;&#26415;&#36827;&#34892;&#20102;&#20215;&#20540;&#35266;&#21644;&#24577;&#24230;&#30340;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;AI&#22914;&#20309;&#24433;&#21709;&#20316;&#32773;&#12289;&#20316;&#21697;&#21644;&#35266;&#20247;&#20043;&#38388;&#30340;&#21033;&#30410;&#20851;&#31995;&#65292;&#20197;&#21450;&#20316;&#32773;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26377;&#28508;&#21147;&#21019;&#36896;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#20132;&#20114;&#24335;&#23186;&#20307;&#65306;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;&#21019;&#24847;&#35821;&#35328;&#33402;&#26415;&#65288;CLA&#65289;&#65292;&#36890;&#36807;&#22312;&#35268;&#27169;&#19978;&#23558;&#20316;&#32773;&#30340;&#24895;&#26223;&#20010;&#24615;&#21270;&#22320;&#21576;&#29616;&#32473;&#35266;&#20247;&#65292;&#20174;&#32780;&#26725;&#25509;&#20102;&#20316;&#32773;&#21644;&#35266;&#20247;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23601;&#20316;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;CLA&#30340;&#20215;&#20540;&#35266;&#21644;&#24577;&#24230;&#32780;&#35328;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35782;&#21035;&#36825;&#20123;&#20215;&#20540;&#35266;&#21644;&#24577;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#21576;&#29616;&#20855;&#26377;&#25512;&#27979;&#24615;&#20294;&#29616;&#23454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26725;&#25509;&#30340;CLA&#22330;&#26223;&#65292;&#23545;&#36328;&#36234;&#20843;&#31181;&#27969;&#27966;&#65288;&#22914;&#35799;&#27468;&#12289;&#28459;&#30011;&#65289;&#30340;18&#20301;&#20316;&#32773;&#36827;&#34892;&#20102;&#35775;&#35848;&#30740;&#31350;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20316;&#32773;&#12289;&#20316;&#21697;&#21644;&#35266;&#20247;&#20043;&#38388;&#30340;&#21160;&#24577;&#24102;&#26469;&#30340;&#19977;&#20010;&#25910;&#30410;&#65306;&#20316;&#32773;&#20174;&#35813;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#25910;&#30410;&#65292;&#35266;&#20247;&#20174;&#20316;&#21697;&#20013;&#33719;&#24471;&#30340;&#25910;&#30410;&#65292;&#20197;&#21450;&#20316;&#32773;&#20174;&#35266;&#20247;&#22788;&#33719;&#24471;&#30340;&#25910;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;AI&#26725;&#25509;&#30340;CLA&#22914;&#20309;&#20250;&#20419;&#36827;&#25110;&#20943;&#23569;&#36825;&#20123;&#21033;&#30410;&#65292;&#20197;&#21450;&#20316;&#32773;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35843;&#26597;&#33021;&#22815;&#26263;&#31034;AI&#22914;&#20309;&#20026;CLA&#35266;&#20247;&#25552;&#20379;&#26377;&#36259;&#30340;&#20307;&#39564;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20316;&#32773;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00439v1 Announce Type: cross  Abstract: Generative AI has the potential to create a new form of interactive media: AI-bridged creative language arts (CLA), which bridge the author and audience by personalizing the author's vision to the audience's context and taste at scale. However, it is unclear what the authors' values and attitudes would be regarding AI-bridged CLA. To identify these values and attitudes, we conducted an interview study with 18 authors across eight genres (e.g., poetry, comics) by presenting speculative but realistic AI-bridged CLA scenarios. We identified three benefits derived from the dynamics between author, artifact, and audience: those that 1) authors get from the process, 2) audiences get from the artifact, and 3) authors get from the audience. We found how AI-bridged CLA would either promote or reduce these benefits, along with authors' concerns. We hope our investigation hints at how AI can provide intriguing experiences to CLA audiences while p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;&#22810;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00437</link><description>&lt;p&gt;
LoMOE: &#36890;&#36807;&#22810;&#25193;&#25955;&#23454;&#29616;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
LoMOE: Localized Multi-Object Editing via Multi-Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00437
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;&#22810;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#23637;&#31034;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#22522;&#20110;&#25552;&#31034;&#26465;&#20214;&#30340;&#22270;&#20687;&#32534;&#36753;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#24403;&#23545;&#22330;&#26223;&#20013;&#21253;&#21547;&#21333;&#20010;/&#22810;&#20010;&#23545;&#35937;&#30340;&#29305;&#23450;&#23545;&#35937;&#25110;&#32454;&#31890;&#24230;&#21306;&#22495;&#36827;&#34892;&#31934;&#30830;&#32534;&#36753;&#26102;&#24448;&#24448;&#19981;&#22826;&#26377;&#25928;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#38646;&#26679;&#26412;&#23616;&#37096;&#22810;&#23545;&#35937;&#32534;&#36753;&#65292;&#20197;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#36171;&#20104;&#29992;&#25143;&#22312;&#22270;&#20687;&#20013;&#23545;&#23545;&#35937;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#22312;&#19968;&#20010;&#22797;&#26434;&#22330;&#26223;&#20013;&#19968;&#27425;&#24615;&#28155;&#21152;&#12289;&#26367;&#25442;&#25110;&#32534;&#36753;$\textbf{&#22810;}$&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21069;&#26223; mask &#21644;&#23545;&#24212;&#30340;&#31616;&#21333;&#25991;&#26412;&#25552;&#31034;&#23545;&#30446;&#26631;&#21306;&#22495;&#26045;&#21152;&#23616;&#37096;&#24433;&#21709;&#65292;&#23454;&#29616;&#39640;&#20445;&#30495;&#24230;&#22270;&#20687;&#32534;&#36753;&#12290;&#36890;&#36807;&#36328;&#27880;&#24847;&#21147;&#21644;&#32972;&#26223;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00437v1 Announce Type: cross  Abstract: Recent developments in the field of diffusion models have demonstrated an exceptional capacity to generate high-quality prompt-conditioned image edits. Nevertheless, previous approaches have primarily relied on textual prompts for image editing, which tend to be less effective when making precise edits to specific objects or fine-grained regions within a scene containing single/multiple objects. We introduce a novel framework for zero-shot localized multi-object editing through a multi-diffusion process to overcome this challenge. This framework empowers users to perform various operations on objects within an image, such as adding, replacing, or editing $\textbf{many}$ objects in a complex scene $\textbf{in one pass}$. Our approach leverages foreground masks and corresponding simple text prompts that exert localized influences on the target regions resulting in high-fidelity image editing. A combination of cross-attention and backgrou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MM-AU&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20107;&#25925;&#35270;&#39057;&#29702;&#35299;&#65292;&#25903;&#25345;&#21508;&#31181;&#20107;&#25925;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22810;&#27169;&#24577;&#35270;&#39057;&#25193;&#25955;&#20197;&#29702;&#35299;&#23433;&#20840;&#39550;&#39542;&#30340;&#20107;&#25925;&#22240;&#26524;&#38142;&#12290;</title><link>https://arxiv.org/abs/2403.00436</link><description>&lt;p&gt;
Abductive Ego-View&#20107;&#25925;&#35270;&#39057;&#29702;&#35299;&#20197;&#23454;&#29616;&#23433;&#20840;&#39550;&#39542;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Abductive Ego-View Accident Video Understanding for Safe Driving Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00436
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MM-AU&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20107;&#25925;&#35270;&#39057;&#29702;&#35299;&#65292;&#25903;&#25345;&#21508;&#31181;&#20107;&#25925;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22810;&#27169;&#24577;&#35270;&#39057;&#25193;&#25955;&#20197;&#29702;&#35299;&#23433;&#20840;&#39550;&#39542;&#30340;&#20107;&#25925;&#22240;&#26524;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MM-AU&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#20107;&#25925;&#35270;&#39057;&#29702;&#35299;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;MM-AU&#21253;&#21547;11,727&#20010;&#37326;&#22806;&#33258;&#25105;&#35270;&#35282;&#20107;&#25925;&#35270;&#39057;&#65292;&#27599;&#20010;&#35270;&#39057;&#37197;&#26377;&#26102;&#38388;&#19978;&#23545;&#40784;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#26631;&#27880;&#20102;&#36229;&#36807;223&#19975;&#20010;&#29289;&#20307;&#26694;&#21644;58,650&#23545;&#22522;&#20110;&#35270;&#39057;&#30340;&#20107;&#25925;&#21407;&#22240;&#65292;&#28085;&#30422;&#20102;58&#31181;&#20107;&#25925;&#31867;&#21035;&#12290;MM-AU&#25903;&#25345;&#21508;&#31181;&#20107;&#25925;&#29702;&#35299;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22810;&#27169;&#24577;&#35270;&#39057;&#25193;&#25955;&#20197;&#29702;&#35299;&#23433;&#20840;&#39550;&#39542;&#30340;&#20107;&#25925;&#22240;&#26524;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00436v1 Announce Type: cross  Abstract: We present MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each with temporally aligned text descriptions. We annotate over 2.23 million object boxes and 58,650 pairs of video-based accident reasons, covering 58 accident categories. MM-AU supports various accident understanding tasks, particularly multimodal video diffusion to understand accident cause-effect chains for safe driving. With MM-AU, we present an Abductive accident Video understanding framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven by an abductive CLIP model. This model involves a contrastive interaction loss to learn the pair co-occurrence of normal, near-accident, accident frames with the corresponding text descriptions, such as accident reasons, prevention advice, and accident categor
&lt;/p&gt;</description></item><item><title>HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.00425</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#28966;&#28857;&#23545;&#27604;&#35299;&#30721;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#65306;HALC
&lt;/p&gt;
&lt;p&gt;
HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00425
&lt;/p&gt;
&lt;p&gt;
HALC&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#35937;&#24187;&#35273;&#30340;&#26032;&#39062;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#23616;&#37096;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#21644;&#20840;&#23616;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#25104;&#21151;&#20943;&#23569;OH&#32780;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#21516;&#26102;&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#22810;&#27169;&#24577;&#29615;&#22659;&#26041;&#38754;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#21463;&#21040;&#23545;&#35937;&#24187;&#35273;&#65288;OH&#65289;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;HALC&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;LVLMs&#20013;&#30340;OH&#12290;HALC&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#29420;&#29305;&#30340;&#32454;&#31890;&#24230;&#26368;&#20339;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#21516;&#26102;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#25805;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HALC&#38598;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#32858;&#28966;&#22522;&#20934;&#26426;&#21046;&#65288;&#23616;&#37096;&#65289;&#65292;&#22312;&#36816;&#34892;&#26102;&#32416;&#27491;&#20135;&#29983;&#24187;&#35273;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;&#19968;&#31181;&#19987;&#38376;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65288;&#20840;&#23616;&#65289;&#65292;&#20197;&#26174;&#30528;&#20943;&#23569;OH&#65292;&#21516;&#26102;&#20445;&#25345;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;HALC&#21487;&#20197;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#38598;&#25104;&#21040;&#20219;&#20309;LVLMs&#20013;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#20102;HALC&#22312;&#20943;&#23569;OH&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00425v1 Announce Type: cross  Abstract: While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00420</link><description>&lt;p&gt;
&#32463;&#30001;&#23545;&#25239;&#25915;&#20987;&#21644;&#35757;&#32451;&#30340;&#31283;&#20581;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#26469;&#25913;&#36827;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#31995;&#32479;&#20998;&#26512;&#20102;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#33258;&#20027;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36731;&#24494;&#26465;&#20214;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#29992;&#24615;&#65292;DRL&#24517;&#39035;&#23637;&#31034;&#20986;&#21487;&#20449;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#39640;DRL&#23545;&#26465;&#20214;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#26159;&#19968;&#31181;&#25913;&#36827;&#26041;&#24335;&#65292;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#38024;&#23545;&#29615;&#22659;&#21160;&#24577;&#30340;&#36866;&#24403;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#23545;&#24403;&#20195;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31995;&#32479;&#22320;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#30446;&#26631;&#21644;&#25805;&#20316;&#26426;&#21046;&#12290;&#36825;&#31181;&#20998;&#31867;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#26377;&#25928;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#24674;&#22797;&#21147;&#30340;&#35814;&#32454;&#35265;&#35299;&#65292;&#20174;&#32780;&#20026;&#24320;&#36767;DRL&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36947;&#36335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00420v1 Announce Type: cross  Abstract: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents across various complex environments. Despite its significant performance in well known environments, it remains susceptible to minor conditions variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve robustness of DRL to unknown changes in the conditions is through Adversarial Training, by training the agent against well suited adversarial attacks on the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary adversarial attack methodologies, systematically categorizing them and comparing their objectives and operational mechanisms. This classification offers a detailed insight into how adversarial attacks effectively act for evaluating the resilience of DRL agents, thereby paving the 
&lt;/p&gt;</description></item><item><title>GLFNET&#25552;&#20986;&#20102;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22120;&#32593;&#32476;&#65288;GLFNet&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#37319;&#29992;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22359;&#30340;&#32452;&#21512;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00396</link><description>&lt;p&gt;
GLFNET: &#29992;&#20110;&#39640;&#25928;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#20840;&#23616;-&#23616;&#37096;&#65288;&#39057;&#29575;&#65289;&#28388;&#27874;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GLFNET: Global-Local (frequency) Filter Networks for efficient medical image segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00396
&lt;/p&gt;
&lt;p&gt;
GLFNET&#25552;&#20986;&#20102;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22120;&#32593;&#32476;&#65288;GLFNet&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#37319;&#29992;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22359;&#30340;&#32452;&#21512;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22120;&#32593;&#32476;&#65288;GLFNet&#65289;&#30340;&#26032;&#22411;&#21464;&#21387;&#22120;&#24335;&#26550;&#26500;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#29992;&#20840;&#23616;-&#23616;&#37096;&#28388;&#27874;&#22359;&#30340;&#32452;&#21512;&#26367;&#25442;&#20102;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20248;&#21270;&#27169;&#22411;&#25928;&#29575;&#12290;&#20840;&#23616;&#28388;&#27874;&#22120;&#20174;&#25972;&#20010;&#29305;&#24449;&#22270;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#32780;&#23616;&#37096;&#28388;&#27874;&#22120;&#34987;&#33258;&#36866;&#24212;&#22320;&#21019;&#24314;&#20026;&#21516;&#19968;&#29305;&#24449;&#22270;&#30340;4x4&#34917;&#19969;&#65292;&#24182;&#28155;&#21152;&#20102;&#21463;&#38480;&#30340;&#23610;&#24230;&#20449;&#24687;&#12290;&#29305;&#24449;&#25552;&#21462;&#21457;&#29983;&#22312;&#39057;&#22495;&#32780;&#19981;&#26159;&#24120;&#29992;&#30340;&#31354;&#38388;&#65288;&#22270;&#20687;&#65289;&#22495;&#65292;&#20197;&#20419;&#36827;&#26356;&#24555;&#30340;&#35745;&#31639;&#12290;&#20174;&#31354;&#38388;&#21644;&#39057;&#29575;&#31354;&#38388;&#20013;&#34701;&#21512;&#20449;&#24687;&#21019;&#36896;&#20986;&#19968;&#20010;&#22797;&#26434;&#24615;&#12289;&#25152;&#38656;&#25968;&#25454;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;GLFNet&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#25928;&#29575;&#19978;&#20960;&#20046;&#25552;&#39640;&#20102;&#19968;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00396v1 Announce Type: cross  Abstract: We propose a novel transformer-style architecture called Global-Local Filter Network (GLFNet) for medical image segmentation and demonstrate its state-of-the-art performance. We replace the self-attention mechanism with a combination of global-local filter blocks to optimize model efficiency. The global filters extract features from the whole feature map whereas the local filters are being adaptively created as 4x4 patches of the same feature map and add restricted scale information. In particular, the feature extraction takes place in the frequency domain rather than the commonly used spatial (image) domain to facilitate faster computations. The fusion of information from both spatial and frequency spaces creates an efficient model with regards to complexity, required data and performance. We test GLFNet on three benchmark datasets achieving state-of-the-art performance on all of them while being almost twice as efficient in terms of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>MS-Net&#26159;&#19968;&#20010;&#22810;&#36335;&#24452;&#31232;&#30095;&#27169;&#22411;&#65292;&#36890;&#36807;&#36827;&#21270;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#65292;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#36873;&#25321;&#24615;&#28608;&#27963;&#20854;&#21442;&#25968;&#23376;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#22330;&#26223;&#19979;&#36816;&#21160;&#39044;&#27979;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00353</link><description>&lt;p&gt;
MS-Net&#65306;&#22810;&#36335;&#24452;&#31232;&#30095;&#27169;&#22411;&#29992;&#20110;&#22810;&#22330;&#26223;&#20013;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00353
&lt;/p&gt;
&lt;p&gt;
MS-Net&#26159;&#19968;&#20010;&#22810;&#36335;&#24452;&#31232;&#30095;&#27169;&#22411;&#65292;&#36890;&#36807;&#36827;&#21270;&#36807;&#31243;&#36827;&#34892;&#35757;&#32451;&#65292;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#36873;&#25321;&#24615;&#28608;&#27963;&#20854;&#21442;&#25968;&#23376;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#22330;&#26223;&#19979;&#36816;&#21160;&#39044;&#27979;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34892;&#20026;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#38543;&#26426;&#29305;&#24449;&#20351;&#24471;&#36816;&#21160;&#39044;&#27979;&#25104;&#20026;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22914;&#20309;&#24314;&#31435;&#22810;&#20010;&#34892;&#39542;&#22330;&#26223;&#65288;&#22914;&#21512;&#24182;&#12289;&#29615;&#23707;&#12289;&#21313;&#23383;&#36335;&#21475;&#65289;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#20043;&#38388;&#30340;&#32852;&#31995;&#20173;&#26410;&#35299;&#20915;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multi-Scenes Network&#65288;&#31616;&#31216;MS-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#36827;&#21270;&#36807;&#31243;&#35757;&#32451;&#30340;&#22810;&#36335;&#24452;&#31232;&#30095;&#27169;&#22411;&#12290;MS-Net&#22312;&#25512;&#26029;&#38454;&#27573;&#36873;&#25321;&#24615;&#22320;&#28608;&#27963;&#20854;&#21442;&#25968;&#23376;&#38598;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#22330;&#26223;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#23558;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#25277;&#35937;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00353v1 Announce Type: cross  Abstract: The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in different scenarios, which may result in sub-optimal results for one individual scene. To address this issue, we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse model trained by an evolutionary process. MS-Net selectively activates a subset of its parameters during the inference stage to produce prediction results for each scene. In the training stage, the motion prediction task under differentiated scenes is abstracted
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.00336</link><description>&lt;p&gt;
&#27704;&#19981;&#20572;&#27490;&#30340;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Never-Ending Embodied Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#36523;&#26426;&#22120;&#20154;&#23398;&#20064;&#20195;&#29702;NBCagent&#65292;&#36890;&#36807;&#25216;&#33021;&#29305;&#23450;&#30340;&#28436;&#21270;&#35268;&#21010;&#22120;&#21644;&#25216;&#33021;&#20849;&#20139;&#30340;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#65292;&#23454;&#29616;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#36830;&#32493;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20855;&#36523;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#35270;&#35273;&#35266;&#27979;&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#35270;&#35273;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#22312;&#36866;&#24212;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26410;&#35265;&#20219;&#21153;&#26102;&#65292;&#20250;&#36973;&#21463;&#25805;&#32437;&#24615;&#33021;&#19979;&#38477;&#20197;&#21450;&#25216;&#33021;&#30693;&#35782;&#36951;&#24536;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;NBCagent&#22312;&#20855;&#36523;&#26426;&#22120;&#20154;&#20013;&#25506;&#35752;&#20102;&#19978;&#36848;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#12289;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#27704;&#19981;&#20572;&#27490;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#21487;&#20197;&#19981;&#26029;&#20174;&#29305;&#23450;&#25216;&#33021;&#21644;&#20849;&#20139;&#25216;&#33021;&#23646;&#24615;&#20013;&#23398;&#20064;&#26032;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#35266;&#23519;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#29305;&#23450;&#25216;&#33021;&#19981;&#26029;&#28436;&#21270;&#30340;&#35268;&#21010;&#22120;&#26469;&#36827;&#34892;&#30693;&#35782;&#35299;&#32806;&#65292;&#36825;&#21487;&#20197;&#20174;&#28508;&#22312;&#21644;&#20302;&#31209;&#31354;&#38388;&#20013;&#19981;&#26029;&#21521;&#25105;&#20204;&#30340;NBCagent&#20195;&#29702;&#23884;&#20837;&#26032;&#30340;&#25216;&#33021;&#29305;&#23450;&#30693;&#35782;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#35821;&#20041;&#28210;&#26579;&#27169;&#22359;&#21644;&#19968;&#20010;&#25216;&#33021;&#20849;&#20139;&#34920;&#31034;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00336v1 Announce Type: cross  Abstract: Relying on large language models (LLMs), embodied robots could perform complex multimodal robot manipulation tasks from visual observations with powerful generalization ability. However, most visual behavior-cloning agents suffer from manipulation performance degradation and skill knowledge forgetting when adapting into a series of challenging unseen tasks. We here investigate the above challenge with NBCagent in embodied robots, a pioneering language-conditioned Never-ending Behavior-Cloning agent, which can continually learn observation knowledge of novel robot manipulation skills from skill-specific and skill-shared attributes. Specifically, we establish a skill-specific evolving planner to perform knowledge decoupling, which can continually embed novel skill-specific knowledge in our NBCagent agent from latent and low-rank space. Meanwhile, we propose a skill-shared semantics rendering module and a skill-shared representation disti
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.00329</link><description>&lt;p&gt;
&#22312;&#19981;&#28385;&#36275;&#25463;&#24452;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Learning with Logical Constraints but without Shortcut Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00329
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23558;&#36923;&#36753;&#30693;&#35782;&#32534;&#30721;&#20026;&#39069;&#22806;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#36923;&#36753;&#32422;&#26463;&#25972;&#21512;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#25463;&#24452;&#34394;&#20551;&#22320;&#28385;&#36275;&#20102;&#36923;&#36753;&#32422;&#26463;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#36923;&#36753;&#32422;&#26463;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#36923;&#36753;&#36830;&#25509;&#35789;&#30340;&#21452;&#37325;&#21464;&#37327;&#26469;&#35299;&#20915;&#25463;&#24452;&#28385;&#36275;&#38382;&#39064;&#65292;&#23545;&#32422;&#26463;&#30340;&#28385;&#36275;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#26694;&#26550;&#65292;&#20854;&#20013;&#32534;&#30721;&#30340;&#36923;&#36753;&#32422;&#26463;&#34987;&#34920;&#36798;&#20026;&#19968;&#20010;&#20998;&#24067;&#25439;&#22833;&#65292;&#19982;&#27169;&#22411;&#30340;&#21407;&#22987;&#35757;&#32451;&#25439;&#22833;&#20860;&#23481;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#29305;&#24615;&#65292;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#20854;&#22312;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#32422;&#26463;&#28385;&#36275;&#26041;&#38754;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00329v1 Announce Type: new  Abstract: Recent studies in neuro-symbolic learning have explored the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model's original training loss. The theoretical analysis shows that the proposed approach bears salient properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#26725;&#25509;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.00323</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#30340;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;
&lt;/p&gt;
&lt;p&gt;
Softened Symbol Grounding for Neuro-symbolic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00323
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#26377;&#25928;&#22320;&#26725;&#25509;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#29420;&#31435;&#19990;&#30028;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#32422;&#26463;&#27714;&#35299;&#65292;&#20854;&#25104;&#21151;&#21462;&#20915;&#20110;&#31526;&#21495;&#25509;&#22320;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#21270;&#31526;&#21495;&#25509;&#22320;&#36807;&#31243;&#65292;&#24357;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#26694;&#26550;&#12290;&#25216;&#26415;&#19978;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;(1)&#23558;&#31526;&#21495;&#35299;&#29366;&#24577;&#24314;&#27169;&#20026;Boltzmann&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#29366;&#24577;&#25628;&#32034;&#65292;&#24182;&#20419;&#36827;&#20102;&#32593;&#32476;&#35757;&#32451;&#21644;&#31526;&#21495;&#25512;&#29702;&#20043;&#38388;&#30340;&#20114;&#24800;&#20114;&#21033;&#20132;&#20114;&#65307;(2)&#21033;&#29992;&#25237;&#24433;&#21644;SMT&#35299;&#31639;&#22120;&#30340;&#26032;&#22411;MCMC&#25216;&#26415;&#65292;&#39640;&#25928;&#22320;&#20174;&#26029;&#24320;&#30340;&#31526;&#21495;&#35299;&#31354;&#38388;&#20013;&#37319;&#26679;&#65307;(3)&#19968;&#31181;&#36864;&#28779;&#26426;&#21046;&#65292;&#21487;&#20197;&#25670;&#33073;&#38519;&#20837;&#27425;&#20248;&#31526;&#21495;&#25509;&#22320;&#30340;&#22256;&#22659;&#12290;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00323v1 Announce Type: new  Abstract: Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, whose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from %being trapped into sub-optimal symbol groundings. Experiments with three representative neuro symbolic learning tasks demonstrate that, owining 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;&#65292;&#22914;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#31649;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#36328;&#39046;&#22495;&#20915;&#31574;&#21327;&#35843;&#65292;&#35777;&#26126;&#20102;&#22312;&#22797;&#26434;&#21160;&#24577;&#21830;&#19994;&#29615;&#22659;&#20013;DRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00318</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;: &#36808;&#21521;&#22823;&#22411;&#31649;&#29702;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Solving Management Problems: Towards A Large Management Mode
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00318
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31649;&#29702;&#38382;&#39064;&#65292;&#22914;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#36229;&#36234;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#35299;&#20915;&#31649;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#36328;&#39046;&#22495;&#20915;&#31574;&#21327;&#35843;&#65292;&#35777;&#26126;&#20102;&#22312;&#22797;&#26434;&#21160;&#24577;&#21830;&#19994;&#29615;&#22659;&#20013;DRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;&#24211;&#23384;&#31649;&#29702;&#12289;&#21160;&#24577;&#23450;&#20215;&#21644;&#25512;&#33616;&#22312;&#20869;&#30340;&#31649;&#29702;&#38382;&#39064;&#12290;&#35813;DRL&#26041;&#27861;&#26377;&#28508;&#21147;&#22522;&#20110;&#29305;&#23450;&#30340;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#24320;&#21457;&#20986;&#19968;&#20010;&#22823;&#22411;&#31649;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#24418;&#25104;&#36866;&#29992;&#20110;&#21508;&#31181;&#31649;&#29702;&#20219;&#21153;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#33539;&#24335;&#12290;&#25105;&#20204;&#35797;&#22270;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#35299;&#20915;&#38382;&#39064;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#29983;&#25104;&#24335;&#20915;&#31574;&#36827;&#34892;&#22522;&#30784;&#20915;&#31574;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#21327;&#35843;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#22522;&#20110;DRL&#30340;&#26694;&#26550;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00318v1 Announce Type: new  Abstract: We introduce a deep reinforcement learning (DRL) approach for solving management problems including inventory management, dynamic pricing, and recommendation. This DRL approach has the potential to lead to a large management model based on certain transformer neural network structures, resulting in an artificial general intelligence paradigm for various management tasks. Traditional methods have limitations for solving complex real-world problems, and we demonstrate how DRL can surpass existing heuristic approaches for solving management tasks. We aim to solve the problems in a unified framework, considering the interconnections between different tasks. Central to our methodology is the development of a foundational decision model coordinating decisions across the different domains through generative decision-making. Our experimental results affirm the effectiveness of our DRL-based framework in complex and dynamic business environments.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#20316;&#20026;&#26367;&#20195;&#8220;XAI&#8221;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#28151;&#20081;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.00315</link><description>&lt;p&gt;
&#20999;&#25481;XAI&#20013;&#30340;X: &#20026;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
Axe the X in XAI: A Plea for Understandable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00315
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36777;&#25252;&#20102;&#37319;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#20316;&#20026;&#26367;&#20195;&#8220;XAI&#8221;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#28151;&#20081;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Erasmus&#31561;&#20154;(2021)&#36777;&#25252;&#20102;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#20013;&#26415;&#35821;&#8220;&#35299;&#37322;&#8221;&#23384;&#22312;&#30340;&#27495;&#20041;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21746;&#23398;&#31185;&#23398;&#20013;&#22235;&#31181;&#19981;&#21516;&#30340;&#29616;&#23384;&#35299;&#37322;&#27169;&#24335;&#20043;&#19968;&#26469;&#35299;&#20915;&#65306;&#28436;&#32462;-&#35268;&#33539;&#12289;&#24402;&#32435;-&#32479;&#35745;&#12289;&#22240;&#26524;&#26426;&#26800;&#21644;&#26032;&#26426;&#21046;&#20027;&#20041;&#27169;&#24335;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#23637;&#31034;&#20102;&#20316;&#32773;&#22768;&#31216;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#20687;&#23545;&#24453;&#20219;&#20309;&#33258;&#28982;&#29616;&#35937;&#19968;&#26679;&#24212;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35828;&#27861;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#36824;&#25552;&#20986;&#20102;&#26356;&#19968;&#33324;&#30340;&#35770;&#28857;&#65292;&#35828;&#26126;&#20102;XAI&#25991;&#29486;&#20013;&#30446;&#21069;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#24615;&#27010;&#24565;&#19982;&#20256;&#32479;&#31185;&#23398;&#35299;&#37322;&#27010;&#24565;&#20960;&#20046;&#27809;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#26356;&#26377;&#25104;&#25928;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#8220;&#21487;&#29702;&#35299;&#30340;&#20154;&#24037;&#26234;&#33021;&#8221;&#26631;&#31614;&#65292;&#20197;&#36991;&#20813;&#22260;&#32469;XAI&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#22256;&#24785;&#12290;&#22312;&#31456;&#33410;&#30340;&#21518;&#21322;&#37096;&#20998;&#65292;&#25105;&#20027;&#24352;&#37319;&#29992;&#26356;&#36866;&#21512;&#25198;&#28436;&#26680;&#24515;&#35282;&#33394;&#30340;&#23454;&#29992;&#29702;&#35299;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00315v1 Announce Type: new  Abstract: In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity of the term "explanation" in explainable AI (XAI) can be solved by adopting any of four different extant accounts of explanation in the philosophy of science: the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New Mechanist models. In this chapter, I show that the authors' claim that these accounts can be applied to deep neural networks as they would to any natural phenomenon is mistaken. I also provide a more general argument as to why the notion of explainability as it is currently used in the XAI literature bears little resemblance to the traditional concept of scientific explanation. It would be more fruitful to use the label "understandable AI" to avoid the confusion that surrounds the goal and purposes of XAI. In the second half of the chapter, I argue for a pragmatic conception of understanding that is better suited to play the centra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;GRROOR&#65292;&#21033;&#29992;&#27491;&#20132;&#22238;&#24402;&#21644;&#29305;&#24449;&#21152;&#26435;&#20197;&#20445;&#30041;&#36275;&#22815;&#30340;&#32479;&#35745;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00307</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#30340;&#27491;&#20132;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Embedded Multi-label Feature Selection via Orthogonal Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;GRROOR&#65292;&#21033;&#29992;&#27491;&#20132;&#22238;&#24402;&#21644;&#29305;&#24449;&#21152;&#26435;&#20197;&#20445;&#30041;&#36275;&#22815;&#30340;&#32479;&#35745;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21560;&#24341;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#23558;&#29305;&#24449;&#23376;&#38598;&#30340;&#25628;&#32034;&#32435;&#20837;&#27169;&#22411;&#20248;&#21270;&#65292;&#20197;&#20934;&#30830;&#35780;&#20272;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#30340;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#36890;&#24120;&#26080;&#27861;&#20445;&#30041;&#36275;&#22815;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#20013;&#30340;&#21028;&#21035;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#24335;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;&#27491;&#20132;&#22238;&#24402;&#20013;&#30340;&#20840;&#23616;&#20887;&#20313;&#21644;&#30456;&#20851;&#24615;&#20248;&#21270;&#65288;GRROOR&#65289;&#65292;&#20197;&#20419;&#36827;&#22810;&#26631;&#31614;&#29305;&#24449;&#36873;&#25321;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#29305;&#24449;&#21152;&#26435;&#30340;&#27491;&#20132;&#22238;&#24402;&#65292;&#22312;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#30041;&#19982;&#22810;&#26631;&#31614;&#25968;&#25454;&#30340;&#23616;&#37096;&#26631;&#31614;&#30456;&#20851;&#24615;&#30456;&#20851;&#30340;&#36275;&#22815;&#32479;&#35745;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#26082;&#32771;&#34385;&#20102;&#20840;&#23616;&#20887;&#20313;&#65292;&#21448;&#32771;&#34385;&#20102;&#30456;&#20851;&#24615;&#20248;&#21270;&#65292;&#20197;&#20419;&#36827;&#20998;&#31867;&#24615;&#33021;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00307v1 Announce Type: cross  Abstract: In the last decade, embedded multi-label feature selection methods, incorporating the search for feature subsets into model optimization, have attracted considerable attention in accurately evaluating the importance of features in multi-label classification tasks. Nevertheless, the state-of-the-art embedded multi-label feature selection algorithms based on least square regression usually cannot preserve sufficient discriminative information in multi-label data. To tackle the aforementioned challenge, a novel embedded multi-label feature selection method, termed global redundancy and relevance optimization in orthogonal regression (GRROOR), is proposed to facilitate the multi-label feature selection. The method employs orthogonal regression with feature weighting to retain sufficient statistical and structural information related to local label correlations of the multi-label data in the feature learning process. Additionally, both glob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00299</link><description>&lt;p&gt;
&#29992;&#20110;MIMO CSI&#21453;&#39304;&#30340;&#36890;&#29992;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Universal Auto-encoder Framework for MIMO CSI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26694;&#26550;&#19987;&#27880;&#20110;&#29305;&#23450;&#37197;&#32622;&#30340;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#21644;&#22522;&#31449;&#65288;BS&#65289;&#65292;&#22240;&#27492;AE&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#21487;&#33021;&#20250;&#26681;&#25454;BS&#21644;UE&#30340;&#22825;&#32447;&#25968;&#37327;&#20197;&#21450;&#22312;&#39057;&#29575;&#32500;&#24230;&#19978;&#20998;&#37197;&#30340;&#36164;&#28304;&#22359;&#25968;&#37327;&#32780;&#21464;&#21270;&#12290;&#25903;&#25345;&#19981;&#21516;&#36755;&#20837;&#21644;&#36755;&#20986;&#22823;&#23567;&#30340;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26159;&#20351;&#29992;&#22810;&#20010;AE&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;UE&#30340;&#26377;&#38480;&#30828;&#20214;&#36164;&#28304;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;AE&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#36755;&#20837;&#22823;&#23567;&#21644;&#22810;&#31181;&#21387;&#32553;&#27604;&#12290;&#25152;&#25552;&#20986;&#30340;AE&#26694;&#26550;&#22312;&#25552;&#20379;&#19982;&#26420;&#32032;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#21487;&#27604;&#30340;&#21387;&#32553;&#27604;-&#22833;&#30495;&#24179;&#34913;&#24615;&#33021;&#30340;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#30828;&#20214;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00299v1 Announce Type: cross  Abstract: Existing auto-encoder (AE)-based channel state information (CSI) frameworks have focused on a specific configuration of user equipment (UE) and base station (BS), and thus the input and output sizes of the AE are fixed. However, in the real-world scenario, the input and output sizes may vary depending on the number of antennas of the BS and UE and the allocated resource block in the frequency dimension. A naive approach to support the different input and output sizes is to use multiple AE models, which is impractical for the UE due to the limited HW resources. In this paper, we propose a universal AE framework that can support different input sizes and multiple compression ratios. The proposed AE framework significantly reduces the HW complexity while providing comparable performance in terms of compression ratio-distortion trade-off compared to the naive and state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#20013;&#30340;&#25104;&#26412;&#21644;&#30456;&#20284;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.00290</link><description>&lt;p&gt;
&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#65306;&#25104;&#26412;-&#30456;&#20284;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Semantic Text Transmission via Prediction with Small Language Models: Cost-Similarity Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#20256;&#36755;&#20013;&#30340;&#25104;&#26412;&#21644;&#30456;&#20284;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#26080;&#22122;&#38899;&#21644;&#23383;&#31526;&#25830;&#38500;&#36890;&#36947;&#19978;&#20174;&#28304;&#21040;&#30446;&#30340;&#22320;&#20256;&#36755;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36890;&#36807;&#20801;&#35768;&#30446;&#30340;&#22320;&#39044;&#27979;&#25110;&#34917;&#20840;&#19982;&#28304;&#25991;&#26412;&#21487;&#33021;&#19981;&#30456;&#20284;&#30340;&#21333;&#35789;&#26469;&#38480;&#21046;&#20256;&#36755;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#21487;&#23454;&#29616;&#30340;$(\bar{c}, \bar{s})$&#23545;&#65292;&#20854;&#20013;$\bar{c}$&#26159;&#28304;&#22836;&#30340;&#24179;&#22343;&#20256;&#36755;&#25104;&#26412;&#65292;$\bar{s}$&#26159;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#28304;&#22836;&#35789;&#21521;&#37327;&#21644;&#30446;&#30340;&#22320;&#39044;&#27979;/&#34917;&#20840;&#35789;&#21521;&#37327;&#20043;&#38388;&#30340;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#20110;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;(SLM)&#36827;&#34892;&#39044;&#27979;&#65292;&#20026;&#20256;&#36755;&#20351;&#29992;&#20102;&#38408;&#20540;&#31574;&#30053;&#65292;&#21363;&#22914;&#26524;&#21333;&#35789;&#19982;&#30446;&#30340;&#22320;&#39044;&#27979;/&#34917;&#20840;&#30340;&#21333;&#35789;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20302;&#20110;&#38408;&#20540;&#65292;&#21017;&#20256;&#36755;&#35813;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00290v1 Announce Type: cross  Abstract: We consider the communication of natural language text from a source to a destination over noiseless and character-erasure channels. We exploit language's inherent correlations and predictability to constrain transmission costs by allowing the destination to predict or complete words with potential dissimilarity with the source text. Concretely, our objective is to obtain achievable $(\bar{c}, \bar{s})$ pairs, where $\bar{c}$ is the average transmission cost at the source and $\bar{s}$ is the average semantic similarity measured via cosine similarity between vector embedding of words at the source and those predicted/completed at the destination. We obtain $(\bar{c}, \bar{s})$ pairs for neural language and first-order Markov chain-based small language models (SLM) for prediction, using both a threshold policy that transmits a word if its cosine similarity with that predicted/completed at the destination is below a threshold, and a peri
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00284</link><description>&lt;p&gt;
&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
A Survey of Route Recommendations: Methods, Applications, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00284
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#65292;&#38543;&#30528;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#37096;&#32626;&#22312;&#25972;&#20010;&#22478;&#24066;&#65292;&#22823;&#37327;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#27491;&#22312;&#20351;&#29616;&#20195;&#22478;&#24066;&#21457;&#23637;&#26234;&#33021;&#21270;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36335;&#32447;&#25512;&#33616;&#21450;&#20854;&#24212;&#29992;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30452;&#25509;&#24433;&#21709;&#24066;&#27665;&#30340;&#20986;&#34892;&#20064;&#24815;&#12290;&#22522;&#20110;&#22823;&#25968;&#25454;&#65288;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#65289;&#24320;&#21457;&#26234;&#33021;&#39640;&#25928;&#30340;&#20986;&#34892;&#36335;&#32447;&#24050;&#25104;&#20026;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#23427;&#20998;&#20026;&#20197;&#19979;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23545;&#22823;&#37327;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#30340;&#21382;&#21490;&#20851;&#31995;&#24182;&#25581;&#31034;&#26368;&#26032;&#36827;&#23637;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#20013;&#36335;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#12290;3&#65289;&#25105;&#20204;&#36842;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#26412;&#22320;&#30340;&#31934;&#35843;&#27169;&#22411;&#65292;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00254</link><description>&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;MRI&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Cloud-based Federated Learning Framework for MRI Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#26412;&#22320;&#30340;&#31934;&#35843;&#27169;&#22411;&#65292;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#20197;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#20892;&#26449;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#35786;&#26029;&#33041;&#37096;&#22270;&#20687;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#25968;&#25454;&#31232;&#32570;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#20854;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#20013;&#24515;&#21270;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20892;&#26449;&#21307;&#30103;&#35774;&#26045;&#30340;&#33041;&#32452;&#32455;&#20998;&#21106;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#20351;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#29615;&#22659;&#21644;&#22312;&#20892;&#26449;&#21307;&#30103;&#32593;&#31449;&#26412;&#22320;&#37096;&#32626;&#30340;&#31934;&#35843;&#27169;&#22411;&#65288;RM&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;DRL&#27169;&#22411;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#20892;&#26449;&#31449;&#28857;&#19978;&#23454;&#29616;&#12290;&#20026;&#20102;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36827;&#34892;&#21512;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00254v1 Announce Type: cross  Abstract: In contemporary rural healthcare settings, the principal challenge in diagnosing brain images is the scarcity of available data, given that most of the existing deep learning models demand extensive training data to optimize their performance, necessitating centralized processing methods that potentially compromise data privacy. This paper proposes a novel framework tailored for brain tissue segmentation in rural healthcare facilities. The framework employs a deep reinforcement learning (DRL) environment in tandem with a refinement model (RM) deployed locally at rural healthcare sites. The proposed DRL model has a reduced parameter count and practicality for implementation across distributed rural sites. To uphold data privacy and enhance model generalization without transgressing privacy constraints, we employ federated learning (FL) for cooperative model training. We demonstrate the efficacy of our approach by training the network wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>&#22312;&#38271;&#23614;&#35782;&#21035;&#20013;&#65292;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;&#32479;&#19968;&#29305;&#24449;&#34920;&#31034;&#30340;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Logits Magnitude&#20316;&#20026;&#26356;&#20339;&#30340;&#24615;&#33021;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.00250</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;Logits Retargeting&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple Logits Retargeting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#23614;&#35782;&#21035;&#20013;&#65292;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;&#32479;&#19968;&#29305;&#24449;&#34920;&#31034;&#30340;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Logits Magnitude&#20316;&#20026;&#26356;&#20339;&#30340;&#24615;&#33021;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#23614;&#35782;&#21035;&#39046;&#22495;&#65292;&#35299;&#32806;&#35757;&#32451;&#33539;&#24335;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#35813;&#33539;&#24335;&#23558;&#35757;&#32451;&#36807;&#31243;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#23581;&#35797;&#21516;&#26102;&#25913;&#36827;&#20004;&#20010;&#38454;&#27573;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#20998;&#31163;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#65292;&#31616;&#21333;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24378;&#35843;&#20102;&#26377;&#24517;&#35201;&#37325;&#26032;&#35780;&#20272;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#32479;&#19968;&#29305;&#24449;&#34920;&#31034;&#30340;&#20998;&#31867;&#22120;&#37325;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#37325;&#26032;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Logits Magnitude&#30340;&#26032;&#24230;&#37327;&#20316;&#20026;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#36234;&#34913;&#37327;&#26631;&#20934;&#65292;&#21462;&#20195;&#24120;&#29992;&#30340;Weight Norm&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30452;&#25509;&#20248;&#21270;&#26032;&#24230;&#37327;&#24456;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#24403;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00250v1 Announce Type: cross  Abstract: In the long-tailed recognition field, the Decoupled Training paradigm has demonstrated remarkable capabilities among various methods. This paradigm decouples the training process into separate representation learning and classifier re-training. Previous works have attempted to improve both stages simultaneously, making it difficult to isolate the effect of classifier re-training. Furthermore, recent empirical studies have demonstrated that simple regularization can yield strong feature representations, emphasizing the need to reassess existing classifier re-training methods. In this study, we revisit classifier re-training methods based on a unified feature representation and re-evaluate their performances. We propose a new metric called Logits Magnitude as a superior measure of model performance, replacing the commonly used Weight Norm. However, since it is hard to directly optimize the new metric during training, we introduce a suita
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00236</link><description>&lt;p&gt;
&#20351;&#29992;FlanT5-XXL&#36827;&#34892;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;&#20174;&#35757;&#32451;&#25968;&#25454;&#12289;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#20013;&#25506;&#35752;&#20854;&#25509;&#36817;SOTA&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from training data, prompting, and decoding strategies into its near-SoTA performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00236
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;FlanT5-XXL&#21644;SemEval 2016&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#21450;&#20854;&#23545;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25935;&#24863;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#33021;&#22815;&#21305;&#25932;&#25110;&#36229;&#36234;&#26368;&#20808;&#36827;&#22522;&#20934;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#22312;&#25512;&#29305;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;FlanT5-XXL&#65292;&#19968;&#20010;&#32463;&#36807;&#35843;&#25972;&#25351;&#20196;&#30340;&#24320;&#28304;LLM&#65292;&#22312;SemEval 2016&#20219;&#21153;6A&#12289;6B&#21644;P-Stance&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#25552;&#31034;&#21644;&#35299;&#30721;&#31574;&#30053;&#19979;&#30340;&#34920;&#29616;&#21450;&#20854;&#21464;&#21270;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#38646;-shot&#26041;&#27861;&#21487;&#20197;&#21305;&#25932;&#29978;&#33267;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20854;&#34920;&#29616;&#30340;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#23545;&#25351;&#20196;&#21644;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#65292;&#35299;&#30721;&#31574;&#30053;&#65292;&#25552;&#31034;&#30340;&#22256;&#24785;&#24230;&#65292;&#20197;&#21450;&#25552;&#31034;&#20013;&#23384;&#22312;&#30340;&#21542;&#23450;&#21644;&#21453;&#23545;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;LLM&#27809;&#26377;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#21487;&#33021;&#37096;&#20998;&#35299;&#37322;&#35299;&#30721;&#31574;&#30053;&#20043;&#38388;&#34920;&#29616;&#24046;&#24322;&#30340;&#31215;&#26497;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00236v1 Announce Type: cross  Abstract: We investigate the performance of LLM-based zero-shot stance detection on tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and its variations under different prompts and decoding strategies, as well as the potential biases of the model. We show that the zero-shot approach can match or outperform state-of-the-art benchmarks, including fine-tuned models. We provide various insights into its performance including the sensitivity to instructions and prompts, the decoding strategies, the perplexity of the prompts, and to negations and oppositions present in prompts. Finally, we ensure that the LLM has not been trained on test datasets, and identify a positivity bias which may partially explain the performance differences across decoding strategie
&lt;/p&gt;</description></item><item><title>AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00198</link><description>&lt;p&gt;
AXOLOTL&#65306;&#36890;&#36807;&#36741;&#21161;&#33258;&#25105;&#21435;&#20559;&#35265;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00198
&lt;/p&gt;
&lt;p&gt;
AXOLOTL&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;&#35782;&#21035;&#21644;&#35299;&#20915;&#20559;&#35265;&#65292;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#24615;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23481;&#26131;&#21463;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#24433;&#21709;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#19981;&#20844;&#24179;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31574;&#30053;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AXOLOTL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#29420;&#31435;&#20110;&#20219;&#21153;&#21644;&#27169;&#22411;&#36816;&#34892;&#65292;&#22312;&#19981;&#30452;&#25509;&#35775;&#38382;&#20869;&#37096;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20844;&#20849;API&#19982;LLMs&#20132;&#20114;&#12290;&#36890;&#36807;&#31867;&#20284;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#19977;&#27493;&#36807;&#31243;&#65292;AXOLOTL&#35782;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#33258;&#25105;&#21435;&#20559;&#35265;&#20854;&#36755;&#20986;&#12290;&#36825;&#31181;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#20445;&#25345;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20351;AXOLOTL&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#26131;&#29992;&#24615;&#30340;&#21435;&#20559;&#35265;LLM&#36755;&#20986;&#30340;&#26377;&#21069;&#26223;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00198v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) have significantly advanced natural language processing capabilities but are susceptible to biases present in their training data, leading to unfair outcomes in various applications. While numerous strategies have been proposed to mitigate bias, they often require extensive computational resources and may compromise model performance. In this work, we introduce AXOLOTL, a novel post-processing framework, which operates agnostically across tasks and models, leveraging public APIs to interact with LLMs without direct access to internal parameters. Through a three-step process resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions, and guides the model to self-debias its outputs. This approach minimizes computational costs and preserves model performance, making AXOLOTL a promising tool for debiasing LLM outputs with broad applicability and ease of use.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#21033;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#20013;&#30340;pix2pix&#26550;&#26500;&#27604;CycleGAN&#34920;&#29616;&#26356;&#22909;&#65292;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#21487;&#20197;&#22686;&#24378;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.00196</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#23398;&#20064;&#25214;&#21040;&#32570;&#22833;&#35270;&#39057;&#24103;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#21450;&#22312;&#20351;&#29992;RGB&#25668;&#20687;&#22836;&#29983;&#25104;&#28909;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Find Missing Video Frames with Synthetic Data Augmentation: A General Framework and Application in Generating Thermal Images Using RGB Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00196
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#21033;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#20013;&#30340;pix2pix&#26550;&#26500;&#27604;CycleGAN&#34920;&#29616;&#26356;&#22909;&#65292;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#21487;&#20197;&#22686;&#24378;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#22312;&#26234;&#33021;&#36710;&#36742;&#20013;&#20381;&#36182;&#20110;&#36710;&#36742;&#36710;&#21410;&#20869;&#30340;&#20934;&#30830;&#39550;&#39542;&#21592;&#24863;&#30693;&#65292;&#36890;&#24120;&#21033;&#29992;&#21508;&#31181;&#20256;&#24863;&#27169;&#24335;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#20197;&#19981;&#21516;&#30340;&#36895;&#29575;&#25805;&#20316;&#65292;&#23545;&#20110;&#23454;&#26102;&#12289;&#20840;&#38754;&#30340;&#39550;&#39542;&#21592;&#29366;&#24577;&#30417;&#27979;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#30001;&#20110;&#20256;&#24863;&#22120;&#24103;&#29575;&#19981;&#21305;&#37197;&#23548;&#33268;&#25968;&#25454;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#26469;&#21019;&#24314;&#21512;&#25104;&#20294;&#36924;&#30495;&#30340;&#28909;&#25104;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#65292;&#20855;&#20307;&#27604;&#36739;&#20102;pix2pix&#21644;CycleGAN&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;pix2pix&#20248;&#20110;CycleGAN&#65292;&#24182;&#19988;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#39118;&#26684;&#65292;&#29305;&#21035;&#26159;&#22534;&#21472;&#35270;&#22270;&#65292;&#22686;&#24378;&#20102;&#28909;&#22270;&#20687;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#20027;&#20307;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#20010;&#24615;&#21270;&#35757;&#32451;&#23545;&#20110;&#26368;&#20339;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00196v1 Announce Type: cross  Abstract: Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on accurate driver perception within the vehicle cabin, often leveraging a combination of sensing modalities. However, these modalities operate at varying rates, posing challenges for real-time, comprehensive driver state monitoring. This paper addresses the issue of missing data due to sensor frame rate mismatches, introducing a generative model approach to create synthetic yet realistic thermal imagery. We propose using conditional generative adversarial networks (cGANs), specifically comparing the pix2pix and CycleGAN architectures. Experimental results demonstrate that pix2pix outperforms CycleGAN, and utilizing multi-view input styles, especially stacked views, enhances the accuracy of thermal image generation. Moreover, the study evaluates the model's generalizability across different subjects, revealing the importance of individualized training for optimal pe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35782;&#21035;&#20449;&#24687;&#20256;&#25773;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#33410;&#28857;&#65292;&#25581;&#31034;&#20102;&#23545;&#32593;&#32476;&#34892;&#20026;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#23545;&#25112;&#30053;&#32593;&#32476;&#20998;&#26512;&#21644;&#20248;&#21270;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.00190</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20449;&#24687;&#20256;&#25773;&#32593;&#32476;&#37325;&#35201;&#33410;&#28857;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Identification of important nodes in the information propagation network based on the artificial intelligence method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35782;&#21035;&#20449;&#24687;&#20256;&#25773;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#33410;&#28857;&#65292;&#25581;&#31034;&#20102;&#23545;&#32593;&#32476;&#34892;&#20026;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#23545;&#25112;&#30053;&#32593;&#32476;&#20998;&#26512;&#21644;&#20248;&#21270;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35782;&#21035;&#20449;&#24687;&#20256;&#25773;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#33410;&#28857;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#23558;&#20915;&#31574;&#35797;&#39564;&#21644;&#35780;&#20272;&#23454;&#39564;&#23460;&#65288;DEMATEL&#65289;&#26041;&#27861;&#19982;&#20840;&#23616;&#32467;&#26500;&#27169;&#22411;&#65288;GSM&#65289;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#20869;&#37096;&#23616;&#37096;&#21644;&#20840;&#23616;&#24433;&#21709;&#30340;&#21327;&#21516;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#32593;&#32476;&#65292;&#22914;&#31038;&#20132;&#12289;&#36816;&#36755;&#21644;&#36890;&#20449;&#31995;&#32479;&#65292;&#21033;&#29992;&#20840;&#29699;&#32593;&#32476;&#24433;&#21709;&#25968;&#25454;&#38598;&#65288;GNID&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#32467;&#26500;&#21160;&#24577;&#24615;&#21644;&#38887;&#24615;&#65292;&#25581;&#31034;&#20102;&#33410;&#28857;&#36830;&#25509;&#24615;&#21644;&#31038;&#21306;&#24418;&#25104;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#23545;&#32593;&#32476;&#34892;&#20026;&#20840;&#38754;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#20110;&#25112;&#30053;&#32593;&#32476;&#20998;&#26512;&#21644;&#20248;&#21270;&#20855;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00190v1 Announce Type: cross  Abstract: This study presents an integrated approach for identifying key nodes in information propagation networks using advanced artificial intelligence methods. We introduce a novel technique that combines the Decision-making Trial and Evaluation Laboratory (DEMATEL) method with the Global Structure Model (GSM), creating a synergistic model that effectively captures both local and global influences within a network. This method is applied across various complex networks, such as social, transportation, and communication systems, utilizing the Global Network Influence Dataset (GNID). Our analysis highlights the structural dynamics and resilience of these networks, revealing insights into node connectivity and community formation. The findings demonstrate the effectiveness of our AI-based approach in offering a comprehensive understanding of network behavior, contributing significantly to strategic network analysis and optimization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Causal Graph Ordinary Differential Equations (CAG-ODE) &#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22788;&#29702;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;ODE&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.00178</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;ODE&#65306;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36830;&#32493;&#22788;&#29702;&#25928;&#24212;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Causal Graph Ordinary Differential Equations (CAG-ODE) &#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#22788;&#29702;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;ODE&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#36890;&#24120;&#26159;&#21160;&#24577;&#21644;&#36830;&#32493;&#30340;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20849;&#21516;&#28436;&#21270;&#65292;&#24182;&#38543;&#26102;&#38388;&#25913;&#21464;&#20854;&#36712;&#36857;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32654;&#22269;&#30340;COVID-19&#20256;&#25773;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#20854;&#20013;&#21508;&#24030;&#20805;&#24403;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#26085;&#24120;&#20154;&#21475;&#27969;&#21160;&#23601;&#26159;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#32467;&#26524;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#26410;&#26469;&#39044;&#27979;&#21644;&#26377;&#25928;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20363;&#22914;&#21046;&#23450;COVID-19&#25919;&#31574;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#26377;&#25928;&#22320;&#27169;&#25311;&#22788;&#29702;&#23545;&#32467;&#26524;&#30340;&#36830;&#32493;&#21160;&#24577;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#21516;&#26102;&#24212;&#29992;&#22810;&#31181;&#22788;&#29702;&#65288;&#20363;&#22914;&#8220;&#23621;&#23478;&#38548;&#31163;&#8221;&#21644;&#8220;&#25509;&#31181;&#30123;&#33495;&#8221;&#25919;&#31574;&#65289;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#22270;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;CAG-ODE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;ODE&#20989;&#25968;&#25429;&#25417;&#26234;&#33021;&#20307;&#20043;&#38388;&#36830;&#32493;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00178v1 Announce Type: cross  Abstract: Real-world multi-agent systems are often dynamic and continuous, where the agents co-evolve and undergo changes in their trajectories and interactions over time. For example, the COVID-19 transmission in the U.S. can be viewed as a multi-agent system, where states act as agents and daily population movements between them are interactions. Estimating the counterfactual outcomes in such systems enables accurate future predictions and effective decision-making, such as formulating COVID-19 policies. However, existing methods fail to model the continuous dynamic effects of treatments on the outcome, especially when multiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) are applied simultaneously. To tackle this challenge, we propose Causal Graph Ordinary Differential Equations (CAG-ODE), a novel model that captures the continuous interaction among agents using a Graph Neural Network (GNN) as the ODE function. The key innovat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#23454;&#29616;&#20102;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#38745;&#24577;&#30830;&#23450;&#65292;&#36827;&#32780;&#36827;&#34892;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#21253;&#25324;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#35745;&#21010;&#21644;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.00176</link><description>&lt;p&gt;
SoD$^2$: &#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SoD$^2$: Statically Optimizing Dynamic Deep Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#26694;&#26550;&#65292;&#29992;&#20110;&#38745;&#24577;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#23454;&#29616;&#20102;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#38745;&#24577;&#30830;&#23450;&#65292;&#36827;&#32780;&#36827;&#34892;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#21253;&#25324;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#35745;&#21010;&#21644;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#24320;&#21457;&#20102;&#35768;&#22810;&#38024;&#23545;DNN&#30340;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31995;&#32479;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;DNN&#19978;&#12290;&#21160;&#24577;DNN&#65292;&#20854;&#20013;&#24352;&#37327;&#24418;&#29366;&#21644;&#22823;&#23567;&#29978;&#33267;&#20351;&#29992;&#30340;&#25805;&#20316;&#31526;&#38598;&#21462;&#20915;&#20110;&#36755;&#20837;&#21644;/&#25110;&#25191;&#34892;&#65292;&#27491;&#22312;&#21464;&#24471;&#24120;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SoD$^2$&#65292;&#19968;&#20010;&#29992;&#20110;&#20248;&#21270;&#21160;&#24577;DNN&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#23545;&#26500;&#25104;DNN&#30340;&#24120;&#35265;&#25805;&#20316;&#31526;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#31209;&#21644;&#32500;&#24230;&#20256;&#25773;&#65288;RDP&#65289;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#38745;&#24577;&#30830;&#23450;&#25805;&#20316;&#31526;&#30340;&#24418;&#29366;&#20026;&#24050;&#30693;&#24120;&#37327;&#12289;&#31526;&#21495;&#24120;&#37327;&#25110;&#36825;&#20123;&#25805;&#20316;&#30340;&#36816;&#31639;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;RDP&#25105;&#20204;&#23454;&#29616;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#22914;&#34701;&#21512;&#20195;&#30721;&#29983;&#25104;&#12289;&#25191;&#34892;&#65288;&#39034;&#24207;&#65289;&#35745;&#21010;&#65292;&#29978;&#33267;&#36816;&#34892;&#26102;&#20869;&#23384;&#20998;&#37197;&#35745;&#21010;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312; 10 &#20010;&#26032;&#20852;&#21160;&#24577;DNN &#19978;&#35780;&#20272;&#35813;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#29616;&#26377;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00176v1 Announce Type: cross  Abstract: Though many compilation and runtime systems have been developed for DNNs in recent years, the focus has largely been on static DNNs. Dynamic DNNs, where tensor shapes and sizes and even the set of operators used are dependent upon the input and/or execution, are becoming common. This paper presents SoD$^2$, a comprehensive framework for optimizing Dynamic DNNs. The basis of our approach is a classification of common operators that form DNNs, and the use of this classification towards a Rank and Dimension Propagation (RDP) method. This framework statically determines the shapes of operators as known constants, symbolic constants, or operations on these. Next, using RDP we enable a series of optimizations, like fused code generation, execution (order) planning, and even runtime memory allocation plan generation. By evaluating the framework on 10 emerging Dynamic DNNs and comparing it against several existing systems, we demonstrate both 
&lt;/p&gt;</description></item><item><title>FusionVision&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#30340;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#22788;&#29702;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#29289;&#20307;&#30340;&#40065;&#26834;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.00175</link><description>&lt;p&gt;
FusionVision&#65306;&#20351;&#29992;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#24847;&#29289;&#20307;&#30340;&#32508;&#21512;&#26041;&#27861;&#36827;&#34892;&#20174;RGB-D&#30456;&#26426;&#37325;&#24314;&#21644;&#20998;&#21106;&#30340;3D&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
FusionVision: A comprehensive approach of 3D object reconstruction and segmentation from RGB-D cameras using YOLO and fast segment anything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00175
&lt;/p&gt;
&lt;p&gt;
FusionVision&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#23558;YOLO&#21644;&#24555;&#36895;&#20998;&#21106;&#20219;&#20309;&#29289;&#20307;&#30340;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#22788;&#29702;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;3D&#29289;&#20307;&#30340;&#40065;&#26834;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#23558;&#20808;&#36827;&#25216;&#26415;&#25972;&#21512;&#21040;RGB-D&#30456;&#26426;&#36755;&#20837;&#22788;&#29702;&#20013;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29615;&#22659;&#26465;&#20214;&#30340;&#22810;&#26679;&#24615;&#21644;&#29289;&#20307;&#22806;&#35266;&#30340;&#21464;&#21270;&#24102;&#26469;&#20102;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;FusionVision&#65292;&#19968;&#31181;&#20026;&#22312;RGB-D&#22270;&#20687;&#20013;&#40065;&#26834;&#22320;&#36827;&#34892;3D&#29289;&#20307;&#20998;&#21106;&#32780;&#35843;&#25972;&#30340;&#35814;&#23613;&#31649;&#36947;&#12290;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#22312;&#21516;&#26102;&#25429;&#25417;&#31934;&#30830;&#30340;&#29289;&#20307;&#36793;&#30028;&#24182;&#22312;&#28145;&#24230;&#22270;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#26159;&#20026;RGB&#25668;&#20687;&#26426;&#25552;&#20986;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;FusionVision&#37319;&#29992;&#20102;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#25216;&#26415;&#19982;&#20808;&#36827;&#30340;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#32452;&#20214;&#30340;&#25972;&#21512;&#20351;&#24471;&#33021;&#22815;&#23545;&#20174;&#24425;&#33394;RGB&#21644;&#28145;&#24230;D&#20449;&#36947;&#33719;&#24471;&#30340;&#20449;&#24687;&#36827;&#34892;&#20840;&#38754;&#32479;&#19968;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00175v1 Announce Type: cross  Abstract: In the realm of computer vision, the integration of advanced techniques into the processing of RGB-D camera inputs poses a significant challenge, given the inherent complexities arising from diverse environmental conditions and varying object appearances. Therefore, this paper introduces FusionVision, an exhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D imagery. Traditional computer vision systems face limitations in simultaneously capturing precise object boundaries and achieving high-precision object detection on depth map as they are mainly proposed for RGB cameras. To address this challenge, FusionVision adopts an integrated approach by merging state-of-the-art object detection techniques, with advanced instance segmentation methods. The integration of these components enables a holistic (unified analysis of information obtained from both color \textit{RGB} and depth \textit{D} channels) interpretation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#37325;&#26032;&#35774;&#35745;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#33410;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00172</link><description>&lt;p&gt;
&#36229;&#36234;&#40657;&#30418;&#31574;&#30053;&#65306;&#37325;&#26032;&#24605;&#32771;&#21487;&#35299;&#37322;&#21644;&#21487;&#39564;&#35777;HVAC&#25511;&#21046;&#23398;&#20064;&#20195;&#29702;&#30340;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Go Beyond Black-box Policies: Rethinking the Design of Learning Agent for Interpretable and Verifiable HVAC Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00172
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#37325;&#26032;&#35774;&#35745;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#33410;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26377;&#28508;&#21147;&#25552;&#39640;&#26262;&#36890;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#33021;&#25928;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#40657;&#30418;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#32570;&#20047;&#21487;&#38752;&#24615;&#20445;&#35777;&#24182;&#23545;&#23621;&#20303;&#32773;&#20581;&#24247;&#26500;&#25104;&#39118;&#38505;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35774;&#35745;&#20351;&#29992;&#29616;&#26377;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#21382;&#21490;&#25968;&#25454;&#25552;&#21462;&#30340;&#20915;&#31574;&#26641;&#30340;HVAC&#25511;&#21046;&#22120;&#65292;&#20811;&#26381;&#20102;&#21487;&#38752;&#24615;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#20915;&#31574;&#26641;&#31574;&#30053;&#26159;&#30830;&#23450;&#24615;&#30340;&#12289;&#21487;&#39564;&#35777;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#19988;&#27604;&#24403;&#21069;&#30340;MBRL&#26041;&#27861;&#26356;&#33410;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;RL&#20195;&#29702;&#22312;HVAC&#25511;&#21046;&#20013;&#30340;&#26032;&#39062;&#39564;&#35777;&#26631;&#20934;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#21487;&#39564;&#35777;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#31574;&#30053;&#25552;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28909;&#21160;&#21147;&#23398;&#27169;&#22411;&#36755;&#20837;&#30340;&#39640;&#32500;&#24230;&#38459;&#30861;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00172v1 Announce Type: cross  Abstract: Recent research has shown the potential of Model-based Reinforcement Learning (MBRL) to enhance energy efficiency of Heating, Ventilation, and Air Conditioning (HVAC) systems. However, existing methods rely on black-box thermal dynamics models and stochastic optimizers, lacking reliability guarantees and posing risks to occupant health. In this work, we overcome the reliability bottleneck by redesigning HVAC controllers using decision trees extracted from existing thermal dynamics models and historical data. Our decision tree-based policies are deterministic, verifiable, interpretable, and more energy-efficient than current MBRL methods. First, we introduce a novel verification criterion for RL agents in HVAC control based on domain knowledge. Second, we develop a policy extraction procedure that produces a verifiable decision tree policy. We found that the high dimensionality of the thermal dynamics model input hinders the efficiency 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#25919;&#27835;&#31185;&#23398;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#65292;&#24182;&#23637;&#31034;Gemini&#22312;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00154</link><description>&lt;p&gt;
&#25919;&#27835;&#31185;&#23398;&#20013;&#30340;LLMs&#65306;&#24320;&#21551;&#35270;&#35273;&#20998;&#26512;&#26032;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
LLMs in Political Science: Heralding a New Era of Visual Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#25919;&#27835;&#31185;&#23398;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#65292;&#24182;&#23637;&#31034;Gemini&#22312;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#23398;&#23478;&#20013;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#35299;&#35835;&#36825;&#20123;&#22270;&#20687;&#30340;&#25361;&#25112;&#22312;&#20110;&#38656;&#35201;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#19987;&#38376;&#30828;&#20214;&#30340;&#35775;&#38382;&#12290;&#22240;&#27492;&#65292;&#22270;&#20687;&#20998;&#26512;&#19968;&#30452;&#23616;&#38480;&#20110;&#25919;&#27835;&#31185;&#23398;&#30028;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#20154;&#32676;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#65292;&#36825;&#31181;&#24773;&#20917;&#26377;&#21487;&#33021;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#20351;&#29992;Gemini&#36827;&#34892;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#24847;&#35782;&#12290;&#23545;688&#24133;&#22270;&#20687;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#22238;&#39038;&#24615;&#20998;&#26512;&#12290;&#23545;&#27599;&#24133;&#22270;&#20687;&#20174;Gemini&#20013;&#33719;&#21462;&#20869;&#23481;&#25253;&#21578;&#65292;&#28982;&#21518;&#30001;&#20316;&#32773;&#25163;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Gemini&#22312;&#25191;&#34892;&#23545;&#35937;&#26816;&#27979;&#26041;&#38754;&#38750;&#24120;&#20934;&#30830;&#65292;&#36825;&#22312;&#25919;&#27835;&#31185;&#23398;&#20013;&#26159;&#21487;&#33021;&#26368;&#24120;&#35265;&#21644;&#22522;&#26412;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;Gemini&#36827;&#34892;&#22270;&#20687;&#20869;&#23481;&#20998;&#26512;&#26159;&#31616;&#21333;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00154v1 Announce Type: cross  Abstract: Interest is increasing among political scientists in leveraging the extensive information available in images. However, the challenge of interpreting these images lies in the need for specialized knowledge in computer vision and access to specialized hardware. As a result, image analysis has been limited to a relatively small group within the political science community. This landscape could potentially change thanks to the rise of large language models (LLMs). This paper aims to raise awareness of the feasibility of using Gemini for image content analysis. A retrospective analysis was conducted on a corpus of 688 images. Content reports were elicited from Gemini for each image and then manually evaluated by the authors. We find that Gemini is highly accurate in performing object detection, which is arguably the most common and fundamental task in image analysis for political scientists. Equally important, we show that it is easy to im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.00144</link><description>&lt;p&gt;
EBBS: &#19968;&#20010;&#20855;&#26377;&#21452;&#23618;&#26463;&#25628;&#32034;&#30340;&#38598;&#25104;&#26041;&#27861;&#29992;&#20110;&#38646;&#32763;&#35793;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;EBBS&#65292;&#37197;&#21512;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#24182;&#23454;&#29616;&#30693;&#35782;&#33976;&#39311;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#29992;&#29305;&#23450;&#30340;&#32763;&#35793;&#26041;&#21521;&#35757;&#32451;&#22810;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38646;&#32763;&#35793;&#30340;&#33021;&#21147;&#23601;&#20250;&#20986;&#29616;&#65307;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#22312;&#26410;&#35265;&#36807;&#30340;&#26041;&#21521;&#36827;&#34892;&#32763;&#35793;&#12290;&#21478;&#22806;&#65292;&#38646;&#32763;&#35793;&#20063;&#21487;&#20197;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#33521;&#35821;&#65289;&#26469;&#23454;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#37117;&#23384;&#22312;&#22122;&#38899;&#65292;&#24182;&#19988;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EBBS&#65292;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#21452;&#23618;&#26463;&#25628;&#32034;&#31639;&#27861;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38598;&#25104;&#32452;&#20214;&#22312;&#19979;&#23618;&#36880;&#27493;&#25506;&#32034;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#19978;&#23618;&#30340;&#8220;&#36719;&#25237;&#31080;&#8221;&#26426;&#21046;&#36827;&#34892;&#21516;&#27493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EBBS&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#21644;&#36890;&#36807;&#31532;&#19977;&#31181;&#35821;&#35328;&#36827;&#34892;&#30340;&#32763;&#35793;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38598;&#25104;&#30340;&#30693;&#35782;&#20256;&#22238;&#21040;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65307;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;E
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00144v1 Announce Type: cross  Abstract: The ability of zero-shot translation emerges when we train a multilingual model with certain translation directions; the model can then directly translate in unseen directions. Alternatively, zero-shot translation can be accomplished by pivoting through a third language (e.g., English). In our work, we observe that both direct and pivot translations are noisy and achieve less satisfactory performance. We propose EBBS, an ensemble method with a novel bi-level beam search algorithm, where each ensemble component explores its own prediction step by step at the lower level but they are synchronized by a "soft voting" mechanism at the upper level. Results on two popular multilingual translation datasets show that EBBS consistently outperforms direct and pivot translations as well as existing ensemble techniques. Further, we can distill the ensemble's knowledge back to the multilingual model to improve inference efficiency; profoundly, our E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;</title><link>https://arxiv.org/abs/2403.00143</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#65306;&#26641;&#24179;&#22343;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#21807;&#19968;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29616;&#26377;&#19981;&#36830;&#32493;&#35299;&#26512;&#22120;&#30340;&#19981;&#21516;&#36816;&#34892;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#39044;&#27979;&#26641;&#26469;&#31283;&#23450;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#20108;&#20803;&#24615;&#21644;&#36830;&#32493;&#24615;&#35774;&#32622;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26641;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#20998;&#26512;&#65288;&#20197;P&#21644;NP&#23436;&#20840;&#20026;&#21333;&#20301;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#30830;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#25152;&#26377;&#26679;&#26412;&#36816;&#34892;&#26102;&#38388;&#22343;&#21512;&#29702;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EROS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#24378;&#35843;&#20102;&#21253;&#21547;&#20851;&#38190;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#21644;&#32452;&#32455;&#29702;&#30001;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00141</link><description>&lt;p&gt;
EROS&#65306;&#22522;&#20110;&#23454;&#20307;&#30340;&#21463;&#25511;&#25919;&#31574;&#25991;&#20214;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
EROS: Entity-Driven Controlled Policy Document Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EROS&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26174;&#33879;&#25913;&#21892;&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615;&#65292;&#24378;&#35843;&#20102;&#21253;&#21547;&#20851;&#38190;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#21644;&#32452;&#32455;&#29702;&#30001;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#25991;&#20214;&#22312;&#21521;&#20010;&#20154;&#20171;&#32461;&#32452;&#32455;&#23545;&#29992;&#25143;&#20010;&#20154;&#25968;&#25454;&#30340;&#25910;&#38598;&#12289;&#20351;&#29992;&#21644;&#20445;&#25252;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20197;&#38271;&#31687;&#12289;&#22797;&#26434;&#21644;&#26214;&#28073;&#30340;&#35821;&#35328;&#32780;&#38395;&#21517;&#65292;&#23588;&#20854;&#28041;&#21450;&#19982;&#38544;&#31169;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#23545;&#35797;&#22270;&#29702;&#35299;&#32452;&#32455;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#30340;&#29992;&#25143;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#21463;&#25511;&#25277;&#35937;&#25688;&#35201;&#26469;&#22686;&#24378;&#25919;&#31574;&#25991;&#20214;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#35835;&#24615; -- &#25105;&#20204;&#24378;&#21046;&#29983;&#25104;&#30340;&#25688;&#35201;&#21253;&#25324;&#20851;&#38190;&#30340;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#65288;&#22914;&#25968;&#25454;&#21644;&#23186;&#20307;&#65289;&#20197;&#21450;&#32452;&#32455;&#30340;&#29702;&#30001;&#65288;&#22914;&#30446;&#26631;&#21644;&#21407;&#22240;&#65289;&#22312;&#25910;&#38598;&#36825;&#20123;&#23454;&#20307;&#26102;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;PD-Sum&#65292;&#19968;&#20010;&#24102;&#26377;&#26631;&#35760;&#30340;&#38544;&#31169;&#30456;&#20851;&#23454;&#20307;&#26631;&#31614;&#30340;&#25919;&#31574;&#25991;&#20214;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;EROS&#36890;&#36807;&#22522;&#20110;&#36328;&#24230;&#30340;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#35782;&#21035;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00141v1 Announce Type: cross  Abstract: Privacy policy documents have a crucial role in educating individuals about the collection, usage, and protection of users' personal data by organizations. However, they are notorious for their lengthy, complex, and convoluted language especially involving privacy-related entities. Hence, they pose a significant challenge to users who attempt to comprehend organization's data usage policy. In this paper, we propose to enhance the interpretability and readability of policy documents by using controlled abstractive summarization -- we enforce the generated summaries to include critical privacy-related entities (e.g., data and medium) and organization's rationale (e.g.,target and reason) in collecting those entities. To achieve this, we develop PD-Sum, a policy-document summarization dataset with marked privacy-related entity labels. Our proposed model, EROS, identifies critical entities through a span-based entity extraction model and em
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#20026;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#20248;&#21270;</title><link>https://arxiv.org/abs/2403.00116</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#32447;&#24615;&#19978;&#19979;&#25991;&#36172;&#33218;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Contextual Bandits with Heterogeneous Clients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00116
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#23458;&#25143;&#30340;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#19979;&#20026;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20998;&#24067;&#24335;&#31995;&#32479;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#36328;&#22810;&#20010;&#20195;&#29702;&#36827;&#34892;&#21327;&#21516;&#21644;&#31169;&#23494;&#30340;&#36172;&#33218;&#23398;&#20064;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#31169;&#23494;&#12289;&#39640;&#25928;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20381;&#36182;&#20110;&#23458;&#25143;&#21516;&#36136;&#24615;&#30340;&#24378;&#20551;&#35774;&#65292;&#21363;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#37117;&#24212;&#20849;&#20139;&#30456;&#21516;&#30340;&#36172;&#33218;&#27169;&#22411;&#65307;&#21542;&#21017;&#65292;&#23427;&#20204;&#23558;&#36973;&#21463;&#32447;&#24615;&#21518;&#24724;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#32852;&#37030;&#36172;&#33218;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#24322;&#26500;&#23458;&#25143;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#36172;&#33218;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#23545;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#65292;&#29992;&#20110;&#21327;&#21516;&#36172;&#33218;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#23545;&#25152;&#26377;&#23458;&#25143;&#32780;&#35328;&#30340;&#38750;&#24179;&#20961;&#27425;&#32447;&#24615;&#21518;&#24724;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#31526;&#21512;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#22312;&#20219;&#20309;&#26102;&#20505;&#21482;&#26377;&#19968;&#20010;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00116v1 Announce Type: cross  Abstract: The demand for collaborative and private bandit learning across multiple agents is surging due to the growing quantity of data generated from distributed systems. Federated bandit learning has emerged as a promising framework for private, efficient, and decentralized online learning. However, almost all previous works rely on strong assumptions of client homogeneity, i.e., all participating clients shall share the same bandit model; otherwise, they all would suffer linear regret. This greatly restricts the application of federated bandit learning in practice. In this work, we introduce a new approach for federated bandits for heterogeneous clients, which clusters clients for collaborative bandit learning under the federated learning setting. Our proposed algorithm achieves non-trivial sub-linear regret and communication cost for all clients, subject to the communication protocol under federated learning that at anytime only one model c
&lt;/p&gt;</description></item><item><title>LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.00108</link><description>&lt;p&gt;
&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#65281;&#22312;Share-and-Play&#22330;&#26223;&#19979;&#31359;&#36879;LLM&#23433;&#20840;
&lt;/p&gt;
&lt;p&gt;
LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00108
&lt;/p&gt;
&lt;p&gt;
LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#28183;&#36879;LLM&#23433;&#20840;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#19979;&#21487;&#33021;&#23454;&#29616;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#23545;&#20110;&#22686;&#24378;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#30830;&#20445;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#20013;&#65292;LoRA&#22240;&#20854;&#25928;&#29575;&#21644;&#26131;&#29992;&#24615;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#36731;&#26494;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#21457;&#24067;&#21644;&#37319;&#29992;&#36731;&#37327;&#30340;LoRA&#27169;&#22359;&#65292;&#20197;&#23450;&#21046;&#20854;&#27169;&#22411;&#20197;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#20415;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#35774;&#32622;&#25171;&#24320;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;LoRA&#20316;&#20026;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#32972;&#38376;&#27880;&#20837;&#65292;&#24182;&#24191;&#27867;&#20998;&#21457;&#23545;&#25239;&#24615;LoRA&#32473;&#31038;&#21306;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#21518;&#26524;&#12290;&#23613;&#31649;&#20849;&#20139;LoRA&#27169;&#22359;&#23384;&#22312;&#24040;&#22823;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20849;&#20139;&#19982;&#29609;&#32781;&#22330;&#26223;&#20013;&#21487;&#33021;&#20570;&#20986;&#30340;&#25915;&#20987;&#26426;&#20250;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#21518;&#38376;&#27880;&#20837;LoRA&#27169;&#22359;&#24182;&#28145;&#20837;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00108v1 Announce Type: cross  Abstract: Fine-tuning LLMs is crucial to enhancing their task-specific performance and ensuring model behaviors are aligned with human preferences. Among various fine-tuning methods, LoRA is popular for its efficiency and ease to use, allowing end-users to easily post and adopt lightweight LoRA modules on open-source platforms to tailor their model for different customization. However, such a handy share-and-play setting opens up new attack surfaces, that the attacker can render LoRA as an attacker, such as backdoor injection, and widely distribute the adversarial LoRA to the community easily. This can result in detrimental outcomes. Despite the huge potential risks of sharing LoRA modules, this aspect however has not been fully explored. To fill the gap, in this study we thoroughly investigate the attack opportunities enabled in the growing share-and-play scenario. Specifically, we study how to inject backdoor into the LoRA module and dive deep
&lt;/p&gt;</description></item><item><title>Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00071</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#65306;&#20849;&#25391; RoPE
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE: Improving Context Length Generalization of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00071
&lt;/p&gt;
&lt;p&gt;
Resonance RoPE&#26159;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#22330;&#26223;&#19979;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#35757;&#32451;&#30701;-&#27979;&#35797;&#38271;&#65288;TSTL&#65289;&#22330;&#26223;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;Rotary Position Embedding&#65288;RoPE&#65289;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#22312;&#36739;&#30701;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#36739;&#38271;&#24207;&#21015;&#20013;&#36935;&#21040;&#20301;&#32622;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Resonance RoPE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;RoPE&#29305;&#24449;&#30340;&#25554;&#20540;&#26469;&#32553;&#23567;TSTL&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PosGen&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#21512;&#25104;&#22522;&#20934;&#65292;&#19987;&#38376;&#38024;&#23545;TSTL&#22330;&#26223;&#20013;&#30340;&#31934;&#32454;&#34892;&#20026;&#20998;&#26512;&#65292;&#26088;&#22312;&#20174;&#38271;&#19978;&#19979;&#25991;&#20013;&#19981;&#26029;&#22686;&#21152;&#30340;&#20196;&#29260;&#29983;&#25104;&#22256;&#38590;&#21644;&#35782;&#21035;&#26032;&#20196;&#29260;&#20301;&#32622;&#30340;&#25361;&#25112;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24212;&#29992;Resonance RoPE&#21518;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#35782;&#21035;OOD&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00071v1 Announce Type: cross  Abstract: This paper addresses the challenge of train-short-test-long (TSTL) scenarios in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE), where models pre-trained on shorter sequences face difficulty with out-of-distribution (OOD) token positions in longer sequences. We introduce Resonance RoPE, a novel approach designed to narrow the generalization gap in TSTL scenarios by refining the interpolation of RoPE features for OOD positions, significantly improving the model performance without additional online computational costs. Furthermore, we present PosGen, a new synthetic benchmark specifically designed for fine-grained behavior analysis in TSTL scenarios, aiming to isolate the constantly increasing difficulty of token generation on long contexts from the challenges of recognizing new token positions. Our experiments on synthetic tasks show that after applying Resonance RoPE, Transformers recognize OOD position bet
&lt;/p&gt;</description></item><item><title>SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#23450;&#20041;&#20197;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00046
&lt;/p&gt;
&lt;p&gt;
SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LLMs&#20197;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#20294;&#23454;&#38469;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#36739;&#24046;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#35843;&#25972;LLMs&#20197;&#36866;&#24212;&#26032;&#22330;&#26223;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#24403;&#21069;&#20195;&#30721;&#29983;&#25104;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21363;Sample-Efficient adaptation with Error-Driven learning for code generation&#12290;SEED&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#21033;&#29992;&#38169;&#35823;&#20462;&#35746;&#26469;&#20811;&#26381;&#33258;&#36523;&#32570;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEED&#28041;&#21450;&#35782;&#21035;LLMs&#29983;&#25104;&#30340;&#38169;&#35823;&#20195;&#30721;&#65292;&#20351;&#29992;Self-revise&#36827;&#34892;&#20195;&#30721;&#20462;&#35746;&#65292;&#20248;&#21270;&#27169;&#22411;&#24182;&#36845;&#20195;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Dirichlet&#20808;&#39564;&#35268;&#33539;&#21644;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#24314;&#65292;&#25193;&#23637;&#20102;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;&#65288;EPM&#65289;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Gibbs&#37319;&#26679;&#22120;&#26469;&#22788;&#29702;&#21518;&#39564;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.00044</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;MCMC&#25193;&#23637;&#21160;&#24577;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Dirichlet&#20808;&#39564;&#35268;&#33539;&#21644;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#24314;&#65292;&#25193;&#23637;&#20102;&#36793;&#32536;&#21010;&#20998;&#27169;&#22411;&#65288;EPM&#65289;&#20197;&#36866;&#24212;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Gibbs&#37319;&#26679;&#22120;&#26469;&#22788;&#29702;&#21518;&#39564;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00044v1&#23459;&#24067;&#31867;&#22411;:cross &#25688;&#35201;: &#36793;&#32536;&#21010;&#20998;&#27169;&#22411;(EPM)&#26159;&#19968;&#31181;&#20174;&#38745;&#24577;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#21472;&#31038;&#21306;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290; &#22312;EPM&#20013;&#65292;&#37319;&#29992;Gamma&#36807;&#31243;&#65288;GaP&#65289;&#20808;&#39564;&#26469;&#25512;&#26029;&#21512;&#36866;&#30340;&#28508;&#22312;&#31038;&#21306;&#25968;&#37327;&#65292;&#24182;&#19988;&#27599;&#20010;&#39030;&#28857;&#34987;&#36171;&#20104;&#19968;&#20010;Gamma&#20998;&#24067;&#30340;&#27491;&#25104;&#21592;&#21521;&#37327;&#12290; &#23613;&#31649;&#20855;&#26377;&#35768;&#22810;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#65292;EPM&#20013;&#30340;&#25512;&#29702;&#36890;&#24120;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#25191;&#34892;&#65292;&#36825;&#38459;&#27490;&#20102;&#20854;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Dirichlet&#20808;&#39564;&#35268;&#33539;&#34920;&#31034;&#27599;&#20010;&#39030;&#28857;&#30340;&#27491;&#25104;&#21592;&#21521;&#37327;&#26469;&#23558;EPM&#27867;&#21270;&#20197;&#32771;&#34385;&#21160;&#24577;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;Dirichlet&#39532;&#23572;&#21487;&#22827;&#38142;&#26500;&#36896;&#25429;&#33719;&#39030;&#28857;&#30340;&#26102;&#38388;&#28436;&#21464;&#34892;&#20026;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#26045;&#30340;Gibbs&#37319;&#26679;&#22120;&#65292;&#20351;&#29992;&#36127;&#20108;&#39033;&#22686;&#24378;&#25216;&#26415;&#25191;&#34892;&#21518;&#39564;&#35745;&#31639;&#12290; &#23545;&#20110;&#22823;&#32593;&#32476;&#36164;&#26009;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00044v1 Announce Type: cross  Abstract: The edge partition model (EPM) is a generative model for extracting an overlapping community structure from static graph-structured data. In the EPM, the gamma process (GaP) prior is adopted to infer the appropriate number of latent communities, and each vertex is endowed with a gamma distributed positive memberships vector. Despite having many attractive properties, inference in the EPM is typically performed using Markov chain Monte Carlo (MCMC) methods that prevent it from being applied to massive network data. In this paper, we generalize the EPM to account for dynamic enviroment by representing each vertex with a positive memberships vector constructed using Dirichlet prior specification, and capturing the time-evolving behaviour of vertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs sampler is proposed to perform posterior computation using Negative- Binomial augmentation technique. For large network d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00041</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Global and Local Prompts Cooperation via Optimal Transport for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316; via Optimal Transport&#65288;FedOTP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20840;&#23616;&#21644;&#26412;&#22320;&#25552;&#31034;&#30340;&#21512;&#20316;&#65292;&#38024;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#23398;&#20064;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23558;&#36825;&#31181;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25972;&#21512;&#21040;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#24182;&#20419;&#36827;&#23545;&#25968;&#25454;&#19981;&#36275;&#30340;&#23616;&#37096;&#35757;&#32451;&#12290;&#20026;&#20102;&#24212;&#23545;&#24403;&#21069;&#32852;&#37030;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#31995;&#32479;&#21270;&#35299;&#20915;&#20005;&#37325;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#21363;&#28041;&#21450;&#26631;&#31614;&#21644;&#29305;&#24449;&#36716;&#31227;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#32852;&#37030;&#25552;&#31034;&#21512;&#20316;&#65288;FedOTP&#65289;&#65292;&#23427;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#21327;&#20316;&#25552;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#22522;&#30784;&#19978;&#25429;&#25417;&#19981;&#21516;&#30340;&#31867;&#21035;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#25552;&#31034;&#26469;&#25552;&#21462;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20849;&#35782;&#30693;&#35782;&#65292;&#36824;&#23398;&#20064;&#19968;&#20010;&#26412;&#22320;&#25552;&#31034;&#26469;&#25429;&#33719;&#29305;&#23450;&#23458;&#25143;&#31471;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00041v1 Announce Type: cross  Abstract: Prompt learning in pretrained visual-language models has shown remarkable flexibility across various downstream tasks. Leveraging its inherent lightweight nature, recent research attempted to integrate the powerful pretrained models into federated learning frameworks to simultaneously reduce communication costs and promote local training on insufficient data. Despite these efforts, current federated prompt learning methods lack specialized designs to systematically address severe data heterogeneities, e.g., data distribution with both label and feature shifts involved. To address this challenge, we present Federated Prompts Cooperation via Optimal Transport (FedOTP), which introduces efficient collaborative prompt learning strategies to capture diverse category traits on a per-client basis. Specifically, for each client, we learn a global prompt to extract consensus knowledge among clients, and a local prompt to capture client-specific
&lt;/p&gt;</description></item><item><title>FhGenie&#26159;&#19968;&#27454;&#19987;&#20026;&#30830;&#20445;&#20445;&#23494;&#24615;&#32780;&#23450;&#21046;&#30340;&#32842;&#22825;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#30693;&#35782;&#24037;&#20316;&#32773;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#25104;&#20026;&#20854;&#20182;&#32452;&#32455;&#25928;&#20223;&#30340;&#20808;&#39537;&#12290;</title><link>https://arxiv.org/abs/2403.00039</link><description>&lt;p&gt;
FhGenie&#65306;&#19968;&#27454;&#19987;&#20026;&#20844;&#21496;&#21644;&#31185;&#30740;&#20351;&#29992;&#23450;&#21046;&#30340;&#20445;&#23494;&#32842;&#22825;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00039
&lt;/p&gt;
&lt;p&gt;
FhGenie&#26159;&#19968;&#27454;&#19987;&#20026;&#30830;&#20445;&#20445;&#23494;&#24615;&#32780;&#23450;&#21046;&#30340;&#32842;&#22825;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#24110;&#21161;&#30693;&#35782;&#24037;&#20316;&#32773;&#25552;&#39640;&#29983;&#20135;&#21147;&#65292;&#25104;&#20026;&#20854;&#20182;&#32452;&#32455;&#25928;&#20223;&#30340;&#20808;&#39537;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;OpenAI&#21457;&#24067;&#20102;ChatGPT&#20197;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#22522;&#20110;AI&#30340;&#32842;&#22825;&#31995;&#32479;&#26377;&#28508;&#21147;&#25552;&#21319;&#30693;&#35782;&#24037;&#20316;&#32773;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#29983;&#20135;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20813;&#36153;&#20844;&#20849;&#26381;&#21153;&#23384;&#22312;&#25968;&#25454;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#26381;&#21153;&#25552;&#20379;&#21830;&#21487;&#33021;&#20250;&#22312;&#27809;&#26377;&#26126;&#30830;&#30028;&#38480;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#21644;&#20248;&#21270;&#12290;&#21363;&#20351;&#26159;&#22522;&#20110;&#35746;&#38405;&#30340;&#26367;&#20195;&#26041;&#26696;&#26377;&#26102;&#20063;&#32570;&#20047;&#22788;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#35753;Fraunhofer&#21592;&#24037;&#22312;&#30830;&#20445;&#20445;&#23494;&#24615;&#30340;&#21516;&#26102;&#21033;&#29992;&#36825;&#39033;&#25216;&#26415;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#24320;&#21457;&#20102;&#19968;&#27454;&#23450;&#21046;&#30340;&#32842;&#22825;&#20154;&#24037;&#26234;&#33021;&#31216;&#20026;FhGenie&#65288;genie&#25351;&#30340;&#26159;&#19968;&#31181;&#26377;&#30410;&#30340;&#31934;&#28789;&#65289;&#12290;&#22312;&#21457;&#24067;&#20960;&#22825;&#21518;&#65292;&#25104;&#21315;&#19978;&#19975;&#30340;Fraunhofer&#21592;&#24037;&#24320;&#22987;&#20351;&#29992;&#36825;&#39033;&#26381;&#21153;&#12290;&#20316;&#20026;&#22312;&#23454;&#26045;&#36825;&#26679;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#20808;&#39537;&#65292;&#35768;&#22810;&#20854;&#20182;&#32452;&#32455;&#20063;&#32439;&#32439;&#25928;&#20223;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;&#21830;&#19994;&#22823;&#22411;&#35821;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00039v1 Announce Type: cross  Abstract: Since OpenAI's release of ChatGPT, generative AI has received significant attention across various domains. These AI-based chat systems have the potential to enhance the productivity of knowledge workers in diverse tasks. However, the use of free public services poses a risk of data leakage, as service providers may exploit user input for additional training and optimization without clear boundaries. Even subscription-based alternatives sometimes lack transparency in handling user data. To address these concerns and enable Fraunhofer staff to leverage this technology while ensuring confidentiality, we have designed and developed a customized chat AI called FhGenie (genie being a reference to a helpful spirit). Within few days of its release, thousands of Fraunhofer employees started using this service. As pioneers in implementing such a system, many other organizations have followed suit. Our solution builds upon commercial large langu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.00037</link><description>&lt;p&gt;
&#26410;&#26469;&#21457;&#23637;&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#30475;&#19981;&#35265;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evolving to the Future: Unseen Event Adaptive Fake News Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00037
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26410;&#30693;&#20107;&#20214;&#30340;&#36866;&#24212;&#24615;&#20551;&#26032;&#38395;&#26816;&#27979;&#26694;&#26550;FADE&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#21516;&#26102;&#29420;&#31435;&#35757;&#32451;&#20107;&#20214;&#39044;&#27979;&#22120;&#65292;&#26368;&#32456;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24191;&#27867;&#20256;&#25773;&#26085;&#30410;&#23041;&#32961;&#20010;&#20154;&#21644;&#31038;&#20250;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26032;&#38395;&#25253;&#36947;&#36807;&#21435;&#20107;&#20214;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#30446;&#26631;&#26159;&#39044;&#27979;&#21644;&#35782;&#21035;&#26377;&#20851;&#26410;&#26469;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#65292;&#36825;&#20123;&#20107;&#20214;&#36890;&#24120;&#19982;&#36807;&#21435;&#23436;&#20840;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#40065;&#26834;&#24615;&#19981;&#36275;&#65292;&#26080;&#27861;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#20107;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26410;&#26469;&#33258;&#36866;&#24212;&#20107;&#20214;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65288;FADE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#36866;&#24212;&#22686;&#24378;&#31574;&#30053;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30446;&#26631;&#39044;&#27979;&#22120;&#65292;&#20197;&#36827;&#34892;&#26356;&#31283;&#20581;&#30340;&#25972;&#20307;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#29420;&#31435;&#35757;&#32451;&#19968;&#20010;&#20165;&#20107;&#20214;&#30340;&#39044;&#27979;&#22120;&#20197;&#33719;&#24471;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#26368;&#32456;&#39044;&#27979;&#26469;&#36827;&#19968;&#27493;&#20943;&#36731;&#20107;&#20214;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00037v1 Announce Type: cross  Abstract: With the rapid development of social media, the wide dissemination of fake news on social media is increasingly threatening both individuals and society. In the dynamic landscape of social media, fake news detection aims to develop a model trained on news reporting past events. The objective is to predict and identify fake news about future events, which often relate to subjects entirely different from those in the past. However, existing fake detection methods exhibit a lack of robustness and cannot generalize to unseen events. To address this, we introduce Future ADaptive Event-based Fake news Detection (FADE) framework. Specifically, we train a target predictor through an adaptive augmentation strategy and graph contrastive learning to make more robust overall predictions. Simultaneously, we independently train an event-only predictor to obtain biased predictions. Then we further mitigate event bias by obtaining the final prediction
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.00036</link><description>&lt;p&gt;
&#24433;&#21709;Bandits&#65306;&#29992;&#20110;&#24418;&#22609;&#20559;&#22909;&#30340;&#25163;&#33218;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Influencing Bandits: Arm Selection for Preference Shaping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#36890;&#36807;&#35266;&#23519;&#22870;&#21169;&#26469;&#31215;&#26497;&#21644;&#28040;&#26497;&#22320;&#24378;&#21270;&#20154;&#32676;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#19981;&#21516;&#24847;&#35265;&#21160;&#24577;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#24182;&#20998;&#26512;&#20102;&#21518;&#24724;&#65292;&#26368;&#21518;&#35752;&#35770;&#20102;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#20849;&#23384;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38750;&#38745;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#22312;&#36825;&#20854;&#20013;&#20154;&#32676;&#30340;&#20559;&#22909;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24378;&#21270;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22609;&#36896;&#20154;&#32676;&#30340;&#20559;&#22909;&#65292;&#20197;&#26368;&#22823;&#21270;&#25903;&#25345;&#39044;&#23450;&#25163;&#33218;&#30340;&#20154;&#21475;&#27604;&#20363;&#12290;&#23545;&#20110;&#20108;&#20803;&#24847;&#35265;&#30340;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#20004;&#31181;&#24847;&#35265;&#21160;&#24577; -- &#36882;&#20943;&#24377;&#24615;&#65288;&#24314;&#27169;&#20026;&#20855;&#26377;&#22686;&#21152;&#29699;&#25968;&#30340;Polya&#37319;&#26679;&#65289;&#21644;&#24120;&#37327;&#24377;&#24615;&#65288;&#20351;&#29992;&#25237;&#31080;&#32773;&#27169;&#22411;&#65289;&#12290;&#23545;&#20110;&#31532;&#19968;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#25506;&#32034;-&#28982;&#21518;-&#25215;&#35834;&#31574;&#30053;&#21644;&#19968;&#31181;Thompson&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#31181;&#31574;&#30053;&#30340;&#21518;&#24724;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#20998;&#26512;&#21487;&#25512;&#24191;&#21040;&#24120;&#24377;&#24615;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;Thompson&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24403;&#23384;&#22312;&#20004;&#31181;&#20197;&#19978;&#31867;&#22411;&#30340;&#24847;&#35265;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23384;&#22312;&#22810;&#20010;&#25512;&#33616;&#31995;&#32479;&#30340;&#24773;&#20917;&#24341;&#21457;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00036v1 Announce Type: cross  Abstract: We consider a non stationary multi-armed bandit in which the population preferences are positively and negatively reinforced by the observed rewards. The objective of the algorithm is to shape the population preferences to maximize the fraction of the population favouring a predetermined arm. For the case of binary opinions, two types of opinion dynamics are considered -- decreasing elasticity (modeled as a Polya urn with increasing number of balls) and constant elasticity (using the voter model). For the first case, we describe an Explore-then-commit policy and a Thompson sampling policy and analyse the regret for each of these policies. We then show that these algorithms and their analyses carry over to the constant elasticity case. We also describe a Thompson sampling based algorithm for the case when more than two types of opinions are present. Finally, we discuss the case where presence of multiple recommendation systems gives ris
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#24577;&#24433;&#21709;&#21333;&#20107;&#20214;&#23884;&#20837;&#27169;&#22411;&#23545;&#24341;&#29992;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;</title><link>https://arxiv.org/abs/2403.00032</link><description>&lt;p&gt;
&#26102;&#38388;&#26631;&#35760;&#65306;&#21033;&#29992;&#21160;&#24577;&#24433;&#21709;&#21333;&#20107;&#20214;&#23884;&#20837;&#27169;&#22411;&#23545;&#24341;&#29992;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Time to Cite: Modeling Citation Networks using the Dynamic Impact Single-Event Embedding Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21160;&#24577;&#24433;&#21709;&#21333;&#20107;&#20214;&#23884;&#20837;&#27169;&#22411;&#23545;&#24341;&#29992;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31185;&#23398;&#30740;&#31350;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#65292;&#21363;&#31185;&#23398;&#30340;&#31185;&#23398;&#65288;SciSci&#65289;&#65292;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#19968;&#20123;&#36843;&#22312;&#30473;&#30571;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#23398;&#32773;&#20204;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#25512;&#36827;&#31185;&#23398;&#21457;&#23637;&#65292;&#23398;&#31185;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#21644;&#28436;&#21464;&#65292;&#20197;&#21450;&#22914;&#20309;&#37327;&#21270;&#21644;&#39044;&#27979;&#30740;&#31350;&#24433;&#21709;&#21147;&#12290;&#30740;&#31350;SciSci&#30340;&#26680;&#24515;&#26159;&#23545;&#24341;&#29992;&#32593;&#32476;&#30340;&#20998;&#26512;&#12290;&#30446;&#21069;&#24050;&#37319;&#29992;&#20102;&#20004;&#31181;&#33879;&#21517;&#30340;&#24314;&#27169;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#21442;&#25968;&#20998;&#24067;&#35780;&#20272;&#35770;&#25991;&#30340;&#24341;&#29992;&#24433;&#21709;&#21160;&#24577;&#65292;&#21478;&#19968;&#31181;&#26159;&#23558;&#24341;&#29992;&#32593;&#32476;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20197;&#26368;&#20339;&#26041;&#24335;&#34920;&#24449;&#35770;&#25991;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#20174;&#32780;&#34913;&#37327;&#20854;&#24341;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24341;&#29992;&#32593;&#32476;&#26159;&#21160;&#24577;&#21333;&#20107;&#20214;&#32593;&#32476;&#30340;&#19968;&#20010;&#26126;&#26174;&#31034;&#20363;&#65292;&#21363;&#27599;&#20010;&#37197;&#23545;&#20165;&#26377;&#19968;&#20010;&#20107;&#20214;&#65288;&#21363;&#24341;&#29992;&#30340;&#26102;&#38388;&#28857;&#65289;&#12290;&#25105;&#20204;&#30446;&#21069;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20284;&#28982;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00032v1 Announce Type: cross  Abstract: Understanding the structure and dynamics of scientific research, i.e., the science of science (SciSci), has become an important area of research in order to address imminent questions including how scholars interact to advance science, how disciplines are related and evolve, and how research impact can be quantified and predicted. Central to the study of SciSci has been the analysis of citation networks. Here, two prominent modeling methodologies have been employed: one is to assess the citation impact dynamics of papers using parametric distributions, and the other is to embed the citation networks in a latent space optimal for characterizing the static relations between papers in terms of their citations. Interestingly, citation networks are a prominent example of single-event dynamic networks, i.e., networks for which each dyad only has a single event (i.e., the point in time of citation). We presently propose a novel likelihood fun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.00030</link><description>&lt;p&gt;
GraphPub: &#20855;&#26377;&#39640;&#21487;&#29992;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphPub: Generation of Differential Privacy Graph with High Availability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphPub&#30340;&#26032;&#22411;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#22312;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#30340;&#21516;&#26102;&#20445;&#35777;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#22270;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;GNN&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#19978;&#28216;&#25968;&#25454;&#25152;&#26377;&#32773;&#21457;&#24067;&#22270;&#25968;&#25454;&#26102;&#65292;&#24448;&#24448;&#20250;&#23384;&#22312;&#35768;&#22810;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#21253;&#21547;&#20687;&#20010;&#20154;&#30340;&#26379;&#21451;&#21015;&#34920;&#31561;&#25935;&#24863;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;DP&#24212;&#29992;&#22312;&#22270;&#19978;&#24448;&#24448;&#20250;&#24433;&#21709;GNN&#27169;&#22411;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#65292;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#36793;&#20445;&#25252;&#26694;&#26550;GraphPub&#65292;&#21487;&#20197;&#20445;&#25252;&#22270;&#25299;&#25169;&#32467;&#26500;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#22522;&#26412;&#19981;&#21464;&#12290;&#36890;&#36807;&#21453;&#21521;&#23398;&#20064;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26426;&#21046;&#65292;&#25105;&#20204;&#25628;&#32034;&#19968;&#20123;&#23545;&#33410;&#28857;&#29305;&#24449;&#32858;&#21512;&#27809;&#26377;&#22826;&#22823;&#36127;&#38754;&#24433;&#21709;&#30340;&#34394;&#20551;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00030v1 Announce Type: cross  Abstract: In recent years, with the rapid development of graph neural networks (GNN), more and more graph datasets have been published for GNN tasks. However, when an upstream data owner publishes graph data, there are often many privacy concerns, because many real-world graph data contain sensitive information like person's friend list. Differential privacy (DP) is a common method to protect privacy, but due to the complex topological structure of graph data, applying DP on graphs often affects the message passing and aggregation of GNN models, leading to a decrease in model accuracy. In this paper, we propose a novel graph edge protection framework, graph publisher (GraphPub), which can protect graph topology while ensuring that the availability of data is basically unchanged. Through reverse learning and the encoder-decoder mechanism, we search for some false edges that do not have a large negative impact on the aggregation of node features, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#23558;MCVRP&#35270;&#20026;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.00026</link><description>&lt;p&gt;
&#23398;&#20064;&#20132;&#20184;&#65306;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00026
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#23558;MCVRP&#35270;&#20026;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33945;&#29305;&#21033;&#23572;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;MCVRP&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FM-MCVRP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#36817;&#20284;&#35299;&#20915;&#19968;&#20010;&#21464;&#20307;&#30340;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#65292;&#35813;&#38382;&#39064;&#25551;&#36848;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;MCVRP&#39318;&#27425;&#30001;Bengio&#31561;&#20154;&#65288;2021&#65289;&#27491;&#24335;&#25551;&#36848;&#65292;&#23450;&#20041;&#22312;&#19968;&#20010;&#22266;&#23450;&#26377;&#38480;&#30340;&#22270;&#19978;&#65292;&#31867;&#20284;&#20110;&#19968;&#20010;&#22478;&#24066;&#12290;&#27599;&#20010;MCVRP&#23454;&#20363;&#26412;&#36136;&#19978;&#26159;&#36830;&#25509;&#22266;&#23450;&#22270;&#20013;&#38543;&#26426;&#25277;&#26679;&#33410;&#28857;&#30340;&#23376;&#22270;&#65292;&#36825;&#20123;&#33410;&#28857;&#20195;&#34920;&#32473;&#23450;&#26085;&#26399;&#23454;&#38469;&#20132;&#20184;&#38382;&#39064;&#20013;&#28508;&#22312;&#22320;&#22336;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#38382;&#39064;&#32467;&#26500;&#65292;&#23558;MCVRP&#26500;&#24314;&#25104;&#31867;&#20284;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#26550;&#26500;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26694;&#26550;&#20013;&#65292;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00026v1 Announce Type: cross  Abstract: In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a superv
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AerisAI&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#30452;&#25509;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.00023</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;&#23646;&#24615;&#30340;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Auditable Homomorphic-based Decentralized Collaborative AI with Attribute-based Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AerisAI&#30340;&#21487;&#23457;&#35745;&#21516;&#24577;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#21033;&#29992;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#30452;&#25509;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#28040;&#38500;&#20102;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#27010;&#24565;&#24341;&#39046;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;FL&#31995;&#32479;&#22240;&#20026;&#38656;&#35201;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#32780;&#23384;&#22312;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#23613;&#31649;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#26469;&#20445;&#25252;&#25968;&#25454;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#24694;&#21270;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;AI&#26694;&#26550;&#65292;&#21517;&#20026;&#20855;&#26377;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#21644;&#32454;&#31890;&#24230;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#23457;&#35745;&#30340;&#20998;&#24067;&#24335;&#21327;&#20316;AI&#65288;AerisAI&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;AerisAI&#30452;&#25509;&#20351;&#29992;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#26234;&#33021;&#21512;&#32422;&#26469;&#32858;&#21512;&#21152;&#23494;&#21442;&#25968;&#65292;&#25670;&#33073;&#20102;&#38656;&#35201;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#28040;&#38500;&#24046;&#20998;&#38544;&#31169;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00023v1 Announce Type: cross  Abstract: In recent years, the notion of federated learning (FL) has led to the new paradigm of distributed artificial intelligence (AI) with privacy preservation. However, most current FL systems suffer from data privacy issues due to the requirement of a trusted third party. Although some previous works introduce differential privacy to protect the data, however, it may also significantly deteriorate the model performance. To address these issues, we propose a novel decentralized collaborative AI framework, named Auditable Homomorphic-based Decentralised Collaborative AI (AerisAI), to improve security with homomorphic encryption and fine-grained differential privacy. Our proposed AerisAI directly aggregates the encrypted parameters with a blockchain-based smart contract to get rid of the need of a trusted third party. We also propose a brand-new concept for eliminating the negative impacts of differential privacy for model performance. Moreove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#32467;&#21512;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00017</link><description>&lt;p&gt;
&#26397;&#30528;&#35299;&#37322;&#22810;&#30446;&#26631;&#29305;&#24449;&#20851;&#32852;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards Interpreting Multi-Objective Feature Associations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00017
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#32467;&#21512;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#20892;&#19994;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#32452;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#20010;&#29305;&#24449;&#22914;&#20309;&#30456;&#20851;&#24182;&#23545;&#29305;&#23450;&#30446;&#26631;&#30340;&#36129;&#29486;&#26159;&#19982;&#29702;&#35299;&#27599;&#20010;&#29305;&#24449;&#22914;&#20309;&#23545;&#29305;&#23450;&#32467;&#26524;&#36129;&#29486;&#21516;&#31561;&#37325;&#35201;&#30340;&#12290;&#22312;&#39044;&#27979;&#20013;&#35299;&#37322;&#21333;&#20010;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#65307;&#28982;&#32780;&#65292;&#22312;&#22810;&#30446;&#26631;&#39044;&#27979;&#20013;&#65292;&#38590;&#20197;&#33719;&#24471;&#29305;&#24449;&#20540;&#32452;&#21512;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#30340;&#23458;&#35266;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#65292;&#20197;&#25214;&#21040;&#20892;&#19994;&#29615;&#22659;&#20013;&#29305;&#24449;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#35813;&#35774;&#35745;&#30340;&#19968;&#39033;&#26032;&#39062;&#20043;&#22788;&#26159;&#30830;&#23450;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#29305;&#24449;&#35299;&#37322;&#19982;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#20445;&#22312;&#22810;&#30446;&#26631;&#29615;&#22659;&#20013;&#36827;&#34892;&#32452;&#21512;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#21487;&#20197;&#25214;&#21040;&#36817;&#20284;&#30340;&#29305;&#24449;&#20540;&#32452;&#21512;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#20351;&#29992;&#20102;&#20004;&#20010;&#20892;&#19994;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00017v1 Announce Type: cross  Abstract: Understanding how multiple features are associated and contribute to a specific objective is as important as understanding how each feature contributes to a particular outcome. Interpretability of a single feature in a prediction may be handled in multiple ways; however, in a multi-objective prediction, it is difficult to obtain interpretability of a combination of feature values. To address this issue, we propose an objective specific feature interaction design using multi-labels to find the optimal combination of features in agricultural settings. One of the novel aspects of this design is the identification of a method that integrates feature explanations with global sensitivity analysis in order to ensure combinatorial optimization in multi-objective settings. We have demonstrated in our preliminary experiments that an approximate combination of feature values can be found to achieve the desired outcome using two agricultural datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#23478;&#31165;&#31649;&#29702;&#20013;&#26368;&#20248;&#21270;&#38477;&#20302;&#22810;&#31181;&#30149;&#21407;&#20307;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.00016</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#30340;&#32452;&#21512;&#20248;&#21270;&#30340;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Sensitivity Analysis for Objective-Oriented Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#22312;&#23478;&#31165;&#31649;&#29702;&#20013;&#26368;&#20248;&#21270;&#38477;&#20302;&#22810;&#31181;&#30149;&#21407;&#20307;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39282;&#20859;&#29615;&#22659;&#20225;&#19994;&#20195;&#30721;&#65306;2403.00016v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#30149;&#21407;&#20307;&#25511;&#21046;&#26159;&#29616;&#20195;&#23478;&#31165;&#20859;&#27542;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23545;&#20844;&#20849;&#20581;&#24247;&#21644;&#29983;&#20135;&#21147;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26377;&#25928;&#30340;&#23478;&#31165;&#31649;&#29702;&#25514;&#26045;&#21487;&#20197;&#38477;&#20302;&#23478;&#31165;&#32676;&#20307;&#20013;&#30340;&#30149;&#21407;&#20307;&#27700;&#24179;&#65292;&#20174;&#32780;&#36890;&#36807;&#38477;&#20302;&#39135;&#28304;&#24615;&#30142;&#30149;&#30340;&#39118;&#38505;&#26469;&#20419;&#36827;&#39135;&#21697;&#23433;&#20840;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36890;&#36807;&#39044;&#38450;&#33021;&#22815;&#36805;&#36895;&#20256;&#25773;&#24182;&#24433;&#21709;&#32676;&#20307;&#29983;&#38271;&#12289;&#34507;&#20135;&#37327;&#21644;&#25972;&#20307;&#20581;&#24247;&#30340;&#20256;&#26579;&#30149;&#26469;&#25903;&#25345;&#21160;&#29289;&#20581;&#24247;&#21644;&#31119;&#21033;&#12290;&#26412;&#30740;&#31350;&#23558;&#23547;&#25214;&#26368;&#20339;&#31649;&#29702;&#23454;&#36341;&#20197;&#26368;&#23567;&#21270;&#22810;&#31181;&#30149;&#21407;&#20307;&#23384;&#22312;&#35270;&#20026;&#19968;&#20010;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#31649;&#29702;&#35774;&#32622;&#30340;&#21487;&#33021;&#32452;&#21512;&#24314;&#27169;&#20026;&#19968;&#20010;&#35299;&#20915;&#31354;&#38388;&#65292;&#21487;&#36890;&#36807;&#39640;&#25928;&#25506;&#32034;&#20197;&#35782;&#21035;&#26368;&#20248;&#38477;&#20302;&#30149;&#21407;&#20307;&#27700;&#24179;&#30340;&#37197;&#32622;&#12290;&#35813;&#35774;&#35745;&#34701;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#29305;&#24449;&#35299;&#37322;&#21644;&#20840;&#23616;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#20445;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00016v1 Announce Type: cross  Abstract: Pathogen control is a critical aspect of modern poultry farming, providing important benefits for both public health and productivity. Effective poultry management measures to reduce pathogen levels in poultry flocks promote food safety by lowering risks of food-borne illnesses. They also support animal health and welfare by preventing infectious diseases that can rapidly spread and impact flock growth, egg production, and overall health. This study frames the search for optimal management practices that minimize the presence of multiple pathogens as a combinatorial optimization problem. Specifically, we model the various possible combinations of management settings as a solution space that can be efficiently explored to identify configurations that optimally reduce pathogen levels. This design incorporates a neural network feedback-based method that combines feature explanations with global sensitivity analysis to ensure combinatorial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GIN-SD&#26694;&#26550;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#35299;&#20915;&#20102;&#22312;&#22270;&#20013;&#26816;&#27979;&#20855;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00014</link><description>&lt;p&gt;
GIN-SD: &#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#22312;&#22270;&#20013;&#26816;&#27979;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GIN-SD&#26694;&#26550;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#35299;&#20915;&#20102;&#22312;&#22270;&#20013;&#26816;&#27979;&#20855;&#26377;&#19981;&#23436;&#25972;&#33410;&#28857;&#30340;&#26469;&#28304;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20013;&#26816;&#27979;&#28304;&#24050;&#32463;&#22312;&#35875;&#35328;&#28304;&#35782;&#21035;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25928;&#21147;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#35201;&#27714;&#23436;&#25972;&#30340;&#29992;&#25143;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#36890;&#36807;&#19981;&#23436;&#25972;&#30340;&#29992;&#25143;&#25968;&#25454;&#36827;&#34892;&#35875;&#35328;&#28304;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;GIN-SD&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#21644;&#20851;&#27880;&#34701;&#21512;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20301;&#32622;&#23884;&#20837;&#27169;&#22359;&#26469;&#21306;&#20998;&#19981;&#23436;&#25972;&#30340;&#33410;&#28857;&#65292;&#24182;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#26469;&#20851;&#27880;&#20855;&#26377;&#26356;&#22823;&#20449;&#24687;&#20256;&#36755;&#33021;&#21147;&#30340;&#33410;&#28857;&#12290;&#20026;&#20102;&#32531;&#35299;&#30001;&#20110;&#28304;&#33410;&#28857;&#21644;&#38750;&#28304;&#33410;&#28857;&#25968;&#37327;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#23548;&#33268;&#30340;&#39044;&#27979;&#20559;&#24046;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#31867;&#24179;&#34913;&#26426;&#21046;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;GIN-SD&#21450;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00014v1 Announce Type: cross  Abstract: Source detection in graphs has demonstrated robust efficacy in the domain of rumor source identification. Although recent solutions have enhanced performance by leveraging deep neural networks, they often require complete user data. In this paper, we address a more challenging task, rumor source detection with incomplete user data, and propose a novel framework, i.e., Source Detection in Graphs with Incomplete Nodes via Positional Encoding and Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach utilizes a positional embedding module to distinguish nodes that are incomplete and employs a self-attention mechanism to focus on nodes with greater information transmission capacity. To mitigate the prediction bias caused by the significant disparity between the numbers of source and non-source nodes, we also introduce a class-balancing mechanism. Extensive experiments validate the effectiveness of GIN-SD and its su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21453;&#20107;&#23454;&#35299;&#37322;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#25552;&#20379;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.00011</link><description>&lt;p&gt;
&#24341;&#20837;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;
&lt;/p&gt;
&lt;p&gt;
Introducing User Feedback-based Counterfactual Explanations (UFCE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21453;&#20107;&#23454;&#35299;&#37322;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22686;&#24378;&#25552;&#20379;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#24120;&#24120;&#20351;&#24471;&#35299;&#37322;&#20854;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CEs&#65289;&#24050;&#32463;&#25104;&#20026;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;CE&#25552;&#20379;&#32473;&#29992;&#25143;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#30340;&#36755;&#20837;&#20462;&#25913;&#23454;&#29616;&#25152;&#26399;&#26395;&#30340;&#32467;&#26524;&#30340;&#21487;&#25805;&#20316;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;CE&#31639;&#27861;&#36890;&#24120;&#22312;&#20248;&#21270;&#21464;&#21270;&#20197;&#36991;&#20813;&#19981;&#26399;&#26395;&#30340;&#32467;&#26524;&#26102;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#20869;&#36816;&#34892;&#65292;&#24573;&#35270;&#20102;&#23545;&#32467;&#26524;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#30340;&#35782;&#21035;&#65292;&#24182;&#24573;&#35270;&#20102;&#24314;&#35758;&#21464;&#21270;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#34987;&#21629;&#21517;&#20026;&#22522;&#20110;&#29992;&#25143;&#21453;&#39304;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;UFCE&#65289;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#26088;&#22312;&#22686;&#24378;&#23545;&#25152;&#25552;&#20379;&#35299;&#37322;&#30340;&#20449;&#24515;&#12290;UFCE&#20801;&#35768;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00011v1 Announce Type: cross  Abstract: Machine learning models are widely used in real-world applications. However, their complexity makes it often challenging to interpret the rationale behind their decisions. Counterfactual explanations (CEs) have emerged as a viable solution for generating comprehensible explanations in eXplainable Artificial Intelligence (XAI). CE provides actionable information to users on how to achieve the desired outcome with minimal modifications to the input. However, current CE algorithms usually operate within the entire feature space when optimizing changes to turn over an undesired outcome, overlooking the identification of key contributors to the outcome and disregarding the practicality of the suggested changes. In this study, we introduce a novel methodology, that is named as user feedback-based counterfactual explanation (UFCE), which addresses these limitations and aims to bolster confidence in the provided explanations. UFCE allows for t
&lt;/p&gt;</description></item><item><title>TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19467</link><description>&lt;p&gt;
TV-TREES&#65306;&#29992;&#20110;&#31070;&#32463;&#31526;&#21495;&#35270;&#39057;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;
&lt;/p&gt;
&lt;p&gt;
TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19467
&lt;/p&gt;
&lt;p&gt;
TV-TREES&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#30005;&#35270;&#21098;&#36753;&#31561;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#38382;&#31572;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#37096;&#20998;&#26159;&#22240;&#20026;&#24403;&#21069;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#25512;&#29702;&#65292;&#22312;&#22788;&#29702;&#38271;&#36755;&#20837;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TV-TREES&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#22120;&#12290;TV-TREES&#20316;&#20026;&#19968;&#31181;&#20419;&#36827;&#21487;&#35299;&#37322;&#32852;&#21512;&#27169;&#24577;&#25512;&#29702;&#30340;&#35270;&#39057;&#29702;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#35270;&#39057;&#30452;&#25509;&#34164;&#28085;&#30340;&#31616;&#21333;&#21069;&#25552;&#19982;&#39640;&#32423;&#32467;&#35770;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#26641;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#34164;&#28085;&#26641;&#29983;&#25104;&#20219;&#21153;&#26469;&#35780;&#20272;&#27492;&#31867;&#26041;&#27861;&#30340;&#25512;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;TVQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#30340;&#12289;&#20855;&#26377;&#26368;&#20808;&#36827;&#38646;-shot&#24615;&#33021;&#30340;&#23436;&#25972;&#35270;&#39057;&#21098;&#36753;&#65292;&#23637;&#31034;&#20102;&#19982;&#40657;&#30418;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19467v1 Announce Type: cross  Abstract: It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.
&lt;/p&gt;</description></item><item><title>$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;</title><link>https://arxiv.org/abs/2402.19457</link><description>&lt;p&gt;
$\texttt{COSMIC}$: &#30456;&#20114;&#20449;&#24687;&#29992;&#20110;&#20219;&#21153;&#26080;&#20851;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19457
&lt;/p&gt;
&lt;p&gt;
$\texttt{COSMIC}$&#26159;&#19968;&#31181;&#20197;&#30456;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#26032;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#65292;&#26377;&#25928;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#24378;&#12290;&#31454;&#20105;&#24615;&#33021;&#20248;&#20110;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24635;&#32467;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26681;&#25454;&#24635;&#32467;&#22120;&#29983;&#25104;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#19988;&#20445;&#30041;&#20219;&#21153;&#32467;&#26524;&#30340;&#25688;&#35201;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#36825;&#20123;&#20219;&#21153;&#30340;&#32467;&#26524;&#38169;&#35823;&#27010;&#29575;&#19982;&#28304;&#25991;&#26412;&#21644;&#29983;&#25104;&#25688;&#35201;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;$\texttt{COSMIC}$&#20316;&#20026;&#36825;&#19968;&#24230;&#37327;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#23637;&#31034;&#20102;&#23427;&#19982;&#22522;&#20110;&#20154;&#31867;&#21028;&#26029;&#30340;&#24230;&#37327;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#24050;&#24314;&#31435;&#30340;&#24230;&#37327;&#22914;$\texttt{BERTScore}$&#21644;$\texttt{ROUGE}$&#30340;&#27604;&#36739;&#20998;&#26512;&#20984;&#26174;&#20102;$\texttt{COSMIC}$&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19457v1 Announce Type: cross  Abstract: Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$ highlight the competitive performance of $\texttt{COSMIC}$.
&lt;/p&gt;</description></item><item><title>PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.19422</link><description>&lt;p&gt;
PEM&#65306;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
PEM: Prototype-based Efficient MaskFormer for Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19422
&lt;/p&gt;
&lt;p&gt;
PEM&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65292;&#36890;&#36807;&#24341;&#20837;&#21407;&#22411;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#30340;&#39640;&#25928;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#22312;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#65292;&#23427;&#20204;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22914;&#35821;&#20041;&#20998;&#21106;&#21644;&#20840;&#26223;&#20998;&#21106;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#19979;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#39640;&#25928;MaskFormer&#65288;PEM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#21106;&#20219;&#21153;&#20013;&#36816;&#34892;&#30340;&#39640;&#25928;transformer&#26550;&#26500;&#12290;PEM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#21033;&#29992;&#35270;&#35273;&#29305;&#24449;&#30340;&#20887;&#20313;&#24615;&#26469;&#38480;&#21046;&#35745;&#31639;&#24182;&#25552;&#39640;&#25928;&#29575;&#32780;&#19981;&#25439;&#23475;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;PEM&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#21462;&#20855;&#26377;&#39640;&#35821;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19422v1 Announce Type: cross  Abstract: Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;Dropout&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;Dropout&#24341;&#20837;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.18929</link><description>&lt;p&gt;
&#36229;&#36234;Dropout&#65306;&#36890;&#21521;&#21487;&#25512;&#24191;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#24341;&#20154;&#27880;&#30446;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;Dropout&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;Dropout&#24341;&#20837;&#30340;&#19981;&#33391;&#21103;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#36817;&#24180;&#26469;&#22312;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#20102;&#31616;&#21333;&#19988;&#22266;&#23450;&#30340;&#38477;&#32423;&#27169;&#22411;&#65288;&#27604;&#22914;&#21452;&#19977;&#27425;&#19979;&#37319;&#26679;&#65289;&#65292;&#20294;&#30450;SR&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26410;&#30693;&#38477;&#32423;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;Kong&#31561;&#20154;&#39318;&#27425;&#25506;&#35752;&#20102;&#20351;&#29992;Dropout&#36827;&#34892;&#30450;SR&#26356;&#21512;&#36866;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#30830;&#23454;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#27867;&#21270;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;Dropout&#21516;&#26102;&#24341;&#20837;&#20102;&#19981;&#33391;&#21103;&#20316;&#29992;&#65292;&#25439;&#23475;&#20102;&#27169;&#22411;&#24544;&#23454;&#37325;&#26500;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#23637;&#31034;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#31616;&#21333;&#35843;&#33410;&#27169;&#22411;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18929v1 Announce Type: cross  Abstract: Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-orde
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#21644;&#34394;&#25311;&#27835;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26356;&#24191;&#27867;&#30340;&#25509;&#35302;&#26426;&#20250;&#65292;&#20294;&#22312;&#23454;&#26045;&#20013;&#38656;&#35201;&#24179;&#34913;&#25928;&#29575;&#21644;&#21516;&#29702;&#24515;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#22987;&#32456;&#26159;&#30001;&#21307;&#25252;&#20154;&#21592;&#30340;&#26234;&#24935;&#25351;&#23548;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.18826</link><description>&lt;p&gt;
&#26426;&#22120;&#26080;&#27861;&#21462;&#20195;&#20154;&#31867;&#30340;&#24515;&#28789;
&lt;/p&gt;
&lt;p&gt;
The Machine Can't Replace the Human Heart
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18826
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#34394;&#25311;&#27835;&#30103;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26356;&#24191;&#27867;&#30340;&#25509;&#35302;&#26426;&#20250;&#65292;&#20294;&#22312;&#23454;&#26045;&#20013;&#38656;&#35201;&#24179;&#34913;&#25928;&#29575;&#21644;&#21516;&#29702;&#24515;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#22987;&#32456;&#26159;&#30001;&#21307;&#25252;&#20154;&#21592;&#30340;&#26234;&#24935;&#25351;&#23548;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18826v1 Announce Type: cross  Abstract: What is the true heart of mental healthcare -- innovation or humanity? Can virtual therapy ever replicate the profound human bonds where healing arises? As artificial intelligence and immersive technologies promise expanded access, safeguards must ensure technologies remain supplementary tools guided by providers' wisdom. Implementation requires nuance balancing efficiency and empathy. If conscious of ethical risks, perhaps AI could restore humanity by automating tasks, giving providers more time to listen. Yet no algorithm can replicate the seat of dignity within. We must ask ourselves: What future has people at its core? One where AI thoughtfully plays a collaborative role? Or where pursuit of progress leaves vulnerability behind? This commentary argues for a balanced approach thoughtfully integrating technology while retaining care's irreplaceable human essence, at the heart of this profoundly human profession. Ultimately, by nurtur
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.17978</link><description>&lt;p&gt;
&#24819;&#35937;&#12289;&#21021;&#22987;&#21270;&#21644;&#25506;&#32034;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#22312;&#22797;&#26434;&#21327;&#35843;&#20219;&#21153;&#20013;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#26368;&#20339;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#23454;&#29616;&#25215;&#35834;&#30340;&#25506;&#32034;&#65292;&#25110;&#32773;&#20351;&#29992;&#22522;&#20110;&#35282;&#33394;&#30340;&#23398;&#20064;&#26469;&#20998;&#35299;&#32852;&#21512;&#21160;&#20316;&#31354;&#38388;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#22312;&#25972;&#20010;&#21160;&#20316;-&#35266;&#23519;&#31354;&#38388;&#20013;&#36827;&#34892;&#38598;&#20307;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#24448;&#24448;&#38754;&#20020;&#33719;&#21462;&#29305;&#23450;&#32852;&#21512;&#21160;&#20316;&#24207;&#21015;&#20197;&#36798;&#21040;&#25104;&#21151;&#29366;&#24577;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Imagine, Initialize, and Explore (IIE)&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;IIE&#21033;&#29992;&#19968;&#20010;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#24819;&#35937;&#26234;&#33021;&#20307;&#22914;&#20309;&#36798;&#21040;&#21487;&#20197;&#24433;&#21709;&#24444;&#27492;&#36716;&#31227;&#20989;&#25968;&#30340;&#20851;&#38190;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#22312;&#25506;&#32034;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22120;&#22312;&#36825;&#20010;&#29366;&#24577;&#19979;&#21021;&#22987;&#21270;&#29615;&#22659;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#23454;&#29616;&#36825;&#31181;&#24819;&#35937;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17978v1 Announce Type: cross  Abstract: Effective exploration is crucial to discovering optimal strategies for multi-agent reinforcement learning (MARL) in complex coordination tasks. Existing methods mainly utilize intrinsic rewards to enable committed exploration or use role-based learning for decomposing joint action spaces instead of directly conducting a collective search in the entire action-observation space. However, they often face challenges obtaining specific joint action sequences to reach successful states in long-horizon tasks. To address this limitation, we propose Imagine, Initialize, and Explore (IIE), a novel method that offers a promising solution for efficient multi-agent exploration in complex scenarios. IIE employs a transformer model to imagine how the agents reach a critical state that can influence each other's transition functions. Then, we initialize the environment at this state using a simulator before the exploration phase. We formulate the imag
&lt;/p&gt;</description></item><item><title>&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;</title><link>https://arxiv.org/abs/2402.16914</link><description>&lt;p&gt;
DrAttack: &#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#20351;&#24378;&#22823;&#30340;LLM&#36234;&#29425;&#32773;
&lt;/p&gt;
&lt;p&gt;
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16914
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#27169;&#31946;&#20854;&#28508;&#22312;&#30340;&#24694;&#24847;&#24847;&#22270;&#65292;&#20351;&#20043;&#20197;&#29255;&#27573;&#21270;&#12289;&#19981;&#26131;&#26816;&#27979;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;&#30340;&#33258;&#21160;&#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#26694;&#26550;&#65288;DrAttack&#65289;&#12290;DrAttack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(a) &#23558;&#21407;&#22987;&#25552;&#31034;&#36827;&#34892;&#8220;&#20998;&#35299;&#8221;&#20026;&#23376;&#25552;&#31034;&#65292;(b) &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#38544;&#21547;&#30340;&#8220;&#37325;&#26500;&#8221;&#36825;&#20123;&#23376;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15987</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20559;&#24046;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Mitigation of Evaluation Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35780;&#20272;&#22120;&#20013;&#30340;&#20284;&#28982;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#36825;&#31181;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#20284;&#28982;&#20316;&#20026;&#34913;&#37327;LLM&#23545;&#21477;&#23376;&#21487;&#20449;&#24230;&#30340;&#25351;&#26631;&#65292;&#21487;&#33021;&#20250;&#22240;&#21477;&#23376;&#34920;&#38754;&#24046;&#24322;&#65288;&#22914;&#35789;&#24207;&#21644;&#21477;&#23376;&#32467;&#26500;&#65289;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#23558;LLMs&#29992;&#20110;&#35780;&#20272;&#65292;&#21487;&#33021;&#23384;&#22312;&#20284;&#28982;&#20559;&#24046;&#65306;&#23427;&#20204;&#21487;&#33021;&#20250;&#39640;&#20272;&#20855;&#26377;&#36739;&#39640;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#65292;&#32780;&#20302;&#20272;&#20855;&#26377;&#36739;&#20302;&#20284;&#28982;&#24615;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#23545;LLM&#35780;&#20272;&#22120;&#20013;&#20284;&#28982;&#20559;&#24046;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#20284;&#28982;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39640;&#24230;&#20559;&#32622;&#30340;&#23454;&#20363;&#20316;&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#35780;&#20272;&#25968;&#25454;&#21040;&#25991;&#26412;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20219;&#21153;&#26102;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#20960;&#31181;LLMs&#26174;&#31034;&#20986;&#20284;&#28982;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20943;&#36731;&#20102;&#36825;&#31181;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15160</link><description>&lt;p&gt;
&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#30340;&#21464;&#21387;&#22120;&#35760;&#24518;&#20307;&#29992;&#20110;&#20307;&#39564;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Spatially-Aware Transformer Memory for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#33410;&#35760;&#24518;&#22312;&#21508;&#31181;&#35748;&#30693;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#27604;&#22914;&#33021;&#22815;&#22312;&#22836;&#33041;&#20013;&#22238;&#24518;&#36807;&#21435;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#35748;&#30693;&#31185;&#23398;&#24378;&#35843;&#31354;&#38388;&#19978;&#19979;&#25991;&#22312;&#24773;&#33410;&#35760;&#24518;&#30340;&#24418;&#25104;&#21644;&#26816;&#32034;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#24773;&#33410;&#35760;&#24518;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#23384;&#20648;&#26102;&#38388;&#39034;&#24207;&#20307;&#39564;&#30340;&#21464;&#21387;&#22120;&#65292;&#36825;&#24573;&#30053;&#20102;&#31354;&#38388;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#23558;&#22522;&#30784;&#32467;&#26500;&#25193;&#23637;&#21040;&#38500;&#20102;&#20165;&#26377;&#26102;&#38388;&#39034;&#24207;&#20043;&#22806;&#30340;&#31354;&#38388;&#36724;&#65292;&#24182;&#30001;&#27492;&#33021;&#22815;&#33719;&#24471;&#21738;&#20123;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21253;&#21547;&#31354;&#38388;&#20449;&#24687;&#30340;&#38754;&#21521;&#31354;&#38388;&#24863;&#30693;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#32771;&#34385;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#30340;&#22330;&#25152;&#20013;&#24515;&#24773;&#33410;&#35760;&#24518;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#35760;&#24518;&#21033;&#29992;&#25928;&#29575;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#65292;&#23548;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15160v1 Announce Type: cross  Abstract: Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#21457;&#29616;&#23558;LLMs&#25972;&#21512;&#21040;Brainwriting&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14978</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#22836;&#33041;&#20889;&#20316;&#65306;&#25506;&#35752;LLMs&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22242;&#20307;&#26500;&#24605;&#20013;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#21457;&#29616;&#23558;LLMs&#25972;&#21512;&#21040;Brainwriting&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#65289;&#26085;&#30410;&#26222;&#21450;&#65292;&#23545;&#21019;&#24847;&#24037;&#20316;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;LLMs&#25972;&#21512;&#21040;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#26041;&#38754; - &#21019;&#24847;&#29983;&#25104;&#30340;&#20998;&#27495;&#38454;&#27573;&#20197;&#21450;&#35780;&#20272;&#21644;&#36873;&#25321;&#24819;&#27861;&#30340;&#25910;&#25947;&#38454;&#27573;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21327;&#20316;&#30340;&#22242;&#20307;-AI&#22836;&#33041;&#20889;&#20316;&#26500;&#24605;&#26694;&#26550;&#65292;&#23558;LLM&#20316;&#20026;&#19968;&#20010;&#22686;&#24378;&#22240;&#32032;&#34701;&#20837;&#21040;&#22242;&#20307;&#26500;&#24605;&#36807;&#31243;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#21019;&#24847;&#29983;&#25104;&#36807;&#31243;&#21644;&#32467;&#26524;&#35299;&#31354;&#38388;&#12290;&#20026;&#35780;&#20272;&#22312;&#24819;&#27861;&#35780;&#20272;&#36807;&#31243;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#24341;&#25806;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#21517;&#19987;&#23478;&#21644;&#20845;&#21517;&#26032;&#25163;&#35780;&#20272;&#32773;&#20998;&#37197;&#30340;&#24819;&#27861;&#35780;&#32423;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23558;LLMs&#25972;&#21512;&#21040;&#22836;&#33041;&#20889;&#20316;&#20013;&#21487;&#20197;&#22686;&#24378;&#26500;&#24605;&#36807;&#31243;&#21450;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;LLMs&#21487;&#20197;&#25903;&#25345;&#24819;&#27861;&#35780;&#20272;&#30340;&#35777;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14978v1 Announce Type: cross  Abstract: The growing availability of generative AI technologies such as large language models (LLMs) has significant implications for creative work. This paper explores twofold aspects of integrating LLMs into the creative process - the divergence stage of idea generation, and the convergence stage of evaluation and selection of ideas. We devised a collaborative group-AI Brainwriting ideation framework, which incorporated an LLM as an enhancement into the group ideation process, and evaluated the idea generation process and the resulted solution space. To assess the potential of using LLMs in the idea evaluation process, we design an evaluation engine and compared it to idea ratings assigned by three expert and six novice evaluators. Our findings suggest that integrating LLM in Brainwriting could enhance both the ideation process and its outcome. We also provide evidence that LLMs can support idea evaluation. We conclude by discussing implicati
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.14875</link><description>&lt;p&gt;
&#21517;&#23383;&#30340;&#21547;&#20041;&#26159;&#20160;&#20040;&#65311;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
What's in a Name? Auditing Large Language Models for Race and Gender Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14875
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;&#65292;&#23588;&#20854;&#23545;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#34920;&#29616;&#26368;&#19981;&#21033;&#12290;&#23457;&#35745;&#22312;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#24378;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#23457;&#35745;&#35774;&#35745;&#26469;&#35843;&#26597;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#21253;&#25324;GPT-4&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#21457;&#27169;&#22411;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#20026;&#20010;&#20154;&#25552;&#20379;&#24314;&#35758;&#65292;&#27604;&#22914;&#22312;&#36141;&#36710;&#35848;&#21028;&#25110;&#36873;&#20030;&#32467;&#26524;&#39044;&#27979;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#24314;&#35758;&#31995;&#32479;&#24615;&#22320;&#23545;&#19982;&#31181;&#26063;&#23569;&#25968;&#32676;&#20307;&#21644;&#22899;&#24615;&#24120;&#35265;&#30456;&#20851;&#30340;&#21517;&#23383;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#19982;&#40657;&#20154;&#22899;&#24615;&#30456;&#20851;&#30340;&#21517;&#23383;&#24471;&#21040;&#30340;&#32467;&#26524;&#26368;&#19981;&#21033;&#12290;&#36825;&#20123;&#20559;&#35265;&#22312;42&#20010;&#25552;&#31034;&#27169;&#26495;&#21644;&#22810;&#20010;&#27169;&#22411;&#20013;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#24615;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#20107;&#20214;&#12290;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#25968;&#20540;&#12289;&#19982;&#20915;&#31574;&#30456;&#20851;&#30340;&#38170;&#28857;&#21487;&#20197;&#25104;&#21151;&#25269;&#28040;&#20559;&#35265;&#65292;&#32780;&#23450;&#24615;&#32454;&#33410;&#30340;&#24433;&#21709;&#24182;&#19981;&#19968;&#33268;&#65292;&#29978;&#33267;&#21487;&#33021;&#20250;&#21152;&#21095;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#37096;&#32626;&#21644;&#23454;&#26045;&#26102;&#36827;&#34892;&#23457;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20854;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14875v1 Announce Type: cross  Abstract: We employ an audit design to investigate biases in state-of-the-art large language models, including GPT-4. In our study, we elicit prompt the models for advice regarding an individual across a variety of scenarios, such as during car purchase negotiations or election outcome predictions. We find that the advice systematically disadvantages names that are commonly associated with racial minorities and women. Names associated with Black women receive the least advantageous outcomes. The biases are consistent across 42 prompt templates and several models, indicating a systemic issue rather than isolated incidents. While providing numerical, decision-relevant anchors in the prompt can successfully counteract the biases, qualitative details have inconsistent effects and may even increase disparities. Our findings underscore the importance of conducting audits at the point of LLM deployment and implementation to mitigate their potential for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14042</link><description>&lt;p&gt;
&#20351;&#29992;GANs&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24310;&#20280;&#19982;&#20445;&#25252;&#8212;&#8212;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21307;&#30103;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Protect and Extend -- Using GANs for Synthetic Data Generation of Time-Series Medical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;GANs&#29983;&#25104;&#20102;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;GAN&#27169;&#22411;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#30340;&#36136;&#37327;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#24182;&#24310;&#20280;&#25968;&#25454;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;: &#20445;&#25252;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#23545;&#20110;&#39640;&#36136;&#37327;&#20307;&#39564;(QoE)&#21644;&#21487;&#25509;&#21463;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#29702;&#25935;&#24863;&#25968;&#25454;&#30340;&#26381;&#21153;&#65292;&#22914;&#22522;&#20110;IT&#30340;&#20581;&#24247;&#26381;&#21153;&#12290;&#23613;&#31649;&#24050;&#32463;&#26174;&#31034;&#21311;&#21517;&#21270;&#25216;&#26415;&#23481;&#26131;&#34987;&#25968;&#25454;&#37325;&#26032;&#35782;&#21035;&#65292;&#20294;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36880;&#28176;&#21462;&#20195;&#20102;&#21311;&#21517;&#21270;&#65292;&#22240;&#20026;&#23427;&#30456;&#23545;&#32791;&#26102;&#21644;&#36164;&#28304;&#32791;&#36153;&#36739;&#23569;&#65292;&#24182;&#19988;&#26356;&#33021;&#25269;&#25239;&#25968;&#25454;&#27844;&#28431;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#36981;&#24490;&#24046;&#20998;&#38544;&#31169;&#29616;&#35937;&#30340;GAN&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#21512;&#25104;&#30196;&#21574;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#26368;&#26032;GAN&#22522;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#22312;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20998;&#21457;&#12290; &#39044;&#27979;&#24314;&#27169;&#12289;&#33258;&#30456;&#20851;&#24615;&#21644;&#20998;&#24067;&#20998;&#26512;&#34987;&#29992;&#26469;&#35780;&#20272;&#29983;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#36136;&#37327;(QoG)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14042v1 Announce Type: cross  Abstract: Preservation of private user data is of paramount importance for high Quality of Experience (QoE) and acceptability, particularly with services treating sensitive data, such as IT-based health services. Whereas anonymization techniques were shown to be prone to data re-identification, synthetic data generation has gradually replaced anonymization since it is relatively less time and resource-consuming and more robust to data leakage. Generative Adversarial Networks (GANs) have been used for generating synthetic datasets, especially GAN frameworks adhering to the differential privacy phenomena. This research compares state-of-the-art GAN-based models for synthetic data generation to generate time-series synthetic medical records of dementia patients which can be distributed without privacy concerns. Predictive modeling, autocorrelation, and distribution analysis are used to assess the Quality of Generating (QoG) of the generated data. T
&lt;/p&gt;</description></item><item><title>E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.14041</link><description>&lt;p&gt;
E2USD&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#29366;&#24577;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14041
&lt;/p&gt;
&lt;p&gt;
E2USD&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;&#36827;&#34892;&#32534;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#20551;&#38452;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;SOTA&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;E2USD&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29366;&#24577;&#26816;&#27979;&#12290;E2USD&#21033;&#29992;&#22522;&#20110;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#30340;&#26102;&#38388;&#24207;&#21015;&#21387;&#32553;&#22120;(FFTCompress)&#21644;&#20998;&#35299;&#30340;&#21452;&#35270;&#22270;&#23884;&#20837;&#27169;&#22359;(DDEM)&#65292;&#19968;&#36215;&#20197;&#20302;&#35745;&#31639;&#24320;&#38144;&#23545;&#36755;&#20837;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#38452;&#24615;&#21462;&#28040;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;(FNCCLearning)&#65292;&#20197;&#25269;&#28040;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#26356;&#21451;&#22909;&#30340;&#31751;&#23884;&#20837;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38408;&#20540;&#26816;&#27979;(ADATD)&#12290;&#36890;&#36807;&#20351;&#29992;&#20845;&#20010;&#22522;&#32447;&#27169;&#22411;&#21644;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;E2USD&#33021;&#22815;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;SOTA&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/AI4CTS/E2Usd &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.12907</link><description>&lt;p&gt;
AI&#23545;&#40784;&#22312;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#65306;&#31435;&#22330;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12907
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#65292;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26085;&#30410;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#65292;&#23545;&#31038;&#20250;&#27835;&#29702;&#21644;&#23433;&#20840;&#24102;&#26469;&#37325;&#35201;&#24433;&#21709;&#12290;&#23613;&#31649;&#22312;&#35299;&#20915;AI&#23545;&#40784;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25216;&#26415;&#26041;&#38754;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;AI&#31995;&#32479;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#24615;&#36136;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24320;&#21457;&#21644;&#37096;&#32626;&#32972;&#26223;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#26032;&#38382;&#39064;&#65306;&#28608;&#21169;&#20860;&#23481;&#24615;&#31038;&#20250;&#25216;&#26415;&#23545;&#40784;&#38382;&#39064;&#65288;ICSAP&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#33021;&#21628;&#21505;&#26356;&#22810;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#21338;&#24328;&#35770;&#20013;&#30340;&#28608;&#21169;&#20860;&#23481;&#24615;&#21407;&#21017;&#26469;&#24357;&#21512;&#25216;&#26415;&#21644;&#31038;&#20250;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#32500;&#25345;AI&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#20849;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23454;&#29616;IC&#30340;&#19977;&#20010;&#32463;&#20856;&#21338;&#24328;&#38382;&#39064;&#65306;&#26426;&#21046;&#35774;&#35745;&#12289;&#22865;&#32422;&#29702;&#35770;&#21644;&#36125;&#21494;&#26031;&#35828;&#26381;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12907v1 Announce Type: new  Abstract: The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion,
&lt;/p&gt;</description></item><item><title>PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.12810</link><description>&lt;p&gt;
PIP-Net&#65306;&#22478;&#24066;&#20013;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PIP-Net: Pedestrian Intention Prediction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12810
&lt;/p&gt;
&lt;p&gt;
PIP-Net&#26159;&#19968;&#20010;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#21160;&#24577;&#23398;&#25968;&#25454;&#21644;&#22330;&#26223;&#31354;&#38388;&#29305;&#24449;&#65292;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#25104;&#21151;&#39044;&#27979;&#34892;&#20154;&#36890;&#36807;&#39532;&#36335;&#30340;&#24847;&#22270;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#23545;&#34892;&#20154;&#24847;&#22270;&#30340;&#39044;&#27979;&#26159;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#19968;&#39033;&#30740;&#31350;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PIP-Net&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39044;&#27979;AVs&#22312;&#29616;&#23454;&#19990;&#30028;&#22478;&#24066;&#22330;&#26223;&#20013;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#38024;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#23433;&#35013;&#21644;&#35774;&#32622;&#35774;&#35745;&#30340;PIP-Net&#21464;&#31181;&#12290;&#21033;&#29992;&#26469;&#33258;&#34892;&#39542;&#22330;&#26223;&#30340;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#24490;&#29615;&#21644;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;&#20026;&#20102;&#22686;&#24378;&#36947;&#36335;&#29992;&#25143;&#30340;&#35270;&#35273;&#34920;&#31034;&#21450;&#20854;&#19982;&#33258;&#36710;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#31867;&#28145;&#24230;&#29305;&#24449;&#22270;&#65292;&#32467;&#21512;&#23616;&#37096;&#36816;&#21160;&#27969;&#29305;&#24449;&#65292;&#20026;&#22330;&#26223;&#21160;&#24577;&#25552;&#20379;&#20016;&#23500;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#25668;&#20687;&#22836;&#30340;&#35270;&#37326;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22260;&#32469;&#33258;&#36710;&#30340;&#19977;&#20010;&#25668;&#20687;&#22836;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12810v1 Announce Type: cross  Abstract: Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.05493</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Investigating White-Box Attacks for On-Device Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#20197;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#30456;&#24212;&#30340;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#36731;&#26131;&#25552;&#21462;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#35774;&#22791;&#19978;&#25915;&#20987;&#26041;&#27861;&#21482;&#33021;&#29983;&#25104;&#40657;&#30418;&#25915;&#20987;&#65292;&#36825;&#31181;&#26041;&#27861;&#36828;&#19981;&#22914;&#30333;&#30418;&#31574;&#30053;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#31227;&#21160;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;TFLite&#19981;&#25903;&#25345;&#26799;&#24230;&#35745;&#31639;&#65292;&#32780;&#26799;&#24230;&#35745;&#31639;&#23545;&#20110;&#30333;&#30418;&#25915;&#20987;&#31639;&#27861;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#21457;&#29616;&#21487;&#33021;&#20302;&#20272;&#20102;&#35774;&#22791;&#19978;&#25915;&#20987;&#30340;&#21361;&#23475;&#24615;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65306;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#30333;&#30418;&#31574;&#30053;&#30452;&#25509;&#21463;&#21040;&#25915;&#20987;&#65311;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23558;&#35774;&#22791;&#19978;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35843;&#35797;&#29256;&#26412;&#30340;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35774;&#22791;&#19978;&#27169;&#22411;&#30340;&#36870;&#21521;&#24037;&#31243;&#26694;&#26550;(REOM)&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#23558;&#32534;&#35793;&#21518;&#30340;&#35774;&#22791;&#19978;TFLite&#27169;&#22411;&#36870;&#21521;&#20026;&#21487;&#35843;&#35797;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REOM
&lt;/p&gt;
&lt;p&gt;
Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.04792</link><description>&lt;p&gt;
&#26469;&#33258;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#30452;&#25509;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Language Model Alignment from Online AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30452;&#25509;&#23545;&#40784;&#20559;&#22909;&#65288;DAP&#65289;&#26041;&#27861;&#22914;DPO&#24050;&#25104;&#20026;&#23545;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#19981;&#35201;&#27714;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DAP&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#35757;&#32451;&#20043;&#21069;&#25910;&#38598;&#65292;&#24182;&#19988;&#20174;&#19981;&#26356;&#26032;&#65292;&#22240;&#27492;&#21453;&#39304;&#32431;&#31929;&#26159;&#31163;&#32447;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#22238;&#24212;&#36890;&#24120;&#26159;&#20174;&#19968;&#20010;&#19982;&#34987;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#30001;&#20110;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21464;&#21270;&#65292;&#23545;&#40784;&#38454;&#27573;&#24517;&#28982;&#26159;&#38750;&#31574;&#30053;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#21453;&#39304;&#26159;&#20851;&#38190;&#65292;&#21487;&#20197;&#25913;&#21892;DAP&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#65288;OAIF&#65289;&#65292;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65306;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#20174;&#24403;&#21069;&#27169;&#22411;&#20013;&#37319;&#26679;&#20004;&#20010;&#22238;&#24212;&#65292;&#24182;&#25552;&#31034;LLM&#26631;&#27880;&#22120;&#36873;&#25321;&#21738;&#20010;&#26356;&#21463;&#27426;&#36814;&#65292;&#20174;&#32780;&#25552;&#20379;&#22312;&#32447;&#21453;&#39304;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;OAIF&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF
&lt;/p&gt;
&lt;p&gt;
Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02648</link><description>&lt;p&gt;
&#38142;&#24335;&#21453;&#39304;&#65306;&#32531;&#35299;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20013;&#20986;&#29616;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#26041;&#27861;&#65292;&#24182;&#25552;&#21040;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#26102;&#32463;&#24120;&#20986;&#29616;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#65292;&#21363;&#20351;&#36755;&#20837;&#30456;&#21516;&#65292;&#20063;&#20250;&#25552;&#20379;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#24403;&#29992;&#25143;&#34920;&#36798;&#22362;&#20915;&#30456;&#21453;&#30340;&#31435;&#22330;&#26102;&#65292;LLMs&#35843;&#25972;&#20854;&#22238;&#31572;&#30340;&#36136;&#37327;&#20250;&#21464;&#24046;&#65292;&#23613;&#31649;&#21021;&#22987;&#22238;&#31572;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#20123;&#34892;&#20026;&#38477;&#20302;&#20102;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#30340;&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#65306;1&#65289;&#36890;&#36807;&#23637;&#31034;&#38142;&#24335;&#21453;&#39304;&#65288;CoF&#65289;&#22914;&#20309;&#23548;&#33268;LLMs&#26356;&#21152;&#20559;&#31163;&#23454;&#38469;&#31572;&#26696;&#65292;&#24341;&#36215;&#36807;&#24230;&#20381;&#36182;ChatGPT&#31561;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#22266;&#26377;&#39118;&#38505;&#65307;2&#65289;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36882;&#24402;&#38142;&#24335;&#21453;&#39304;&#65288;R-CoF&#65289;&#65292;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;CoF&#31995;&#32479;&#25509;&#25910;&#19968;&#20010;&#24320;&#25918;&#24335;&#22810;&#27493;&#38382;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#37325;&#22797;&#25552;&#20379;&#26080;&#24847;&#20041;&#30340;&#21453;&#39304;&#65292;&#35201;&#27714;&#20877;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#21453;&#39304;&#21482;&#20250;&#38477;&#20302;&#22238;&#31572;&#30340;&#36136;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth
&lt;/p&gt;</description></item><item><title>&#23558;RNA&#35774;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#30340;&#36890;&#29992;&#20248;&#21270;&#26694;&#26550;&#65292;&#23558;&#20505;&#36873;&#24207;&#21015;&#30340;&#20998;&#24067;&#36880;&#27493;&#20248;&#21270;&#20026;&#21333;&#19968;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2401.00037</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#21644;&#36830;&#32493;&#20248;&#21270;&#35774;&#35745;&#20449;&#20351;RNA
&lt;/p&gt;
&lt;p&gt;
Messenger RNA Design via Expected Partition Function and Continuous Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00037
&lt;/p&gt;
&lt;p&gt;
&#23558;RNA&#35774;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;&#30340;&#36890;&#29992;&#20248;&#21270;&#26694;&#26550;&#65292;&#23558;&#20505;&#36873;&#24207;&#21015;&#30340;&#20998;&#24067;&#36880;&#27493;&#20248;&#21270;&#20026;&#21333;&#19968;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RNA&#35774;&#35745;&#30340;&#20219;&#21153;&#26159;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#26377;&#20960;&#20010;&#29256;&#26412;&#30340;&#36825;&#20123;&#38382;&#39064;&#26159;NP&#38590;&#39064;&#12290;&#20026;&#20102;&#26367;&#20195;&#24120;&#29992;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#34920;&#36848;&#20026;&#36830;&#32493;&#20248;&#21270;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;"&#26399;&#26395;&#20998;&#21306;&#20989;&#25968;"&#30340;&#32463;&#20856;&#20998;&#21306;&#20989;&#25968;&#30340;&#27867;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#20174;&#25152;&#26377;&#21487;&#33021;&#30340;&#20505;&#36873;&#24207;&#21015;&#20013;&#24320;&#22987;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#23558;&#30446;&#26631;&#20989;&#25968;&#20174;&#19968;&#20010;&#24207;&#21015;&#25193;&#23637;&#21040;&#19968;&#20010;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#26041;&#27861;&#25913;&#21892;&#25193;&#23637;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20998;&#24067;&#23558;&#36880;&#28176;&#25910;&#32553;&#21040;&#19968;&#20010;&#29420;&#28909;&#24207;&#21015;&#65288;&#21363;&#19968;&#20010;&#24207;&#21015;&#65289;&#12290;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#30123;&#33495;&#21644;&#27835;&#30103;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#30340;mRNA&#35774;&#35745;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00037v2 Announce Type: replace-cross  Abstract: The tasks of designing RNAs are discrete optimization problems, and several versions of these problems are NP-hard. As an alternative to commonly used local search methods, we formulate these problems as continuous optimization and develop a general framework for this optimization based on a generalization of classical partition function which we call "expected partition function". The basic idea is to start with a distribution over all possible candidate sequences, and extend the objective function from a sequence to a distribution. We then use gradient descent-based optimization methods to improve the extended objective function, and the distribution will gradually shrink towards a one-hot sequence (i.e., a single sequence). As a case study, we consider the important problem of mRNA design with wide applications in vaccines and therapeutics. While the recent work of LinearDesign can efficiently optimize mRNAs for minimum free
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12462</link><description>&lt;p&gt;
&#23454;&#29616;&#31471;&#21040;&#31471;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards an end-to-end artificial intelligence driven global weather forecasting system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#23545;&#31185;&#23398;&#21644;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20381;&#36182;&#20110;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#31995;&#32479;&#30340;&#20998;&#26512;&#25110;&#20877;&#20998;&#26512;&#20135;&#21697;&#20316;&#20026;&#39044;&#27979;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#21021;&#22987;&#29366;&#24577;&#36890;&#24120;&#30001;&#20256;&#32479;&#25968;&#25454;&#21516;&#21270;&#32452;&#20214;&#29983;&#25104;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#21363;Adas&#65292;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;Adas&#19982;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65288;&#21363;FengWu&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#22522;&#20110;AI&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65306;FengWu-Adas&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Adas&#33021;&#22815;&#21516;&#21270;&#31232;&#30095;&#30340;&#20840;&#29699;&#35266;&#27979;&#25968;&#25454;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
&lt;/p&gt;</description></item><item><title>LatestEval&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#19981;&#21463;&#25968;&#25454;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.12343</link><description>&lt;p&gt;
LatestEval: &#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12343
&lt;/p&gt;
&lt;p&gt;
LatestEval&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21644;&#26102;&#38388;&#25935;&#24863;&#30340;&#27979;&#35797;&#26500;&#24314;&#19981;&#21463;&#25968;&#25454;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#65292;&#36991;&#20813;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#26356;&#22909;&#22320;&#25512;&#26029;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#20808;&#22312;&#36229;&#22823;&#35268;&#27169;&#33258;&#21160;&#25235;&#21462;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#22312;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LatestEval&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#36817;&#30340;&#25991;&#26412;&#21019;&#24314;&#19981;&#21463;&#27745;&#26579;&#30340;&#38405;&#35835;&#29702;&#35299;&#35780;&#20272;&#12290;LatestEval&#36890;&#36807;&#20165;&#20351;&#29992;&#22312;&#26368;&#36817;&#26102;&#38388;&#31383;&#21475;&#20869;&#21457;&#24067;&#30340;&#25991;&#26412;&#26469;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#65292;&#30830;&#20445;&#19981;&#20250;&#19982;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#37325;&#21472;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;LatestEval&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;1&#65289;&#25910;&#38598;&#26368;&#26032;&#25991;&#26412;&#65307;2&#65289;&#35782;&#21035;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#21450;3&#65289;&#26500;&#24314;&#38024;&#23545;&#35813;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#29616;&#26377;&#31572;&#26696;&#12290;&#36825;&#40723;&#21169;&#27169;&#22411;&#26681;&#25454;&#21097;&#20313;&#19978;&#19979;&#25991;&#25512;&#26029;&#31572;&#26696;&#65292;&#32780;&#19981;&#20165;&#26159;&#22797;&#21046;&#31896;&#36148;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12343v3 Announce Type: replace-cross  Abstract: Data contamination in evaluation is getting increasingly prevalent with the emergence of language models pre-trained on super large, automatically crawled corpora. This problem leads to significant challenges in the accurate assessment of model capabilities and generalisations. In this paper, we propose LatestEval, an automatic method that leverages the most recent texts to create uncontaminated reading comprehension evaluations. LatestEval avoids data contamination by only using texts published within a recent time window, ensuring no overlap with the training corpora of pre-trained language models. We develop the LatestEval automated pipeline to 1) gather the latest texts; 2) identify key information, and 3) construct questions targeting the information while removing the existing answers from the context. This encourages models to infer the answers themselves based on the remaining context, rather than just copy-paste. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>ARIA&#20803;&#32032;&#24517;&#39035;&#20849;&#21516;&#36873;&#25321;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#20219;&#21153;&#19979;&#27599;&#20010;&#20803;&#32032;&#30340;&#33391;&#22909;&#36873;&#25321;&#21644;&#26631;&#20934;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2311.14625</link><description>&lt;p&gt;
ARIA&#65306;&#20851;&#20110;&#32852;&#37030;&#35270;&#35273;&#20998;&#31867;&#20013;&#26550;&#26500;&#12289;&#21021;&#22987;&#21270;&#21644;&#32858;&#21512;&#26041;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
ARIA: On the Interaction Between Architectures, Initialization and Aggregation Methods for Federated Visual Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14625
&lt;/p&gt;
&lt;p&gt;
ARIA&#20803;&#32032;&#24517;&#39035;&#20849;&#21516;&#36873;&#25321;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#20219;&#21153;&#19979;&#27599;&#20010;&#20803;&#32032;&#30340;&#33391;&#22909;&#36873;&#25321;&#21644;&#26631;&#20934;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#28040;&#38500;&#25935;&#24863;&#25968;&#25454;&#20132;&#25442;&#24182;&#20381;&#36182;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#27169;&#22411;&#21442;&#25968;&#30340;&#20132;&#25442;&#65292;&#23454;&#29616;&#36328;&#26426;&#26500;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#12290;&#23613;&#31649;&#20851;&#20110;&#23458;&#25143;&#31471;&#27169;&#22411;&#22914;&#20309;&#32858;&#21512;&#20197;&#21450;&#26368;&#36817;&#23545;ImageNet&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#36827;&#34892;&#20102;&#29420;&#31435;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#32852;&#37030;&#36873;&#25321;&#30340;&#26550;&#26500;&#21450;&#20854;&#22914;&#20309;&#30456;&#20114;&#36830;&#25509;&#30340;&#24433;&#21709;&#32570;&#20047;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#39318;&#27425;&#32852;&#21512;&#26550;&#26500;-&#21021;&#22987;&#21270;-&#32858;&#21512;&#30740;&#31350;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23545;ARIA&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#24403;&#21069;&#20570;&#27861;&#30456;&#21453;&#65292;&#24517;&#39035;&#20849;&#21516;&#36873;&#25321;ARIA&#20803;&#32032;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#26681;&#25454;&#20219;&#21153;&#36873;&#25321;&#27599;&#20010;&#20803;&#32032;&#30340;&#33391;&#22909;&#36873;&#25321;&#65292;&#26631;&#20934;&#21270;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14625v2 Announce Type: replace-cross  Abstract: Federated Learning (FL) is a collaborative training paradigm that allows for privacy-preserving learning of cross-institutional models by eliminating the exchange of sensitive data and instead relying on the exchange of model parameters between the clients and a server. Despite individual studies on how client models are aggregated, and, more recently, on the benefits of ImageNet pre-training, there is a lack of understanding of the effect the architecture chosen for the federation has, and of how the aforementioned elements interconnect. To this end, we conduct the first joint ARchitecture-Initialization-Aggregation study and benchmark ARIAs across a range of medical image classification tasks. We find that, contrary to current practices, ARIA elements have to be chosen together to achieve the best possible performance. Our results also shed light on good choices for each element depending on the task, the effect of normalisat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.02198</link><description>&lt;p&gt;
&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Bootstrapped Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02198
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#26679;&#26412;-efficient RL&#65292;&#36890;&#36807;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20027;&#35201;&#20381;&#36182;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#26159;&#22240;&#20026;&#20854;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#33021;&#20351;IL&#25512;&#24191;&#21040;&#25152;&#26377;&#21487;&#33021;&#22330;&#26223;&#30340;&#20840;&#38754;&#19987;&#23478;&#28436;&#31034;&#26159;&#26114;&#36149;&#30340;&#65292;&#20219;&#20309;&#20998;&#24067;&#30340;&#36716;&#21464;&#37117;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;RL&#21487;&#20197;&#24314;&#31435;&#22312;IL&#30340;&#22522;&#30784;&#19978;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#31243;&#24207;&#65292;&#37027;&#20040;&#23427;&#23558;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#24341;&#23548;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;IBRL&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#31034;&#33539;&#30340;&#39640;&#25928;&#25277;&#26679;RL&#65292;&#39318;&#20808;&#22312;&#25552;&#20379;&#30340;&#31034;&#33539;&#19978;&#35757;&#32451;IL&#31574;&#30053;&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#25552;&#20986;&#26367;&#20195;&#21160;&#20316;&#36827;&#34892;&#22312;&#32447;&#25506;&#32034;&#21644;&#24341;&#23548;&#30446;&#26631;&#20540;&#12290;&#19982;&#20808;&#21069;&#36807;&#24230;&#37319;&#26679;&#31034;&#33539;&#25110;&#29992;&#39069;&#22806;&#30340;&#27169;&#20223;&#25439;&#22833;&#23545;RL&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;IBRL&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;IL&#30340;&#39640;&#36136;&#37327;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02198v4 Announce Type: replace-cross  Abstract: Despite the considerable potential of reinforcement learning (RL), robotic control tasks predominantly rely on imitation learning (IL) due to its better sample efficiency. However, it is costly to collect comprehensive expert demonstrations that enable IL to generalize to all possible scenarios, and any distribution shift would require recollecting data for finetuning. Therefore, RL is appealing if it can build upon IL as an efficient autonomous self-improvement procedure. We propose imitation bootstrapped reinforcement learning (IBRL), a novel framework for sample-efficient RL with demonstrations that first trains an IL policy on the provided demonstrations and then uses it to propose alternative actions for both online exploration and bootstrapping target values. Compared to prior works that oversample the demonstrations or regularize RL with an additional imitation loss, IBRL is able to utilize high quality actions from IL p
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2308.06960</link><description>&lt;p&gt;
&#20026;&#22270;&#32423;&#20219;&#21153;&#35843;&#20248;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.06960
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;GNN&#38754;&#20020;&#30528;&#26631;&#31614;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#23581;&#35797;&#22312;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#30340;&#22270;&#19978;&#39044;&#35757;&#32451;GNN&#65292;&#24182;&#23558;&#20174;&#26410;&#26631;&#35760;&#30340;&#22270;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#36866;&#24212;&#21040;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#35813;&#36866;&#24212;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;GNN&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;GNN&#39044;&#35757;&#32451;&#24037;&#20316;&#24448;&#24448;&#24573;&#35270;&#20102;&#35774;&#35745;&#19968;&#20010;&#33391;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#36716;&#31227;&#30340;&#30693;&#35782;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#24320;&#22987;&#30740;&#31350;&#39044;&#35757;&#32451;GNN&#30340;&#26356;&#22909;&#24494;&#35843;&#31574;&#30053;&#12290;&#20294;&#20182;&#20204;&#30340;&#35774;&#35745;&#35201;&#20040;&#26377;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#24573;&#35270;&#20102;&#21508;&#31181;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24863;&#30693;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#39044;&#35757;&#32451;&#30340;GNN&#35774;&#35745;&#26356;&#22909;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.06960v2 Announce Type: replace-cross  Abstract: Recently, graph neural networks (GNNs) have shown its unprecedented success in many graph-related tasks. However, GNNs face the label scarcity issue as other neural networks do. Thus, recent efforts try to pre-train GNNs on a large-scale unlabeled graph and adapt the knowledge from the unlabeled graph to the target downstream task. The adaptation is generally achieved by fine-tuning the pre-trained GNNs with a limited number of labeled data. Despite the importance of fine-tuning, current GNNs pre-training works often ignore designing a good fine-tuning strategy to better leverage transferred knowledge and improve the performance on downstream tasks. Only few works start to investigate a better fine-tuning strategy for pre-trained GNNs. But their designs either have strong assumptions or overlook the data-aware issue for various downstream datasets. Therefore, we aim to design a better fine-tuning strategy for pre-trained GNNs t
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#28041;&#21450;&#23398;&#20064;&#23558;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23884;&#20837;&#24182;&#22788;&#29702;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#19978;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#22810;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2302.00389</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65306;&#28436;&#36827;&#12289;&#39044;&#35757;&#32451;&#21450;&#20854;&#24212;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00389
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#28041;&#21450;&#23398;&#20064;&#23558;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23884;&#20837;&#24182;&#22788;&#29702;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#19978;&#21462;&#24471;&#26174;&#33879;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#22810;&#31181;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#23398;&#20064;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#21450;&#20854;&#30456;&#20851;&#24615;&#30340;&#20449;&#24687;&#23884;&#20837;&#30340;&#25216;&#26415;&#65292;&#22312;&#35832;&#22914;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#12289;&#35270;&#35273;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NLVR&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#26816;&#32034;&#65288;VLR&#65289;&#31561;&#21508;&#31181;&#24212;&#29992;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34917;&#20805;&#20449;&#24687;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#25191;&#34892;&#20219;&#20309;&#22810;&#27169;&#24577;&#20219;&#21153;&#65288;&#22914;&#29702;&#35299;&#12289;&#35782;&#21035;&#12289;&#26816;&#32034;&#25110;&#29983;&#25104;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#12290;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#19981;&#21516;&#21464;&#20307;&#26550;&#26500;&#22312;&#22810;&#20010;&#27169;&#24577;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#35843;&#26597;&#23545;&#28145;&#24230;&#23398;&#20064;&#22810;&#27169;&#24577;&#26550;&#26500;&#30340;&#28436;&#21464;&#21644;&#22686;&#24378;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#20197;&#22788;&#29702;&#25991;&#26412;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#29305;&#24449;&#65292;&#20197;&#24212;&#23545;&#22810;&#26679;&#30340;&#36328;&#27169;&#24577;&#21644;&#29616;&#20195;&#22810;&#27169;&#24577;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00389v2 Announce Type: replace  Abstract: Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2210.01282</link><description>&lt;p&gt;
&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#19982;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#65292;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32467;&#26500;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#21487;&#35266;&#27979;&#30340;&#34892;&#20026;&#21382;&#21490;&#21644;&#35775;&#38382;&#29366;&#24577;&#26469;&#20272;&#35745;&#20154;&#31867;&#20195;&#29702;&#21160;&#24577;&#20915;&#31574;&#30340;&#32467;&#26500;&#27169;&#22411;&#30340;&#20219;&#21153;&#12290;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#23884;&#22871;&#32467;&#26500;&#65306;&#22312;&#20869;&#37096;&#38382;&#39064;&#20013;&#65292;&#30830;&#23450;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#22806;&#37096;&#38382;&#39064;&#20013;&#65292;&#26368;&#22823;&#21270;&#36866;&#21512;&#24230;&#24230;&#37327;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23884;&#22871;&#24490;&#29615;&#32467;&#26500;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#20294;&#24403;&#29366;&#24577;&#31354;&#38388;&#35201;&#20040;&#26159;&#20855;&#26377;&#22823;&#22522;&#25968;&#30340;&#31163;&#25955;&#31354;&#38388;&#65292;&#35201;&#20040;&#26159;&#39640;&#32500;&#36830;&#32493;&#31354;&#38388;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#39640;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#25991;&#29486;&#20013;&#30340;&#20854;&#20182;&#26041;&#27861;&#24378;&#35843;&#31574;&#30053;&#20272;&#35745;&#65292;&#20294;&#21364;&#20197;&#38477;&#20302;&#22870;&#21169;&#20272;&#35745;&#31934;&#24230;&#20026;&#20195;&#20215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#38480;&#26102;&#38388;&#20445;&#35777;&#30340;&#21333;&#24490;&#29615;&#20272;&#35745;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#32780;&#19981;&#20250;&#25439;&#23475;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GUI&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65288;GUI-TOD&#65289;&#26550;&#26500;&#65292;&#24182;&#21457;&#24067;&#20102;&#29992;&#20110;&#22312;&#31227;&#21160;GUI&#19978;&#35757;&#32451;&#22810;&#27169;&#24577;&#23545;&#35805;&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;META-GUI&#12290;</title><link>https://arxiv.org/abs/2205.11029</link><description>&lt;p&gt;
META-GUI&#65306;&#38754;&#21521;&#31227;&#21160;GUI&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.11029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GUI&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65288;GUI-TOD&#65289;&#26550;&#26500;&#65292;&#24182;&#21457;&#24067;&#20102;&#29992;&#20110;&#22312;&#31227;&#21160;GUI&#19978;&#35757;&#32451;&#22810;&#27169;&#24577;&#23545;&#35805;&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;META-GUI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31227;&#21160;&#30005;&#35805;&#26234;&#33021;&#21161;&#25163;&#65292;&#29992;&#20110;&#23436;&#25104;&#35832;&#22914;&#26085;&#21382;&#23433;&#25490;&#25110;&#37202;&#24215;&#39044;&#35746;&#31561;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;TOD&#31995;&#32479;&#36890;&#24120;&#19987;&#27880;&#20110;&#22810;&#36718;&#25991;&#26412;/&#35821;&#38899;&#20132;&#20114;&#65292;&#28982;&#21518;&#35843;&#29992;&#20026;TOD&#35774;&#35745;&#30340;&#21518;&#31471;API&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;API&#30340;&#26550;&#26500;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#26234;&#33021;&#21161;&#25163;&#30340;&#20449;&#24687;&#25628;&#32034;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#65292;&#22914;&#26524;TOD&#29305;&#23450;&#30340;API&#19981;&#21487;&#29992;&#25110;&#20219;&#21153;&#36807;&#20110;&#22797;&#26434;&#32780;&#26080;&#27861;&#36890;&#36807;&#25552;&#20379;&#30340;API&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;TOD&#26550;&#26500;&#65306;&#22522;&#20110;GUI&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65288;GUI-TOD&#65289;&#12290;GUI-TOD&#31995;&#32479;&#21487;&#20197;&#30452;&#25509;&#22312;&#30495;&#23454;&#24212;&#29992;&#31243;&#24207;&#19978;&#25191;&#34892;GUI&#25805;&#20316;&#24182;&#25191;&#34892;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#35843;&#29992;&#29305;&#23450;&#20110;TOD&#30340;&#21518;&#31471;API&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;META-GUI&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#31227;&#21160;GUI&#19978;&#35757;&#32451;&#22810;&#27169;&#24577;&#23545;&#35805;&#20195;&#29702;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.11029v2 Announce Type: cross  Abstract: Task-oriented dialogue (TOD) systems have been widely used by mobile phone intelligent assistants to accomplish tasks such as calendar scheduling or hotel reservation. Current TOD systems usually focus on multi-turn text/speech interaction, then they would call back-end APIs designed for TODs to perform the task. However, this API-based architecture greatly limits the information-searching capability of intelligent assistants and may even lead to task failure if TOD-specific APIs are not available or the task is too complicated to be executed by the provided APIs. In this paper, we propose a new TOD architecture: GUI-based task-oriented dialogue system (GUI-TOD). A GUI-TOD system can directly perform GUI operations on real APPs and execute tasks without invoking TOD-specific backend APIs. Furthermore, we release META-GUI, a dataset for training a Multi-modal convErsaTional Agent on mobile GUI. We also propose a multi-model action predi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#25552;&#20379;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25429;&#25417;&#30452;&#35266;&#30340;&#20542;&#21521;&#12290;</title><link>https://arxiv.org/abs/2203.16464</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#25552;&#20379;&#20840;&#23616;&#35299;&#37322;&#65292;&#24182;&#25429;&#25417;&#30452;&#35266;&#30340;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#23588;&#20854;&#26159;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#34987;&#21487;&#38752;&#22320;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#33021;&#22815;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#22914;&#20309;&#23558;&#20854;&#36755;&#20837;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#35299;&#37322;&#25104;&#20026;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36825;&#31181;&#29305;&#24615;&#38459;&#30861;&#20102;&#30740;&#31350;&#20154;&#21592;&#23398;&#20064;&#21644;&#25552;&#20379;&#23545;&#27169;&#22411;&#34892;&#20026;&#21644;&#26368;&#32456;&#39044;&#27979;&#30340;&#35299;&#37322;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.16464v3 Announce Type: replace-cross  Abstract: Artificial intelligence, particularly through recent advancements in deep learning, has achieved exceptional performances in many tasks in fields such as natural language processing and computer vision. In addition to desirable evaluation metrics, a high level of interpretability is often required for these models to be reliably utilized. Therefore, explanations that offer insight into the process by which a model maps its inputs onto its outputs are much sought-after. Unfortunately, the current black box nature of machine learning models is still an unresolved issue and this very nature prevents researchers from learning and providing explicative descriptions for a model's behavior and final predictions. In this work, we propose a novel framework utilizing Adversarial Inverse Reinforcement Learning that can provide global explanations for decisions made by a Reinforcement Learning model and capture intuitive tendencies that th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2202.06834</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Tries&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Sequential Pattern Mining with Hybrid Tries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.06834
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;trie&#30340;&#20869;&#23384;&#39640;&#25928;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#26377;&#26174;&#33879;&#25913;&#21892;&#65292;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;256GB&#31995;&#32479;&#20869;&#23384;&#19979;&#22823;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#23545;&#20110;&#33021;&#22815;&#22788;&#29702;&#22914;&#27492;&#24222;&#22823;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#25366;&#25496;&#31639;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#36843;&#20999;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#27169;&#24335;&#25366;&#25496;&#65288;SPM&#65289;&#65292;&#36825;&#26159;&#30693;&#35782;&#21457;&#29616;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20027;&#39064;&#65292;&#38754;&#20020;&#30528;&#38024;&#23545;&#22823;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#20869;&#23384;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;trie&#25968;&#25454;&#32467;&#26500;&#65292;&#21033;&#29992;&#37325;&#22797;&#27169;&#24335;&#32039;&#20945;&#22320;&#23384;&#20648;&#20869;&#23384;&#20013;&#30340;&#25968;&#25454;&#38598;; &#20197;&#21450;&#19968;&#20010;&#30456;&#24212;&#30340;&#25366;&#25496;&#31639;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#20174;&#27492;&#32039;&#20945;&#34920;&#31034;&#20013;&#25552;&#21462;&#27169;&#24335;&#12290;&#23545;&#30495;&#23454;&#27979;&#35797;&#23454;&#20363;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;&#23545;&#20110;&#23567;&#21040;&#20013;&#31561;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20869;&#23384;&#28040;&#32791;&#24179;&#22343;&#25552;&#39640;&#20102;88&#65285;&#65292;&#35745;&#31639;&#26102;&#38388;&#25552;&#39640;&#20102;41&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#21807;&#19968;&#19968;&#20010;&#22312;&#31995;&#32479;&#20869;&#23384;&#20026;256GB&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#30340;SPM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.06834v2 Announce Type: replace-cross  Abstract: As modern data sets continue to grow exponentially in size, the demand for efficient mining algorithms capable of handling such large data sets becomes increasingly imperative. This paper develops a memory-efficient approach for Sequential Pattern Mining (SPM), a fundamental topic in knowledge discovery that faces a well-known memory bottleneck for large data sets. Our methodology involves a novel hybrid trie data structure that exploits recurring patterns to compactly store the data set in memory; and a corresponding mining algorithm designed to effectively extract patterns from this compact representation. Numerical results on real-life test instances show an average improvement of 88% in memory consumption and 41% in computation time for small to medium-sized data sets compared to the state of the art. Furthermore, our algorithm stands out as the only capable SPM approach for large data sets within 256GB of system memory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2109.07319</link><description>&lt;p&gt;
InceptionXML&#65306;&#19968;&#31181;&#24102;&#26377;&#21516;&#27493;&#36127;&#37319;&#26679;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#25968;&#25454;&#23545;&#22823;&#37327;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#34987;&#31216;&#20026;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#30456;&#20851;&#25628;&#32034;&#39044;&#27979;&#21644;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#26550;&#26500;InceptionXML&#65292;&#20854;&#36731;&#37327;&#20294;&#21151;&#33021;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#25628;&#32034;&#21644;&#25512;&#33616;&#20219;&#21153;&#20013;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#32570;&#20047;&#21333;&#35789;&#39034;&#24207;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21367;&#31215;&#30340;&#25805;&#20316;&#27839;&#30528;&#23884;&#20837;&#32500;&#24230;&#37325;&#26032;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;CNNs&#19968;&#26679;&#27839;&#30528;&#21333;&#35789;&#32500;&#24230;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#24212;&#29992;&#21367;&#31215;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#26631;&#31614;&#31579;&#36873;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22312;&#21021;&#22987;&#26465;&#20214;&#20013;&#22914;&#20309;&#24433;&#21709;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#26368;&#32456;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21508;&#21521;&#24322;&#24615;&#28155;&#21152;&#20102;&#19968;&#20123;&#39069;&#22806;&#20449;&#24687;&#37327;&#12290;</title><link>https://arxiv.org/abs/2011.10577</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#23431;&#23449;&#32467;&#26500;&#24418;&#25104;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Deep learning insights into cosmological structure formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.10577
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22312;&#21021;&#22987;&#26465;&#20214;&#20013;&#22914;&#20309;&#24433;&#21709;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#26368;&#32456;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21508;&#21521;&#24322;&#24615;&#28155;&#21152;&#20102;&#19968;&#20123;&#39069;&#22806;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23431;&#23449;&#27169;&#25311;&#21487;&#20197;&#35745;&#31639;&#26089;&#26399;&#23431;&#23449;&#20013;&#30340;&#32447;&#24615;&#21021;&#22987;&#26465;&#20214;&#22914;&#20309;&#28436;&#21464;&#25104;&#20026;&#21518;&#26469;&#30340;&#26263;&#29289;&#36136;&#25193;&#23637;&#26263;&#22242;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#19968;&#22797;&#26434;&#36807;&#31243;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65307;&#29305;&#21035;&#26159;&#65292;&#21021;&#22987;&#26465;&#20214;&#20013;&#21508;&#21521;&#24322;&#24615;&#20449;&#24687;&#22914;&#20309;&#30830;&#23450;&#26368;&#32456;&#26263;&#29289;&#36136;&#26263;&#22242;&#36136;&#37327;&#30340;&#35282;&#33394;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20174;&#21021;&#22987;&#26465;&#20214;&#39044;&#27979;&#26263;&#29289;&#36136;&#26263;&#22242;&#30340;&#36136;&#37327;&#65292;&#24182;&#20840;&#38754;&#37327;&#21270;&#20102;&#21021;&#22987;&#23494;&#24230;&#22330;&#30340;&#21508;&#21521;&#21516;&#24615;&#21644;&#21508;&#21521;&#24322;&#24615;&#26041;&#38754;&#20851;&#20110;&#26368;&#32456;&#26263;&#22242;&#36136;&#37327;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21508;&#21521;&#24322;&#24615;&#30830;&#23454;&#27604;&#23494;&#24230;&#22330;&#30340;&#29699;&#38754;&#24179;&#22343;&#20540;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#37327;&#30053;&#24494;&#22686;&#21152;&#20102;&#19968;&#20123;&#65292;&#23613;&#31649;&#22312;&#32479;&#35745;&#19978;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2011.10577v3 Announce Type: replace-cross  Abstract: The evolution of linear initial conditions present in the early universe into extended halos of dark matter at late times can be computed using cosmological simulations. However, a theoretical understanding of this complex process remains elusive; in particular, the role of anisotropic information in the initial conditions in establishing the final mass of dark matter halos remains a long-standing puzzle. Here, we build a deep learning framework to investigate this question. We train a three-dimensional convolutional neural network (CNN) to predict the mass of dark matter halos from the initial conditions, and quantify in full generality the amounts of information in the isotropic and anisotropic aspects of the initial density field about final halo masses. We find that anisotropies add a small, albeit statistically significant amount of information over that contained within spherical averages of the density field about final 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.13662</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#32456;&#26497;&#25351;&#21335;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#21644;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Definitive Guide to Policy Gradients in Deep Reinforcement Learning: Theory, Algorithms and Implementations. (arXiv:2401.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#21253;&#25324;&#29702;&#35770;&#22522;&#30784;&#12289;&#23454;&#38469;&#23454;&#29616;&#21644;&#27604;&#36739;&#32467;&#26524;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#34429;&#28982;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#24314;&#31435;&#22312;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#22522;&#30784;&#19978;&#65292;&#20294;&#20855;&#20307;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#31639;&#27861;&#20043;&#38388;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#20307;&#30340;&#35270;&#35282;&#26469;&#27010;&#36848;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20415;&#29702;&#35299;&#23427;&#20204;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#38469;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#36830;&#32493;&#29256;&#26412;&#30340;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#30340;&#35814;&#32454;&#35777;&#26126;&#12289;&#25910;&#25947;&#32467;&#26524;&#21644;&#23545;&#23454;&#38469;&#31639;&#27861;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#26368;&#37325;&#35201;&#30340;&#31639;&#27861;&#65292;&#24182;&#23545;&#27491;&#21017;&#21270;&#30340;&#30410;&#22788;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#25152;&#26377;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/Matt00n/PolicyGradientsJax&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, various powerful policy gradient algorithms have been proposed in deep reinforcement learning. While all these algorithms build on the Policy Gradient Theorem, the specific design choices differ significantly across algorithms. We provide a holistic overview of on-policy policy gradient algorithms to facilitate the understanding of both their theoretical foundations and their practical implementations. In this overview, we include a detailed proof of the continuous version of the Policy Gradient Theorem, convergence results and a comprehensive discussion of practical algorithms. We compare the most prominent algorithms on continuous control environments and provide insights on the benefits of regularization. All code is available at https://github.com/Matt00n/PolicyGradientsJax.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#20869;&#30465;&#30340;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#35753;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#31995;&#32479;&#22312;&#19982;&#20854;&#20182;&#31995;&#32479;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.10910</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#21527;&#65311;&#22312;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#20013;&#20351;&#29992;&#20869;&#30465;&#20197;&#25552;&#39640;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior. (arXiv:2401.10910v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#20869;&#30465;&#30340;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21487;&#20197;&#35753;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26469;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#31995;&#32479;&#22312;&#19982;&#20854;&#20182;&#31995;&#32479;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#20248;&#21183;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;LLMs&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#27867;&#21270;&#22256;&#38590;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20803;&#35748;&#30693;&#27169;&#22359;&#29992;&#20110;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#65292;&#20351;&#20854;&#33021;&#22815;&#35266;&#23519;&#33258;&#24049;&#30340;&#24605;&#32771;&#36807;&#31243;&#21644;&#34892;&#21160;&#12290;&#36825;&#31181;&#20803;&#35748;&#30693;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#31995;&#32479;1&#21644;&#31995;&#32479;2&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#20462;&#25913;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#27979;&#35797;&#20102;&#20803;&#35748;&#30693;&#27169;&#22359;&#65292;&#21253;&#25324;&#29983;&#25104;&#24335;&#26234;&#33021;&#20307;&#24517;&#39035;&#22312;&#20725;&#23608;&#21551;&#31034;&#24405;&#20013;&#23384;&#27963;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#34920;&#29616;&#19978;&#36229;&#36807;&#20854;&#20182;&#31995;&#32479;&#65292;&#21516;&#26102;&#26234;&#33021;&#20307;&#33021;&#22815;&#38543;&#30528;&#26102;&#38388;&#36866;&#24212;&#21644;&#25913;&#36827;&#31574;&#30053;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have shown impressive capabilities in various applications, yet LLMs face challenges such as limited context windows and difficulties in generalization. In this paper, we introduce a metacognition module for generative agents, enabling them to observe their own thought processes and actions. This metacognitive approach, designed to emulate System 1 and System 2 cognitive processes, allows agents to significantly enhance their performance by modifying their strategy. We tested the metacognition module on a variety of scenarios, including a situation where generative agents must survive a zombie apocalypse, and observe that our system outperform others, while agents adapt and improve their strategies to complete tasks over time.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.11046</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#30340;&#24555;&#36895;&#22270;&#32467;&#26500;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#36716;&#21270;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#21033;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#26102;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#24320;&#38144;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#22823;&#22411;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#36739;&#23567;&#30340;&#38598;&#21512;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21452;&#23618;&#20248;&#21270;&#26550;&#26500;&#26469;&#21387;&#32553;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21516;&#26679;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#26412;&#25991;&#23558;&#22270;&#32467;&#26500;&#21387;&#32553;&#38382;&#39064;&#25913;&#20026;&#26680;&#23725;&#22238;&#24402;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#22312;&#21452;&#23618;&#20248;&#21270;&#30340;&#20869;&#24490;&#29615;&#20013;&#36845;&#20195;&#35757;&#32451;GNN&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65288;GC-SNTK&#65289;&#65292;&#20854;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#30340;&#31070;&#32463;&#20999;&#32447;&#20869;&#26680;&#65288;SNTK&#65289;&#26469;&#25429;&#25417;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03309</link><description>&lt;p&gt;
&#31616;&#26126;&#26377;&#24207;&#30340;&#24863;&#30693;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning. (arXiv:2310.03309v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03309
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#26126;&#26377;&#24207;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#24182;&#19988;&#20154;&#31867;&#21270;&#22320;&#32452;&#32455;&#24605;&#32500;&#65292;&#20197;&#25552;&#39640;&#28436;&#32462;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#28436;&#32462;&#25512;&#29702;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#22797;&#26434;&#30340;&#28436;&#32462;&#38382;&#39064;&#20013;&#20173;&#28982;&#24456;&#38590;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#31867;&#38382;&#39064;&#20855;&#26377;&#22823;&#37327;&#21069;&#25552;&#65288;&#21363;&#20107;&#23454;&#25110;&#35268;&#21017;&#65289;&#65292;&#20854;&#20013;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#12290;&#19968;&#31181;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#21407;&#22987;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20197;&#21069;&#21521;&#65288;&#20363;&#22914;&#36873;&#25321;-&#25512;&#29702;&#65289;&#25110;&#21453;&#21521;&#65288;&#20363;&#22914;LAMBADA&#65289;&#26041;&#24335;&#23558;&#22810;&#20010;&#22240;&#26524;&#25512;&#29702;&#27493;&#39588;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#22823;&#37327;&#30340;&#24635;&#20307;&#38454;&#27573;&#65292;&#23548;&#33268;&#35745;&#31639;&#24320;&#38144;&#22823;&#65292;&#24182;&#19988;&#26377;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#20135;&#29983;&#35823;&#23548;&#24615;&#30340;&#27493;&#39588;&#12290;&#38500;&#20102;&#36880;&#38454;&#27573;&#20998;&#35299;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#30340;&#21478;&#19968;&#20010;&#26041;&#38754;&#33719;&#24471;&#20102;&#21551;&#21457;&#12290;&#20154;&#31867;&#20542;&#21521;&#20110;&#25552;&#28860;&#20986;&#26368;&#30456;&#20851;&#30340;&#20449;&#24687;&#24182;&#26377;&#24207;&#22320;&#32452;&#32455;&#24605;&#32500;&#65288;&#20363;&#22914;&#21019;&#24314;&#24605;&#32500;&#23548;&#22270;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#20182;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploiting large language models (LLMs) to tackle deductive reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex deductive problems, characterized by plenty of premises (i.e., facts or rules) entailing intricate relationships among entities and requiring multi-hop reasoning. One intuitive solution is to decompose the original task into smaller sub-tasks, and then chain the multiple casual reasoning steps together in a forward (e.g., Selection-Inference) or backward (e.g., LAMBADA) direction. However, these techniques inevitably necessitate a large number of overall stages, leading to computationally expensive operations and a higher possibility of making misleading steps. In addition to stage-by-stage decomposition, we draw inspiration from another aspect of human problem-solving. Humans tend to distill the most relevant information and organize their thoughts systematically (e.g., creating mind maps), which assists th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2310.02304</link><description>&lt;p&gt;
&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65306;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. (arXiv:2310.02304v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20248;&#21270;&#22120;&#65288;STOP&#65289;&#65292;&#36890;&#36807;&#36882;&#24402;&#33258;&#25105;&#25913;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#29992;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#65292;&#20174;&#32780;&#29983;&#25104;&#24615;&#33021;&#26356;&#22909;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;&#24605;&#32500;&#26641;&#21644;&#31243;&#24207;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#8220;&#33050;&#25163;&#26550;&#8221;&#31243;&#24207;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#31243;&#24207;&#26500;&#24314;&#20102;&#22810;&#27425;&#35843;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;&#33050;&#25163;&#26550;&#31243;&#24207;&#36890;&#24120;&#20351;&#29992;Python&#31561;&#32534;&#31243;&#35821;&#35328;&#32534;&#20889;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#33050;&#25163;&#26550;&#31243;&#24207;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#31181;&#23376;&#8220;&#25913;&#36827;&#22120;&#8221;&#24320;&#22987;&#65292;&#36890;&#36807;&#22810;&#27425;&#26597;&#35810;&#35821;&#35328;&#27169;&#22411;&#24182;&#36820;&#22238;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#26681;&#25454;&#32473;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25913;&#36827;&#36755;&#20837;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36816;&#34892;&#36825;&#20010;&#31181;&#23376;&#25913;&#36827;&#22120;&#26469;&#25913;&#36827;&#33258;&#36523;&#12290;&#22312;&#19968;&#31995;&#21015;&#32454;&#20998;&#20219;&#21153;&#20013;&#65292;&#24471;&#21040;&#30340;&#25913;&#36827;&#25913;&#36827;&#22120;&#29983;&#25104;&#30340;&#31243;&#24207;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#31181;&#23376;&#25913;&#36827;&#22120;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#30340;&#21508;&#31181;&#33258;&#25105;&#25913;&#36827;&#31574;&#30053;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21253;&#25324;&#27874;&#26463;&#25628;&#32034;&#12289;&#36951;&#20256;&#31639;&#27861;&#21644;&#27169;&#25311;&#36864;&#28779;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#24182;&#19981;&#26159;&#19968;&#31181;&#22686;&#38271;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent advances in AI systems (e.g., Tree-of-Thoughts and Program-Aided Language Models) solve problems by providing a "scaffolding" program that structures multiple calls to language models to generate better outputs. A scaffolding program is written in a programming language such as Python. In this work, we use a language-model-infused scaffolding program to improve itself. We start with a seed "improver" that improves an input program according to a given utility function by querying a language model several times and returning the best solution. We then run this seed improver to improve itself. Across a small set of downstream tasks, the resulting improved improver generates programs with significantly better performance than its seed improver. Afterward, we analyze the variety of self-improvement strategies proposed by the language model, including beam search, genetic algorithms, and simulated annealing. Since the language models themselves are not altered, this is not fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16739</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33267;6G&#36793;&#32536;&#65306;&#35270;&#37326;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#37096;&#32626;&#22312;6G&#36793;&#32536;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;LLMs&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#24182;&#20174;&#21709;&#24212;&#26102;&#38388;&#12289;&#24102;&#23485;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#31561;&#26041;&#38754;&#20998;&#26512;&#20102;&#20113;&#31471;&#37096;&#32626;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#24182;&#35752;&#35770;&#20102;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#30340;&#21019;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#27491;&#22312;&#25913;&#21464;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#26377;&#21487;&#33021;&#22609;&#36896;&#25105;&#20204;&#30340;&#26410;&#26469;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#27169;&#24577;&#29305;&#24615;&#65292;&#24403;&#21069;&#30340;&#22522;&#20110;&#20113;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65306;1) &#21709;&#24212;&#26102;&#38388;&#38271;&#65307;2) &#39640;&#24102;&#23485;&#25104;&#26412;&#65307;&#20197;&#21450;3) &#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#31995;&#32479;&#21487;&#33021;&#35299;&#20915;&#36825;&#20123;&#36843;&#20999;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;6G&#36793;&#32536;&#37096;&#32626;LLMs&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#30001;&#22810;&#27169;&#24577;LLMs&#25552;&#20379;&#25903;&#25345;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21307;&#30103;&#20445;&#20581;&#65292;&#20197;&#31361;&#20986;&#22312;&#32456;&#31471;&#29992;&#25143;&#38468;&#36817;&#37096;&#32626;LLMs&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#36793;&#32536;&#37096;&#32626;LLMs&#26102;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#35774;&#24819;&#20102;&#36866;&#29992;&#20110;LLMs&#30340;6G MEC&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20004;&#20010;&#35774;&#35745;&#26041;&#38754;&#65292;&#21363;LLMs&#30340;&#36793;&#32536;&#35757;&#32451;&#21644;&#36793;&#32536;&#25512;&#29702;&#12290;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#36793;&#32536;&#30340;&#22266;&#26377;&#36164;&#28304;&#38480;&#21046;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21508;&#31181;&#21069;&#27839;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.04455</link><description>&lt;p&gt;
&#21311;&#21517;&#21270;&#35821;&#38899;&#65306;&#35780;&#20272;&#21644;&#35774;&#35745;&#35828;&#35805;&#20154;&#21311;&#21517;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques. (arXiv:2308.04455v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#65292;&#20197;&#24212;&#23545;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#29992;&#25143;&#30028;&#38754;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#35821;&#38899;&#25968;&#25454;&#30340;&#25910;&#38598;&#21644;&#23384;&#20648;&#20063;&#22823;&#22823;&#22686;&#21152;&#12290;&#34429;&#28982;&#25968;&#25454;&#25910;&#38598;&#21487;&#20197;&#20026;&#22823;&#22810;&#25968;&#35821;&#38899;&#26381;&#21153;&#25552;&#20379;&#39640;&#25928;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20063;&#32473;&#29992;&#25143;&#30340;&#38544;&#31169;&#36896;&#25104;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#38598;&#20013;&#23384;&#20648;&#20351;&#20010;&#20154;&#30340;&#35821;&#38899;&#25968;&#25454;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#23041;&#32961;&#30340;&#20405;&#23475;&#12290;&#38543;&#30528;&#20122;&#39532;&#36874;&#30340;Alexa&#65292;&#35895;&#27468;&#30340;Home&#21644;&#33529;&#26524;&#30340;Siri&#31561;&#22522;&#20110;&#35821;&#38899;&#30340;&#25968;&#23383;&#21161;&#25163;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20197;&#21450;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#25910;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#65292;&#22768;&#38899;&#20811;&#38534;&#21644;&#35828;&#35805;&#20154;/&#24615;&#21035;/&#30149;&#29702;&#31561;&#35782;&#21035;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#20063;&#22686;&#21152;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21311;&#21517;&#21270;&#35821;&#38899;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#21311;&#21517;&#21270;&#30340;&#31243;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21311;&#21517;&#21270;&#26159;&#25351;&#20351;&#20010;&#20154;&#35821;&#38899;&#25968;&#25454;&#19982;&#36523;&#20221;&#26080;&#27861;&#20851;&#32852;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#38899;&#20449;&#21495;&#30340;&#23454;&#29992;&#24615;&#65288;&#20363;&#22914;&#65292;&#35775;&#38382;&#35821;&#35328;&#20869;&#23481;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#20960;&#20010;&#35780;&#20272;&#21327;&#35758;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protoco
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.17366</link><description>&lt;p&gt;
$\lambda$-AC&#65306;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
$\lambda$-AC: Learning latent decision-aware models for reinforcement learning in continuous state-spaces. (arXiv:2306.17366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;$\lambda$-AC&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#30830;&#23450;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#35774;&#35745;&#36873;&#25321;&#23545;&#31639;&#27861;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24863;&#30693;&#27169;&#22411;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21363;&#27169;&#22411;&#22312;&#20915;&#31574;&#21046;&#23450;&#26102;&#24212;&#35813;&#26159;&#20934;&#30830;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#26159;&#22312;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;&#20915;&#31574;&#24863;&#30693;&#25439;&#22833;&#30340;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#33021;&#22815;&#23454;&#29616;&#33391;&#22909;&#31639;&#27861;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#37325;&#35201;&#31639;&#27861;&#24605;&#24819;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#22312;MuZero&#31995;&#21015;&#24037;&#20316;&#20013;&#25152;&#24314;&#31435;&#30340;&#32463;&#39564;&#24615;&#35774;&#35745;&#20915;&#31574;&#23545;&#20110;&#30456;&#20851;&#31639;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#65292;&#19981;&#21516;&#30340;&#20215;&#20540;&#24863;&#30693;&#31639;&#27861;&#23454;&#20363;&#20043;&#38388;&#34892;&#20026;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#35265;&#35299;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#27169;&#22411;&#39537;&#21160;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;$\lambda$-AC&#12290;
&lt;/p&gt;
&lt;p&gt;
The idea of decision-aware model learning, that models should be accurate where it matters for decision-making, has gained prominence in model-based reinforcement learning. While promising theoretical results have been established, the empirical performance of algorithms leveraging a decision-aware loss has been lacking, especially in continuous control problems. In this paper, we present a study on the necessary components for decision-aware reinforcement learning models and we showcase design choices that enable well-performing algorithms. To this end, we provide a theoretical and empirical investigation into prominent algorithmic ideas in the field. We highlight that empirical design decisions established in the MuZero line of works are vital to achieving good performance for related algorithms, and we showcase differences in behavior between different instantiations of value-aware algorithms in stochastic environments. Using these insights, we propose the Latent Model-Based Decisio
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01334</link><description>&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20998;&#24067;&#26159;&#30456;&#21516;&#30340;&#65292;&#25968;&#25454;&#26159;&#38598;&#20013;&#23384;&#20648;&#20379;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#35774;&#22791;&#12289;&#32452;&#32455;&#25110;&#36793;&#32536;&#33410;&#28857;&#19978;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270; (FDG) &#30340;&#26497;&#22823;&#20852;&#36259;&#12290;FDG &#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064; (FL) &#21644;&#39046;&#22495;&#27867;&#21270; (DG) &#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#20351;&#22810;&#20010;&#28304;&#39046;&#22495;&#33021;&#22815;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#32780;&#21448;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#20171;&#20132;&#27969;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11897</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#20171;&#20132;&#27969;&#30340;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Critical Appraisal of Artificial Intelligence-Mediated Communication. (arXiv:2305.11897v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#20171;&#20132;&#27969;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#25216;&#26415;&#24212;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#21457;&#23637;&#65292;&#29616;&#22312;&#34987;&#31216;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35821;&#35328;&#23398;&#20064;&#65288;CALL&#65289;&#12290;&#26368;&#36817;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25972;&#21512;&#21040;CALL&#20013;&#65292;&#26174;&#33879;&#25913;&#21464;&#20102;&#35821;&#35328;&#25945;&#32946;&#22312;&#35838;&#22530;&#20869;&#22806;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#25506;&#35752;&#20102;AI&#20013;&#20171;&#20132;&#27969;&#22312;&#35821;&#35328;&#25945;&#32946;&#20013;&#30340;&#20248;&#32570;&#28857;&#12290;&#25105;&#39318;&#20808;&#31616;&#35201;&#22238;&#39038;&#20102;&#25945;&#32946;&#20013;&#30340;AI&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20171;&#32461;&#20102;ICALL&#65292;&#24182;&#23545;AI&#39537;&#21160;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12289;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#12289;AI&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#30340;&#28508;&#21147;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#35748;&#20026;&#65292;&#35821;&#35328;&#25945;&#24072;&#31215;&#26497;&#21442;&#19982;CALL&#25945;&#24072;&#25945;&#32946;&#21644;&#19987;&#19994;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#36319;&#19978;&#19981;&#26029;&#21457;&#23637;&#30340;&#25216;&#26415;&#26223;&#35266;&#24182;&#25552;&#39640;&#33258;&#24049;&#30340;&#25945;&#23398;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last two decades, technology use in language learning and teaching has significantly advanced and is now referred to as Computer-Assisted Language Learning (CALL). Recently, the integration of Artificial Intelligence (AI) into CALL has brought about a significant shift in the traditional approach to language education both inside and outside the classroom. In line with this book's scope, I explore the advantages and disadvantages of AI-mediated communication in language education. I begin with a brief review of AI in education. I then introduce the ICALL and give a critical appraisal of the potential of AI-powered automatic speech recognition (ASR), Machine Translation (MT), Intelligent Tutoring Systems (ITSs), AI-powered chatbots, and Extended Reality (XR). In conclusion, I argue that it is crucial for language teachers to engage in CALL teacher education and professional development to keep up with the ever-evolving technology landscape and improve their teaching effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00188</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Search for Integer Linear Programming. (arXiv:2305.00188v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#20135;&#19994;&#21644;&#31649;&#29702;&#37096;&#38376;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;&#65292;&#20999;&#25442;&#19977;&#31181;&#27169;&#24335;&#65292;&#20998;&#21035;&#20026;&#25628;&#32034;&#65292;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#24182;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#23450;&#21046;&#31639;&#23376;&#65292;&#20174;&#32780;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#25628;&#32034;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32039;&#36523;&#21160;&#20316;&#8221;&#30340;&#31639;&#23376;&#65292;&#23427;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#21464;&#37327;&#30340;&#20540;&#65292;&#35797;&#22270;&#20351;&#26576;&#20123;&#32422;&#26463;&#21464;&#24471;&#26356;&#32039;&#12290;&#23545;&#20110;&#25913;&#36827;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#23376;&#8220;&#20030;&#21319;&#21160;&#20316;&#8221;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21487;&#34892;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30446;&#26631;&#20989;&#25968;&#30340;&#36136;&#37327;&#12290;&#32467;&#21512;&#36825;&#20123;&#20869;&#23481;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Local-ILP&#12290;&#23545;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Local-ILP&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integer linear programming models a wide range of practical combinatorial optimization problems and has significant impacts in industry and management sectors. This work develops the first standalone local search solver for general integer linear programming validated on a large heterogeneous problem dataset. We propose a local search framework that switches in three modes, namely Search, Improve, and Restore modes, and design tailored operators adapted to different modes, thus improve the quality of the current solution according to different situations. For the Search and Restore modes, we propose an operator named tight move, which adaptively modifies variables' values trying to make some constraint tight. For the Improve mode, an efficient operator lift move is proposed to improve the quality of the objective function while maintaining feasibility. Putting these together, we develop a local search solver for integer linear programming called Local-ILP. Experiments conducted on the 
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2302.11351</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#27934;&#23519;&#21147;
&lt;/p&gt;
&lt;p&gt;
Regularised neural networks mimic human insight. (arXiv:2302.11351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21160;&#24577;&#21644;&#34892;&#20026;&#29305;&#24449;&#19978;&#23494;&#20999;&#27169;&#20223;&#20102;&#20154;&#31867;&#30340;&#27934;&#23519;&#21147;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#26102;&#20250;&#23637;&#29616;&#20986;&#31361;&#28982;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#30340;&#24773;&#20917;&#65292;&#36825;&#19982;&#27934;&#23519;&#21147;&#26102;&#21051;&#30456;&#20851;&#12290;&#36825;&#31181;&#27934;&#23519;&#21147;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#20284;&#20046;&#24456;&#29305;&#27530;&#65292;&#22240;&#20026;&#23427;&#20204;&#21069;&#38754;&#26377;&#19968;&#20010;&#36739;&#38271;&#26102;&#38388;&#30340;&#20725;&#23616;&#65292;&#21464;&#21270;&#24322;&#24120;&#31361;&#28982;&#65292;&#24182;&#19988;&#21482;&#21457;&#29983;&#22312;&#19968;&#37096;&#20998;&#23398;&#20064;&#32773;&#36523;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#21542;&#20063;&#23384;&#22312;&#31867;&#20284;&#27934;&#23519;&#21147;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#20915;&#31574;&#20219;&#21153;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#38544;&#34255;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20542;&#21521;&#20110;&#36890;&#36807;&#27934;&#23519;&#21147;&#21457;&#29616;&#36825;&#31181;&#35268;&#24459;&#65292;&#32780;&#19981;&#26159;&#36880;&#28176;&#21457;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24102;&#26377;&#27491;&#21017;&#21270;&#38376;&#25511;&#35843;&#33410;&#30340;&#31070;&#32463;&#32593;&#32476;&#32039;&#23494;&#27169;&#20223;&#20102;&#20154;&#31867;&#27934;&#23519;&#21147;&#30340;&#34892;&#20026;&#29305;&#24449;&#65292;&#34920;&#29616;&#20986;&#27934;&#23519;&#21147;&#30340;&#24310;&#36831;&#12289;&#31361;&#28982;&#24615;&#21644;&#36873;&#25321;&#24615;&#21457;&#29983;&#12290;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#27934;&#23519;&#21147;&#34892;&#20026;&#20851;&#38190;&#22320;&#21462;&#20915;&#20110;&#22122;&#22768;&#28155;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans sometimes show sudden improvements in task performance that have been linked to moments of insight. Such insight-related performance improvements appear special because they are preceded by an extended period of impasse, are unusually abrupt, and occur only in some, but not all, learners. Here, we ask whether insight-like behaviour also occurs in artificial neural networks trained with gradient descent algorithms. We compared learning dynamics in humans and regularised neural networks in a perceptual decision task that provided a hidden opportunity which allowed to solve the task more efficiently. We show that humans tend to discover this regularity through insight, rather than gradually. Notably, neural networks with regularised gate modulation closely mimicked behavioural characteristics of human insights, exhibiting delay of insight, suddenness and selective occurrence. Analyses of network learning dynamics revealed that insight-like behaviour crucially depended on noise adde
&lt;/p&gt;</description></item></channel></rss>