<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14274</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#35752;&#35770;&#23454;&#29616;&#28431;&#27934;&#26816;&#27979;&#30340;&#22810;&#35282;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-role Consensus through LLMs Discussions for Vulnerability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#36719;&#20214;&#36136;&#37327;&#20445;&#35777;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#19968;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#36890;&#24120;&#26159;&#27979;&#35797;&#20154;&#21592;&#65292;&#32570;&#20047;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;&#22810;&#20803;&#35266;&#28857;&#65292;&#21253;&#25324;&#24320;&#21457;&#20154;&#21592;&#21644;&#27979;&#35797;&#20154;&#21592;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#29616;&#23454;&#20195;&#30721;&#23457;&#26597;&#36807;&#31243;&#65292;&#36827;&#34892;&#35752;&#35770;&#20197;&#36798;&#25104;&#20851;&#20110;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;&#31934;&#30830;&#29575;&#22686;&#21152;&#20102;4.73&#65285;&#65292;&#21484;&#22238;&#29575;&#22686;&#21152;&#20102;58.9&#65285;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;28.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14274v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13728</link><description>&lt;p&gt;
M-HOF-Opt: &#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#65306;&#22522;&#20110;&#20056;&#23376;&#35825;&#23548;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#35768;&#22810;&#39033;&#32452;&#25104;&#26102;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23545;&#26435;&#37325;&#20056;&#23376;&#30340;&#32452;&#21512;&#36873;&#25321;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#29992;&#20110;&#32852;&#21512;&#27169;&#22411;&#21442;&#25968;&#21644;&#20056;&#23376;&#28436;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#20284;&#28982;&#65292;&#20419;&#36827;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#22810;&#30446;&#26631;&#19979;&#38477;&#12290;&#30456;&#24212;&#30340;&#21442;&#25968;&#21644;&#20056;&#23376;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#34987;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#30446;&#26631;&#19979;&#38477;&#30446;&#26631;&#34987;&#20998;&#23618;&#22320;&#20998;&#27966;&#21040;&#19968;&#31995;&#21015;&#32422;&#26463;&#20248;&#21270;&#23376;&#38382;&#39064;&#20013;&#12290;&#23376;&#38382;&#39064;&#32422;&#26463;&#26681;&#25454;&#24085;&#32047;&#25176;&#25903;&#37197;&#33258;&#21160;&#36866;&#24212;&#24182;&#20316;&#20026;&#20302;&#23618;&#20056;&#23376;&#25511;&#21046;&#22120;&#35843;&#24230;&#25439;&#22833;&#26223;&#35266;&#30340;&#35774;&#23450;&#28857;&#65292;&#36890;&#36807;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#36755;&#20986;&#21453;&#39304;&#26469;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#20056;&#23376;&#30340;&#65292;&#24182;&#19988;&#22312;&#26102;&#20195;&#23610;&#24230;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13728v1 Announce Type: new  Abstract: When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT4&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20339;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.05916</link><description>&lt;p&gt;
GPT&#20316;&#20026;&#24515;&#29702;&#23398;&#23478;&#65311;GPT-4V&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#19978;&#30340;&#21021;&#27493;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT4&#22312;&#35270;&#35273;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20854;&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20339;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#26088;&#22312;&#22788;&#29702;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#25991;&#26412;&#12289;&#35821;&#38899;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#23613;&#31649;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#23545;&#20110;&#26356;&#22909;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;MLMs&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#65292;&#36328;&#36234;&#35270;&#35273;&#24773;&#24863;&#20219;&#21153;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;5&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT4&#22312;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#35782;&#21035;&#21644;&#24494;&#34920;&#24773;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#65292;&#32780;&#20854;&#19968;&#33324;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#24615;&#33021;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23454;&#29616;&#32454;&#31890;&#24230;&#24494;&#34920;&#24773;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#19982;&#20219;&#21153;&#30456;&#20851;&#20195;&#29702;&#32467;&#21512;&#23637;&#31034;&#20102;GPT4&#22788;&#29702;&#24773;&#24863;&#35782;&#21035;&#21644;&#30456;&#20851;&#39046;&#22495;&#39640;&#32423;&#20219;&#21153;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#28508;&#21147;&#65292;&#20197;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05916v1 Announce Type: cross  Abstract: Multimodal language models (MLMs) are designed to process and integrate information from multiple sources, such as text, speech, images, and videos. Despite its success in language understanding, it is critical to evaluate the performance of downstream tasks for better human-centric applications. This paper assesses the application of MLMs with 5 crucial abilities for affective computing, spanning from visual affective tasks and reasoning tasks. The results show that GPT4 has high accuracy in facial action unit recognition and micro-expression detection while its general facial expression recognition performance is not accurate. We also highlight the challenges of achieving fine-grained micro-expression recognition and the potential for further study and demonstrate the versatility and potential of GPT4 for handling advanced tasks in emotion recognition and related fields by integrating with task-related agents for more complex tasks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05326</link><description>&lt;p&gt;
ChatASU&#65306;&#21796;&#36215;LLM&#30340;&#21453;&#24605;&#65292;&#30495;&#27491;&#29702;&#35299;&#23545;&#35805;&#20013;&#30340;&#26041;&#38754;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;Aspect Chain Reasoning&#65288;ACR&#65289;&#20219;&#21153;&#26469;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#21160;&#22330;&#26223;&#65288;&#20363;&#22914;&#65292;&#38382;&#31572;&#21644;&#23545;&#35805;&#65289;&#20013;&#36827;&#34892;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ASU&#65289;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#24573;&#30053;&#20102;&#24847;&#35265;&#30446;&#26631;&#65288;&#21363;&#26041;&#38754;&#65289;&#30340;&#20849;&#25351;&#38382;&#39064;&#65292;&#32780;&#36825;&#31181;&#29616;&#35937;&#22312;&#20114;&#21160;&#22330;&#26223;&#29305;&#21035;&#26159;&#23545;&#35805;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;ASU&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#23558;&#21508;&#31181;NLP&#20219;&#21153;&#19982;&#32842;&#22825;&#33539;&#24335;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#26041;&#38754;&#24773;&#32490;&#29702;&#35299;&#65288;ChatASU&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#25506;&#32034;LLMs&#22312;&#23545;&#35805;&#22330;&#26223;&#20013;&#29702;&#35299;&#26041;&#38754;&#24773;&#32490;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#39033;ChatASU&#20219;&#21153;&#24341;&#20837;&#20102;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#26041;&#38754;&#38142;&#25512;&#29702;&#65288;ACR&#65289;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#26041;&#38754;&#20849;&#25351;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20449;&#30340;&#33258;&#21453;&#24605;&#26041;&#27861;&#65288;TSA&#65289;&#19982;ChatGLM&#20316;&#20026;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05326v1 Announce Type: cross  Abstract: Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as back
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22836;&#37096;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40060;&#30524;&#22270;&#20687;&#36827;&#34892;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#19979;&#65292;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.18320</link><description>&lt;p&gt;
&#22522;&#20110;&#20301;&#32622;&#24341;&#23548;&#30340;&#40060;&#30524;&#22270;&#20687;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Location-guided Head Pose Estimation for Fisheye Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18320
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22836;&#37096;&#20301;&#32622;&#20449;&#24687;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#40060;&#30524;&#22270;&#20687;&#36827;&#34892;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#31471;&#21040;&#31471;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#19979;&#65292;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#40060;&#30524;&#25110;&#36229;&#24191;&#35282;&#38236;&#22836;&#30340;&#30456;&#26426;&#35206;&#30422;&#30340;&#35270;&#22330;&#24191;&#38420;&#65292;&#26080;&#27861;&#36890;&#36807;&#36879;&#35270;&#25237;&#24433;&#26469;&#24314;&#27169;&#12290;&#22270;&#20687;&#36793;&#32536;&#22788;&#20005;&#37325;&#30340;&#40060;&#30524;&#38236;&#22836;&#30072;&#21464;&#23548;&#33268;&#20102;&#22312;&#38750;&#30072;&#21464;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#29616;&#26377;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#20013;&#22836;&#37096;&#20301;&#32622;&#30340;&#30693;&#35782;&#26469;&#20943;&#23569;&#40060;&#30524;&#30072;&#21464;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22836;&#37096;&#23039;&#24577;&#21644;&#22836;&#37096;&#20301;&#32622;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#20174;&#40060;&#30524;&#22270;&#20687;&#20013;&#20272;&#35745;&#22836;&#37096;&#23039;&#24577;&#65292;&#26080;&#38656;&#26657;&#27491;&#25110;&#26631;&#23450;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#20026;BIWI&#12289;300W-LP&#21644;AFLW2000&#36825;&#19977;&#20010;&#28909;&#38376;&#22836;&#37096;&#23039;&#24577;&#20272;&#35745;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#40060;&#30524;&#30072;&#21464;&#29256;&#26412;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18320v1 Announce Type: cross  Abstract: Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye \textcolor{blue}{lens} distortion in the peripheral region of the image leads to degraded performance of the \textcolor{blue}{existing} head pose estimation models trained on undistorted images. This paper presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multi-task learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created \textcolor{blue}{a} fisheye-\textcolor{blue}{distorted} version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experim
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11319</link><description>&lt;p&gt;
&#20351;&#29992;RGBD&#24863;&#30693;&#21644;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#23545;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30340;&#28382;&#21518;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Hysteresis Compensation of Flexible Continuum Manipulator using RGBD Sensing and Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11319
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#25429;&#25417;&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#30005;&#32518;&#39537;&#21160;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#28382;&#21518;&#34917;&#20607;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36830;&#32493;&#26426;&#26800;&#33218;&#22240;&#33021;&#22815;&#36890;&#36807;&#38750;&#32447;&#24615;&#36335;&#24452;&#36827;&#20837;&#29421;&#31364;&#31354;&#38388;&#32780;&#34987;&#37325;&#35270;&#20110;&#24494;&#21019;&#25163;&#26415;&#12290;&#20294;&#26159;&#21463;&#21040;&#30005;&#32518;&#25928;&#24212;&#65288;&#22914;&#25705;&#25830;&#12289;&#20280;&#38271;&#21644;&#32806;&#21512;&#65289;&#24341;&#36215;&#30340;&#28382;&#21518;&#25928;&#24212;&#23548;&#33268;&#30005;&#32518;&#39537;&#21160;&#26426;&#26500;&#38754;&#20020;&#25511;&#21046;&#22256;&#38590;&#12290;&#36825;&#20123;&#25928;&#24212;&#30001;&#20110;&#38750;&#32447;&#24615;&#32780;&#24456;&#38590;&#24314;&#27169;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38271;&#19988;&#22810;&#33410;&#27573;&#26426;&#26800;&#33218;&#26102;&#36825;&#20123;&#22256;&#38590;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#30005;&#32518;&#39537;&#21160;&#30340;&#36825;&#31181;&#38750;&#32447;&#24615;&#21644;&#20197;&#24448;&#29366;&#24577;&#20381;&#36182;&#29305;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#23450;&#21046;&#30340;&#22522;&#20934;&#26631;&#35760;&#26469;&#25910;&#38598;&#29289;&#29702;&#20851;&#33410;&#37197;&#32622;&#20316;&#20026;&#25968;&#25454;&#38598;&#12290;&#23545;&#22235;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#65288;TCN&#65289;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#21033;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;TCN&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25511;&#21046;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11319v1 Announce Type: cross  Abstract: Flexible continuum manipulators are valued for minimally invasive surgery, offering access to confined spaces through nonlinear paths. However, cable-driven manipulators face control difficulties due to hysteresis from cabling effects such as friction, elongation, and coupling. These effects are difficult to model due to nonlinearity and the difficulties become even more evident when dealing with long and multi-segmented manipulator. This paper proposes a data-driven approach based on recurrent neural networks to capture these nonlinear and previous states-dependent characteristics of cable actuation. We design customized fiducial markers to collect physical joint configurations as a dataset. Result on a study comparing the learning performance of four Deep Neural Network (DNN) models show that the Temporal Convolution Network (TCN) demonstrates the highest predictive capability. Leveraging trained TCNs, we build a control algorithm to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15487</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#26368;&#23567;&#21270;$\mathcal{L}^2$&#20195;&#20215;&#20989;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#27969;&#65292;&#24182;&#24341;&#20837;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65307;&#19968;&#20010;&#36866;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#21478;&#19968;&#20010;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#65292;&#32771;&#34385;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25289;&#22238;&#21521;&#37327;&#19995;&#32467;&#26500;&#21644;&#22312;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25512;&#21069;&#21521;&#37327;&#19995;&#32467;&#26500;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#28385;&#36275;&#31209;&#26465;&#20214;&#65292;&#25913;&#36827;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25152;&#26377;&#36712;&#36947;&#23558;&#20197;&#22343;&#21248;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#23558;$\mathcal{L}^2$&#20195;&#20215;&#39537;&#21160;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65307;&#22240;&#27492;&#65292;&#23545;&#20110;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#25509;&#36817;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20808;&#39564;&#20572;&#27490;&#26102;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#21518;&#32773;&#19982;&#27425;Riemann&#20960;&#20309;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2306.08543</link><description>&lt;p&gt;
MiniLLM&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniLLM: Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#26356;&#23567;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;KLD&#26367;&#25442;&#26631;&#20934;KD&#26041;&#27861;&#20013;&#30340;&#21069;&#21521;KLD&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#23398;&#29983;&#27169;&#22411;&#39640;&#20272;&#25945;&#24072;&#20998;&#24067;&#30340;&#20302;&#27010;&#29575;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;KD&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#30333;&#30418;&#20998;&#31867;&#27169;&#22411;&#25110;&#35757;&#32451;&#23567;&#27169;&#22411;&#26469;&#27169;&#20223;&#22914;ChatGPT&#20043;&#31867;&#30340;&#40657;&#30418;&#27169;&#22411;API&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#30333;&#30418;LLMs&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#23567;&#27169;&#22411;&#20013;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38543;&#30528;&#24320;&#28304;LLMs&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#36825;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;KD&#26041;&#27861;&#65292;&#23558;LLMs&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08543v2 Announce Type: replace-cross  Abstract: Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named Mi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.13138</link><description>&lt;p&gt;
&#23545;AI&#20195;&#29702;&#30340;&#21487;&#35265;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visibility into AI Agents. (arXiv:2401.13138v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#12290;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#20943;&#36731;AI&#20195;&#29702;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21830;&#19994;&#12289;&#31185;&#23398;&#12289;&#25919;&#24220;&#21644;&#20010;&#20154;&#27963;&#21160;&#22996;&#25176;&#32473;&#20855;&#26377;&#26377;&#38480;&#30417;&#30563;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#21152;&#21095;&#29616;&#26377;&#30340;&#31038;&#20250;&#39118;&#38505;&#24182;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#12290;&#29702;&#35299;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#23545;&#29616;&#26377;&#27835;&#29702;&#32467;&#26500;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20462;&#35746;&#21644;&#35843;&#25972;&#65292;&#24182;&#30830;&#20445;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#38382;&#36131;&#21046;&#12290;&#25105;&#20204;&#23558;AI&#20195;&#29702;&#30340;&#20351;&#29992;&#22320;&#28857;&#12289;&#21407;&#22240;&#12289;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#32773;&#31561;&#20449;&#24687;&#31216;&#20026;&#8220;&#21487;&#35265;&#24615;&#8221;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#19978;&#36848;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31867;&#22686;&#21152;&#23545;AI&#20195;&#29702;&#21487;&#35265;&#24615;&#30340;&#25514;&#26045;&#65306;&#20195;&#29702;&#26631;&#35782;&#31526;&#12289;&#23454;&#26102;&#30417;&#25511;&#21644;&#27963;&#21160;&#35760;&#24405;&#12290;&#23545;&#20110;&#27599;&#19968;&#31181;&#25514;&#26045;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21487;&#33021;&#30340;&#23454;&#26045;&#26041;&#24335;&#65292;&#36825;&#20123;&#26041;&#24335;&#22312;&#20405;&#20837;&#24615;&#21644;&#20449;&#24687;&#24615;&#26041;&#38754;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25514;&#26045;&#22312;&#38598;&#20013;&#21270;&#21644;&#21435;&#20013;&#24515;&#21270;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increased delegation of commercial, scientific, governmental, and personal activities to AI agents -- systems capable of pursuing complex goals with limited supervision -- may exacerbate existing societal risks and introduce new risks. Understanding and mitigating these risks involves critically evaluating existing governance structures, revising and adapting these structures where needed, and ensuring accountability of key stakeholders. Information about where, why, how, and by whom certain AI agents are used, which we refer to as \textbf{visibility}, is critical to these objectives. In this paper, we assess three categories of measures to increase visibility into AI agents: \textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity logging}. For each, we outline potential implementations that vary in intrusiveness and informativeness. We analyze how the measures apply across a spectrum of centralized through decentralized deployment contexts, accounting for vario
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10831</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#27010;&#24565;&#21457;&#29616;&#29702;&#35299;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#39057;Transformer&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#20102;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#22522;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#39640;&#23618;&#26102;&#31354;&#27010;&#24565;&#30340;&#35270;&#39057;Transformer&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20197;&#24448;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#20165;&#38598;&#20013;&#22312;&#22270;&#20687;&#32423;&#20219;&#21153;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35270;&#39057;&#27169;&#22411;&#22788;&#29702;&#20102;&#39069;&#22806;&#30340;&#26102;&#38388;&#32500;&#24230;&#65292;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#21160;&#24577;&#27010;&#24565;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#35270;&#39057;Transformer&#27010;&#24565;&#21457;&#29616;(VTCD)&#31639;&#27861;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#35270;&#39057;Transformer&#34920;&#31034;&#30340;&#21333;&#20803;&#65288;&#27010;&#24565;&#65289;&#24182;&#23545;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#24471;&#21040;&#30340;&#27010;&#24565;&#20855;&#26377;&#24456;&#24378;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#35270;&#39057;&#20013;&#30340;&#26102;&#31354;&#25512;&#29702;&#26426;&#21046;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of concept-based interpretability of transformer representations for videos. Concretely, we seek to explain the decision-making process of video transformers based on high-level, spatiotemporal concepts that are automatically discovered. Prior research on concept-based interpretability has concentrated solely on image-level tasks. Comparatively, video models deal with the added temporal dimension, increasing complexity and posing challenges in identifying dynamic concepts over time. In this work, we systematically address these challenges by introducing the first Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose an efficient approach for unsupervised identification of units of video transformer representations - concepts, and ranking their importance to the output of a model. The resulting concepts are highly interpretable, revealing spatio-temporal reasoning mechanisms and object-centric representations in unstructured video m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11884</link><description>&lt;p&gt;
&#23558;Wikidata&#20998;&#31867;&#20307;&#31995;&#38598;&#25104;&#21040;YAGO&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating the Wikidata Taxonomy into YAGO. (arXiv:2308.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikidata&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#36890;&#29992;&#30693;&#35782;&#24211;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#30340;&#21512;&#20316;&#24615;&#36136;&#65292;&#20854;&#27169;&#24335;&#21644;&#20998;&#31867;&#20307;&#31995;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;YAGO 4&#30693;&#35782;&#24211;&#20013;&#65292;&#25105;&#20204;&#23558;Wikidata&#19982;Schema.org&#30340;&#26412;&#20307;&#35770;&#32467;&#21512;&#36215;&#26469;&#65292;&#20943;&#23569;&#21644;&#28165;&#29702;&#20998;&#31867;&#20307;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#25968;&#25454;&#19978;&#36816;&#34892;&#33258;&#21160;&#25512;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#33293;&#24323;&#20102;&#22823;&#37096;&#20998;&#30340;Wikidata&#20998;&#31867;&#20307;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36923;&#36753;&#32422;&#26463;&#21644;&#31867;&#19982;&#23454;&#20363;&#30340;&#32454;&#33268;&#21306;&#20998;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21019;&#24314;&#20102;YAGO 4.5&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#20013;&#30340;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12220</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#21152;&#24555;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#30340;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision. (arXiv:2307.12220v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#24314;&#31569;&#29289;&#20998;&#21106;&#20013;&#30340;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#23485;&#26494;&#30417;&#30563;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#30340;&#26377;&#25928;&#24615;&#19968;&#30452;&#21463;&#21040;&#27169;&#22411;&#20256;&#36882;&#25928;&#26524;&#30340;&#38459;&#30861;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#24314;&#31569;&#29289;&#20998;&#21106;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;U-Net&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#24320;&#21457;&#30340;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#26159;&#20174;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#26032;&#24320;&#21457;&#30340;&#39592;&#24178;&#32593;&#32476;&#24494;&#35843;&#32780;&#26469;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#30721;&#22120;&#35774;&#35745;&#30340;&#22823;&#37327;&#35745;&#31639;&#36127;&#25285;&#38459;&#30861;&#20102;&#36825;&#20123;&#29616;&#20195;&#32534;&#30721;&#22120;&#32593;&#32476;&#25104;&#21151;&#36716;&#31227;&#21040;&#36965;&#24863;&#20219;&#21153;&#19978;&#12290;&#21363;&#20351;&#26159;&#24191;&#27867;&#37319;&#29992;&#30340;&#28145;&#24230;&#30417;&#30563;&#31574;&#30053;&#20063;&#26080;&#27861;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#28151;&#21512;&#21306;&#22495;&#65292;&#21069;&#26223;&#21644;&#32972;&#26223;&#20687;&#32032;&#26159;&#20132;&#38169;&#30340;&#65292;&#23548;&#33268;&#20854;&#25439;&#22833;&#26080;&#25928;&#12290;&#26412;&#25991;&#23545;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#22120;&#32593;&#32476;&#35774;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BFSeg&#30340;&#39640;&#25928;&#26694;&#26550;&#26469;&#22686;&#24378;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#36830;&#25509;&#30340;&#31895;&#21040;&#32454;&#29305;&#24449;&#34701;&#21512;&#35299;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
The efficacy of building footprint segmentation from remotely sensed images has been hindered by model transfer effectiveness. Many existing building segmentation methods were developed upon the encoder-decoder architecture of U-Net, in which the encoder is finetuned from the newly developed backbone networks that are pre-trained on ImageNet. However, the heavy computational burden of the existing decoder designs hampers the successful transfer of these modern encoder networks to remote sensing tasks. Even the widely-adopted deep supervision strategy fails to mitigate these challenges due to its invalid loss in hybrid regions where foreground and background pixels are intermixed. In this paper, we conduct a comprehensive evaluation of existing decoder network designs for building footprint segmentation and propose an efficient framework denoted as BFSeg to enhance learning efficiency and effectiveness. Specifically, a densely-connected coarse-to-fine feature fusion decoder network that
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10054</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#33402;&#26415;&#23454;&#36341;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
A Shift In Artistic Practices through Artificial Intelligence. (arXiv:2306.10054v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10054
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22823;&#37327;&#20869;&#23481;&#30340;&#29190;&#28856;&#24341;&#21457;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#30340;&#25991;&#21270;&#36716;&#21464;&#65292;&#35282;&#33394;&#21464;&#21270;&#12289;&#20215;&#20540;&#35266;&#36716;&#21464;&#21644;&#20256;&#32479;&#21463;&#21040;&#25361;&#25112;&#12290;&#20114;&#32852;&#32593;&#19978;&#21487;&#33719;&#24471;&#30340;&#24191;&#38420;&#25968;&#25454;&#38598;&#20026;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35757;&#32451;&#21019;&#36896;&#20102;&#19968;&#20010;&#29615;&#22659;&#12290;AI&#27169;&#22411;&#30340;&#20844;&#24320;&#20849;&#20139;&#21644;&#20840;&#29699;&#20351;&#29992;&#65292;&#22914;&#20309;&#25361;&#25112;&#33402;&#26415;&#23454;&#36341;&#20013;&#30340;&#29616;&#29366;&#65311;AI&#25216;&#26415;&#23558;&#32473;&#38899;&#20048;&#12289;&#33402;&#26415;&#21644;&#26032;&#23186;&#20307;&#24102;&#26469;&#20160;&#20040;&#26679;&#30340;&#21464;&#38761;&#65311;
&lt;/p&gt;
&lt;p&gt;
The explosion of content generated by Artificial Intelligence models has initiated a cultural shift in arts, music, and media, where roles are changing, values are shifting, and conventions are challenged. The readily available, vast dataset of the internet has created an environment for AI models to be trained on any content on the web. With AI models shared openly, and used by many, globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring into music, arts, and new media?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16351</link><description>&lt;p&gt;
WeiAvg&#65306;&#20419;&#36827;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20026;&#21033;&#29992;&#22823;&#35268;&#27169;&#31169;&#26377;&#36793;&#32536;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#31561;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21442;&#19982;&#32773;&#23545;&#32852;&#37030;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#65288;WeiAvg&#65289;&#30340;&#26694;&#26550;&#65292;&#30528;&#37325;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20915;&#31574;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;RKGCN&#65292;&#23427;&#21160;&#24577;&#20998;&#26512;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#23427;&#22312;&#29289;&#21697;&#21644;&#29992;&#25143;&#21452;&#26041;&#38754;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#20016;&#23500;&#23427;&#20204;&#30340;&#34920;&#31034;&#65292;&#26368;&#22823;&#21270;&#30693;&#35782;&#22270;&#35889;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290; RKGCN&#33021;&#22815;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09820</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#21452;&#38454;&#27573;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23427;&#21033;&#29992;&#25110;&#37325;&#29992;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#20016;&#23500;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#19987;&#27880;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#24573;&#30053;&#21487;&#33021;&#23384;&#22312;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#39046;&#22495;&#24863;&#30693;&#29305;&#24449;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#21644;&#26469;&#33258;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#65288;SSD&#65289;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20854;&#24615;&#33021;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain text classification aims to adapt models to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12306</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logical Expressiveness of Graph Neural Network for Knowledge Graph Reasoning. (arXiv:2303.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#24341;&#20837;&#29992;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32570;&#20047;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#23545;&#20110;&#24402;&#32435;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21482;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#23427;&#20204;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#19978;&#36848;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36923;&#36753;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;GNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25214;&#20986;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#20197;&#25429;&#33719;&#21738;&#20123;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#20808;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20026;&#20998;&#26512;GNN&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65307;&#32780;&#19968;&#20010;&#26597;&#35810;&#26631;&#35760;&#25216;&#24039;&#20351;&#24471;GNN&#26356;&#23481;&#26131;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;&#25216;&#24039;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#20419;&#36827;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GNN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the 
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.10538</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning for Solving the Travelling Salesman Problem. (arXiv:2303.10538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10538
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;UTSP&#33021;&#22815;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20445;&#35777;&#36335;&#24452;&#20026;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#21069;&#25552;&#19979;&#65292;&#33021;&#22815;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;UTSP&#22312;&#35757;&#32451;&#26679;&#26412;&#19982;&#21442;&#25968;&#25968;&#37327;&#19978;&#21344;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;UTSP&#65292;&#19968;&#31181;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#27714;&#35299;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#25439;&#22833;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;GNN&#36755;&#20986;&#19968;&#20010;&#28909;&#21147;&#22270;&#34920;&#31034;&#27599;&#20010;&#36793;&#25104;&#20026;&#26368;&#20248;&#36335;&#24452;&#30340;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#26681;&#25454;&#28909;&#21147;&#22270;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#19968;&#37096;&#20998;&#25512;&#21160;&#27169;&#22411;&#25214;&#21040;&#26368;&#30701;&#30340;&#36335;&#24452;&#65292;&#21478;&#19968;&#37096;&#20998;&#20316;&#20026;&#32422;&#26463;&#26465;&#20214;&#65292;&#30830;&#20445;&#36335;&#24452;&#24418;&#25104;&#21704;&#23494;&#39039;&#24490;&#29615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UTSP&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;TSP&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21442;&#25968;&#25928;&#29575;&#21644;&#25968;&#25454;&#25928;&#29575;&#22343;&#36739;&#39640;&#65306;&#19982;&#24378;&#21270;&#23398;&#20064;&#25110;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#20165;&#21344;&#29992;&#32422;10&#65285;&#30340;&#21442;&#25968;&#21644;&#32422;0.2&#65285;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose UTSP, an unsupervised learning (UL) framework for solving the Travelling Salesman Problem (TSP). We train a Graph Neural Network (GNN) using a surrogate loss. The GNN outputs a heat map representing the probability for each edge to be part of the optimal path. We then apply local search to generate our final prediction based on the heat map. Our loss function consists of two parts: one pushes the model to find the shortest path and the other serves as a surrogate for the constraint that the route should form a Hamiltonian Cycle. Experimental results show that UTSP outperforms the existing data-driven TSP heuristics. Our approach is parameter efficient as well as data efficient: the model takes $\sim$ 10\% of the number of parameters and $\sim$ 0.2\% of training samples compared with reinforcement learning or supervised learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#32972;&#26223;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2302.10035</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey. (arXiv:2302.10035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#32972;&#26223;&#12289;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#31561;&#26041;&#38754;&#30340;&#30456;&#20851;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#36890;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#27169;&#22411;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;BERT&#65292;ViT&#65292;GPT&#31561;&#12290;&#21463;&#21040;&#36825;&#20123;&#27169;&#22411;&#22312;&#21333;&#19968;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#36817;&#24180;&#26469;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24182;&#24076;&#26395;&#26412;&#35770;&#25991;&#33021;&#25552;&#20379;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#24110;&#21161;&#26032;&#30740;&#31350;&#20154;&#21592;&#36861;&#36394;&#26368;&#21069;&#27839;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22238;&#39038;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#30340;&#39044;&#35757;&#32451;&#30740;&#31350;&#24037;&#20316;&#65292;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;MM-PTMs&#65289;&#30340;&#20219;&#21153;&#23450;&#20041;&#12289;&#20851;&#38190;&#25361;&#25112;&#21644;&#20248;&#21183;&#65292;&#24182;&#37325;&#28857;&#35752;&#35770;&#20102;&#25968;&#25454;&#12289;&#30446;&#26631;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;MM-PTMs&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21518;&#32493;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item></channel></rss>