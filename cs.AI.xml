<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.09477</link><description>&lt;p&gt;
&#36827;&#23637;&#20013;&#30340;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Ordinal Data Science. (arXiv:2307.09477v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#30340;&#21457;&#23637;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#65306;&#20351;&#29992;&#24207;&#21015;&#32467;&#26500;&#26469;&#34913;&#37327;&#21644;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20174;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23398;&#31185;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#24207;&#26159;&#34913;&#37327;&#65288;&#32463;&#39564;&#65289;&#25968;&#25454;&#20013;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#20351;&#29992;&#23545;&#35937;&#30340;&#25968;&#23383;&#23646;&#24615;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21457;&#23637;&#20986;&#30340;&#24207;&#21015;&#26041;&#27861;&#25968;&#37327;&#30456;&#23545;&#36739;&#23569;&#12290;&#36896;&#25104;&#36825;&#19968;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#22312;&#19978;&#20010;&#19990;&#32426;&#65292;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#26080;&#27861;&#28385;&#36275;&#24207;&#21015;&#35745;&#31639;&#25152;&#38656;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#26469;&#35828;&#65292;&#21478;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#22522;&#20110;&#39034;&#24207;&#30340;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#23545;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#36807;&#20110;&#25968;&#23398;&#20005;&#35880;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#35752;&#35770;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#21644;&#8220;&#35745;&#31639;&#8221;&#24207;&#21015;&#32467;&#26500;&#65288;&#19968;&#31867;&#29305;&#23450;&#30340;&#26377;&#21521;&#22270;&#65289;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20174;&#20854;&#20013;&#25512;&#26029;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#24207;&#21015;&#25968;&#25454;&#31185;&#23398;&#24314;&#31435;&#20026;&#19968;&#39033;&#20840;&#26032;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;&#38500;&#20102;&#19982;&#20854;&#20182;&#37325;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#30340;&#20132;&#21449;&#20114;&#34917;&#22806;&#65292;&#24191;&#27867;&#30340;&#23398;&#31185;&#39046;&#22495;&#20063;&#23558;&#21463;&#30410;&#20110;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order is one of the main instruments to measure the relationship between objects in (empirical) data. However, compared to methods that use numerical properties of objects, the amount of ordinal methods developed is rather small. One reason for this is the limited availability of computational resources in the last century that would have been required for ordinal computations. Another reason -- particularly important for this line of research -- is that order-based methods are often seen as too mathematically rigorous for applying them to real-world data. In this paper, we will therefore discuss different means for measuring and 'calculating' with ordinal structures -- a specific class of directed graphs -- and show how to infer knowledge from them. Our aim is to establish Ordinal Data Science as a fundamentally new research agenda. Besides cross-fertilization with other cornerstone machine learning and knowledge representation methods, a broad range of disciplines will benefit from t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09476</link><description>&lt;p&gt;
&#36807;&#24230;&#24605;&#32771;&#30495;&#30456;&#65306;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Overthinking the Truth: Understanding how Language Models Process False Demonstrations. (arXiv:2307.09476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34394;&#20551;&#28436;&#31034;&#26102;&#20986;&#29616;&#30340;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#29616;&#35937;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#20043;&#21518;&#23545;&#38169;&#35823;&#28436;&#31034;&#30340;&#22788;&#29702;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#65292;&#24182;&#25351;&#20986;&#20102;&#38169;&#35823;&#24402;&#32435;&#22836;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#36827;&#34892;&#22797;&#26434;&#27169;&#24335;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27169;&#20223;&#20063;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20013;&#37325;&#29616;&#19981;&#20934;&#30830;&#25110;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#26469;&#30740;&#31350;&#26377;&#23475;&#30340;&#27169;&#20223;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#30456;&#20851;&#29616;&#35937;&#65306;&#36807;&#24230;&#24605;&#32771;&#21644;&#38169;&#35823;&#24402;&#32435;&#22836;&#12290;&#31532;&#19968;&#20010;&#29616;&#35937;&#65292;&#36807;&#24230;&#24605;&#32771;&#65292;&#22312;&#32473;&#20986;&#27491;&#30830;&#19982;&#38169;&#35823;&#30340;&#23569;&#37327;&#31034;&#33539;&#26102;&#65292;&#25105;&#20204;&#20174;&#20013;&#38388;&#23618;&#35299;&#30721;&#39044;&#27979;&#12290;&#22312;&#26089;&#26399;&#23618;&#20013;&#65292;&#20004;&#31181;&#31034;&#33539;&#24341;&#36215;&#20102;&#30456;&#20284;&#30340;&#27169;&#22411;&#34892;&#20026;&#65292;&#20294;&#22312;&#26576;&#20010;&#8220;&#20851;&#38190;&#23618;&#8221;&#20043;&#21518;&#65292;&#32473;&#20986;&#38169;&#35823;&#31034;&#33539;&#30340;&#20934;&#30830;&#24615;&#36880;&#28176;&#38477;&#20302;&#12290;&#31532;&#20108;&#20010;&#29616;&#35937;&#65292;&#38169;&#35823;&#24402;&#32435;&#22836;&#65292;&#21487;&#33021;&#26159;&#36807;&#24230;&#24605;&#32771;&#30340;&#19968;&#31181;&#26426;&#21046;&#24615;&#21407;&#22240;&#65306;&#36825;&#20123;&#26159;&#20301;&#20110;&#36739;&#26202;&#23618;&#30340;&#22836;&#37096;&#65292;&#23427;&#20204;&#20851;&#27880;&#24182;&#22797;&#21046;&#20808;&#21069;&#31034;&#33539;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20854;&#21066;&#24369;&#20250;&#20943;&#23569;&#36807;&#24230;&#24605;&#32771;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.09437</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#29992;&#20110;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Conditional Slot Attention for Object Centric Learning. (arXiv:2307.09437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#23454;&#29616;&#20102;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#21644;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#65292;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#29289;&#20307;&#23618;&#27425;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25512;&#29702;&#20219;&#21153;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#28044;&#29616;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#20219;&#24847;&#25968;&#37327;&#30340;&#29289;&#20307;&#23454;&#20363;&#32465;&#23450;&#21040;&#19987;&#38376;&#30340;&#29289;&#20307;&#27133;&#20301;&#12290;&#26368;&#36817;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#26041;&#27861;&#22914;&#27133;&#20301;&#27880;&#24847;&#21147;&#21033;&#29992;&#36845;&#20195;&#24335;&#27880;&#24847;&#21147;&#23398;&#20064;&#20855;&#26377;&#21160;&#24577;&#25512;&#29702;&#23618;&#32423;&#32465;&#23450;&#30340;&#21487;&#32452;&#21512;&#34920;&#31034;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#19987;&#38376;&#30340;&#27133;&#20301;&#32465;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#39062;&#30340;&#27010;&#29575;&#24615;&#27133;&#23383;&#20856;&#65288;PSD&#65289;&#30340;&#26080;&#30417;&#30563;&#26465;&#20214;&#27133;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#23558;PSD&#23450;&#20041;&#20026;&#65288;i&#65289;&#25277;&#35937;&#30340;&#29289;&#20307;&#23618;&#27425;&#23646;&#24615;&#21521;&#37327;&#20316;&#20026;&#38190;&#65292;&#65288;ii&#65289;&#21442;&#25968;&#21270;&#39640;&#26031;&#20998;&#24067;&#20316;&#20026;&#30456;&#24212;&#30340;&#20540;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#21518;&#32493;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#20855;&#20307;&#29289;&#20307;&#23618;&#27425;&#35843;&#33410;&#20998;&#24067;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#29289;&#20307;&#21457;&#29616;&#12289;&#32452;&#21512;&#24335;&#22330;&#26223;&#29983;&#25104;&#21644;&#32452;&#21512;&#24335;&#35270;&#35273;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting object-level representations for downstream reasoning tasks is an emerging area in AI. Learning object-centric representations in an unsupervised setting presents multiple challenges, a key one being binding an arbitrary number of object instances to a specialized object slot. Recent object-centric representation methods like Slot Attention utilize iterative attention to learn composable representations with dynamic inference level binding but fail to achieve specialized slot level binding. To address this, in this paper we propose Unsupervised Conditional Slot Attention using a novel Probabilistic Slot Dictionary (PSD). We define PSD with (i) abstract object-level property vectors as key and (ii) parametric Gaussian distribution as its corresponding value. We demonstrate the benefits of the learnt specific object-level conditioning distributions in multiple downstream tasks, namely object discovery, compositional scene generation, and compositional visual reasoning. We show
&lt;/p&gt;</description></item><item><title>SLMGAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09435</link><description>&lt;p&gt;
SLMGAN: &#22312;GAN&#20013;&#21033;&#29992;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs. (arXiv:2307.09435v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09435
&lt;/p&gt;
&lt;p&gt;
SLMGAN&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#22312;&#21508;&#31181;&#29983;&#25104;&#24335;&#35821;&#38899;&#24314;&#27169;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#22914;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#35821;&#38899;&#36716;&#25442;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#23558;&#25991;&#26412;&#25110;&#35821;&#38899;&#36755;&#20837;&#26144;&#23556;&#21040;&#39044;&#35757;&#32451;SLM&#34920;&#31034;&#65292;&#28982;&#21518;&#35299;&#30721;&#30446;&#26631;&#35821;&#38899;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SLMGAN&#65292;&#20197;&#21033;&#29992;SLM&#34920;&#31034;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26694;&#26550;&#20013;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#35821;&#38899;&#36716;&#25442;&#12290;&#22312;StarGANv2-VC&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;mel&#30340;&#37492;&#21035;&#22120;&#19978;&#28155;&#21152;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;SLM&#30340;WavLM&#37492;&#21035;&#22120;&#65292;&#20197;&#21450;&#25105;&#20204;&#26032;&#35774;&#35745;&#30340;SLM&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#31995;&#32479;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#25991;&#26412;&#26631;&#31614;&#12290;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;SLMGAN&#22312;&#38646;&#26679;&#26412;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26102;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09426</link><description>&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#65306;&#22522;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#21307;&#23398;&#30740;&#31350;&#21644;&#25945;&#32946;&#20013;&#30340;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Progress in Artificial Intelligence: Anonymization in Histopathology for Biomedical Research and Education. (arXiv:2307.09426v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26102;&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24179;&#34913;&#38544;&#31169;&#21644;&#36827;&#27493;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#30340;&#36827;&#23637;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#21307;&#23398;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#21644;&#20020;&#24202;&#30149;&#29702;&#23398;&#20449;&#24687;&#23545;&#20110;&#24320;&#21457;&#25968;&#23383;&#30149;&#29702;&#23398;&#65288;DP&#65289;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23558;&#21307;&#23398;&#25968;&#25454;&#20256;&#36755;&#8220;&#23613;&#37327;&#24320;&#25918;&#8221;&#25552;&#39640;&#20102;&#25968;&#25454;&#29992;&#20110;&#27425;&#32423;&#30446;&#30340;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#23545;&#24739;&#32773;&#38544;&#31169;&#26500;&#25104;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#27861;&#35268;&#35201;&#27714;&#20445;&#25345;&#21307;&#23398;&#25968;&#25454;&#8220;&#23613;&#37327;&#23553;&#38381;&#8221;&#20197;&#36991;&#20813;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27861;&#35268;&#35201;&#27714;&#21024;&#38500;&#25935;&#24863;&#25968;&#25454;&#65292;&#20294;&#19981;&#32771;&#34385;&#29616;&#20195;&#22270;&#20687;&#21305;&#37197;&#31639;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#25968;&#25454;&#38142;&#25509;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;DP&#30340;&#26631;&#20934;&#21270;&#19981;&#36275;&#20351;&#24471;&#22312;&#25152;&#26377;WSI&#26684;&#24335;&#19978;&#24314;&#31435;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#36825;&#20123;&#25361;&#25112;&#32473;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#32773;&#22312;&#24320;&#21457;AI&#31639;&#27861;&#26102;&#22312;&#38544;&#31169;&#19982;&#36827;&#27493;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#24102;&#26469;&#20102;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of biomedical research heavily relies on access to large amounts of medical data. In the case of histopathology, Whole Slide Images (WSI) and clinicopathological information are valuable for developing Artificial Intelligence (AI) algorithms for Digital Pathology (DP). Transferring medical data "as open as possible" enhances the usability of the data for secondary purposes but poses a risk to patient privacy. At the same time, existing regulations push towards keeping medical data "as closed as necessary" to avoid re-identification risks. Generally, these legal regulations require the removal of sensitive data but do not consider the possibility of data linkage attacks due to modern image-matching algorithms. In addition, the lack of standardization in DP makes it harder to establish a single solution for all formats of WSIs. These challenges raise problems for bio-informatics researchers in balancing privacy and progress while developing AI algorithms. This paper explo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.09423</link><description>&lt;p&gt;
&#22312;NetHack&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#35268;&#27169;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Imitation Learning in NetHack. (arXiv:2307.09423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;NetHack&#28216;&#25103;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#21457;&#29616;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#21487;&#20197;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#24314;&#31435;&#20102;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;IL&#20195;&#29702;&#20154;&#30340;&#24130;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064; (IL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#24378;&#22823;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#23427;&#24448;&#24448;&#19981;&#33021;&#23436;&#20840;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#19987;&#23478;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#28145;&#20837;&#25506;&#31350;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#25193;&#22823;&#22312;&#20854;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#22312;&#37027;&#37324;&#8220;&#25193;&#22823;&#35268;&#27169;&#8221;&#24050;&#32463;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#26377;&#33021;&#21147;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20180;&#32454;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#24102;&#26469;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312; NetHack &#28216;&#25103;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31243;&#24207;&#29983;&#25104;&#12289;&#38543;&#26426;&#24615;&#12289;&#38271;&#26399;&#20381;&#36182;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#21457;&#29616; IL &#30340;&#25439;&#22833;&#21644;&#24179;&#22343;&#22238;&#25253;&#38543;&#30528;&#35745;&#31639;&#39044;&#31639;&#30340;&#21464;&#21270;&#32780;&#24179;&#28369;&#21464;&#21270;&#19988;&#24378;&#30456;&#20851;&#65292;&#20174;&#32780;&#22312;&#27169;&#22411;&#22823;&#23567;&#21644;&#26679;&#26412;&#25968;&#37327;&#26041;&#38754;&#20026;&#35757;&#32451;&#35745;&#31639;&#26368;&#20248;&#30340; IL &#20195;&#29702;&#20154;&#30340;&#35745;&#31639;&#39044;&#31639;&#24314;&#31435;&#20102;&#24130;&#24459;&#12290;&#25105;&#20204;&#39044;&#27979;&#24182;&#35757;&#32451;&#20102;&#20960;&#20010;&#20855;&#26377; IL &#30340;NetHack&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet, while powerful, many works find it is often not able to fully recover the underlying expert behavior. However, none of these works deeply investigate the role of scaling up the model and data size. Inspired by recent work in Natural Language Processing (NLP) where "scaling up" has resulted in increasingly more capable LLMs, we investigate whether carefully scaling up model and data size can bring similar improvements in the imitation learning setting. To demonstrate our findings, we focus on the game of NetHack, a challenging environment featuring procedural generation, stochasticity, long-term dependencies, and partial observability. We find IL loss and mean return scale smoothly with the compute budget and are strongly correlated, resulting in power laws for training compute-optimal IL agents with respect to model size and number of samples. We forecast and train several NetHack agents with IL an
&lt;/p&gt;</description></item><item><title>CertPri&#26159;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#31227;&#21160;&#25104;&#26412;&#30340;&#21487;&#35777;&#26126;&#20248;&#20808;&#32423;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36719;&#20214;&#30340;&#36136;&#37327;&#12290;&#23427;&#25552;&#20379;&#20102;&#24418;&#24335;&#19978;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#30456;&#23545;&#36739;&#39640;&#30340;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.09375</link><description>&lt;p&gt;
CertPri: &#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#31227;&#21160;&#25104;&#26412;&#30340;&#21487;&#35777;&#26126;&#20248;&#20808;&#32423;&#25216;&#26415;&#65292;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space. (arXiv:2307.09375v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09375
&lt;/p&gt;
&lt;p&gt;
CertPri&#26159;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#31227;&#21160;&#25104;&#26412;&#30340;&#21487;&#35777;&#26126;&#20248;&#20808;&#32423;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36719;&#20214;&#30340;&#36136;&#37327;&#12290;&#23427;&#25552;&#20379;&#20102;&#24418;&#24335;&#19978;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#30456;&#23545;&#36739;&#39640;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#34920;&#29616;&#20986;&#22833;&#25511;&#34892;&#20026;&#65292;&#29978;&#33267;&#23548;&#33268;&#19981;&#21487;&#36870;&#36716;&#30340;&#28798;&#38590;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#22522;&#20110;DNN&#30340;&#36719;&#20214;&#30340;&#22833;&#25511;&#34892;&#20026;&#24182;&#25552;&#39640;DNN&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#27979;&#35797;&#36755;&#20837;&#20248;&#20808;&#32423;&#26159;&#30830;&#20445;DNN&#36136;&#37327;&#30340;&#19968;&#31181;&#26368;&#21560;&#24341;&#20154;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#23427;&#26681;&#25454;&#27979;&#35797;&#36755;&#20837;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#20415;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#21644;&#25163;&#21160;&#26631;&#35760;&#24037;&#20316;&#20013;&#33021;&#22815;&#26356;&#26089;&#22320;&#35782;&#21035;&#20986;&#26356;&#22810;&#30340;&#26377;&#32570;&#38519;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20248;&#20808;&#32423;&#26041;&#27861;&#22312;&#21487;&#35777;&#26126;&#24615;&#12289;&#25928;&#26524;&#24615;&#21644;&#36890;&#29992;&#24615;&#19977;&#20010;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CertPri&#65292;&#19968;&#31181;&#22522;&#20110;DNN&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#27979;&#35797;&#36755;&#20837;&#31227;&#21160;&#25104;&#26412;&#35282;&#24230;&#35774;&#35745;&#30340;&#27979;&#35797;&#36755;&#20837;&#20248;&#20808;&#32423;&#25216;&#26415;&#12290;CertPri&#22312;&#20197;&#19979;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65306;&#65288;1&#65289;&#21487;&#35777;&#26126;&#24615;&#65306;&#23427;&#20026;&#31227;&#21160;&#25104;&#26412;&#25552;&#20379;&#20102;&#24418;&#24335;&#19978;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65307;&#65288;2&#65289;&#26377;&#25928;&#24615;&#65306;&#23427;&#21033;&#29992;&#24418;&#24335;&#19978;&#20445;&#35777;&#30340;&#31227;&#21160;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters. Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality. Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts. However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability. To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space. CertPri differs from previous works in three key aspects: (1) certifiable: it provides a formal robustness guarantee for the movement cost; (2) effective: it leverages formally guaranteed movement c
&lt;/p&gt;</description></item><item><title>&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#21512;&#20316;&#20219;&#21153;&#35201;&#27714;&#20195;&#29702;&#22312;&#36866;&#24403;&#30340;&#26102;&#21051;&#36890;&#36807;&#36890;&#20449;&#26469;&#35843;&#25972;&#24847;&#22270;&#65292;&#20197;&#36798;&#21040;&#20840;&#23616;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.09364</link><description>&lt;p&gt;
&#22312;&#21512;&#20316;&#20114;&#21160;&#20013;&#65292;&#23616;&#37096;&#26497;&#23567;&#20540;&#39537;&#21160;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Local Minima Drive Communications in Cooperative Interaction. (arXiv:2307.09364v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09364
&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#21512;&#20316;&#20219;&#21153;&#35201;&#27714;&#20195;&#29702;&#22312;&#36866;&#24403;&#30340;&#26102;&#21051;&#36890;&#36807;&#36890;&#20449;&#26469;&#35843;&#25972;&#24847;&#22270;&#65292;&#20197;&#36798;&#21040;&#20840;&#23616;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#65288;HRI&#65289;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#26159;&#20160;&#20040;&#26102;&#20505;&#19968;&#20010;&#20195;&#29702;&#24212;&#35813;&#20915;&#23450;&#36827;&#34892;&#36890;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#12290;&#24863;&#30693;&#25511;&#21046;&#29702;&#35770;&#65288;PCT&#65289;&#21578;&#35785;&#25105;&#20204;&#65292;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#30456;&#21516;&#30340;&#8220;&#24847;&#22270;&#8221;&#26469;&#21512;&#20316;&#23436;&#25104;&#19968;&#20010;&#32852;&#21512;&#20219;&#21153;&#65292;&#20174;&#32780;&#23558;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#24037;&#20316;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#20195;&#29702;&#20043;&#38388;&#12290;&#21363;&#20351;&#23545;&#20110;&#19981;&#20855;&#22791;&#30456;&#21516;&#33021;&#21147;&#21644;&#30446;&#26631;&#21487;&#35266;&#27979;&#30340;&#20195;&#29702;&#65292;&#21482;&#35201;&#20854;&#32452;&#21512;&#34892;&#21160;&#36275;&#20197;&#23436;&#25104;&#20219;&#21153;&#19988;&#25628;&#32034;&#31354;&#38388;&#20013;&#27809;&#26377;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#21512;&#20316;&#20219;&#21153;&#20063;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#20219;&#21153;&#65292;&#21482;&#26377;&#22312;&#36866;&#24403;&#26102;&#21051;&#33267;&#23569;&#19968;&#20010;&#20195;&#29702;&#25913;&#21464;&#20854;&#24847;&#22270;&#65292;&#25165;&#33021;&#36798;&#21040;&#20840;&#23616;&#35299;&#65292;&#32780;&#36825;&#21482;&#33021;&#36890;&#36807;&#36866;&#26102;&#30340;&#36890;&#20449;&#26469;&#23454;&#29616;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#36866;&#26102;&#30340;&#36890;&#20449;&#21487;&#20197;&#20419;&#20351;&#20195;&#29702;&#22312;&#36866;&#24403;&#30340;&#26102;&#21051;&#35843;&#25972;&#20854;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important open question in human-robot interaction (HRI) is precisely when an agent should decide to communicate, particularly in a cooperative task. Perceptual Control Theory (PCT) tells us that agents are able to cooperate on a joint task simply by sharing the same 'intention', thereby distributing the effort required to complete the task among the agents. This is even true for agents that do not possess the same abilities, so long as the goal is observable, the combined actions are sufficient to complete the task, and there is no local minimum in the search space. If these conditions hold, then a cooperative task can be accomplished without any communication between the contributing agents. However, for tasks that do contain local minima, the global solution can only be reached if at least one of the agents adapts its intention at the appropriate moments, and this can only be achieved by appropriately timed communication. In other words, it is hypothesised that in cooperative tas
&lt;/p&gt;</description></item><item><title>MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.09361</link><description>&lt;p&gt;
MOCA: &#33258;&#30417;&#30563;&#23398;&#20064;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments. (arXiv:2307.09361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09361
&lt;/p&gt;
&lt;p&gt;
MOCA&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#25513;&#30721;&#24335;&#22312;&#32447;&#30721;&#26412;&#20998;&#37197;&#26469;&#23454;&#29616;&#34920;&#31034;&#23398;&#20064;&#12290;&#23427;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#21644;&#23545;&#22270;&#20687;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#21644;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#35757;&#32451;&#36895;&#24230;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#32531;&#35299;Vision Transformer&#32593;&#32476;&#23545;&#22823;&#22411;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#36138;&#23146;&#38656;&#27714;&#12290;&#19981;&#21516;&#31867;&#21035;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#20855;&#26377;&#33391;&#22909;&#35821;&#22659;&#25512;&#29702;&#23646;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#31574;&#30053;&#65292;&#25110;&#32773;&#23545;&#22270;&#20687;&#25200;&#21160;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#34920;&#31034;&#65292;&#20363;&#22914;&#20351;&#29992;&#23545;&#27604;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#12289;&#29420;&#31435;&#30340;&#26041;&#27861;MOCA&#65292;&#20351;&#29992;&#22522;&#20110;&#39640;&#32423;&#29305;&#24449;&#65288;&#32780;&#19981;&#26159;&#20687;&#32032;&#32423;&#32454;&#33410;&#65289;&#23450;&#20041;&#30340;&#26032;&#22411;&#25513;&#30721;&#21644;&#39044;&#27979;&#30446;&#26631;&#26469;&#32479;&#19968;&#36825;&#20004;&#31181;&#26399;&#26395;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#21327;&#21516;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26377;&#25928;&#22320;&#24212;&#29992;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#22312;&#20302;&#26679;&#26412;&#35774;&#32622;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20854;&#35757;&#32451;&#36895;&#24230;&#33267;&#23569;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning can be used for mitigating the greedy needs of Vision Transformer networks for very large fully-annotated datasets. Different classes of self-supervised learning offer representations with either good contextual reasoning properties, e.g., using masked image modeling strategies, or invariance to image perturbations, e.g., with contrastive methods. In this work, we propose a single-stage and standalone method, MOCA, which unifies both desired properties using novel mask-and-predict objectives defined with high-level features (instead of pixel-level details). Moreover, we show how to effectively employ both learning paradigms in a synergistic and computation-efficient way. Doing so, we achieve new state-of-the-art results on low-shot settings and strong experimental results in various evaluation protocols with a training that is at least 3 times faster than prior methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09342</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Learning to Select SAT Encodings for Pseudo-Boolean and Linear Integer Constraints. (arXiv:2307.09342v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#25972;&#25968;&#32422;&#26463;&#30340;SAT&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#32452;&#29305;&#24449;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65292;&#24182;&#19988;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#26032;&#29305;&#24449;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32422;&#26463;&#28385;&#36275;&#21644;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#23427;&#20204;&#32534;&#30721;&#20026;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#65288;SAT&#65289;&#30340;&#23454;&#20363;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#32422;&#26463;&#31867;&#22411;&#22312;&#25991;&#29486;&#20013;&#20063;&#26377;&#24456;&#22810;&#32534;&#30721;&#26041;&#24335;&#65292;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#32534;&#30721;&#26041;&#24335;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#23454;&#20363;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#37319;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;&#36873;&#25321;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#30340;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#32422;&#26463;&#38382;&#39064;&#30340;&#29305;&#24449;&#38598;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#38376;&#20026;&#20266;&#24067;&#23572;&#21644;&#32447;&#24615;&#32422;&#26463;&#35774;&#35745;&#30340;&#19968;&#32452;&#26032;&#29305;&#24449;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#36873;&#25321;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#31867;&#21035;&#30340;&#32534;&#30721;&#26041;&#24335;&#26102;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;&#24403;&#20351;&#29992;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;AutoFolio&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23454;&#20363;&#29305;&#24449;&#23545;&#20110;&#36873;&#25321;&#32534;&#30721;&#26041;&#24335;&#20219;&#21153;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many constraint satisfaction and optimisation problems can be solved effectively by encoding them as instances of the Boolean Satisfiability problem (SAT). However, even the simplest types of constraints have many encodings in the literature with widely varying performance, and the problem of selecting suitable encodings for a given problem instance is not trivial. We explore the problem of selecting encodings for pseudo-Boolean and linear constraints using a supervised machine learning approach. We show that it is possible to select encodings effectively using a standard set of features for constraint problems; however we obtain better performance with a new set of features specifically designed for the pseudo-Boolean and linear constraints. In fact, we achieve good results when selecting encodings for unseen problem classes. Our results compare favourably to AutoFolio when using the same feature set. We discuss the relative importance of instance features to the task of selecting the
&lt;/p&gt;</description></item><item><title>Company2Vec&#26159;&#19968;&#31181;&#22522;&#20110;&#20225;&#19994;&#32593;&#31449;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#20225;&#19994;&#27963;&#21160;&#24182;&#20445;&#25345;&#35821;&#20041;&#32467;&#26500;&#65292;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20225;&#19994;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#35821;&#20041;&#21830;&#19994;&#20998;&#26512;&#12289;&#34892;&#19994;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09332</link><description>&lt;p&gt;
&#22522;&#20110;&#20225;&#19994;&#32593;&#31449;&#30340;&#24503;&#22269;&#20225;&#19994;&#23884;&#20837;Company2Vec
&lt;/p&gt;
&lt;p&gt;
Company2Vec -- German Company Embeddings based on Corporate Websites. (arXiv:2307.09332v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09332
&lt;/p&gt;
&lt;p&gt;
Company2Vec&#26159;&#19968;&#31181;&#22522;&#20110;&#20225;&#19994;&#32593;&#31449;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#20225;&#19994;&#27963;&#21160;&#24182;&#20445;&#25345;&#35821;&#20041;&#32467;&#26500;&#65292;&#29983;&#25104;&#32454;&#31890;&#24230;&#30340;&#20225;&#19994;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#35821;&#20041;&#21830;&#19994;&#20998;&#26512;&#12289;&#34892;&#19994;&#39044;&#27979;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#24212;&#29992;&#65292;&#21363;Company2Vec&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;Word2Vec&#21644;&#38477;&#32500;&#25216;&#26415;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#20225;&#19994;&#32593;&#31449;&#25968;&#25454;&#20013;&#20998;&#26512;&#20225;&#19994;&#27963;&#21160;&#12290;Company2Vec&#20445;&#25345;&#35821;&#20041;&#35821;&#35328;&#32467;&#26500;&#65292;&#20174;&#32780;&#22312;&#32454;&#31890;&#24230;&#34892;&#19994;&#20013;&#21019;&#24314;&#39640;&#25928;&#30340;&#20225;&#19994;&#23884;&#20837;&#12290;&#36825;&#20123;&#35821;&#20041;&#23884;&#20837;&#21487;&#20197;&#22312;&#38134;&#34892;&#19994;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#20844;&#21496;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#21487;&#20197;&#36827;&#34892;&#35821;&#20041;&#21830;&#19994;&#20998;&#26512;&#65288;&#20363;&#22914;&#26576;&#20844;&#21496;&#30340;&#21069;n&#20010;&#21333;&#35789;&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#21363;&#34892;&#19994;&#39044;&#27979;&#12290;&#23884;&#20837;&#30340;&#21521;&#37327;&#32467;&#26500;&#21487;&#20197;&#36890;&#36807;&#20313;&#24358;&#36317;&#31163;&#26469;&#27979;&#37327;&#20844;&#21496;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#19982;&#26631;&#20934;&#34892;&#19994;&#26631;&#31614;&#65288;NACE&#65289;&#30456;&#27604;&#65292;Company2Vec&#21487;&#20197;&#26356;&#31934;&#32454;&#22320;&#27604;&#36739;&#20844;&#21496;&#12290;&#36825;&#19968;&#29305;&#24615;&#23545;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65288;&#22914;&#32858;&#31867;&#65289;&#26159;&#30456;&#20851;&#30340;&#12290;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;k-means&#32858;&#31867;&#30340;&#26367;&#20195;&#34892;&#19994;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
With Company2Vec, the paper proposes a novel application in representation learning. The model analyzes business activities from unstructured company website data using Word2Vec and dimensionality reduction. Company2Vec maintains semantic language structures and thus creates efficient company embeddings in fine-granular industries. These semantic embeddings can be used for various applications in banking. Direct relations between companies and words allow semantic business analytics (e.g. top-n words for a company). Furthermore, industry prediction is presented as a supervised learning application and evaluation method. The vectorized structure of the embeddings allows measuring companies similarities with the cosine distance. Company2Vec hence offers a more fine-grained comparison of companies than the standard industry labels (NACE). This property is relevant for unsupervised learning tasks, such as clustering. An alternative industry segmentation is shown with k-means clustering on 
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;&#23383;&#27573;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#24182;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#25913;&#36827;&#20102;&#23383;&#27573;&#20043;&#38388;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09321</link><description>&lt;p&gt;
&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#24615;&#36827;&#34892;&#20998;&#31867;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Field Dependencies for Learning on Categorical Data. (arXiv:2307.09321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09321
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;&#23383;&#27573;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#27573;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#24182;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#25913;&#36827;&#20102;&#23383;&#27573;&#20043;&#38388;&#30340;&#24314;&#27169;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#38598;&#20013;&#23383;&#27573;&#65288;&#20063;&#31216;&#20026;&#29305;&#24449;&#65289;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#28857;&#23884;&#20837;&#30340;&#20998;&#31867;/&#22238;&#24402;&#25439;&#22833;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#23383;&#27573;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#19981;&#26159;&#20840;&#23616;&#24314;&#27169;&#29305;&#24449;&#30340;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#29305;&#24449;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#65289;&#65292;&#32780;&#26159;&#23398;&#20064;&#19968;&#20010;&#25429;&#25417;&#23383;&#27573;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#26435;&#37325;&#65288;&#31216;&#20026;&#23616;&#37096;&#20381;&#36182;&#24314;&#27169;&#65289;&#22312;&#23454;&#20363;&#32423;&#21035;&#19978;&#25913;&#36827;&#20840;&#23616;&#23383;&#27573;&#20381;&#36182;&#30697;&#38453;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#20869;&#37096;&#24490;&#29615;&#20013;&#26080;&#38656;&#20351;&#29992;&#26631;&#31614;&#23601;&#21487;&#20197;&#32454;&#21270;&#20381;&#36182;&#30697;&#38453;&#65292;&#32780;&#22806;&#37096;&#24490;&#29615;&#21017;&#20132;&#32455;&#20102;&#23884;&#20837;&#30697;&#38453;&#21644;&#20381;&#36182;&#30697;&#38453;&#30340;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches for learning on categorical data underexploit the dependencies between columns (\aka fields) in a dataset because they rely on the embedding of data points driven alone by the classification/regression loss. In contrast, we propose a novel method for learning on categorical data with the goal of exploiting dependencies between fields. Instead of modelling statistics of features globally (i.e., by the covariance matrix of features), we learn a global field dependency matrix that captures dependencies between fields and then we refine the global field dependency matrix at the instance-wise level with different weights (so-called local dependency modelling) w.r.t. each field to improve the modelling of the field dependencies. Our algorithm exploits the meta-learning paradigm, i.e., the dependency matrices are refined in the inner loop of the meta-learning algorithm without the use of labels, whereas the outer loop intertwines the updates of the embedding matrix (the
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#39033;&#30446;&#65292;&#21033;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#22312;GPU&#19978;&#36890;&#36807;Python JAX&#26694;&#26550;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#20171;&#32461;&#20102;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#21644;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#30340;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.09320</link><description>&lt;p&gt;
&#29983;&#29289;&#21046;&#36896;&#32773;CA&#65306;&#20351;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#30340;&#29983;&#29289;&#21046;&#36896;&#32773;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Biomaker CA: a Biome Maker project using Cellular Automata. (arXiv:2307.09320v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09320
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#39033;&#30446;&#65292;&#21033;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#22312;GPU&#19978;&#36890;&#36807;Python JAX&#26694;&#26550;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#12290;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#20171;&#32461;&#20102;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#21644;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#30340;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#65306;&#20351;&#29992;&#32454;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#30340;&#29983;&#29289;&#21046;&#36896;&#32773;&#39033;&#30446;&#12290;&#22312;&#29983;&#29289;&#21046;&#36896;&#32773;CA&#20013;&#65292;&#24418;&#24577;&#21457;&#29983;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20027;&#39064;&#65292;&#23567;&#31181;&#23376;&#38656;&#35201;&#22312;&#20859;&#20998;&#21294;&#20047;&#30340;&#29615;&#22659;&#20013;&#25104;&#38271;&#20026;&#26893;&#29289;&#29366;&#30340;&#29983;&#29289;&#20307;&#65292;&#26368;&#32456;&#36890;&#36807;&#21464;&#24322;&#26469;&#32321;&#27542;&#65292;&#20197;&#20351;&#29983;&#29289;&#32676;&#31995;&#24471;&#20197;&#38271;&#26399;&#23384;&#27963;&#12290;&#25105;&#20204;&#36890;&#36807;2D&#32593;&#26684;&#19978;&#30340;CA&#35268;&#21017;&#27169;&#25311;&#22797;&#26434;&#30340;&#29983;&#29289;&#32676;&#31995;&#65292;&#24182;&#36890;&#36807;Python JAX&#26694;&#26550;&#22312;GPU&#19978;&#24182;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#39033;&#30446;&#20801;&#35768;&#22810;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#29615;&#22659;&#21644;&#8220;&#29289;&#29702;&#8221;&#23450;&#24459;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#31361;&#21464;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19968;&#20123;&#37197;&#32622;&#65292;&#23637;&#31034;&#20102;&#26893;&#29289;&#20195;&#29702;&#22914;&#20309;&#29983;&#38271;&#12289;&#23384;&#27963;&#12289;&#32321;&#27542;&#21644;&#36827;&#21270;&#65292;&#24418;&#25104;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#30340;&#29983;&#29289;&#32676;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31471;&#21040;&#31471;&#20803;&#36827;&#21270;&#25110;&#26356;&#31934;&#30830;&#39640;&#25928;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;&#22521;&#20859;&#30399;&#20803;&#36827;&#21270;&#65289;&#26469;&#20351;&#27169;&#22411;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#29983;&#23384;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;&#39640;&#24615;&#33021;&#35745;&#31639;&#20197;&#35780;&#20272;&#29983;&#29289;&#32676;&#31995;&#30340;&#31283;&#23450;&#24615;&#21644;&#36827;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA). In Biomaker CA, morphogenesis is a first class citizen and small seeds need to grow into plant-like organisms to survive in a nutrient starved environment and eventually reproduce with variation so that a biome survives for long timelines. We simulate complex biomes by means of CA rules in 2D grids and parallelize all of its computation on GPUs through the Python JAX framework. We show how this project allows for several different kinds of environments and laws of 'physics', alongside different model architectures and mutation strategies. We further analyze some configurations to show how plant agents can grow, survive, reproduce, and evolve, forming stable and unstable biomes. We then demonstrate how one can meta-evolve models to survive in a harsh environment either through end-to-end meta-evolution or by a more surgical and efficient approach, called Petri dish meta-evolution. Finally, we show how to perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DCE-RD&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#20214;&#22270;&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#20316;&#20026;&#22810;&#35270;&#35282;&#35299;&#37322;&#65292;&#36827;&#19968;&#27493;&#32858;&#21512;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#35875;&#35328;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09296</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26679;&#30340;&#21453;&#20107;&#23454;&#35777;&#25454;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rumor Detection with Diverse Counterfactual Evidence. (arXiv:2307.09296v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DCE-RD&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#20214;&#22270;&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#20316;&#20026;&#22810;&#35270;&#35282;&#35299;&#37322;&#65292;&#36827;&#19968;&#27493;&#32858;&#21512;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#35875;&#35328;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#22686;&#38271;&#21152;&#21095;&#20102;&#34394;&#20551;&#26032;&#38395;&#23545;&#20010;&#20154;&#21644;&#31038;&#21306;&#30340;&#23041;&#32961;&#12290;&#36825;&#23545;&#20110;&#24320;&#21457;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20511;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#21033;&#29992;&#35875;&#35328;&#20256;&#25773;&#36807;&#31243;&#30340;&#21518;&#20256;&#25773;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GNNs&#30340;&#40657;&#31665;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;&#35875;&#35328;&#26816;&#27979;&#30340;&#22266;&#26377;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#37319;&#29992;&#20102;&#25152;&#26377;&#20256;&#25773;&#27169;&#24335;&#65292;&#32467;&#26524;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DCE-RD&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#26694;&#26550;&#65292;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#20107;&#20214;&#22270;&#30340;&#22810;&#26679;&#36870;&#20107;&#23454;&#35777;&#25454;&#20316;&#20026;&#22810;&#35270;&#35282;&#35299;&#37322;&#65292;&#36827;&#32780;&#32858;&#21512;&#20197;&#33719;&#24471;&#40065;&#26834;&#30340;&#35875;&#35328;&#26816;&#27979;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#23376;&#22270;&#29983;&#25104;&#31574;&#30053;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#20107;&#20214;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth in social media has exacerbated the threat of fake news to individuals and communities. This draws increasing attention to developing efficient and timely rumor detection methods. The prevailing approaches resort to graph neural networks (GNNs) to exploit the post-propagation patterns of the rumor-spreading process. However, these methods lack inherent interpretation of rumor detection due to the black-box nature of GNNs. Moreover, these methods suffer from less robust results as they employ all the propagation patterns for rumor detection. In this paper, we address the above issues with the proposed Diverse Counterfactual Evidence framework for Rumor Detection (DCE-RD). Our intuition is to exploit the diverse counterfactual evidence of an event graph to serve as multi-view interpretations, which are further aggregated for robust rumor detection results. Specifically, our method first designs a subgraph generation strategy to efficiently generate different subgraphs of the e
&lt;/p&gt;</description></item><item><title>Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2307.09288</link><description>&lt;p&gt;
Llama 2: &#24320;&#25918;&#22522;&#30784;&#21644;&#20248;&#21270;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Llama 2: Open Foundation and Fine-Tuned Chat Models. (arXiv:2307.09288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09288
&lt;/p&gt;
&lt;p&gt;
Llama 2&#26159;&#19968;&#20010;&#20248;&#21270;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36890;&#36807;fine-tuned&#25216;&#26415;&#21644;&#23433;&#20840;&#25913;&#36827;&#65292;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#21457;&#24067;&#20102;Llama 2&#65292;&#19968;&#20010;&#21253;&#21547;&#39044;&#35757;&#32451;&#21644;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20854;&#35268;&#27169;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#19981;&#31561;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;LLM&#65292;&#31216;&#20026;Llama 2-Chat&#65292;&#22312;&#23545;&#35805;&#20351;&#29992;&#26696;&#20363;&#20013;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#12290;&#26681;&#25454;&#25105;&#20204;&#23545;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#65292;&#23427;&#20204;&#21487;&#33021;&#26159;&#38381;&#28304;&#27169;&#22411;&#30340;&#21512;&#36866;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;Llama 2-Chat&#30340;&#20248;&#21270;&#21644;&#23433;&#20840;&#24615;&#25913;&#36827;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#35753;&#31038;&#21306;&#33021;&#22815;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#26500;&#24314;&#24182;&#20026;LLM&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#19977;&#32500;&#36830;&#20307;&#32593;&#32476;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#23618;&#27425;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#36827;&#34892;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#27010;&#35272;&#12290;</title><link>http://arxiv.org/abs/2307.09274</link><description>&lt;p&gt;
&#36890;&#36807;3D&#36830;&#20307;&#32593;&#32476;&#25913;&#36827;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Improving Text Semantic Similarity Modeling through a 3D Siamese Network. (arXiv:2307.09274v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09274
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#19977;&#32500;&#36830;&#20307;&#32593;&#32476;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20445;&#30041;&#20102;&#23618;&#27425;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#36827;&#34892;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#27010;&#35272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36830;&#20307;&#32593;&#32476;&#24050;&#25104;&#20026;&#24314;&#27169;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#27744;&#21270;&#25805;&#20316;&#26469;&#21387;&#32553;&#32534;&#30721;&#20013;Transformer&#22359;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#20174;&#32780;&#24471;&#21040;&#20108;&#32500;&#35821;&#20041;&#21521;&#37327;&#24182;&#20002;&#22833;Transformer&#22359;&#20013;&#30340;&#23618;&#27425;&#21270;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26377;&#38480;&#30340;&#35821;&#20041;&#21521;&#37327;&#32467;&#26500;&#31867;&#20284;&#20110;&#19968;&#20010;&#34987;&#38138;&#24179;&#30340;&#22320;&#24418;&#65292;&#38480;&#21046;&#20102;&#19979;&#28216;&#24314;&#27169;&#20013;&#21487;&#20197;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#22312;&#36825;&#20010;&#24179;&#38754;&#19978;&#36827;&#34892;&#23548;&#33322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24314;&#27169;&#30340;3D&#36830;&#20307;&#32593;&#32476;&#65292;&#23427;&#23558;&#35821;&#20041;&#20449;&#24687;&#26144;&#23556;&#21040;&#19968;&#20010;&#26356;&#39640;&#32500;&#30340;&#31354;&#38388;&#20013;&#12290;&#19977;&#32500;&#35821;&#20041;&#24352;&#37327;&#19981;&#20165;&#20445;&#30041;&#20102;&#26356;&#31934;&#30830;&#30340;&#31354;&#38388;&#21644;&#29305;&#24449;&#39046;&#22495;&#20449;&#24687;&#65292;&#36824;&#20026;&#20840;&#38754;&#30340;&#19979;&#28216;&#24314;&#27169;&#31574;&#30053;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#32467;&#26500;&#26465;&#20214;&#26469;&#25429;&#25417;&#36825;&#20123;&#20449;&#24687;&#12290;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#19978;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Siamese networks have gained popularity as a method for modeling text semantic similarity. Traditional methods rely on pooling operation to compress the semantic representations from Transformer blocks in encoding, resulting in two-dimensional semantic vectors and the loss of hierarchical semantic information from Transformer blocks. Moreover, this limited structure of semantic vectors is akin to a flattened landscape, which restricts the methods that can be applied in downstream modeling, as they can only navigate this flat terrain. To address this issue, we propose a novel 3D Siamese network for text semantic similarity modeling, which maps semantic information to a higher-dimensional space. The three-dimensional semantic tensors not only retains more precise spatial and feature domain information but also provides the necessary structural condition for comprehensive downstream modeling strategies to capture them. Leveraging this structural advantage, we introduce several modules to 
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09244</link><description>&lt;p&gt;
&#38754;&#21521;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#26159;&#20174;&#21333;&#20010;&#35745;&#37327;&#28857;&#33719;&#21462;&#23478;&#24237;&#25110;&#20225;&#19994;&#24635;&#30005;&#21147;&#28040;&#32791;&#30340;&#30005;&#22120;&#32423;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#30005;&#22120;&#32423;&#25968;&#25454;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#38656;&#27714;&#21709;&#24212;&#24212;&#29992;&#12289;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20197;&#21450;&#25552;&#39640;&#33021;&#25928;&#21644;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#24847;&#35782;&#25552;&#39640;&#21644;&#28608;&#21169;&#12290;&#26368;&#36817;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#22312;NILM&#20998;&#31867;&#20013;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#24182;&#35777;&#26126;&#22312;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#19979;&#23545;NILM&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38543;&#30528;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DL&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#26469;&#22686;&#24378;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#20174;&#27979;&#37327;&#25968;&#25454;&#38598;&#21512;&#25104;&#30340;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
&lt;/p&gt;</description></item><item><title>&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#65292;&#28085;&#30422;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#24320;&#21457;&#65292;&#24182;&#24378;&#35843;&#20102;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09225</link><description>&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;: &#19968;&#20010;&#24635;&#20307;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Human Body Digital Twin: A Master Plan. (arXiv:2307.09225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09225
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#65292;&#28085;&#30422;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#31561;&#32452;&#25104;&#37096;&#20998;&#30340;&#24320;&#21457;&#65292;&#24182;&#24378;&#35843;&#20102;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#20855;&#26377;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#24247;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36127;&#36131;&#21644;&#26377;&#25928;&#30340;&#23454;&#26045;&#38656;&#35201;&#32771;&#34385;&#21508;&#31181;&#22240;&#32032;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#21069;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#32423;&#21457;&#23637;&#36335;&#32447;&#22270;&#12290;&#36335;&#32447;&#22270;&#28085;&#30422;&#20102;&#21508;&#31181;&#32452;&#25104;&#37096;&#20998;&#30340;&#21457;&#23637;&#65292;&#22914;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#25968;&#25454;&#25910;&#38598;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#31995;&#32479;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24517;&#39035;&#35299;&#20915;&#30340;&#25903;&#25345;&#12289;&#23433;&#20840;&#12289;&#25104;&#26412;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#30340;&#36127;&#36131;&#21644;&#26377;&#25928;&#23454;&#26045;&#12290;&#25152;&#25552;&#20986;&#30340;&#36335;&#32447;&#22270;&#20026;&#25351;&#23548;&#26410;&#26469;&#21457;&#23637;&#25552;&#20379;&#20102;&#26694;&#26550;&#65292;&#24182;&#20026;&#25506;&#32034;&#20154;&#20307;&#25968;&#23383;&#23402;&#29983;&#30340;&#26410;&#26469;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#20419;&#36827;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#20013;&#30340;&#26032;&#23398;&#31185;&#30740;&#31350;&#21644;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The human body DT has the potential to revolutionize healthcare and wellness, but its responsible and effective implementation requires consideration of various factors. This article presents a comprehensive overview of the current status and future prospects of the human body DT and proposes a five-level roadmap for its development. The roadmap covers the development of various components, such as wearable devices, data collection, data analysis, and decision-making systems. The article also highlights the necessary support, security, cost, and ethical considerations that must be addressed in order to ensure responsible and effective implementation of the human body DT. The proposed roadmap provides a framework for guiding future development and offers a unique perspective on the future of the human body DT, facilitating new interdisciplinary research and innovative solutions in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2307.09218</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#36229;&#36234;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning. (arXiv:2307.09218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09218
&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#19981;&#20165;&#38480;&#20110;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#19982;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#36951;&#24536;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#65292;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#24536;&#25351;&#30340;&#26159;&#20808;&#21069;&#33719;&#21462;&#30340;&#20449;&#24687;&#25110;&#30693;&#35782;&#30340;&#20007;&#22833;&#25110;&#24694;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20851;&#20110;&#36951;&#24536;&#30340;&#35843;&#26597;&#20027;&#35201;&#38598;&#20013;&#22312;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36951;&#24536;&#26159;&#19968;&#31181;&#26222;&#36941;&#29616;&#35937;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#35266;&#23519;&#21040;&#12290;&#36951;&#24536;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20363;&#22914;&#30001;&#20110;&#29983;&#25104;&#22120;&#28418;&#31227;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26469;&#65292;&#20197;&#21450;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#32780;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26469;&#12290;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#28041;&#21450;&#21040;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#24179;&#34913;&#20445;&#30041;&#26087;&#20219;&#21153;&#30693;&#35782;&#65292;&#31649;&#29702;&#20219;&#21153;&#24178;&#25200;&#19982;&#20914;&#31361;&#30446;&#26631;&#65292;&#20197;&#21450;&#38450;&#27490;&#38544;&#31169;&#27844;&#38706;&#31561;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#35843;&#26597;&#37117;&#40664;&#35748;&#35748;&#20026;&#36951;&#24536;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#35748;&#20026;&#36951;&#24536;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26159;&#26377;&#30410;&#19988;&#21487;&#21462;&#30340;&#65292;&#20363;&#22914;&#38544;&#31169;&#20445;&#25252;&#22330;&#26223;&#12290;&#36890;&#36807;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#36951;&#24536;&#29616;&#35937;&#65292;
&lt;/p&gt;
&lt;p&gt;
Forgetting refers to the loss or deterioration of previously acquired information or knowledge. While the existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new tasks, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2307.09209</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#27531;&#38556;&#20027;&#20041;&#65306;&#25506;&#32034;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models. (arXiv:2307.09209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09209
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#25506;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#30740;&#31350;&#20351;&#29992;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#23545;&#35805;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#35821;&#26009;&#24211;&#26469;&#37327;&#21270;&#26126;&#26174;&#30340;&#27531;&#38556;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#30740;&#31350;&#30340;&#27169;&#22411;&#22343;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#23545;&#27531;&#38556;&#20154;&#22763;&#30340;&#26126;&#26174;&#20559;&#35265;&#12290;&#25105;&#20204;&#37319;&#29992;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#20559;&#35265;&#35782;&#21035;&#26694;&#26550;&#65292;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#19982;&#27531;&#38556;&#20154;&#22763;&#30456;&#20851;&#30340;&#23545;&#35805;&#65292;&#29305;&#21035;&#26159;Twitter&#21644;Reddit&#65292;&#22312;&#30495;&#23454;&#31038;&#20132;&#29615;&#22659;&#20013;&#20102;&#35299;&#27531;&#38556;&#20559;&#35265;&#26159;&#22914;&#20309;&#20256;&#25773;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#8220;&#24773;&#24863;&#20013;&#30340;&#20559;&#35265;&#35782;&#21035;&#27979;&#35797;&#8221;&#65288;BITS&#65289;&#35821;&#26009;&#24211;&#65292;&#20197;&#37327;&#21270;&#20219;&#20309;&#24773;&#24863;&#20998;&#26512;&#21644;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26126;&#26174;&#27531;&#38556;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;BITS&#25581;&#31034;&#20102;&#22235;&#31181;&#24320;&#25918;&#30340;AIaaS&#65288;AI&#21363;&#26381;&#21153;&#65289;&#24773;&#24863;&#20998;&#26512;&#24037;&#20855;&#65288;TextBlob&#65292;VADER&#65292;Google Cloud Natural Language API&#65292;DistilBERT&#65289;&#21644;&#20004;&#31181;&#27602;&#24615;&#26816;&#27979;&#27169;&#22411;&#65288;&#20004;&#20010;&#29256;&#26412;&#30340;Toxic-BERT&#65289;&#20013;&#23384;&#22312;&#26174;&#30528;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze sentiment analysis and toxicity detection models to detect the presence of explicit bias against people with disability (PWD). We employ the bias identification framework of Perturbation Sensitivity Analysis to examine conversations related to PWD on social media platforms, specifically Twitter and Reddit, in order to gain insight into how disability bias is disseminated in real-world social settings. We then create the \textit{Bias Identification Test in Sentiment} (BITS) corpus to quantify explicit disability bias in any sentiment analysis and toxicity detection models. Our study utilizes BITS to uncover significant biases in four open AIaaS (AI as a Service) sentiment analysis tools, namely TextBlob, VADER, Google Cloud Natural Language API, DistilBERT and two toxicity detection models, namely two versions of Toxic-BERT. Our findings indicate that all of these models exhibit statistically significant explicit bias against PWD.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09193</link><description>&lt;p&gt;
ESMC:&#25972;&#20010;&#31354;&#38388;&#22810;&#20219;&#21153;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#29992;&#20110;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;
&lt;/p&gt;
&lt;p&gt;
ESMC: Entire Space Multi-Task Model for Post-Click Conversion Rate via Parameter Constraint. (arXiv:2307.09193v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ESMC&#27169;&#22411;&#65292;&#36890;&#36807;&#21442;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#20272;&#35745;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25193;&#23637;&#20915;&#31574;&#36335;&#24452;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#65292;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#22312;&#20114;&#32852;&#32593;&#19978;&#24191;&#27867;&#20351;&#29992;&#65292;&#36127;&#36131;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#21644;&#28857;&#20987;&#21518;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;CVR&#20272;&#35745;&#22120;&#23384;&#22312;&#30528;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#36861;&#36394;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#36141;&#20080;&#8221;&#36825;&#20010;&#20915;&#31574;&#36335;&#24452;&#65292;&#25552;&#20986;&#20102;&#25972;&#20010;&#31354;&#38388;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#30740;&#31350;&#32773;&#35266;&#23519;&#21040;&#22312;&#28857;&#20987;&#21644;&#36141;&#20080;&#20043;&#38388;&#23384;&#22312;&#36141;&#20080;&#30456;&#20851;&#34892;&#20026;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#29992;&#25143;&#30340;&#20915;&#31574;&#24847;&#22270;&#24182;&#25552;&#39640;&#25512;&#33616;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20915;&#31574;&#36335;&#24452;&#24050;&#25193;&#23637;&#20026;&#8220;&#26333;&#20809;_&#28857;&#20987;_&#24215;&#20869;&#21160;&#20316;_&#36141;&#20080;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#26041;&#27861;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26465;&#20214;&#27010;&#29575;&#30340;&#38142;&#24335;&#27861;&#21017;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#27010;&#29575;&#31354;&#38388;&#28151;&#28102;&#65288;PSC&#65289;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20102;&#22320;&#38754;&#23454;&#20917;&#19982;&#20272;&#35745;&#25968;&#23398;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale online recommender system spreads all over the Internet being in charge of two basic tasks: Click-Through Rate (CTR) and Post-Click Conversion Rate (CVR) estimations. However, traditional CVR estimators suffer from well-known Sample Selection Bias and Data Sparsity issues. Entire space models were proposed to address the two issues via tracing the decision-making path of "exposure_click_purchase". Further, some researchers observed that there are purchase-related behaviors between click and purchase, which can better draw the user's decision-making intention and improve the recommendation performance. Thus, the decision-making path has been extended to "exposure_click_in-shop action_purchase" and can be modeled with conditional probability approach. Nevertheless, we observe that the chain rule of conditional probability does not always hold. We report Probability Space Confusion (PSC) issue and give a derivation of difference between ground-truth and estimation mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#65292;&#20197;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2307.09177</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#23454;&#29616;&#23545;&#26234;&#33021;&#25163;&#26426;&#35774;&#32622;&#30340;&#30452;&#35266;&#35775;&#38382;
&lt;/p&gt;
&lt;p&gt;
Intuitive Access to Smartphone Settings Using Relevance Model Trained by Contrastive Learning. (arXiv:2307.09177v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#30456;&#20851;&#27169;&#22411;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#65292;&#20197;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36816;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#30340;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#22686;&#21152;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#21151;&#33021;&#65292;&#29992;&#25143;&#36234;&#26469;&#36234;&#38590;&#20197;&#25214;&#21040;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#21151;&#33021;&#30340;&#21517;&#31216;&#36890;&#24120;&#24456;&#30701;&#65292;&#35760;&#19981;&#20303;&#22826;&#22810;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#25552;&#20986;&#25551;&#36848;&#20182;&#20204;&#35201;&#23547;&#25214;&#30340;&#21151;&#33021;&#30340;&#19978;&#19979;&#25991;&#26597;&#35810;&#65292;&#20294;&#26631;&#20934;&#30340;&#22522;&#20110;&#35789;&#39057;&#30340;&#25628;&#32034;&#26080;&#27861;&#22788;&#29702;&#23427;&#20204;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31227;&#21160;&#21151;&#33021;&#26816;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#25509;&#21463;&#30452;&#35266;&#21644;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#30456;&#20851;&#27169;&#22411;&#65292;&#20197;&#24863;&#30693;&#26597;&#35810;&#23884;&#20837;&#21644;&#32034;&#24341;&#30340;&#31227;&#21160;&#21151;&#33021;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#35774;&#22791;&#19978;&#20351;&#29992;&#26368;&#20302;&#36164;&#28304;&#39640;&#25928;&#36816;&#34892;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#30693;&#35782;&#33976;&#39311;&#26469;&#21387;&#32553;&#27169;&#22411;&#32780;&#19981;&#38477;&#20302;&#22826;&#22810;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#27979;&#35797;&#26597;&#35810;&#65292;&#24182;&#19982;&#24403;&#21069;&#37096;&#32626;&#30340;&#25628;&#32034;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The more new features that are being added to smartphones, the harder it becomes for users to find them. This is because the feature names are usually short, and there are just too many to remember. In such a case, the users may want to ask contextual queries that describe the features they are looking for, but the standard term frequency-based search cannot process them. This paper presents a novel retrieval system for mobile features that accepts intuitive and contextual search queries. We trained a relevance model via contrastive learning from a pre-trained language model to perceive the contextual relevance between query embeddings and indexed mobile features. Also, to make it run efficiently on-device using minimal resources, we applied knowledge distillation to compress the model without degrading much performance. To verify the feasibility of our method, we collected test queries and conducted comparative experiments with the currently deployed search baselines. The results show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#26412;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#36923;&#36753;&#31243;&#24207;&#20013;&#23545;&#20110;"&#30456;&#20851;"&#37096;&#20998;&#30340;&#26368;&#22823;&#26410;&#23450;&#38598;&#21512;&#20013;&#30340;&#26368;&#23567;&#38598;&#21512;&#65292;&#30456;&#27604;&#20110;&#22522;&#26412;&#24490;&#29615;&#65292;&#22522;&#26412;&#38598;&#21512;&#26356;&#31616;&#21333;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#30456;&#20132;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.09168</link><description>&lt;p&gt;
&#36923;&#36753;&#31243;&#24207;&#30340;&#22522;&#26412;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Elementary Sets for Logic Programs. (arXiv:2307.09168v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#26412;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#23427;&#26159;&#36923;&#36753;&#31243;&#24207;&#20013;&#23545;&#20110;"&#30456;&#20851;"&#37096;&#20998;&#30340;&#26368;&#22823;&#26410;&#23450;&#38598;&#21512;&#20013;&#30340;&#26368;&#23567;&#38598;&#21512;&#65292;&#30456;&#27604;&#20110;&#22522;&#26412;&#24490;&#29615;&#65292;&#22522;&#26412;&#38598;&#21512;&#26356;&#31616;&#21333;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#30456;&#20132;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#21644;&#24490;&#29615;&#20844;&#24335;&#30340;&#27010;&#24565;&#65292;Lin&#21644;Zhao&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#19981;&#30456;&#20132;&#36923;&#36753;&#31243;&#24207;&#30340;&#31572;&#26696;&#38598;&#23601;&#26159;&#28385;&#36275;&#25152;&#26377;&#24490;&#29615;&#30340;&#24490;&#29615;&#20844;&#24335;&#30340;Clark&#23436;&#25104;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;Gebser&#21644;Schaub&#35777;&#26126;&#20102;&#21363;&#20351;&#25105;&#20204;&#23558;&#24490;&#29615;&#20844;&#24335;&#38480;&#21046;&#22312;&#19968;&#31181;&#31216;&#20026;"&#22522;&#26412;&#24490;&#29615;"&#30340;&#29305;&#27530;&#31867;&#22411;&#30340;&#24490;&#29615;&#20013;&#65292;Lin-Zhao&#23450;&#29702;&#20173;&#28982;&#27491;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31616;&#21270;&#21644;&#25512;&#24191;&#20102;&#22522;&#26412;&#24490;&#29615;&#30340;&#27010;&#24565;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#26412;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#23427;&#20960;&#20046;&#31561;&#20215;&#20110;&#38750;&#19981;&#30456;&#20132;&#31243;&#24207;&#30340;&#22522;&#26412;&#24490;&#29615;&#30340;&#27010;&#24565;&#65292;&#20294;&#26356;&#31616;&#21333;&#65292;&#24182;&#19988;&#19982;&#22522;&#26412;&#24490;&#29615;&#19981;&#21516;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#21547;&#38750;&#30452;&#35266;&#32467;&#26524;&#30340;&#19981;&#30456;&#20132;&#31243;&#24207;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#31243;&#24207;&#30340;"&#30456;&#20851;"&#37096;&#20998;&#26469;&#35828;&#65292;&#26368;&#22823;&#30340;&#26410;&#23450;&#22522;&#26412;&#38598;&#21512;&#24688;&#22909;&#26159;&#38750;&#31354;&#26410;&#23450;&#38598;&#21512;&#20013;&#30340;&#26368;&#23567;&#38598;&#21512;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#35770;&#30340;&#38750;&#19981;&#30456;&#20132;&#31243;&#24207;&#22522;&#26412;&#38598;&#21512;&#30340;&#29305;&#24449;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
By introducing the concepts of a loop and a loop formula, Lin and Zhao showed that the answer sets of a nondisjunctive logic program are exactly the models of its Clark's completion that satisfy the loop formulas of all loops. Recently, Gebser and Schaub showed that the Lin-Zhao theorem remains correct even if we restrict loop formulas to a special class of loops called ``elementary loops.'' In this paper, we simplify and generalize the notion of an elementary loop, and clarify its role. We propose the notion of an elementary set, which is almost equivalent to the notion of an elementary loop for nondisjunctive programs, but is simpler, and, unlike elementary loops, can be extended to disjunctive programs without producing unintuitive results. We show that the maximal unfounded elementary sets for the ``relevant'' part of a program are exactly the minimal sets among the nonempty unfounded sets. We also present a graph-theoretic characterization of elementary sets for nondisjunctive pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#29702;&#35770;&#20013;&#30340;&#23433;&#20840;&#20844;&#24335;&#65292;&#35777;&#26126;&#20102;&#23433;&#20840;&#21477;&#23376;&#21644;&#20854;&#32487;&#25215;&#32467;&#26524;&#20855;&#26377;&#30456;&#21516;&#30340;&#31283;&#23450;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#21477;&#27861;&#24418;&#24335;&#30340;&#20844;&#24335;&#26469;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.09166</link><description>&lt;p&gt;
&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#29702;&#35770;&#20013;&#30340;&#23433;&#20840;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Safe Formulas in the General Theory of Stable Models. (arXiv:2307.09166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31283;&#23450;&#27169;&#22411;&#29702;&#35770;&#20013;&#30340;&#23433;&#20840;&#20844;&#24335;&#65292;&#35777;&#26126;&#20102;&#23433;&#20840;&#21477;&#23376;&#21644;&#20854;&#32487;&#25215;&#32467;&#26524;&#20855;&#26377;&#30456;&#21516;&#30340;&#31283;&#23450;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#21477;&#27861;&#24418;&#24335;&#30340;&#20844;&#24335;&#26469;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#30340;&#19968;&#38454;&#20844;&#24335;&#25512;&#24191;&#20102;&#23433;&#20840;&#35268;&#21017;&#30340;&#27010;&#24565;&#65292;&#22312;&#31572;&#26696;&#38598;&#27714;&#35299;&#22120;&#30340;&#35774;&#35745;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#65292;&#20219;&#20309;&#23433;&#20840;&#30340;&#35821;&#21477;&#37117;&#31561;&#20215;&#20110;&#20854;&#32487;&#25215;&#30340;&#32467;&#26524;-&#36890;&#36807;&#23558;&#25152;&#26377;&#37327;&#35789;&#26367;&#25442;&#20026;&#22810;&#20010;&#21512;&#21462;&#21644;&#26512;&#21462;&#32780;&#33719;&#24471;&#30340;&#26080;&#21464;&#37327;&#21477;&#23376;&#12290;&#30001;&#27492;&#21487;&#35265;&#65292;&#23433;&#20840;&#21477;&#23376;&#21644;&#20854;&#32487;&#25215;&#32467;&#26524;&#20855;&#26377;&#30456;&#21516;&#30340;&#31283;&#23450;&#27169;&#22411;&#65292;&#24182;&#19988;&#19968;&#20010;&#23433;&#20840;&#21477;&#23376;&#30340;&#31283;&#23450;&#27169;&#22411;&#21487;&#20197;&#30001;&#19968;&#20010;&#31616;&#21333;&#21477;&#27861;&#24418;&#24335;&#30340;&#20844;&#24335;&#26469;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe first-order formulas generalize the concept of a safe rule, which plays an important role in the design of answer set solvers. We show that any safe sentence is equivalent, in a certain sense, to the result of its grounding -to the variable-free sentence obtained from it by replacing all quantifiers with multiple conjunctions and disjunctions. It follows that a safe sentence and the result of its grounding have the same stable models, and that the stable models of a safe sentence can be characterized by a formula of a simple syntactic form.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#23454;&#39564;&#32593;&#32476;&#38598;&#25104;&#26469;&#22686;&#24378;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#65292;&#22312;&#28385;&#36275;&#19981;&#21516;&#39046;&#22495;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25512;&#36827;6G&#39640;&#24230;&#38656;&#27714;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.09151</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65292;&#23433;&#20840;&#24615;&#65292;&#21487;&#25345;&#32493;&#24615;&#21644;&#23454;&#39564;&#32593;&#32476;&#38598;&#25104;&#22686;&#24378;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Enhancing Network Slicing Architectures with Machine Learning, Security, Sustainability and Experimental Networks Integration. (arXiv:2307.09151v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#23454;&#39564;&#32593;&#32476;&#38598;&#25104;&#26469;&#22686;&#24378;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#65292;&#22312;&#28385;&#36275;&#19981;&#21516;&#39046;&#22495;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25512;&#36827;6G&#39640;&#24230;&#38656;&#27714;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20999;&#29255;&#26159;&#19968;&#31181;&#22312;5G&#32593;&#32476;&#35745;&#31639;&#31574;&#30053;&#65292;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65292;&#31227;&#21160;&#20113;&#35745;&#31639;&#20197;&#21450;&#29289;&#32852;&#32593;&#36710;&#36742;&#20114;&#32852;&#32593;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#31561;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#32593;&#32476;&#20999;&#29255;&#34987;&#35270;&#20026;6G&#26410;&#26469;&#21644;&#39640;&#24230;&#38656;&#35201;&#30340;&#24212;&#29992;&#30340;&#20027;&#35201;&#25903;&#25345;&#32773;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20248;&#21270;&#21644;&#23450;&#21046;&#21160;&#24577;&#12289;&#38656;&#27714;&#39640;&#12289;&#20855;&#26377;&#39640;&#24230;&#19981;&#21516;&#24212;&#29992;&#38656;&#27714;&#30340;&#23458;&#25143;&#20043;&#38388;&#20105;&#22842;&#30340;&#26377;&#38480;&#36164;&#28304;&#12290;&#21508;&#31181;&#26631;&#20934;&#21270;&#32452;&#32455;&#65292;&#22914;3GPP&#30340;&#26032;&#19968;&#20195;&#32593;&#32476;&#25552;&#26696;&#21644;&#26368;&#26032;&#30340;5G/6G&#30740;&#31350;&#39033;&#30446;&#65292;&#27491;&#22312;&#25552;&#20986;&#26032;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#24517;&#39035;&#22788;&#29702;&#24191;&#27867;&#30340;&#38656;&#27714;&#33539;&#22260;&#65292;&#36825;&#23548;&#33268;&#20102;&#36890;&#24120;&#21482;&#28385;&#36275;&#20855;&#26377;&#20849;&#24615;&#22495;&#38598;&#21512;&#38656;&#27714;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#25552;&#35758;&#12290;&#20999;&#29255;&#26410;&#26469;&#20114;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;(SFI2)&#26550;&#26500;&#25552;&#35758;&#25506;&#35752;&#20102;&#30001;&#22810;&#26679;&#24615;&#23548;&#33268;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network Slicing (NS) is an essential technique extensively used in 5G networks computing strategies, mobile edge computing, mobile cloud computing, and verticals like the Internet of Vehicles and industrial IoT, among others. NS is foreseen as one of the leading enablers for 6G futuristic and highly demanding applications since it allows the optimization and customization of scarce and disputed resources among dynamic, demanding clients with highly distinct application requirements. Various standardization organizations, like 3GPP's proposal for new generation networks and state-of-the-art 5G/6G research projects, are proposing new NS architectures. However, new NS architectures have to deal with an extensive range of requirements that inherently result in having NS architecture proposals typically fulfilling the needs of specific sets of domains with commonalities. The Slicing Future Internet Infrastructures (SFI2) architecture proposal explores the gap resulting from the diversity of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SAT&#27714;&#35299;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#26435;&#20132;&#32473;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#36716;&#25442;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;SAT&#38382;&#39064;&#30340;Graph-Q-SAT&#30340;&#20462;&#25913;&#29256;&#12290;</title><link>http://arxiv.org/abs/2307.09141</link><description>&lt;p&gt;
&#29992;&#20110;SAT&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#38480;&#21046;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26032;&#30340;&#22270;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for SAT: Restricted Heuristics and New Graph Representations. (arXiv:2307.09141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;SAT&#27714;&#35299;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#26435;&#20132;&#32473;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#36716;&#25442;&#26469;&#33258;&#20854;&#20182;&#39046;&#22495;&#30340;SAT&#38382;&#39064;&#30340;Graph-Q-SAT&#30340;&#20462;&#25913;&#29256;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26377;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#35268;&#21010;&#21644;&#35843;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#23454;&#20363;&#65292;SAT&#27714;&#35299;&#22120;&#24517;&#39035;&#20381;&#36182;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;DPLL&#21644;CDCL&#27714;&#35299;&#22120;&#20013;&#36873;&#25321;&#20998;&#25903;&#21464;&#37327;&#12290;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#25913;&#36827;&#65292;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#20294;&#36890;&#24120;&#20250;&#24433;&#21709;&#36816;&#34892;&#26102;&#38388;&#65292;&#22240;&#20026;&#26377;&#29992;&#30340;&#27169;&#22411;&#30456;&#23545;&#36739;&#22823;&#19988;&#36739;&#24930;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#23569;&#37327;&#21021;&#22987;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#26435;&#20132;&#32473;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#65307;&#36825;&#31616;&#21270;&#20102;SAT&#27714;&#35299;&#30340;&#20919;&#21551;&#21160;&#36807;&#31243;&#65292;&#21487;&#20197;&#20943;&#23569;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#25972;&#20307;&#36816;&#34892;&#26102;&#38388;&#65292;&#20294;&#38656;&#35201;&#21478;&#22806;&#20915;&#23450;&#20309;&#26102;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#27714;&#35299;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#20174;&#20854;&#20182;&#39046;&#22495;&#36716;&#25442;&#32780;&#26469;&#30340;SAT&#38382;&#39064;&#36827;&#34892;&#23450;&#21046;&#30340;Graph-Q-SAT&#20462;&#25913;&#29256;&#65292;&#20363;&#22914;&#24320;&#25918;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#21644;&#24037;&#19994;SAT&#38382;&#39064;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean satisfiability (SAT) is a fundamental NP-complete problem with many applications, including automated planning and scheduling. To solve large instances, SAT solvers have to rely on heuristics, e.g., choosing a branching variable in DPLL and CDCL solvers. Such heuristics can be improved with machine learning (ML) models; they can reduce the number of steps but usually hinder the running time because useful models are relatively large and slow. We suggest the strategy of making a few initial steps with a trained ML model and then releasing control to classical heuristics; this simplifies cold start for SAT solving and can decrease both the number of steps and overall runtime, but requires a separate decision of when to release control to the solver. Moreover, we introduce a modification of Graph-Q-SAT tailored to SAT problems converted from other domains, e.g., open shop scheduling problems. We validate the feasibility of our approach with random and industrial SAT problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DropMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#19968;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20043;&#21069;&#22240;&#20026;MSDA&#32780;&#19979;&#38477;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.09136</link><description>&lt;p&gt;
DropMix: &#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
DropMix: Reducing Class Dependency in Mixed Sample Data Augmentation. (arXiv:2307.09136v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DropMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#19968;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#20013;&#30340;&#31867;&#21035;&#30456;&#20851;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20043;&#21069;&#22240;&#20026;MSDA&#32780;&#19979;&#38477;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#22686;&#21152;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#21508;&#31181;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MSDA&#30340;&#25928;&#26524;&#26159;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#65292;&#19968;&#20123;&#31867;&#21035;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#32780;&#20854;&#20182;&#31867;&#21035;&#21017;&#20986;&#29616;&#20102;&#19979;&#38477;&#12290;&#20026;&#20102;&#20943;&#23569;&#31867;&#21035;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DropMix&#26041;&#27861;&#65292;&#22312;MSDA&#35745;&#31639;&#20013;&#25490;&#38500;&#20102;&#29305;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;MSDA&#21644;&#38750;MSDA&#25968;&#25454;&#30340;&#32452;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#20043;&#21069;&#30001;MSDA&#38477;&#20302;&#30340;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#25972;&#20307;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#20351;&#29992;&#19977;&#31181;MSDA&#26041;&#27861;&#65288;Mixup&#12289;CutMix&#21644;PuzzleMix&#65289;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CIFAR-100&#21644;ImageNet&#65289;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed sample data augmentation (MSDA) is a widely used technique that has been found to improve performance in a variety of tasks. However, in this paper, we show that the effects of MSDA are class-dependent, with some classes seeing an improvement in performance while others experience a decline. To reduce class dependency, we propose the DropMix method, which excludes a specific percentage of data from the MSDA computation. By training on a combination of MSDA and non-MSDA data, the proposed method not only improves the performance of classes that were previously degraded by MSDA, but also increases overall average accuracy, as shown in experiments on two datasets (CIFAR-100 and ImageNet) using three MSDA methods (Mixup, CutMix and PuzzleMix).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22522;&#20110;Kubernetes&#30340;&#20113;&#21407;&#29983;&#25216;&#26415;&#22312;Hopsworks&#19978;&#23454;&#29616;&#30340;&#22810;&#31199;&#25143;RStudio&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#22810;&#31199;&#25143;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#38548;&#31163;&#12289;&#23433;&#20840;&#24615;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#21644;&#29992;&#25143;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.09132</link><description>&lt;p&gt;
&#29992;&#20110;Hopsworks&#30340;&#22522;&#20110;Kubernetes&#30340;&#20113;&#21407;&#29983;RStudio
&lt;/p&gt;
&lt;p&gt;
Cloud-native RStudio on Kubernetes for Hopsworks. (arXiv:2307.09132v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09132
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22522;&#20110;Kubernetes&#30340;&#20113;&#21407;&#29983;&#25216;&#26415;&#22312;Hopsworks&#19978;&#23454;&#29616;&#30340;&#22810;&#31199;&#25143;RStudio&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#22810;&#31199;&#25143;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#38548;&#31163;&#12289;&#23433;&#20840;&#24615;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#21644;&#29992;&#25143;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20113;&#35745;&#31639;&#65292;&#26381;&#21153;&#37319;&#29992;&#8220;&#22810;&#31199;&#25143;&#8221;&#26550;&#26500;&#27169;&#22411;&#35774;&#35745;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#23454;&#29616;&#29992;&#25143;&#20043;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#12290;&#28982;&#32780;&#65292;&#22810;&#31199;&#25143;&#24341;&#20837;&#20102;&#23433;&#20840;&#12289;&#24615;&#33021;&#38548;&#31163;&#12289;&#25193;&#23637;&#24615;&#21644;&#23450;&#21046;&#21270;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;RStudio&#26381;&#21153;&#22120;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#65292;&#36890;&#36807;Web&#27983;&#35272;&#22120;&#35775;&#38382;&#65292;&#29992;&#20110;R&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#23494;&#38598;&#22411;AI&#24179;&#21488;Hopsworks&#19978;&#35774;&#35745;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#29992;&#25143;&#20998;&#24067;&#24335;&#31995;&#32479;&#65292;&#37319;&#29992;&#22810;&#31199;&#25143;&#27169;&#22411;&#65292;&#20197;&#25552;&#20379;RStudio&#20316;&#20026;&#36719;&#20214;&#21363;&#26381;&#21153;&#65288;SaaS&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#27969;&#34892;&#30340;&#20113;&#21407;&#29983;&#25216;&#26415;&#65306;Docker&#21644;Kubernetes&#65292;&#35299;&#20915;&#20102;&#22810;&#31199;&#25143;&#29615;&#22659;&#20013;&#23384;&#22312;&#30340;&#24615;&#33021;&#38548;&#31163;&#12289;&#23433;&#20840;&#24615;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;RStudio&#26381;&#21153;&#22120;&#23454;&#20363;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#20197;&#25552;&#20379;&#25968;&#25454;&#38544;&#31169;&#24182;&#20801;&#35768;RStudio&#29992;&#25143;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;Apache Spark&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to fully benefit from cloud computing, services are designed following the "multi-tenant" architectural model, which is aimed at maximizing resource sharing among users. However, multi-tenancy introduces challenges of security, performance isolation, scaling, and customization. RStudio server is an open-source Integrated Development Environment (IDE) accessible over a web browser for the R programming language. We present the design and implementation of a multi-user distributed system on Hopsworks, a data-intensive AI platform, following the multi-tenant model that provides RStudio as Software as a Service (SaaS). We use the most popular cloud-native technologies: Docker and Kubernetes, to solve the problems of performance isolation, security, and scaling that are present in a multi-tenant environment. We further enable secure data sharing in RStudio server instances to provide data privacy and allow collaboration among RStudio users. We integrate our system with Apache Spark
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BOLD&#22522;&#20934;&#21644;&#27169;&#25311;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;&#38142;&#25509;&#25968;&#25454;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#25311;&#21160;&#24577;&#38142;&#25509;&#25968;&#25454;&#29615;&#22659;&#21644;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#26816;&#26597;&#20219;&#21153;&#25191;&#34892;&#30340;&#25163;&#27573;&#24182;&#27979;&#37327;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09114</link><description>&lt;p&gt;
BOLD&#65306;&#19968;&#20010;&#38754;&#21521;&#38142;&#25509;&#25968;&#25454;&#29992;&#25143;&#20195;&#29702;&#30340;&#22522;&#20934;&#21644;&#19968;&#20010;&#29992;&#20110;&#21160;&#24577;&#38142;&#25509;&#25968;&#25454;&#29615;&#22659;&#30340;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BOLD: A Benchmark for Linked Data User Agents and a Simulation Framework for Dynamic Linked Data Environments. (arXiv:2307.09114v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BOLD&#22522;&#20934;&#21644;&#27169;&#25311;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;&#38142;&#25509;&#25968;&#25454;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#25311;&#21160;&#24577;&#38142;&#25509;&#25968;&#25454;&#29615;&#22659;&#21644;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#26816;&#26597;&#20219;&#21153;&#25191;&#34892;&#30340;&#25163;&#27573;&#24182;&#27979;&#37327;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#38142;&#25509;&#25968;&#25454;&#20195;&#29702;&#30340;BOLD&#65288;Buildings on Linked Data&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#21160;&#24577;&#38142;&#25509;&#25968;&#25454;&#29615;&#22659;&#30340;&#26694;&#26550;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;BOLD&#12290;BOLD&#22522;&#20934;&#36890;&#36807;&#20026;&#26234;&#33021;&#24314;&#31569;&#25552;&#20379;&#35835;&#20889;&#38142;&#25509;&#25968;&#25454;&#25509;&#21475;&#26469;&#23454;&#20363;&#21270;BOLD&#26694;&#26550;&#65292;&#27169;&#25311;&#26102;&#38388;&#12289;&#20154;&#21592;&#31227;&#21160;&#20197;&#21450;&#22260;&#32469;&#29031;&#26126;&#30340;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#12290;&#20195;&#29702;&#22312;&#35813;&#29615;&#22659;&#30340;&#38142;&#25509;&#25968;&#25454;&#34920;&#31034;&#19978;&#25191;&#34892;&#22810;&#20010;&#25351;&#23450;&#20219;&#21153;&#65292;&#22914;&#25511;&#21046;&#29031;&#26126;&#12290;&#27169;&#25311;&#29615;&#22659;&#25552;&#20379;&#20102;&#26816;&#26597;&#20219;&#21153;&#27491;&#30830;&#25191;&#34892;&#21644;&#27979;&#37327;&#20195;&#29702;&#24615;&#33021;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#22522;&#20110;&#26465;&#20214;-&#21160;&#20316;&#35268;&#21017;&#23545;&#38142;&#25509;&#25968;&#25454;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the BOLD (Buildings on Linked Data) benchmark for Linked Data agents, next to the framework to simulate dynamic Linked Data environments, using which we built BOLD. The BOLD benchmark instantiates the BOLD framework by providing a read-write Linked Data interface to a smart building with simulated time, occupancy movement and sensors and actuators around lighting. On the Linked Data representation of this environment, agents carry out several specified tasks, such as controlling illumination. The simulation environment provides means to check for the correct execution of the tasks and to measure the performance of agents. We conduct measurements on Linked Data agents based on condition-action rules.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MONAS&#65289;&#30340;&#20027;&#35201;&#30740;&#31350;&#24037;&#20316;&#65292;&#20171;&#32461;&#20102;&#39046;&#22495;&#20869;&#30340;&#20998;&#31867;&#21644;&#34920;&#36848;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30446;&#26631;&#21015;&#34920;&#21644;&#19968;&#20123;&#26032;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.09099</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multi-Objective Neural Architecture Search. (arXiv:2307.09099v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09099
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MONAS&#65289;&#30340;&#20027;&#35201;&#30740;&#31350;&#24037;&#20316;&#65292;&#20171;&#32461;&#20102;&#39046;&#22495;&#20869;&#30340;&#20998;&#31867;&#21644;&#34920;&#36848;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#30446;&#26631;&#21015;&#34920;&#21644;&#19968;&#20123;&#26032;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#33258;&#21160;&#21270;&#29983;&#25104;&#65288;&#21644;&#35843;&#25972;&#65289;&#32593;&#32476;&#32467;&#26500;&#65292;&#39046;&#22495;&#20869;&#20351;&#29992;&#19987;&#23478;&#35774;&#35745;&#30340;&#31070;&#32463;&#26550;&#26500;&#30340;&#36235;&#21183;&#27491;&#22312;&#36880;&#28176;&#34987;&#21462;&#20195;&#12290;&#36825;&#19982;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#36807;&#21435;&#65292;NAS&#20165;&#20248;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#32780;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MONAS&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#23427;&#32771;&#34385;&#20102;&#26356;&#22810;&#30340;&#30446;&#26631;&#65292;&#22914;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#21151;&#32791;&#21644;&#32593;&#32476;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#19982;&#35745;&#31639;&#25104;&#26412;&#31561;&#20854;&#20182;&#29305;&#24449;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;MONAS&#39046;&#22495;&#30340;&#20027;&#35201;&#21644;&#26368;&#26032;&#30740;&#31350;&#24037;&#20316;&#12290;&#20174;&#23545;NAS&#30340;&#20998;&#31867;&#21644;&#34920;&#36848;&#24320;&#22987;&#65292;&#25105;&#20204;&#32416;&#27491;&#20102;&#20043;&#21069;&#19968;&#20123;&#20851;&#20110;NAS&#39046;&#22495;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#24050;&#30693;&#30446;&#26631;&#30340;&#21015;&#34920;&#65292;&#24182;&#34917;&#20805;&#20102;&#19968;&#20123;&#26032;&#30446;&#26631;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the expert-crafted neural architectures is increasing overtaken by the utilization of neural architecture search (NAS) and automatic generation (and tuning) of network structures which has a close relation to the Hyperparameter Optimization and Auto Machine Learning (AutoML). After the earlier NAS attempts to optimize only the prediction accuracy, Multi-Objective Neural architecture Search (MONAS) has been attracting attentions which considers more goals such as computational complexity, power consumption, and size of the network for optimization, reaching a trade-off between the accuracy and other features like the computational cost. In this paper, we present an overview of principal and state-of-the-art works in the field of MONAS. Starting from a well-categorized taxonomy and formulation for the NAS, we address and correct some miscategorizations in previous surveys of the NAS field. We also provide a list of all known objectives used and add a number of new ones and elab
&lt;/p&gt;</description></item><item><title>DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09072</link><description>&lt;p&gt;
DiTTO&#65306;&#21463;&#25193;&#25955;&#21551;&#21457;&#30340;&#26102;&#31354;&#36716;&#25442;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09072
&lt;/p&gt;
&lt;p&gt;
DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26368;&#36817;&#30340;&#31639;&#23376;&#23398;&#20064;&#33539;&#24335;&#30340;&#21457;&#23637;&#20351;&#24471;&#35299;&#20915;&#26356;&#24191;&#27867;PDE&#30456;&#20851;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#30340;PDEs&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;DiTTO&#65292;&#21463;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#20294;&#20854;&#26102;&#38388;&#26465;&#20214;&#26426;&#21046;&#23545;PDEs&#38750;&#24120;&#26377;&#29992;&#12290;&#25193;&#25955;&#21551;&#21457;&#30340;&#26694;&#26550;&#19982;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#26041;&#27861;&#22312;&#22810;&#32500;&#24230;&#30340;&#24191;&#27867;PDE&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;1&#32500;Burgers&#26041;&#31243;&#65292;2&#32500;Navier-Stokes&#26041;&#31243;&#21644;2&#32500;&#21644;3&#32500;&#22768;&#27874;&#26041;&#31243;&#12290;DiTTO&#22312;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.09059</link><description>&lt;p&gt;
&#25991;&#23383;&#24819;&#35937;&#30340;&#37322;&#25918;&#65306;&#36890;&#36807;&#25506;&#32034;&#25991;&#23383;&#30340;&#21147;&#37327;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words. (arXiv:2307.09059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#26412;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#20154;&#29289;&#26816;&#32034;&#30340;&#30446;&#26631;&#26159;&#20174;&#22823;&#22411;&#22270;&#24211;&#20013;&#26816;&#32034;&#19982;&#32473;&#23450;&#25991;&#26412;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#34920;&#31034;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#25991;&#26412;&#27169;&#24577;&#36890;&#36807;&#35789;&#27719;&#21644;&#35821;&#27861;&#32467;&#26500;&#20256;&#36882;&#25277;&#35937;&#21644;&#31934;&#30830;&#30340;&#20449;&#24687;&#65292;&#32780;&#35270;&#35273;&#27169;&#24577;&#36890;&#36807;&#22270;&#20687;&#20256;&#36882;&#20855;&#20307;&#21644;&#30452;&#35266;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25991;&#23383;&#34920;&#31034;&#30340;&#34920;&#36798;&#21147;&#65292;&#20934;&#30830;&#22320;&#23558;&#25277;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#26144;&#23556;&#21040;&#20855;&#20307;&#22270;&#20687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#21477;&#23376;&#20013;&#30340;&#25991;&#23383;&#30340;&#21147;&#37327;&#65292;&#37322;&#25918;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20154;&#29289;&#26816;&#32034;&#20013;&#30340;&#25991;&#23383;&#24819;&#35937;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#20840;&#38754;CLIP&#27169;&#22411;&#20316;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#20808;&#21069;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Text-to-image person retrieval is to retrieve person images from a large gallery that match the given textual descriptions. The main challenge of this task lies in the significant differences in information representation between the visual and textual modalities. The textual modality conveys abstract and precise information through vocabulary and grammatical structures, while the visual modality conveys concrete and intuitive information through images. To fully leverage the expressive power of textual representations, it is essential to accurately map abstract textual descriptions to specific images.  To address this issue, we propose a novel framework to Unleash the Imagination of Text (UIT) in text-to-image person retrieval, aiming to fully explore the power of words in sentences. Specifically, the framework employs the pre-trained full CLIP model as a dual encoder for the images and texts , taking advantage of prior cross-modal alignment knowledge. The Text-guided Imag
&lt;/p&gt;</description></item><item><title>QMNet&#26159;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#28040;&#24687;&#20132;&#25442;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#37325;&#35201;&#24615;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#21644;&#21033;&#29992;&#28040;&#24687;&#39044;&#27979;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09051</link><description>&lt;p&gt;
QMNet: &#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#28040;&#24687;&#20132;&#25442;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
QMNet: Importance-Aware Message Exchange for Decentralized Multi-Agent Reinforcement Learning. (arXiv:2307.09051v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09051
&lt;/p&gt;
&lt;p&gt;
QMNet&#26159;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#30340;&#28040;&#24687;&#20132;&#25442;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#37325;&#35201;&#24615;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#21644;&#21033;&#29992;&#28040;&#24687;&#39044;&#27979;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#26080;&#32447;&#36164;&#28304;&#38480;&#21046;&#19979;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#37325;&#35201;&#24615;&#24230;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#24863;&#30693;&#35843;&#24230;&#31574;&#30053;&#26469;&#26377;&#25928;&#20132;&#25442;&#28040;&#24687;&#12290;&#20851;&#38190;&#27934;&#23519;&#26159;&#23558;&#23453;&#36149;&#30340;&#36890;&#20449;&#36164;&#28304;&#29992;&#20110;&#37325;&#35201;&#30340;&#28040;&#24687;&#19978;&#12290;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#19981;&#20165;&#21462;&#20915;&#20110;&#28040;&#24687;&#26412;&#36523;&#65292;&#36824;&#21462;&#20915;&#20110;&#25509;&#25910;&#28040;&#24687;&#30340;&#26234;&#33021;&#20307;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#28040;&#24687;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;QMNet&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#29615;&#22659;&#35266;&#27979;&#29983;&#25104;&#26597;&#35810;&#21644;&#28040;&#24687;&#12290;&#20849;&#20139;&#26597;&#35810;&#21487;&#20197;&#24110;&#21161;&#35745;&#31639;&#28040;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#20132;&#25442;&#28040;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#26356;&#22909;&#22320;&#21327;&#20316;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#28040;&#24687;&#37325;&#35201;&#24615;&#26469;&#22788;&#29702;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#20013;&#30340;&#38543;&#26426;&#25509;&#20837;&#20914;&#31361;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#24687;&#39044;&#27979;&#26426;&#21046;&#26469;&#34917;&#20607;&#26410;&#20256;&#36755;&#30340;&#28040;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20132;&#36890;&#36335;&#21475;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the performance of multi-agent reinforcement learning under the constraint of wireless resources, we propose a message importance metric and design an importance-aware scheduling policy to effectively exchange messages. The key insight is spending the precious communication resources on important messages. The message importance depends not only on the messages themselves, but also on the needs of agents who receive them. Accordingly, we propose a query-message-based architecture, called QMNet. Agents generate queries and messages with the environment observation. Sharing queries can help calculate message importance. Exchanging messages can help agents cooperate better. Besides, we exploit the message importance to deal with random access collisions in decentralized systems. Furthermore, a message prediction mechanism is proposed to compensate for messages that are not transmitted. Finally, we evaluate the proposed schemes in a traffic junction environment, where only a fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;Transformer&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#36755;&#20986;&#21644;&#21098;&#20999;&#20004;&#20010;&#27169;&#22359;&#65292;&#29983;&#25104;&#23494;&#38598;&#30340;&#31867;&#21035;&#29305;&#23450;&#21487;&#35299;&#37322;&#24615;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.09050</link><description>&lt;p&gt;
R-Cut: &#20351;&#29992;&#21152;&#26435;&#36755;&#20986;&#21644;&#21098;&#20999;&#22686;&#24378;Transformer&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut. (arXiv:2307.09050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;Transformer&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#36755;&#20986;&#21644;&#21098;&#20999;&#20004;&#20010;&#27169;&#22359;&#65292;&#29983;&#25104;&#23494;&#38598;&#30340;&#31867;&#21035;&#29305;&#23450;&#21487;&#35299;&#37322;&#24615;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65288;NLP&#65289;&#20013;&#24191;&#21463;&#27426;&#36814;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;GPT4&#31561;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;Transformer&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#20998;&#31867;&#32467;&#26524;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#31867;&#21035;&#29305;&#23450;&#30340;&#21487;&#35270;&#21270;&#22320;&#22270;&#65292;&#20351;&#29992;&#25143;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20197;&#36827;&#34892;&#21518;&#32493;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#27169;&#22359;&#65306;``&#21152;&#26435;&#36755;&#20986;&#20851;&#31995;"&#21644;``&#21098;&#20999;"&#27169;&#22359;&#12290;``&#21152;&#26435;&#36755;&#20986;&#20851;&#31995;"&#27169;&#22359;&#19987;&#27880;&#20110;&#20174;&#20013;&#38388;&#23618;&#25552;&#21462;&#31867;&#21035;&#29305;&#23450;&#20449;&#24687;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#31361;&#20986;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;``&#21098;&#20999;"&#27169;&#22359;&#23545;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#30340;&#20998;&#35299;&#65292;&#32771;&#34385;&#20301;&#32622;&#12289;&#32441;&#29702;&#21644;&#39068;&#33394;&#31561;&#22240;&#32032;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#27169;&#22359;&#65292;&#25105;&#20204;&#29983;&#25104;&#23494;&#38598;&#30340;&#31867;&#21035;&#29305;&#23450;&#21487;&#35299;&#37322;&#24615;&#22320;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have gained popularity in the field of natural language processing (NLP) and are extensively utilized in computer vision tasks and multi-modal models such as GPT4. This paper presents a novel method to enhance the explainability of Transformer-based image classification models. Our method aims to improve trust in classification results and empower users to gain a deeper understanding of the model for downstream tasks by providing visualizations of class-specific maps. We introduce two modules: the ``Relationship Weighted Out" and the ``Cut" modules. The ``Relationship Weighted Out" module focuses on extracting class-specific information from intermediate layers, enabling us to highlight relevant features. Additionally, the ``Cut" module performs fine-grained feature decomposition, taking into account factors such as position, texture, and color. By integrating these modules, we generate dense class-specific visual explainability maps. We validate our method wit
&lt;/p&gt;</description></item><item><title>FedDefender&#26159;&#19968;&#31181;&#38754;&#21521;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#25239;&#25915;&#20987;&#30340;&#26412;&#22320;&#20803;&#26356;&#26032;&#21644;&#20840;&#23616;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#33391;&#24615;&#23458;&#25143;&#31471;&#35757;&#32451;&#31283;&#20581;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09048</link><description>&lt;p&gt;
FedDefender&#65306;&#38754;&#21521;&#23458;&#25143;&#31471;&#25239;&#25915;&#20987;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDefender: Client-Side Attack-Tolerant Federated Learning. (arXiv:2307.09048v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09048
&lt;/p&gt;
&lt;p&gt;
FedDefender&#26159;&#19968;&#31181;&#38754;&#21521;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#25239;&#25915;&#20987;&#30340;&#26412;&#22320;&#20803;&#26356;&#26032;&#21644;&#20840;&#23616;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#33391;&#24615;&#23458;&#25143;&#31471;&#35757;&#32451;&#31283;&#20581;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#19981;&#25439;&#23475;&#38544;&#31169;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#28304;&#36827;&#34892;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#24694;&#24847;&#23458;&#25143;&#31471;&#24178;&#25200;&#35757;&#32451;&#36807;&#31243;&#12290;&#20197;&#24448;&#30340;&#38450;&#24481;&#26426;&#21046;&#20027;&#35201;&#38598;&#20013;&#22312;&#26381;&#21153;&#22120;&#31471;&#65292;&#36890;&#36807;&#31934;&#24515;&#30340;&#27169;&#22411;&#32858;&#21512;&#26469;&#38450;&#24481;&#65292;&#20294;&#26159;&#24403;&#25968;&#25454;&#19981;&#26159;&#30456;&#21516;&#20998;&#24067;&#30340;&#65292;&#25110;&#32773;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#33391;&#24615;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#26102;&#65292;&#36825;&#31181;&#38450;&#24481;&#21487;&#33021;&#26080;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21517;&#20026;FedDefender&#65292;&#23427;&#19987;&#27880;&#20110;&#23458;&#25143;&#31471;&#65292;&#22312;&#26381;&#21153;&#22120;&#31471;&#26080;&#27861;&#35782;&#21035;&#25110;&#31227;&#38500;&#23545;&#25163;&#26102;&#65292;&#24110;&#21161;&#33391;&#24615;&#23458;&#25143;&#31471;&#35757;&#32451;&#31283;&#20581;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#24694;&#24847;&#27169;&#22411;&#26356;&#26032;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#25239;&#25915;&#20987;&#30340;&#26412;&#22320;&#20803;&#26356;&#26032;&#21644;&#65288;2&#65289;&#25239;&#25915;&#20987;&#30340;&#20840;&#23616;&#30693;&#35782;&#33976;&#39311;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#29992;&#26469;&#25214;&#21040;&#25239;&#22122;&#22768;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#20934;&#30830;&#22320;&#20256;&#36882;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables learning from decentralized data sources without compromising privacy, which makes it a crucial technique. However, it is vulnerable to model poisoning attacks, where malicious clients interfere with the training process. Previous defense mechanisms have focused on the server-side by using careful model aggregation, but this may not be effective when the data is not identically distributed or when attackers can access the information of benign clients. In this paper, we propose a new defense mechanism that focuses on the client-side, called FedDefender, to help benign clients train robust local models and avoid the adverse impact of malicious model updates from attackers, even when a server-side defense cannot identify or remove adversaries. Our method consists of two main components: (1) attack-tolerant local meta update and (2) attack-tolerant global knowledge distillation. These components are used to find noise-resilient model parameters while accurately 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#36827;&#34892;&#34701;&#21512;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#30446;&#26631;&#12290;&#22312;&#25991;&#26412;&#27169;&#24577;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#22312;160 GB&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.09047</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature. (arXiv:2307.09047v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09047
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#36827;&#34892;&#34701;&#21512;&#20998;&#31867;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#23450;&#29702;&#21644;&#35777;&#26126;&#30340;&#30446;&#26631;&#12290;&#22312;&#25991;&#26412;&#27169;&#24577;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#22312;160 GB&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#39046;&#22495;&#30340;&#23398;&#26415;&#25991;&#31456;&#20013;&#21253;&#21547;&#23450;&#29702;&#12289;&#21629;&#39064;&#31561;&#25968;&#23398;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#12290;&#20174;&#25991;&#31456;&#30340;PDF&#34920;&#31034;&#20013;&#25552;&#21462;&#23427;&#20204;&#38656;&#35201;&#29702;&#35299;&#31185;&#23398;&#25991;&#26412;&#20197;&#21450;&#35270;&#35273;&#21644;&#22522;&#20110;&#23383;&#20307;&#30340;&#25351;&#31034;&#31526;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#20316;&#20026;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#31867;&#38382;&#39064;&#65292;&#20351;&#29992;&#25991;&#26412;&#12289;&#23383;&#20307;&#29305;&#24449;&#21644;PDF&#30340;&#20301;&#22270;&#22270;&#20687;&#28210;&#26579;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#31867;&#23450;&#29702;&#29615;&#22659;&#21644;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;&#21333;&#19968;&#21333;&#27169;&#24577;&#20998;&#31867;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#30340;&#21518;&#26399;&#34701;&#21512;&#65292;&#32771;&#34385;&#25991;&#26723;&#20013;&#22359;&#30340;&#39034;&#24207;&#12290;&#23545;&#20110;&#25991;&#26412;&#27169;&#24577;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;11 GB&#30340;&#31185;&#23398;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#19982;&#19968;&#20010;&#22312;160 GB&#19978;&#39044;&#35757;&#32451;&#30340;&#65288;RoBERTa&#65289;&#27169;&#22411;&#30456;&#27604;&#25317;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#22312;&#35201;&#27714;&#26356;&#23569;&#30340;&#24494;&#35843;&#25968;&#25454;&#30340;&#21516;&#26102;&#25910;&#25947;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly articles in mathematical fields feature mathematical statements such as theorems, propositions, etc., as well as their proofs. Extracting them from the PDF representation of the articles requires understanding of scientific text along with visual and font-based indicators. We pose this problem as a multimodal classification problem using text, font features, and bitmap image rendering of the PDF as different modalities. In this paper we propose a multimodal machine learning approach for extraction of theorem-like environments and proofs, based on late fusion of features extracted by individual unimodal classifiers, taking into account the sequential succession of blocks in the document. For the text modality, we pretrain a new language model on a 11 GB scientific corpus; experiments shows similar performance for our task than a model (RoBERTa) pretrained on 160 GB, with faster convergence while requiring much less fine-tuning data. Font-based information relies on training a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37325;&#28857;&#32771;&#23519;&#20102;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;LLMs&#22312;&#24773;&#24863;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2307.09042</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Emotional Intelligence of Large Language Models. (arXiv:2307.09042v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09042
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37325;&#28857;&#32771;&#23519;&#20102;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#33021;&#21147;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;LLMs&#22312;&#24773;&#24863;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20027;&#35201;&#36890;&#36807;&#35821;&#35328;&#29983;&#25104;&#12289;&#30693;&#35782;&#21033;&#29992;&#21644;&#22797;&#26434;&#25512;&#29702;&#31561;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#24773;&#24863;&#21644;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#35780;&#20272;&#65292;&#32780;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#65292;&#21253;&#25324;&#24773;&#24863;&#35782;&#21035;&#12289;&#35299;&#37322;&#21644;&#29702;&#35299;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#30340;&#27807;&#36890;&#21644;&#31038;&#20132;&#20114;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#29702;&#27979;&#37327;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#24773;&#24863;&#29702;&#35299;&#65288;EU&#65289;&#65292;&#36825;&#26159;EI&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#36866;&#29992;&#20110;&#20154;&#31867;&#21644;LLMs&#12290;&#36825;&#20010;&#27979;&#35797;&#38656;&#35201;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#35780;&#20272;&#22797;&#26434;&#30340;&#24773;&#24863;&#65288;&#20363;&#22914;&#65292;&#24778;&#35766;&#12289;&#24841;&#24555;&#12289;&#22256;&#24785;&#12289;&#33258;&#35946;&#65289;&#65292;&#22914;&#23613;&#31649;&#24863;&#35273;&#34920;&#29616;&#19981;&#20339;&#65292;&#32422;&#32752;&#21364;&#24847;&#22806;&#22320;&#33719;&#24471;&#20102;&#26368;&#39640;&#20998;&#12290;&#36890;&#36807;&#20174;500&#22810;&#21517;&#25104;&#24180;&#20154;&#26500;&#24314;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#20027;&#27969;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achie
&lt;/p&gt;</description></item><item><title>PromptMagician&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25552;&#31034;&#24037;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#21457;&#23637;&#20986;&#39640;&#25928;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#21487;&#35270;&#21270;&#21644;&#20010;&#24615;&#21270;&#25506;&#32034;&#25903;&#25345;&#29992;&#25143;&#22312;&#36755;&#20837;&#25552;&#31034;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#21644;&#36845;&#20195;&#12290;</title><link>http://arxiv.org/abs/2307.09036</link><description>&lt;p&gt;
PromptMagician&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#20132;&#20114;&#24335;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation. (arXiv:2307.09036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09036
&lt;/p&gt;
&lt;p&gt;
PromptMagician&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25552;&#31034;&#24037;&#31243;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#21457;&#23637;&#20986;&#39640;&#25928;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#32423;&#21487;&#35270;&#21270;&#21644;&#20010;&#24615;&#21270;&#25506;&#32034;&#25903;&#25345;&#29992;&#25143;&#22312;&#36755;&#20837;&#25552;&#31034;&#36807;&#31243;&#20013;&#30340;&#20132;&#20114;&#21644;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#22240;&#20854;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#22823;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#65292;&#20026;&#26399;&#26395;&#30340;&#22270;&#20687;&#24320;&#21457;&#26377;&#25928;&#30340;&#25552;&#31034;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PromptMagician&#65292;&#19968;&#20010;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#29992;&#25143;&#25506;&#32034;&#22270;&#20687;&#32467;&#26524;&#24182;&#32454;&#21270;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#20027;&#24178;&#26159;&#19968;&#20010;&#25552;&#31034;&#25512;&#33616;&#27169;&#22411;&#65292;&#23427;&#20197;&#29992;&#25143;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;DiffusionDB&#20013;&#26816;&#32034;&#31867;&#20284;&#30340;&#25552;&#31034;-&#22270;&#20687;&#23545;&#65292;&#24182;&#35782;&#21035;&#20986;&#29305;&#27530;&#30340;&#65288;&#37325;&#35201;&#30340;&#21644;&#30456;&#20851;&#30340;&#65289;&#25552;&#31034;&#20851;&#38190;&#35789;&#12290;&#20026;&#20102;&#20419;&#36827;&#20132;&#20114;&#24335;&#25552;&#31034;&#32454;&#21270;&#65292;PromptMagician&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#32423;&#21487;&#35270;&#21270;&#65292;&#29992;&#20110;&#26816;&#32034;&#30340;&#22270;&#20687;&#21644;&#25512;&#33616;&#30340;&#20851;&#38190;&#35789;&#30340;&#36328;&#27169;&#24577;&#23884;&#20837;&#65292;&#24182;&#25903;&#25345;&#29992;&#25143;&#25351;&#23450;&#22810;&#20010;&#20010;&#24615;&#21270;&#25506;&#32034;&#30340;&#26631;&#20934;&#12290;&#36890;&#36807;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#12289;&#29992;&#25143;&#30740;&#31350;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#35777;&#26126;&#20102;PromptMagician&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;KeyBERT&#21644;SNA&#26041;&#27861;&#65292;&#25506;&#32034;&#24037;&#31243;&#23398;&#29983;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#20998;&#26512;&#30740;&#31350;&#29983;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#30340;&#20998;&#26512;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.09014</link><description>&lt;p&gt;
&#20351;&#29992;KeyBERT&#21644;SNA&#25506;&#32034;&#38024;&#23545;&#24037;&#31243;&#23398;&#29983;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#25509;&#21463;&#24230;
&lt;/p&gt;
&lt;p&gt;
Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students. (arXiv:2307.09014v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;KeyBERT&#21644;SNA&#26041;&#27861;&#65292;&#25506;&#32034;&#24037;&#31243;&#23398;&#29983;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36890;&#36807;&#25991;&#26412;&#25366;&#25496;&#20998;&#26512;&#30740;&#31350;&#29983;&#30340;&#24847;&#35265;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#30340;&#20998;&#26512;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#25913;&#36827;&#30340;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#65292;&#25506;&#32034;&#29992;&#25143;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25919;&#31574;&#30340;&#25509;&#21463;&#24230;&#12290;&#36817;&#24180;&#26469;&#65292;&#38889;&#22269;&#25919;&#31574;&#21046;&#23450;&#32773;&#23558;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;ADC&#65289;&#21644;&#33258;&#21160;&#39550;&#39542;&#26426;&#22120;&#20154;&#65288;ADR&#65289;&#35270;&#20026;&#19979;&#19968;&#20195;&#20132;&#36890;&#24037;&#20855;&#65292;&#21487;&#20197;&#38477;&#20302;&#20056;&#23458;&#21644;&#36135;&#29289;&#36816;&#36755;&#25104;&#26412;&#12290;&#20182;&#20204;&#25903;&#25345;&#20026;ADC&#24314;&#35774;V2I&#21644;V2V&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#35748;&#35782;&#21040;ADR&#31561;&#21516;&#20110;&#34892;&#20154;&#65292;&#20197;&#20419;&#36827;&#20854;&#22312;&#20154;&#34892;&#36947;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#22635;&#34917;&#32456;&#31471;&#29992;&#25143;&#23545;&#36825;&#20123;&#25919;&#31574;&#25509;&#21463;&#24230;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#30340;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23558;&#20004;&#31181;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#24212;&#29992;&#20110;&#24037;&#19994;&#12289;&#26426;&#26800;&#21644;&#30005;&#23376;-&#30005;&#27668;-&#35745;&#31639;&#26426;&#39046;&#22495;&#30340;&#30740;&#31350;&#29983;&#24847;&#35265;&#12290;&#19968;&#31181;&#26159;&#22522;&#20110;TF-IWF&#21644;Dice&#31995;&#25968;&#30340;&#20849;&#29616;&#32593;&#32476;&#20998;&#26512;&#65288;CNA&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#26041;&#24335;&#34920;&#31034;&#24847;&#35265;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#24037;&#20855;KeyBERT&#30340;&#19978;&#19979;&#25991;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#65288;C-SNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improved text-mining methods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied two text-mining methods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and dou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09009</link><description>&lt;p&gt;
ChatGPT&#30340;&#34892;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#19978;&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#21464;&#21270;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#34920;&#29616;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#24046;&#24322;&#65292;&#21253;&#25324;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12289;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#12289;&#29983;&#25104;&#20195;&#30721;&#21644;&#35270;&#35273;&#25512;&#29702;&#31561;&#20219;&#21153;&#19978;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#30456;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5&#21644;GPT-4&#26159;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#26356;&#26032;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#30340;2023&#24180;3&#26376;&#21644;2023&#24180;6&#26376;&#29256;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28041;&#21450;&#22235;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;1&#65289;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;2&#65289;&#22238;&#31572;&#25935;&#24863;/&#21361;&#38505;&#38382;&#39064;&#65292;3&#65289;&#29983;&#25104;&#20195;&#30721;&#21644;4&#65289;&#35270;&#35273;&#25512;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3.5&#21644;GPT-4&#30340;&#24615;&#33021;&#21644;&#34892;&#20026;&#22312;&#26102;&#38388;&#19978;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;GPT-4&#65288;2023&#24180;3&#26376;&#65289;&#22312;&#35782;&#21035;&#36136;&#25968;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65288;&#20934;&#30830;&#29575;&#20026;97.6%&#65289;&#65292;&#20294;GPT-4&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#30456;&#21516;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#38750;&#24120;&#24046;&#65288;&#20934;&#30830;&#29575;&#20026;2.4%&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;GPT-3.5&#65288;2023&#24180;6&#26376;&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#27604;GPT-3.5&#65288;2023&#24180;3&#26376;&#65289;&#35201;&#22909;&#24471;&#22810;&#12290;GPT-4&#22312;6&#26376;&#20221;&#23545;&#22238;&#31572;&#25935;&#24863;&#38382;&#39064;&#30340;&#24847;&#24895;&#36739;3&#26376;&#20221;&#35201;&#20302;&#65292;&#32780;&#26080;&#35770;&#26159;GPT-4&#36824;&#26159;GPT-3.5&#22312;6&#26376;&#20221;&#30340;&#20195;&#30721;&#29983;&#25104;&#20013;&#37117;&#26377;&#26356;&#22810;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#30456;&#21516;LLM&#26381;&#21153;&#30340;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#26102;&#38388;&#20869;&#21487;&#20197;&#21457;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
&lt;/p&gt;</description></item><item><title>Ord2Seq&#26159;&#19968;&#31181;&#23558;&#24207;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.09004</link><description>&lt;p&gt;
Ord2Seq: &#23558;&#24207;&#22238;&#24402;&#35270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ord2Seq: Regard Ordinal Regression as Label Sequence Prediction. (arXiv:2307.09004v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09004
&lt;/p&gt;
&lt;p&gt;
Ord2Seq&#26159;&#19968;&#31181;&#23558;&#24207;&#22238;&#24402;&#38382;&#39064;&#36716;&#21270;&#20026;&#26631;&#31614;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#36798;&#21040;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#22238;&#24402;&#26159;&#23558;&#23545;&#35937;&#23454;&#20363;&#20998;&#20026;&#24207;&#21015;&#31867;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22914;&#21307;&#30103;&#30142;&#30149;&#20998;&#32423;&#12289;&#30005;&#24433;&#35780;&#20998;&#31561;&#26041;&#38754;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#23398;&#20064;&#31867;&#38388;&#24207;&#20851;&#31995;&#65292;&#20294;&#22312;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;Ord2Seq&#65292;&#23427;&#39318;&#27425;&#23558;&#27599;&#20010;&#24207;&#31867;&#21035;&#26631;&#31614;&#36716;&#21270;&#20026;&#29305;&#27530;&#30340;&#26631;&#31614;&#24207;&#21015;&#65292;&#20174;&#32780;&#23558;&#24207;&#22238;&#24402;&#20219;&#21153;&#35270;&#20026;&#24207;&#21015;&#39044;&#27979;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#24207;&#22238;&#24402;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36882;&#24402;&#30340;&#20108;&#20803;&#20998;&#31867;&#27493;&#39588;&#65292;&#20197;&#24494;&#22937;&#22320;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21306;&#20998;&#30456;&#37051;&#31867;&#21035;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#20195;&#30721;&#23558;&#22312;&#23436;&#25104;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading, movie rating, etc. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes will be available upon a
&lt;/p&gt;</description></item><item><title>EVIL&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35777;&#25454;&#25512;&#29702;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#29983;&#25104;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08988</link><description>&lt;p&gt;
EVIL: &#29992;&#20110;&#21487;&#20449;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#35777;&#25454;&#25512;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EVIL: Evidential Inference Learning for Trustworthy Semi-supervised Medical Image Segmentation. (arXiv:2307.08988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08988
&lt;/p&gt;
&lt;p&gt;
EVIL&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35777;&#25454;&#25512;&#29702;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#25512;&#26029;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#29983;&#25104;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23384;&#22312;&#19968;&#20010;&#38590;&#20197;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#24179;&#34913;&#35745;&#31639;&#25104;&#26412;&#12289;&#20272;&#35745;&#31934;&#24230;&#21644;&#29702;&#35770;&#25903;&#25345;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35777;&#25454;&#29702;&#35770;&#24341;&#20837;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#31216;&#20043;&#20026;&#35777;&#25454;&#25512;&#29702;&#23398;&#20064;&#65288;EVIL&#65289;&#12290;EVIL&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25512;&#26029;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21518;&#65292;&#26681;&#25454;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#24378;&#21046;&#23545;&#25200;&#21160;&#39044;&#27979;&#36827;&#34892;&#19968;&#33268;&#24615;&#32422;&#26463;&#26469;&#22686;&#24378;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EVIL&#22312;&#19982;&#20960;&#31181;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, uncertainty-aware methods have attracted increasing attention in semi-supervised medical image segmentation. However, current methods usually suffer from the drawback that it is difficult to balance the computational cost, estimation accuracy, and theoretical support in a unified framework. To alleviate this problem, we introduce the Dempster-Shafer Theory of Evidence (DST) into semi-supervised medical image segmentation, dubbed Evidential Inference Learning (EVIL). EVIL provides a theoretically guaranteed solution to infer accurate uncertainty quantification in a single forward pass. Trustworthy pseudo labels on unlabeled data are generated after uncertainty estimation. The recently proposed consistency regularization-based training paradigm is adopted in our framework, which enforces the consistency on the perturbed predictions to enhance the generalization with few labeled data. Experimental results show that EVIL achieves competitive performance in comparison with several
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#36741;&#21161;&#30340;5G NR&#19979;&#20302;&#24310;&#36831;XR&#26381;&#21153;&#25552;&#20379;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#24103;&#36827;&#34892;&#22788;&#29702;&#32780;&#38750;&#20165;&#20165;&#20381;&#36182;&#23454;&#38469;&#24103;&#65292;&#23454;&#29616;&#20102;&#23545;XR&#26381;&#21153;&#30340;&#25913;&#36827;&#21644;&#22810;&#20493;&#22686;&#21152;&#25903;&#25345;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#32593;&#32476;&#35774;&#35745;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.08987</link><description>&lt;p&gt;
&#22522;&#20110;AI&#36741;&#21161;&#30340;5G NR&#19979;&#20302;&#24310;&#36831;XR&#26381;&#21153;&#25552;&#20379;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
AI-assisted Improved Service Provisioning for Low-latency XR over 5G NR. (arXiv:2307.08987v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#36741;&#21161;&#30340;5G NR&#19979;&#20302;&#24310;&#36831;XR&#26381;&#21153;&#25552;&#20379;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#27979;&#24103;&#36827;&#34892;&#22788;&#29702;&#32780;&#38750;&#20165;&#20165;&#20381;&#36182;&#23454;&#38469;&#24103;&#65292;&#23454;&#29616;&#20102;&#23545;XR&#26381;&#21153;&#30340;&#25913;&#36827;&#21644;&#22810;&#20493;&#22686;&#21152;&#25903;&#25345;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#32593;&#32476;&#35774;&#35745;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#26159;5G/6G&#23186;&#20307;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#20043;&#19968;&#65292;&#23558;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20154;&#31867;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#20302;&#24310;&#36831;&#12289;&#39640;&#25968;&#25454;&#36895;&#29575;&#21644;&#21487;&#38752;&#24615;&#20197;&#25903;&#25345;XR&#26381;&#21153;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AI&#36741;&#21161;&#26381;&#21153;&#25552;&#20379;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#39044;&#27979;&#24103;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#23454;&#38469;&#24103;&#12290;&#35813;&#26041;&#27861;&#34394;&#25311;&#22686;&#21152;&#20102;&#32593;&#32476;&#24310;&#36831;&#39044;&#31639;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26381;&#21153;&#25552;&#20379;&#65292;&#23613;&#31649;&#20250;&#20135;&#29983;&#19968;&#20123;&#39044;&#27979;&#38169;&#35823;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#65292;&#22312;&#25903;&#25345;XR&#29992;&#25143;&#26041;&#38754;&#23454;&#29616;&#20102;&#22810;&#20493;&#22686;&#21152;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#32593;&#32476;&#35774;&#35745;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extended Reality (XR) is one of the most important 5G/6G media applications that will fundamentally transform human interactions. However, ensuring low latency, high data rate, and reliability to support XR services poses significant challenges. This letter presents a novel AI-assisted service provisioning scheme that leverages predicted frames for processing rather than relying solely on actual frames. This method virtually increases the network delay budget and consequently improves service provisioning, albeit at the expense of minor prediction errors. The proposed scheme is validated by extensive simulations demonstrating a multi-fold increase in supported XR users and also provides crucial network design insights.
&lt;/p&gt;</description></item><item><title>PromptCrafter&#26159;&#19968;&#31181;&#28151;&#21512;&#20513;&#35758;&#31995;&#32479;&#65292;&#36890;&#36807;&#19982;LLM&#30340;&#23545;&#35805;&#36880;&#27493;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#12290;&#29992;&#25143;&#21487;&#20197;&#39640;&#25928;&#25506;&#32034;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22238;&#31572;&#28548;&#28165;&#38382;&#39064;&#26469;&#20248;&#21270;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.08985</link><description>&lt;p&gt;
PromptCrafter&#65306;&#36890;&#36807;&#19982;LLM&#30340;&#28151;&#21512;&#20513;&#35758;&#23545;&#35805;&#26469;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM. (arXiv:2307.08985v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08985
&lt;/p&gt;
&lt;p&gt;
PromptCrafter&#26159;&#19968;&#31181;&#28151;&#21512;&#20513;&#35758;&#31995;&#32479;&#65292;&#36890;&#36807;&#19982;LLM&#30340;&#23545;&#35805;&#36880;&#27493;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#12290;&#29992;&#25143;&#21487;&#20197;&#39640;&#25928;&#25506;&#32034;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22238;&#31572;&#28548;&#28165;&#38382;&#39064;&#26469;&#20248;&#21270;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21333;&#20010;&#25552;&#31034;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#21644;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#20132;&#20114;&#26041;&#27861;&#65292;&#24110;&#21161;&#29992;&#25143;&#20102;&#35299;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#21033;&#29992;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#25903;&#25345;&#29992;&#25143;&#39640;&#25928;&#22320;&#25506;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptCrafter&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#20513;&#35758;&#31995;&#32479;&#65292;&#21487;&#20197;&#36880;&#27493;&#21019;&#24314;&#25991;&#26412;&#21040;&#22270;&#20687;&#25552;&#31034;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#36807;&#31243;&#65292;&#29992;&#25143;&#21487;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#26126;&#30830;&#33258;&#24049;&#30340;&#24847;&#22270;&#12290;PromptCrafter&#36824;&#36890;&#36807;&#22238;&#31572;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#28548;&#28165;&#38382;&#39064;&#30340;&#21508;&#31181;&#22238;&#31572;&#26469;&#25903;&#25345;&#29992;&#25143;&#20248;&#21270;&#25552;&#31034;&#12290;&#26368;&#21518;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#24037;&#20316;&#21382;&#21490;&#26469;&#24674;&#22797;&#21040;&#25152;&#38656;&#30340;&#27493;&#39588;&#12290;&#22312;&#26412;&#30740;&#35752;&#20250;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;PromptCrafter&#30340;&#35774;&#35745;&#36807;&#31243;&#21644;&#21518;&#32493;&#30740;&#31350;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model's capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;ChatGPT&#30340;&#24320;&#21457;&#21644;&#19982;&#20043;&#31867;&#20284;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#22914;&#20309;&#36947;&#24503;&#24212;&#29992;&#12289;&#20351;&#29992;&#21644;&#25259;&#38706;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CANGARU&#25351;&#21335;&#12290;&#35813;&#25351;&#21335;&#26088;&#22312;&#20419;&#36827;&#20840;&#29699;&#23398;&#26415;&#30028;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#36947;&#24503;&#20351;&#29992;&#12289;&#25259;&#38706;&#21644;&#36866;&#24403;&#25253;&#21578;&#30340;&#32479;&#19968;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.08974</link><description>&lt;p&gt;
ChatGPT&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#36127;&#36131;&#20219;&#30340;&#25253;&#36947;&#21644;&#20351;&#29992;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#65288;CANGARU&#65289;
&lt;/p&gt;
&lt;p&gt;
Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines. (arXiv:2307.08974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08974
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;ChatGPT&#30340;&#24320;&#21457;&#21644;&#19982;&#20043;&#31867;&#20284;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#22914;&#20309;&#36947;&#24503;&#24212;&#29992;&#12289;&#20351;&#29992;&#21644;&#25259;&#38706;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CANGARU&#25351;&#21335;&#12290;&#35813;&#25351;&#21335;&#26088;&#22312;&#20419;&#36827;&#20840;&#29699;&#23398;&#26415;&#30028;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#36947;&#24503;&#20351;&#29992;&#12289;&#25259;&#38706;&#21644;&#36866;&#24403;&#25253;&#21578;&#30340;&#32479;&#19968;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ChatGPT&#30340;&#24320;&#21457;&#20197;&#21450;&#19982;&#20854;&#31867;&#20284;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#12289;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#31185;&#23398;&#25104;&#26524;&#20013;&#36947;&#24503;&#24212;&#29992;&#12289;&#20351;&#29992;&#21644;&#25259;&#38706;&#30340;&#38382;&#39064;&#12290;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#20986;&#29256;&#21830;&#21644;&#26399;&#21002;&#26368;&#36817;&#21046;&#23450;&#20102;&#33258;&#24049;&#30340;&#35268;&#21017;&#65292;&#20294;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#8220;&#24052;&#21035;&#22612;&#25928;&#24212;&#8221;&#65292;&#21487;&#33021;&#20135;&#29983;&#28151;&#28102;&#32780;&#19981;&#26159;&#26399;&#26395;&#30340;&#26631;&#20934;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatGPT&#12289;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#36127;&#36131;&#20219;&#30340;&#25253;&#36947;&#21644;&#20351;&#29992;&#30340;&#25351;&#21335;&#65288;CANGARU&#65289;&#35745;&#21010;&#65292;&#26088;&#22312;&#20419;&#36827;&#36328;&#23398;&#31185;&#30340;&#20840;&#29699;&#20849;&#35782;&#65292;&#20851;&#20110;&#22312;&#23398;&#26415;&#30028;&#23545;GAI/GPT/LLM&#25216;&#26415;&#30340;&#36947;&#24503;&#20351;&#29992;&#12289;&#25259;&#38706;&#21644;&#36866;&#24403;&#25253;&#21578;&#12290;&#26412;&#21327;&#35758;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#30340;&#37096;&#20998;: a) &#23545;GAI/GPT/LLM&#24212;&#29992;&#30340;&#25345;&#32493;&#31995;&#32479;&#24615;&#23457;&#26597;&#65292;&#20197;&#20102;&#35299;&#20854;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The swift progress and ubiquitous adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have spurred queries about their ethical application, use, and disclosure in scholarly research and scientific productions. A few publishers and journals have recently created their own sets of rules; however, the absence of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization. In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia. The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08964</link><description>&lt;p&gt;
&#26223;&#35266;&#26367;&#20195;&#21697;&#65306;&#22312;&#37096;&#20998;&#20449;&#24687;&#19979;&#23398;&#20064;&#25968;&#23398;&#20248;&#21270;&#30340;&#20915;&#31574;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Landscape Surrogate: Learning Decision Losses for Mathematical Optimization Under Partial Information. (arXiv:2307.08964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26223;&#35266;&#26367;&#20195;&#21697;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37096;&#20998;&#20449;&#24687;&#19979;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#20248;&#21270;&#22120;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23398;&#20064;&#38598;&#25104;&#20248;&#21270;&#24037;&#20316;&#22312;&#20248;&#21270;&#38382;&#39064;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#25110;&#36890;&#29992;&#20248;&#21270;&#22120;&#22312;&#26080;&#19987;&#23478;&#35843;&#20248;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#24773;&#20917;&#19979;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20248;&#21270;&#22120;$ \mathbf{g} $&#26469;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;&#20248;&#21270;&#22120;&#21487;&#20197;&#36890;&#36807;&#24050;&#30693;&#26368;&#20248;&#35299;&#30340;&#30417;&#30563;&#25110;&#36890;&#36807;&#20248;&#21270;&#22797;&#21512;&#20989;&#25968;$ f\circ \mathbf{g} $&#30340;&#38544;&#24335;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#38544;&#24335;&#26041;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#26368;&#20248;&#35299;&#20316;&#20026;&#26631;&#31614;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#39057;&#32321;&#35843;&#29992;&#20248;&#21270;&#22120;$ \mathbf{g} $&#65292;&#22240;&#27492;&#35757;&#32451;&#21644;&#37096;&#32626;&#32531;&#24930;&#12290;&#23545;&#20110;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#30001;&#20110;$ \mathbf{g} $&#30340;&#31232;&#30095;&#26799;&#24230;&#65292;&#35757;&#32451;&#36827;&#19968;&#27493;&#21463;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#28369;&#21487;&#23398;&#20064;&#30340;&#26223;&#35266;&#26367;&#20195;&#21697;$ M $&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in learning-integrated optimization have shown promise in settings where the optimization problem is only partially observed or where general-purpose optimizers perform poorly without expert tuning. By learning an optimizer $\mathbf{g}$ to tackle these challenging problems with $f$ as the objective, the optimization process can be substantially accelerated by leveraging past experience. The optimizer can be trained with supervision from known optimal solutions or implicitly by optimizing the compound function $f\circ \mathbf{g}$. The implicit approach may not require optimal solutions as labels and is capable of handling problem uncertainty; however, it is slow to train and deploy due to frequent calls to optimizer $\mathbf{g}$ during both training and testing. The training is further challenged by sparse gradients of $\mathbf{g}$, especially for combinatorial solvers. To address these challenges, we propose using a smooth and learnable Landscape Surrogate $M$ as a replace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08962</link><description>&lt;p&gt;
REX: &#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#22686;&#24378;&#22411;AI&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
REX: Rapid Exploration and eXploitation for AI Agents. (arXiv:2307.08962v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;REX&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#21644;&#31867;&#20284;&#20110;UCB&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31163;&#32447;&#34892;&#20026;&#21033;&#29992;&#21644;&#19982;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;AI&#20195;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;REX&#12290;&#29616;&#26377;&#30340;AutoGPT&#39118;&#26684;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#22914;&#23545;&#20110;&#20915;&#31574;&#30340;&#31934;&#30830;&#25551;&#36848;&#30340;&#36807;&#24230;&#20381;&#36182;&#65292;&#20197;&#21450;&#32570;&#20047;&#31867;&#20284;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning&#65292;RL)&#20013;&#30340;&#23581;&#35797;&#21644;&#22833;&#36133;&#31243;&#24207;&#30340;&#31995;&#32479;&#24615;&#26041;&#27861;&#12290;REX&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22870;&#21169;&#23618;&#65292;&#24182;&#38598;&#25104;&#20102;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#38480;(UCB)&#20998;&#25968;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;&#30340;AI&#20195;&#29702;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#26159;&#21487;&#20197;&#21033;&#29992;&#26469;&#33258;&#26085;&#24535;&#30340;&#31163;&#32447;&#34892;&#20026;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#12290;&#36890;&#36807;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#24605;&#32500;&#38142;(CoT)&#21644;&#35268;&#21010;&#25512;&#29702;(RAP)&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#22522;&#20110;REX&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#36229;&#36807;&#20102;&#36825;&#20123;&#29616;&#26377;&#25216;&#26415;&#25152;&#21462;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08944</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;Siamese&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Siamese Networks for Weakly Supervised Human Activity Recognition. (arXiv:2307.08944v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#24369;&#30417;&#30563;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26126;&#30830;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#22810;&#20010;Siamese&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#20165;&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#30693;&#36947;&#26126;&#30830;&#30340;&#26631;&#31614;&#12290;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#23558;&#27963;&#21160;&#25968;&#25454;&#26679;&#26412;&#26144;&#23556;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#21521;&#37327;&#65292;&#20351;&#24471;&#34920;&#31034;&#31354;&#38388;&#20013;&#21521;&#37327;&#20043;&#38388;&#30340;&#36317;&#31163;&#36817;&#20284;&#20110;&#36755;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#26679;&#26412;&#30340;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#21518;&#30340;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#24191;&#27867;&#30340;&#19981;&#21516;&#32858;&#31867;&#31639;&#27861;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35757;&#32451;&#36807;&#31243;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#20284;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#21516;&#19968;&#31181;&#27963;&#21160;&#30340;&#26679;&#26412;&#23545;&#30340;&#36317;&#31163;&#24230;&#37327;&#23567;&#65292;&#24182;&#20351;&#24471;&#19981;&#21516;&#31181;&#27963;&#21160;&#30340;&#26679;&#26412;&#23545;&#30340;&#36317;&#31163;&#24230;&#37327;&#22823;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#39564;&#35777;&#20854;&#22312;&#20998;&#21106;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been successfully applied to human activity recognition. However, training deep neural networks requires explicitly labeled data which is difficult to acquire. In this paper, we present a model with multiple siamese networks that are trained by using only the information about the similarity between pairs of data samples without knowing the explicit labels. The trained model maps the activity data samples into fixed size representation vectors such that the distance between the vectors in the representation space approximates the similarity of the data samples in the input space. Thus, the trained model can work as a metric for a wide range of different clustering algorithms. The training process minimizes a similarity loss function that forces the distance metric to be small for pairs of samples from the same kind of activity, and large for pairs of samples from different kinds of activities. We evaluate the model on three datasets to verify its effectiveness in segm
&lt;/p&gt;</description></item><item><title>IxDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#25805;&#20316;&#21592;&#23545;RL&#20195;&#29702;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.08933</link><description>&lt;p&gt;
IxDRL:&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
IxDRL: A Novel Explainable Deep Reinforcement Learning Toolkit based on Analyses of Interestingness. (arXiv:2307.08933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08933
&lt;/p&gt;
&lt;p&gt;
IxDRL&#26159;&#19968;&#31181;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20154;&#31867;&#25805;&#20316;&#21592;&#23545;RL&#20195;&#29702;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#20855;&#26377;&#39640;&#32500;&#36755;&#20837;&#30340;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#20247;&#22810;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#32570;&#20047;&#24517;&#35201;&#30340;&#26426;&#21046;&#26469;&#25552;&#20379;&#20154;&#31867;&#23545;&#20854;&#33021;&#21147;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#36825;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#25104;&#20026;&#37319;&#29992;RL&#30340;&#38556;&#30861;&#65292;&#22240;&#20026;&#20195;&#29702;&#31243;&#24207;&#25152;&#20570;&#30340;&#20915;&#31574;&#21487;&#33021;&#20855;&#26377;&#37325;&#22823;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#31995;&#32479;&#26412;&#36136;&#19978;&#19981;&#20855;&#22791;&#33021;&#21147;&#24863;&#30693;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#24517;&#35201;&#30340;&#35299;&#37322;&#26426;&#21046;&#65292;&#20351;&#20154;&#31867;&#25805;&#20316;&#21592;&#33021;&#22815;&#23545;&#20854;&#33021;&#21147;&#26377;&#28145;&#20837;&#12289;&#25972;&#20307;&#30340;&#20102;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;RL&#65288;xDRL&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#26377;&#36259;&#20998;&#26512;&#25552;&#20379;&#22810;&#31181;RL&#20195;&#29702;&#30340;&#33021;&#21147;&#24230;&#37327;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;RL&#31639;&#27861;&#65292;&#24182;&#21407;&#29983;&#25903;&#25345;&#27969;&#34892;&#30340;RLLib&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24037;&#20855;&#21253;&#22312;...&#65288;&#21407;&#25991;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in deep learning have resulted in a plethora of successes in the use of reinforcement learning (RL) to solve complex sequential decision tasks with high-dimensional inputs. However, existing systems lack the necessary mechanisms to provide humans with a holistic view of their competence, presenting an impediment to their adoption, particularly in critical applications where the decisions an agent makes can have significant consequences. Yet, existing RL-based systems are essentially competency-unaware in that they lack the necessary interpretation mechanisms to allow human operators to have an insightful, holistic view of their competency. Towards more explainable Deep RL (xDRL), we propose a new framework based on analyses of interestingness. Our tool provides various measures of RL agent competence stemming from interestingness analysis and is applicable to a wide range of RL algorithms, natively supporting the popular RLLib toolkit. We showcase the use of o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.08930</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Graph Matching Based on Cycle Consistency. (arXiv:2307.08930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#19968;&#33268;&#24615;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#30495;&#23454;&#23545;&#24212;&#30340;&#20851;&#38190;&#28857;&#23545;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#24378;&#21046;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#31232;&#30095;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#22270;&#21305;&#37197;&#20013;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24212;&#29992;&#20110;&#22270;&#20687;&#20013;&#30340;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;&#19982;&#26631;&#20934;&#30340;&#8220;&#30417;&#30563;&#8221;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20851;&#38190;&#28857;&#23545;&#20043;&#38388;&#30340;&#30495;&#23454;&#23545;&#24212;&#12290;&#30456;&#21453;&#65292;&#23427;&#36890;&#36807;&#24378;&#21046;&#21516;&#19968;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#12290;&#30001;&#20110;&#21305;&#37197;&#21644;&#19968;&#33268;&#24615;&#25439;&#22833;&#26159;&#31163;&#25955;&#30340;&#65292;&#23427;&#20204;&#30340;&#23548;&#25968;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#32452;&#21512;&#27714;&#35299;&#22120;&#30340;&#40657;&#30418;&#24494;&#20998;&#30340;&#26368;&#26032;&#32467;&#26524;&#22522;&#30784;&#19978;&#26500;&#24314;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#19982;&#20219;&#24847;&#32593;&#32476;&#26550;&#26500;&#21644;&#32452;&#21512;&#27714;&#35299;&#22120;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#26080;&#30417;&#30563;&#22270;&#21305;&#37197;&#26041;&#38754;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We address this issue in a principled way by building our method upon the recent results on black-box differentiation of combinatorial solvers. This makes our method exceptionally flexible, as it is compatible with arbitrary network architectures and combinatorial solvers. Our experimental evaluation suggests that our technique sets a new state-of-the-art for unsupervised graph matching.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#31561;&#35774;&#35745;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08920</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;: &#20855;&#26377;&#29702;&#35770;&#27934;&#23519;&#21147;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#26032;&#35774;&#35745;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time Reinforcement Learning: New Design Algorithms with Theoretical Insights and Performance Guarantees. (arXiv:2307.08920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#30340;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#31561;&#35774;&#35745;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#32463;&#21382;&#20102;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#35774;&#35745;&#26041;&#27861;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#23545;&#29616;&#26377;&#30340;&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#65288;CT-RL&#65289;&#26041;&#27861;&#36827;&#34892;&#30340;&#20840;&#38754;&#20998;&#26512;&#25581;&#31034;&#20102;&#23427;&#20204;&#38754;&#20020;&#37325;&#22823;&#30340;&#35774;&#35745;&#25361;&#25112;&#65292;&#21253;&#25324;&#22797;&#26434;&#24615;&#12289;&#25968;&#20540;&#26465;&#20214;&#21644;&#32500;&#24230;&#25193;&#23637;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#20808;&#36827;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20294;&#29616;&#26377;&#30340;ADP CT-RL&#32508;&#21512;&#26041;&#27861;&#22312;&#35299;&#20915;&#21363;&#20351;&#26159;&#23567;&#30340;&#23398;&#26415;&#38382;&#39064;&#26102;&#37117;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#19968;&#22871;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26032;&#30340;CT-RL&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21487;&#20197;&#21010;&#20998;&#20026;&#36739;&#23567;&#23376;&#38382;&#39064;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#31181;&#26500;&#36896;&#24615;&#30340;&#32771;&#34385;&#20351;&#24471;&#38382;&#39064;&#30340;&#32500;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time nonlinear optimal control problems hold great promise in real-world applications. After decades of development, reinforcement learning (RL) has achieved some of the greatest successes as a general nonlinear control design method. However, a recent comprehensive analysis of state-of-the-art continuous-time RL (CT-RL) methods, namely, adaptive dynamic programming (ADP)-based CT-RL algorithms, reveals they face significant design challenges due to their complexity, numerical conditioning, and dimensional scaling issues. Despite advanced theoretical results, existing ADP CT-RL synthesis methods are inadequate in solving even small, academic problems. The goal of this work is thus to introduce a suite of new CT-RL algorithms for control of affine nonlinear systems. Our design approach relies on two important factors. First, our methods are applicable to physical systems that can be partitioned into smaller subproblems. This constructive consideration results in reduced dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;&#34880;&#31958;&#25511;&#21046;&#12289;&#20943;&#23569;&#34880;&#31958;&#21464;&#24322;&#24615;&#20197;&#21450;&#39044;&#38450;&#20302;&#34880;&#31958;&#20107;&#20214;&#31561;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08897</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#24739;&#32773;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
Basal-Bolus Advisor for Type 1 Diabetes (T1D) Patients Using Multi-Agent Reinforcement Learning (RL) Methodology. (arXiv:2307.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#26174;&#33879;&#25913;&#21892;&#34880;&#31958;&#25511;&#21046;&#12289;&#20943;&#23569;&#34880;&#31958;&#21464;&#24322;&#24615;&#20197;&#21450;&#39044;&#38450;&#20302;&#34880;&#31958;&#20107;&#20214;&#31561;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#22411;&#31958;&#23615;&#30149;&#65288;T1D&#65289;&#24739;&#32773;&#20010;&#24615;&#21270;&#34880;&#31958;&#25511;&#21046;&#30340;&#26032;&#22411;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#38381;&#29615;&#31995;&#32479;&#65292;&#21253;&#25324;&#34880;&#31958;&#20195;&#35874;&#27169;&#22411;&#21644;&#20316;&#20026;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;&#30340;&#22810;&#26234;&#33021;&#20307;&#36719;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#27169;&#22411;&#12290;&#36890;&#36807;&#27604;&#36739;RL&#20195;&#29702;&#19982;&#20256;&#32479;&#27835;&#30103;&#22312;&#19977;&#20010;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35780;&#20272;&#25351;&#26631;&#21253;&#25324;&#34880;&#31958;&#27700;&#24179;&#65288;&#26368;&#20302;&#12289;&#26368;&#39640;&#21644;&#24179;&#22343;&#65289;&#65292;&#22312;&#19981;&#21516;&#34880;&#31958;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#65292;&#20197;&#21450;&#24179;&#22343;&#27599;&#26085;&#25512;&#33616;&#21058;&#37327;&#21644;&#22522;&#30784;&#33008;&#23707;&#32032;&#21058;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;RL&#30340;&#22522;&#30784;-&#25512;&#33616;&#21058;&#37327;&#39038;&#38382;&#26174;&#33879;&#25913;&#21892;&#20102;&#34880;&#31958;&#25511;&#21046;&#65292;&#20943;&#23567;&#20102;&#34880;&#31958;&#21464;&#24322;&#24615;&#65292;&#22686;&#21152;&#20102;&#22312;&#30446;&#26631;&#33539;&#22260;&#65288;70-180 mg/dL&#65289;&#20869;&#30340;&#26102;&#38388;&#12290;&#20302;&#34880;&#31958;&#20107;&#20214;&#24471;&#21040;&#26377;&#25928;&#39044;&#38450;&#65292;&#20005;&#37325;&#39640;&#34880;&#31958;&#20107;&#20214;&#24471;&#21040;&#20943;&#23569;&#12290;RL&#26041;&#27861;&#36824;&#23548;&#33268;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#24179;&#22343;&#27599;&#26085;&#22522;&#30784;&#33008;&#23707;&#32032;&#21058;&#37327;&#26377;&#32479;&#35745;&#23398;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel multi-agent reinforcement learning (RL) approach for personalized glucose control in individuals with type 1 diabetes (T1D). The method employs a closed-loop system consisting of a blood glucose (BG) metabolic model and a multi-agent soft actor-critic RL model acting as the basal-bolus advisor. Performance evaluation is conducted in three scenarios, comparing the RL agents to conventional therapy. Evaluation metrics include glucose levels (minimum, maximum, and mean), time spent in different BG ranges, and average daily bolus and basal insulin dosages. Results demonstrate that the RL-based basal-bolus advisor significantly improves glucose control, reducing glycemic variability and increasing time spent within the target range (70-180 mg/dL). Hypoglycemia events are effectively prevented, and severe hyperglycemia events are reduced. The RL approach also leads to a statistically significant reduction in average daily basal insulin dosage compared to conventio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#36824;&#26080;&#27861;&#25903;&#25345;&#24605;&#24819;&#30340;&#21487;&#36861;&#28335;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25351;&#20986;&#29983;&#25104;&#24335;AI&#22312;&#30495;&#23454;&#24615;&#12289;&#21442;&#32771;&#21644;&#20934;&#30830;&#22320;&#22320;&#22270;&#31561;&#26041;&#38754;&#36824;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2307.08876</link><description>&lt;p&gt;
AI&#29992;&#20110;&#21019;&#24847;&#30340;&#29983;&#25104;&#21644;&#27979;&#35797;&#65306;&#21521;AI&#25903;&#25345;&#30340;&#30693;&#35782;&#21457;&#23637;&#29615;&#22659;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
AI for the Generation and Testing of Ideas Towards an AI Supported Knowledge Development Environment. (arXiv:2307.08876v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;AI&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#36824;&#26080;&#27861;&#25903;&#25345;&#24605;&#24819;&#30340;&#21487;&#36861;&#28335;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25351;&#20986;&#29983;&#25104;&#24335;AI&#22312;&#30495;&#23454;&#24615;&#12289;&#21442;&#32771;&#21644;&#20934;&#30830;&#22320;&#22320;&#22270;&#31561;&#26041;&#38754;&#36824;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#31995;&#32479;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#22823;&#22411;&#30693;&#35782;&#28304;&#36827;&#34892;&#31579;&#36873;&#65292;&#21019;&#24314;&#28789;&#27963;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20998;&#36776;&#19981;&#21516;&#36890;&#20449;&#24418;&#24335;&#20013;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#39044;&#27979;&#39034;&#24207;&#20449;&#24687;&#12290;&#29983;&#25104;&#24335;AI&#21033;&#29992;Transformer&#29983;&#25104;&#25991;&#26412;&#25110;&#35270;&#35273;&#36755;&#20986;&#65292;&#27169;&#20223;&#20154;&#31867;&#22238;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#25110;&#22810;&#20010;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#20379;&#29992;&#25143;&#24605;&#32771;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#24335;AI&#30446;&#21069;&#19981;&#25903;&#25345;&#24605;&#24819;&#30340;&#21487;&#36861;&#28335;&#24615;&#65292;&#32780;&#25628;&#32034;&#24341;&#25806;&#21487;&#20197;&#25552;&#20379;&#20449;&#24687;&#26469;&#28304;&#30340;&#26377;&#29992;&#21151;&#33021;&#12290;&#29983;&#25104;&#24335;AI&#30340;&#21465;&#36848;&#39118;&#26684;&#21463;&#21040;&#20102;&#31215;&#26497;&#30340;&#35780;&#20215;&#12290;&#20154;&#20204;&#36890;&#36807;&#25925;&#20107;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;ChatGPT&#22312;&#30495;&#23454;&#24615;&#12289;&#21442;&#32771;&#12289;&#35745;&#31639;&#21644;&#20934;&#30830;&#22320;&#22320;&#22270;&#31561;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#24403;&#21069;&#30340;&#33021;&#21147;&#21442;&#32771;&#20301;&#32622;&#21644;&#38142;&#25509;&#21040;&#24212;&#29992;&#20284;&#20046;&#26356;&#36866;&#21512;&#25105;&#20204;&#20351;&#29992;&#20102;&#20108;&#21313;&#24180;&#30340;&#20197;&#38142;&#25509;&#20026;&#20013;&#24515;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#37096;&#32626;&#30495;&#27491;&#21487;&#20449;&#30340;&#35299;&#20915;&#26041;&#26696;&#36229;&#36234;&#20102;&#27169;&#25311;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
New systems employ Machine Learning to sift through large knowledge sources, creating flexible Large Language Models. These models discern context and predict sequential information in various communication forms. Generative AI, leveraging Transformers, generates textual or visual outputs mimicking human responses. It proposes one or multiple contextually feasible solutions for a user to contemplate. However, generative AI does not currently support traceability of ideas, a useful feature provided by search engines indicating origin of information. The narrative style of generative AI has gained positive reception. People learn from stories. Yet, early ChatGPT efforts had difficulty with truth, reference, calculations, and aspects like accurate maps. Current capabilities of referencing locations and linking to apps seem to be better catered by the link-centric search methods we've used for two decades. Deploying truly believable solutions extends beyond simulating contextual relevance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.08873</link><description>&lt;p&gt;
&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#26367;&#20195;&#65306;&#22522;&#23612;&#31163;&#24046;
&lt;/p&gt;
&lt;p&gt;
An Alternative to Variance: Gini Deviation for Risk-averse Policy Gradient. (arXiv:2307.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#23612;&#31163;&#24046;&#26469;&#26367;&#20195;&#26041;&#24046;&#65292;&#32531;&#35299;&#20102;&#26041;&#24046;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39118;&#38505;&#21388;&#24694;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#38480;&#21046;&#31574;&#30053;&#22238;&#25253;&#30340;&#26041;&#24046;&#26159;&#19968;&#31181;&#24120;&#35265;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26126;&#30830;&#30340;&#25968;&#23398;&#23450;&#20041;&#21644;&#26131;&#20110;&#35299;&#37322;&#12290;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#38480;&#21046;&#24635;&#22238;&#25253;&#26041;&#24046;&#65292;&#32780;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#27599;&#27493;&#22870;&#21169;&#26041;&#24046;&#20316;&#20026;&#20195;&#29702;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#36825;&#20123;&#22522;&#20110;&#26041;&#24046;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#25968;&#23383;&#23610;&#24230;&#30340;&#25935;&#24863;&#24615;&#21644;&#38459;&#30861;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#26367;&#20195;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#8212;&#8212;&#22522;&#23612;&#31163;&#24046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#23567;&#21270;&#22522;&#23612;&#31163;&#24046;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;&#22312;&#39118;&#38505;&#21388;&#24694;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#30340;&#39046;&#22495;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#22522;&#20110;&#26041;&#24046;&#30340;&#39118;&#38505;&#34913;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#31574;&#30053;&#26080;&#27861;&#23398;&#21040;&#21512;&#29702;&#31574;&#30053;&#26102;&#23454;&#29616;&#39640;&#22238;&#25253;&#21644;&#20302;&#39118;&#38505;&#65292;&#20197;&#26041;&#24046;&#21644;&#22522;&#23612;&#31163;&#24046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restricting the variance of a policy's return is a popular choice in risk-averse Reinforcement Learning (RL) due to its clear mathematical definition and easy interpretability. Traditional methods directly restrict the total return variance. Recent methods restrict the per-step reward variance as a proxy. We thoroughly examine the limitations of these variance-based methods, such as sensitivity to numerical scale and hindering of policy learning, and propose to use an alternative risk measure, Gini deviation, as a substitute. We study various properties of this new risk measure and derive a policy gradient algorithm to minimize it. Empirical evaluation in domains where risk-aversion can be clearly defined, shows that our algorithm can mitigate the limitations of variance-based risk measures and achieves high return with low risk in terms of variance and Gini deviation when others fail to learn a reasonable policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.08859</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#22810;&#35270;&#35282;&#21644;&#33021;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach. (arXiv:2307.08859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#21644;&#27169;&#22411;&#33021;&#21147;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65292;&#20197;&#21450;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#32454;&#31890;&#24230;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#26159;&#19968;&#31181;&#35745;&#21010;&#22909;&#30340;&#23398;&#20064;&#26448;&#26009;&#24207;&#21015;&#65292;&#26377;&#25928;&#30340;&#35838;&#31243;&#23398;&#20064;&#21487;&#20197;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#23398;&#20064;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35821;&#35328;&#24212;&#29992;&#20013;&#20026;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#36890;&#24120;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#35270;&#35282;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24314;&#31435;&#22312;&#22270;&#22797;&#26434;&#24615;&#24418;&#24335;&#21270;&#65288;&#20316;&#20026;&#22256;&#38590;&#24230;&#26631;&#20934;&#65289;&#21644;&#27169;&#22411;&#33021;&#21147;&#20043;&#19978;&#30340;&#26032;&#26041;&#27861;&#26469;&#36827;&#34892;&#35838;&#31243;&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#35843;&#24230;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#26679;&#26412;&#22256;&#38590;&#24230;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#21516;&#35270;&#35282;&#22312;&#35757;&#32451;&#26399;&#38388;&#25512;&#23548;&#20986;&#26377;&#25928;&#30340;&#35838;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35838;&#31243;&#23398;&#20064;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#33539;&#24335;&#20013;&#32435;&#20837;&#32454;&#31890;&#24230;&#30340;&#22270;&#22256;&#38590;&#24230;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. E
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#23450;&#20041;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#21644;&#35774;&#35745;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#20197;&#21450;&#21435;&#22122;&#32593;&#32476;&#65292;&#33021;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;</title><link>http://arxiv.org/abs/2307.08849</link><description>&lt;p&gt;
&#22270;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Diffusion Model for Graph Generation. (arXiv:2307.08849v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22270;&#29983;&#25104;&#65292;&#36890;&#36807;&#23450;&#20041;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#21644;&#35774;&#35745;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#20197;&#21450;&#21435;&#22122;&#32593;&#32476;&#65292;&#33021;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#22823;&#22810;&#26159;&#19968;&#27425;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20204;&#22312;&#21435;&#37327;&#21270;&#30340;&#37051;&#25509;&#30697;&#38453;&#31354;&#38388;&#20013;&#24212;&#29992;&#39640;&#26031;&#25193;&#25955;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#33021;&#22312;&#27169;&#22411;&#35757;&#32451;&#22256;&#38590;&#12289;&#37319;&#26679;&#36895;&#24230;&#24930;&#21644;&#26080;&#27861;&#38598;&#25104;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#22270;&#31354;&#38388;&#20013;&#30452;&#25509;&#36827;&#34892;&#30340;&#33410;&#28857;&#21560;&#25910;&#25193;&#25955;&#36807;&#31243;&#12290;&#23545;&#20110;&#21069;&#21521;&#25193;&#25955;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#25193;&#25955;&#25490;&#24207;&#32593;&#32476;&#8221;&#30340;&#32593;&#32476;&#65292;&#23427;&#20174;&#22270;&#30340;&#25299;&#25169;&#20013;&#23398;&#20064;&#21040;&#20102;&#25968;&#25454;&#30456;&#20851;&#30340;&#33410;&#28857;&#21560;&#25910;&#39034;&#24207;&#12290;&#23545;&#20110;&#36870;&#21521;&#29983;&#25104;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#21435;&#22122;&#32593;&#32476;&#8221;&#30340;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#36870;&#21521;&#33410;&#28857;&#25490;&#24207;&#20197;&#21450;&#20043;&#21069;&#21435;&#22122;&#30340;&#33410;&#28857;&#26469;&#39640;&#25928;&#22320;&#37325;&#26500;&#22270;&#65292;&#36890;&#36807;&#39044;&#27979;&#26032;&#33410;&#28857;&#30340;&#31867;&#22411;&#21644;&#36793;&#19982;&#20043;&#21069;&#21435;&#22122;&#33410;&#28857;&#30340;&#26102;&#38388;&#12290;&#22522;&#20110;&#32622;&#25442;&#30340;&#22270;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39640;&#36136;&#37327;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based graph generative models have recently obtained promising results for graph generation. However, existing diffusion-based graph generative models are mostly one-shot generative models that apply Gaussian diffusion in the dequantized adjacency matrix space. Such a strategy can suffer from difficulty in model training, slow sampling speed, and incapability of incorporating constraints. We propose an \emph{autoregressive diffusion} model for graph generation. Unlike existing methods, we define a node-absorbing diffusion process that operates directly in the discrete graph space. For forward diffusion, we design a \emph{diffusion ordering network}, which learns a data-dependent node absorbing ordering from graph topology. For reverse generation, we design a \emph{denoising network} that uses the reverse node ordering to efficiently reconstruct the graph by predicting the node type of the new node and its edges with previously denoised nodes at a time. Based on the permutatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08816</link><description>&lt;p&gt;
&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#35797;&#22270;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#20248;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#38656;&#35201;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#26223;&#25968;&#37327;&#20197;&#21450;&#29616;&#23454;&#35268;&#21010;&#38382;&#39064;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#32463;&#20856;&#24418;&#24335;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#36341;&#32773;&#20204;&#36716;&#21521;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#20998;&#35299;&#26041;&#27861;&#26159;Benders&#20998;&#35299;&#65288;BD&#65289;&#65292;&#23427;&#26681;&#25454;&#24773;&#26223;&#29420;&#31435;&#24615;&#23545;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;BD&#30340;&#26041;&#27861;&#65292;&#35813;&#20195;&#29702;&#27169;&#22411;&#21462;&#20195;&#20102;NP&#38590;&#30340;&#25972;&#25968;&#20027;&#38382;&#39064;&#12290;&#36890;&#36807;&#21152;&#36895;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#21152;&#36895;&#30340;BD&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;30%&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20316;&#20026;&#26367;&#20195;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#38543;&#26426;&#24211;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#36890;&#36807;&#23545;&#27604;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08810</link><description>&lt;p&gt;
AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Operator Guidance Informed by AI-Augmented Simulations. (arXiv:2307.08810v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#22686;&#24378;&#27169;&#25311;&#25351;&#23548;&#30340;&#25805;&#20316;&#21592;&#25351;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;LSTM&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#36890;&#36807;&#23545;&#27604;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#22810;&#20445;&#30495;&#24230;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;(LSTM)&#26469;&#20272;&#35745;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#19979;&#33337;&#33334;&#21709;&#24212;&#32479;&#35745;&#37327;&#12290;&#30740;&#31350;&#23558;&#37319;&#29992;&#24555;&#36895;&#20302;&#20445;&#30495;&#24230;&#30340;&#22522;&#20110;&#20307;&#31215;&#30340;&#24037;&#20855;SimpleCode&#21644;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#24037;&#20855;Large Amplitude Motion Program (LAMP)&#12290;&#35757;&#32451;&#25968;&#25454;&#26159;&#36890;&#36807;&#24120;&#35265;&#30340;&#21452;&#23792;&#21452;&#21521;&#28023;&#20917;&#22312;&#21271;&#22823;&#35199;&#27915;&#29983;&#25104;&#30340;SimpleCode&#21644;LAMP&#25968;&#25454;&#12290;&#22312;&#29992;LAMP&#33337;&#33334;&#36816;&#21160;&#21709;&#24212;&#25968;&#25454;&#35757;&#32451;LSTM&#32593;&#32476;&#21518;&#65292;&#26679;&#26412;&#36335;&#32447;&#34987;&#31359;&#36807;&#65292;&#38543;&#26426;&#36873;&#21462;&#21382;&#21490;&#22825;&#27668;&#36755;&#20837;SimpleCode&#21644;LSTM&#32593;&#32476;&#65292;&#24182;&#19982;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will present a multi-fidelity, data-adaptive approach with a Long Short-Term Memory (LSTM) neural network to estimate ship response statistics in bimodal, bidirectional seas. The study will employ a fast low-fidelity, volume-based tool SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). SimpleCode and LAMP data were generated by common bi-modal, bi-directional sea conditions in the North Atlantic as training data. After training an LSTM network with LAMP ship motion response data, a sample route was traversed and randomly sampled historical weather was input into SimpleCode and the LSTM network, and compared against the higher fidelity results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FedLabel&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#25968;&#25454;&#30340;&#19987;&#19994;&#24615;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.08809</link><description>&lt;p&gt;
&#26412;&#22320;&#25110;&#20840;&#23616;&#65306;&#22522;&#20110;&#26377;&#38480;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels. (arXiv:2307.08809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FedLabel&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#25968;&#25454;&#30340;&#19987;&#19994;&#24615;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20266;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#26631;&#35760;&#36807;&#31243;&#30340;&#26114;&#36149;&#21644;&#36153;&#21147;&#65292;&#23458;&#25143;&#31471;&#21482;&#26377;&#26377;&#38480;&#30340;&#26631;&#31614;&#12290;&#23458;&#25143;&#31471;&#26377;&#38480;&#30340;&#26631;&#35760;&#26412;&#22320;&#25968;&#25454;&#24120;&#24120;&#23548;&#33268;&#23427;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#23545;&#20854;&#26356;&#22823;&#30340;&#26080;&#26631;&#31614;&#26412;&#22320;&#25968;&#25454;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20363;&#22914;&#19982;&#26080;&#26631;&#31614;&#25968;&#25454;&#23384;&#22312;&#31867;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#36873;&#25321;&#20174;&#36328;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#21463;&#30410;&#65292;&#20197;&#21033;&#29992;&#20182;&#20204;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#36825;&#20063;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedLabel&#65292;&#23458;&#25143;&#31471;&#26681;&#25454;&#21738;&#20010;&#27169;&#22411;&#23545;&#25968;&#25454;&#26356;&#20855;&#19987;&#19994;&#30693;&#35782;&#36873;&#25321;&#26412;&#22320;&#25110;&#20840;&#23616;&#27169;&#22411;&#26469;&#20266;&#26631;&#35760;&#20854;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20840;&#23616;-&#26412;&#22320;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#21033;&#29992;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#24403;&#23427;&#20204;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#30340;&#20266;&#26631;&#31614;&#26102;&#65292;&#26368;&#23567;&#21270;&#20004;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many existing FL methods assume clients with fully-labeled data, while in realistic settings, clients have limited labels due to the expensive and laborious process of labeling. Limited labeled local data of the clients often leads to their local model having poor generalization abilities to their larger unlabeled local data, such as having class-distribution mismatch with the unlabeled data. As a result, clients may instead look to benefit from the global model trained across clients to leverage their unlabeled data, but this also becomes difficult due to data heterogeneity across clients. In our work, we propose FedLabel where clients selectively choose the local or global model to pseudo-label their unlabeled data depending on which is more of an expert of the data. We further utilize both the local and global models' knowledge via global-local consistency regularization which minimizes the divergence between the two models' outputs when they have identical pseudo-labels for the unl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#20182;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.08794</link><description>&lt;p&gt;
&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Policy Learning for Multi-Timescale Multi-Agent Reinforcement Learning. (arXiv:2307.08794v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#20182;&#20204;&#21033;&#29992;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#20449;&#24687;&#23450;&#20041;&#20102;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#23545;&#20110;&#21463;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#36215;&#30340;&#26102;&#38388;&#20381;&#36182;&#34892;&#20026;&#65292;&#31574;&#30053;&#26159;&#38750;&#24179;&#31283;&#30340;&#12290;&#23398;&#20064;&#38750;&#24179;&#31283;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24448;&#24448;&#38656;&#35201;&#22797;&#26434;&#25110;&#20302;&#25928;&#30340;&#31639;&#27861;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#31995;&#32479;&#20013;&#25511;&#21046;&#38382;&#39064;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20851;&#20110;&#26234;&#33021;&#20307;&#26102;&#38388;&#23610;&#24230;&#30340;&#21487;&#29992;&#20449;&#24687;&#26469;&#23450;&#20041;&#21608;&#26399;&#24615;&#26102;&#38388;&#32534;&#30721;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36890;&#36807;&#21608;&#26399;&#24615;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#21487;&#20197;&#23398;&#20064;&#22810;&#26102;&#38388;&#23610;&#24230;&#24341;&#20837;&#30340;&#38750;&#24179;&#31283;&#24615;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#30456;&#20301;&#20989;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#65292;&#20026;&#21608;&#26399;&#24615;&#25552;&#20379;&#24402;&#32435;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-timescale multi-agent reinforcement learning (MARL), agents interact across different timescales. In general, policies for time-dependent behaviors, such as those induced by multiple timescales, are non-stationary. Learning non-stationary policies is challenging and typically requires sophisticated or inefficient algorithms. Motivated by the prevalence of this control problem in real-world complex systems, we introduce a simple framework for learning non-stationary policies for multi-timescale MARL. Our approach uses available information about agent timescales to define a periodic time encoding. In detail, we theoretically demonstrate that the effects of non-stationarity introduced by multiple timescales can be learned by a periodic multi-agent policy. To learn such policies, we propose a policy gradient algorithm that parameterizes the actor and critic with phase-functioned neural networks, which provide an inductive bias for periodicity. The framework's ability to effective
&lt;/p&gt;</description></item><item><title>GEAR&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19981;&#20381;&#36182;&#20219;&#21153;&#31034;&#33539;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08775</link><description>&lt;p&gt;
GEAR: &#19982;&#36890;&#29992;&#21270;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEAR: Augmenting Language Models with Generalizable and Efficient Tool Resolution. (arXiv:2307.08775v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08775
&lt;/p&gt;
&lt;p&gt;
GEAR&#26159;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#19981;&#20381;&#36182;&#20219;&#21153;&#31034;&#33539;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#22806;&#37096;&#24037;&#20855;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#25552;&#39640;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#36807;&#20110;&#20381;&#36182;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#20351;&#29992;&#31034;&#33539;&#65292;&#38480;&#21046;&#20102;&#20854;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#30001;&#20110;&#23545;&#22823;&#35268;&#27169;LLM&#36827;&#34892;&#22810;&#27425;&#35843;&#29992;&#32780;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GEAR&#65292;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26597;&#35810;-&#24037;&#20855;&#23545;&#24212;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#20381;&#36182;&#29305;&#23450;&#20219;&#21153;&#31034;&#33539;&#30340;&#21508;&#31181;&#38656;&#35201;&#20351;&#29992;&#24037;&#20855;&#30340;&#20219;&#21153;&#12290;GEAR&#36890;&#36807;&#23558;&#24037;&#20855;&#23545;&#24212;&#21644;&#25191;&#34892;&#20998;&#21035;&#22996;&#25176;&#32473;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#21644;LLM&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#25928;&#29575;&#65307;&#21516;&#26102;&#21033;&#29992;&#35821;&#20041;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#35780;&#20272;&#22312;&#38382;&#39064;&#21644;&#31572;&#26696;&#32423;&#21035;&#19978;&#36827;&#34892;&#36890;&#29992;&#21270;&#30340;&#24037;&#20855;&#23545;&#24212;&#12290;&#25105;&#20204;&#22312;6&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;14&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GEAR&#65292;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#26032;&#20219;&#21153;&#12289;&#26032;&#24037;&#20855;&#21644;&#19981;&#21516;SLM&#30340;&#24378;&#22823;&#36890;&#29992;&#24615;&#12290;&#23613;&#31649;&#25552;&#20379;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#20294;GEAR&#22312;&#24037;&#20855;&#23545;&#24212;&#20013;&#30340;&#31934;&#30830;&#24615;&#27604;&#20351;&#29992;LLM&#30340;&#20808;&#21069;&#31574;&#30053;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting large language models (LLM) to use external tools enhances their performance across a variety of tasks. However, prior works over-rely on task-specific demonstration of tool use that limits their generalizability and computational cost due to making many calls to large-scale LLMs. We introduce GEAR, a computationally efficient query-tool grounding algorithm that is generalizable to various tasks that require tool use while not relying on task-specific demonstrations. GEAR achieves better efficiency by delegating tool grounding and execution to small language models (SLM) and LLM, respectively; while leveraging semantic and pattern-based evaluation at both question and answer levels for generalizable tool grounding. We evaluate GEAR on 14 datasets across 6 downstream tasks, demonstrating its strong generalizability to novel tasks, tools and different SLMs. Despite offering more efficiency, GEAR achieves higher precision in tool grounding compared to prior strategies using LLM
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#35752;&#20250;&#24635;&#32467;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#22312;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35268;&#21010;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#20445;&#25252;&#24178;&#39044;&#20013;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#21628;&#21505;&#21512;&#20316;&#21162;&#21147;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#20445;&#25252;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.08774</link><description>&lt;p&gt;
AI&#36741;&#21161;&#20915;&#31574;&#22312;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#30740;&#35752;&#20250;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Reflections from the Workshop on AI-Assisted Decision Making for Conservation. (arXiv:2307.08774v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08774
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#35752;&#20250;&#24635;&#32467;&#20102;AI&#36741;&#21161;&#20915;&#31574;&#22312;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35268;&#21010;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#20445;&#25252;&#24178;&#39044;&#20013;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#21628;&#21505;&#21512;&#20316;&#21162;&#21147;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#20445;&#25252;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#30333;&#30382;&#20070;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22312;2022&#24180;10&#26376;20-21&#26085;&#21704;&#20315;&#22823;&#23398;&#35745;&#31639;&#19982;&#31038;&#20250;&#30740;&#31350;&#20013;&#24515;&#20030;&#21150;&#30340;AI&#36741;&#21161;&#20915;&#31574;&#22312;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;&#30740;&#35752;&#20250;&#30340;&#28436;&#35762;&#21644;&#35752;&#35770;&#20013;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#25105;&#20204;&#35782;&#21035;&#20986;&#36164;&#28304;&#20998;&#37197;&#12289;&#35268;&#21010;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#20445;&#25252;&#24178;&#39044;&#20013;&#30340;&#20851;&#38190;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#19981;&#20165;&#38656;&#35201;AI&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#38656;&#35201;&#26032;&#26041;&#27861;&#23398;&#36827;&#23637;&#30340;&#20445;&#25252;&#25361;&#25112;&#12290;&#38500;&#20102;&#25552;&#20379;&#30740;&#35752;&#20250;&#28436;&#35762;&#21644;&#35752;&#35770;&#30340;&#25688;&#35201;&#22806;&#65292;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#25991;&#20214;&#33021;&#22815;&#21628;&#21505;&#29983;&#24577;&#23398;&#23478;&#12289;&#20445;&#25252;&#20915;&#31574;&#32773;&#21644;AI&#30740;&#31350;&#20154;&#21592;&#30340;&#21512;&#20316;&#21162;&#21147;&#65292;&#20351;&#31639;&#27861;&#20915;&#31574;&#26041;&#27861;&#30340;&#25193;&#23637;&#33021;&#22815;&#20248;&#20808;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#20445;&#25252;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this white paper, we synthesize key points made during presentations and discussions from the AI-Assisted Decision Making for Conservation workshop, hosted by the Center for Research on Computation and Society at Harvard University on October 20-21, 2022. We identify key open research questions in resource allocation, planning, and interventions for biodiversity conservation, highlighting conservation challenges that not only require AI solutions, but also require novel methodological advances. In addition to providing a summary of the workshop talks and discussions, we hope this document serves as a call-to-action to orient the expansion of algorithmic decision-making approaches to prioritize real-world conservation challenges, through collaborative efforts of ecologists, conservation decision-makers, and AI researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.08767</link><description>&lt;p&gt;
&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#30340;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A mixed policy to improve performance of language models on math problems. (arXiv:2307.08767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;&#25277;&#35937;&#23618;&#21644;&#31532;&#20108;&#23618;&#37319;&#29992;&#19981;&#21516;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#21462;&#24471;&#20102;&#36229;&#36807;2%&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26102;&#65292;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37319;&#29992;&#37319;&#26679;&#31574;&#30053;&#26681;&#25454;&#26465;&#20214;&#27010;&#29575;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#12290;&#22312;&#25968;&#23398;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#29983;&#25104;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#32771;&#34385;&#21040;&#25968;&#23398;&#38382;&#39064;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31574;&#30053;&#30340;&#25506;&#32034;&#26041;&#27861;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#32423;&#26631;&#35760;&#25506;&#32034;&#31574;&#30053;&#65306;&#25277;&#35937;&#23618;&#20197;&#27010;&#29575;&#37319;&#26679;&#26469;&#20915;&#23450;&#19979;&#19968;&#20010;&#26631;&#35760;&#26159;&#36816;&#31639;&#31526;&#36824;&#26159;&#25805;&#20316;&#25968;&#65292;&#32780;&#31532;&#20108;&#23618;&#21017;&#20197;&#36138;&#23146;&#26041;&#24335;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36229;&#36807;2&#65285;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#20195;&#30721;&#21487;&#22312;https://github.com/vividitytech/math_lm_rl&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When to solve math problems, most language models take a sampling strategy to predict next word according conditional probabilities. In the math reasoning step, it may generate wrong answer. Considering math problems are deterministic, we propose a mixed policy exploration approach to solve math problems with reinforcement learning. In peculiar, we propose a two level token exploration policy: the abstract level explores next token with probability and the second level is deterministic. Specifically, the abstract level policy will decide whether the token is operator or operand with probability sampling, while the second level is deterministic to select next token with the highest score in a greedy way. We test our method on GSM8K dataset with GPT-2 model, and demonstrate more than $2\%$ performance gain. Our implementation is available at https://github.com/vividitytech/math_lm_rl.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24178;&#25200;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08766</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#24515;&#34880;&#31649;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20809;&#30005;&#23481;&#25239;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality Assessment of Photoplethysmography Signals For Cardiovascular Biomarkers Monitoring Using Wearable Devices. (arXiv:2307.08766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#20449;&#21495;&#36827;&#34892;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24178;&#25200;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;&#65288;PPG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#65292;&#29992;&#20110;&#27979;&#37327;&#24494;&#34880;&#31649;&#32452;&#32455;&#20013;&#30340;&#34880;&#23481;&#37327;&#21464;&#21270;&#12290;&#23427;&#24120;&#34987;&#29992;&#20110;&#21307;&#30103;&#35774;&#22791;&#65292;&#22914;&#33033;&#25615;&#34880;&#27687;&#20202;&#21644;&#25163;&#33109;&#24335;&#24515;&#29575;&#30417;&#27979;&#22120;&#65292;&#29992;&#20110;&#30417;&#27979;&#24515;&#34880;&#31649;&#34880;&#28082;&#21160;&#21147;&#23398;&#12290;PPG&#21487;&#20197;&#35780;&#20272;&#24515;&#29575;&#12289;&#33033;&#25615;&#27874;&#24418;&#21644;&#22806;&#21608;&#28748;&#27880;&#31561;&#21442;&#25968;&#65292;&#20197;&#25351;&#31034;&#34880;&#31649;&#25910;&#32553;&#25110;&#25193;&#24352;&#31561;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#24494;&#34880;&#31649;&#34880;&#27969;&#30340;&#20449;&#24687;&#65292;&#26159;&#30417;&#27979;&#24515;&#34880;&#31649;&#20581;&#24247;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;PPG&#21463;&#21040;&#22810;&#31181;&#21464;&#21270;&#28304;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36830;&#32493;&#30417;&#27979;&#26102;&#65292;&#22914;&#36816;&#21160;&#20266;&#24433;&#12289;&#30382;&#32932;&#33394;&#32032;&#21644;&#34880;&#31649;&#36816;&#21160;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;PPG&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;27&#20010;&#32479;&#35745;&#29305;&#24449;&#65292;&#29992;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#21644;CatBoost&#65289;&#21644;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to asse
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25991;&#31456;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#22122;&#22768;&#24178;&#25200;&#65292;&#20026;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;</title><link>http://arxiv.org/abs/2307.08721</link><description>&lt;p&gt;
&#19978;&#21608;&#24635;&#32479;&#21435;&#20102;&#21738;&#37324;&#65311;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;
&lt;/p&gt;
&lt;p&gt;
Where Did the President Visit Last Week? Detecting Celebrity Trips from News Articles. (arXiv:2307.08721v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08721
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#21517;&#20154;&#34892;&#31243;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#25991;&#31456;&#38388;&#30340;&#24322;&#36136;&#24615;&#21644;&#22122;&#22768;&#24178;&#25200;&#65292;&#20026;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21517;&#20154;&#30340;&#34892;&#36394;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#20363;&#22914;&#65292;&#25919;&#27835;&#23478;&#21435;&#21738;&#37324;&#65292;&#20182;&#20204;&#22810;&#20037;&#35775;&#38382;&#19968;&#27425;&#65292;&#20197;&#21450;&#20182;&#20204;&#20250;&#35265;&#35841;&#65292;&#37117;&#24102;&#26377;&#28145;&#36828;&#30340;&#22320;&#32536;&#25919;&#27835;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#34429;&#28982;&#26032;&#38395;&#25991;&#31456;&#21253;&#21547;&#20102;&#21517;&#20154;&#30340;&#26053;&#34892;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#33258;&#21160;&#34892;&#31243;&#26816;&#27979;&#24037;&#20855;&#65292;&#26080;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#21644;&#32593;&#32476;&#20998;&#26512;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#24517;&#39035;&#20811;&#26381;&#26032;&#38395;&#25991;&#31456;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#65306;1)&#19968;&#20010;&#21333;&#29420;&#30340;&#25991;&#31456;&#21487;&#33021;&#22122;&#38899;&#24456;&#22823;&#65292;&#28041;&#21450;&#26080;&#20851;&#30340;&#20154;&#29289;&#21644;&#22320;&#28857;&#65292;&#29305;&#21035;&#26159;&#24403;&#25991;&#31456;&#24456;&#38271;&#26102;&#12290;2)&#34429;&#28982;&#32771;&#34385;&#22810;&#31687;&#25991;&#31456;&#19968;&#36215;&#26469;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#30340;&#34892;&#31243;&#21487;&#33021;&#20250;&#26377;&#24110;&#21161;&#65292;&#20294;&#20851;&#38190;&#30340;&#35821;&#20041;&#20173;&#28982;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#31456;&#20013;&#65292;&#19982;&#21508;&#31181;&#22122;&#22768;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20351;&#20854;&#38590;&#20197;&#26377;&#25928;&#22320;&#27719;&#24635;&#12290;3)&#36229;&#36807;20%&#30340;&#25991;&#31456;&#38388;&#25509;&#25552;&#21450;&#20102;&#21517;&#20154;&#30340;&#34892;&#31243;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20351;&#29992;&#20934;&#30830;&#30340;&#21517;&#20154;&#22995;&#21517;&#25110;&#22320;&#28857;&#21517;&#31216;&#65292;&#23548;&#33268;&#20102;&#22823;&#37096;&#20998;&#34892;&#31243;&#20449;&#24687;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Celebrities' whereabouts are of pervasive importance. For instance, where politicians go, how often they visit, and who they meet, come with profound geopolitical and economic implications. Although news articles contain travel information of celebrities, it is not possible to perform large-scale and network-wise analysis due to the lack of automatic itinerary detection tools. To design such tools, we have to overcome difficulties from the heterogeneity among news articles: 1)One single article can be noisy, with irrelevant people and locations, especially when the articles are long. 2)Though it may be helpful if we consider multiple articles together to determine a particular trip, the key semantics are still scattered across different articles intertwined with various noises, making it hard to aggregate them effectively. 3)Over 20% of the articles refer to the celebrities' trips indirectly, instead of using the exact celebrity names or location names, leading to large portions of tri
&lt;/p&gt;</description></item><item><title>TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08674</link><description>&lt;p&gt;
TableGPT&#65306;&#23558;&#34920;&#26684;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#32479;&#19968;&#21040;&#19968;&#20010;GPT&#20013;
&lt;/p&gt;
&lt;p&gt;
TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08674
&lt;/p&gt;
&lt;p&gt;
TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24211;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#38656;&#35201;&#20154;&#20204;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#36827;&#34892;&#20998;&#26512;&#21644;&#25805;&#20316;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#19982;&#34920;&#26684;&#20132;&#20114;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#24471;&#36825;&#31181;&#33021;&#21147;&#26356;&#21152;&#25509;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TableGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#31934;&#35843;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#29702;&#35299;&#21644;&#25805;&#20316;&#34920;&#26684;&#12290;&#23427;&#24341;&#20837;&#20102;&#19982;&#34920;&#26684;&#26080;&#32541;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25968;&#25454;&#25805;&#20316;&#65288;&#20363;&#22914;&#25554;&#20837;&#12289;&#21024;&#38500;&#12289;&#26597;&#35810;&#21644;&#20462;&#25913;&#25805;&#20316;&#65289;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#20998;&#26512;&#25253;&#21578;&#29983;&#25104;&#21644;&#33258;&#21160;&#39044;&#27979;&#12290;TableGPT&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;TableGPT&#30340;&#26680;&#24515;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#26032;&#27010;&#24565;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#24182;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#25805;&#20316;&#23545;&#34920;&#26684;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.08671</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#34920;&#31034;&#30340;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Deep Cross-Modal Steganography Using Neural Representations. (arXiv:2307.08671v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#26159;&#23558;&#31192;&#23494;&#25968;&#25454;&#23884;&#20837;&#21040;&#21478;&#19968;&#26465;&#28040;&#24687;&#25110;&#25968;&#25454;&#20013;&#65292;&#20197;&#19981;&#23481;&#26131;&#34987;&#23519;&#35273;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#22312;&#38544;&#20889;&#26415;&#20013;&#24320;&#22987;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#38544;&#20889;&#26415;&#25216;&#26415;&#22312;&#33539;&#22260;&#19978;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#19988;&#23545;&#20110;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#19981;&#22815;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#36827;&#34892;&#28145;&#24230;&#36328;&#27169;&#24577;&#38544;&#20889;&#26415;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35206;&#30422;&#22270;&#20687;&#20013;&#38544;&#34255;&#21508;&#31181;&#26684;&#24335;&#30340;&#31192;&#23494;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;INRs&#26469;&#34920;&#31034;&#31192;&#23494;&#25968;&#25454;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#24418;&#24335;&#21644;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#31192;&#23494;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography is the process of embedding secret data into another message or data, in such a way that it is not easily noticeable. With the advancement of deep learning, Deep Neural Networks (DNNs) have recently been utilized in steganography. However, existing deep steganography techniques are limited in scope, as they focus on specific data types and are not effective for cross-modal steganography. Therefore, We propose a deep cross-modal steganography framework using Implicit Neural Representations (INRs) to hide secret data of various formats in cover images. The proposed framework employs INRs to represent the secret data, which can handle data of various modalities and resolutions. Experiments on various secret datasets of diverse types demonstrate that the proposed approach is expandable and capable of accommodating different modalities.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2307.08533</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#22788;&#29702;&#19982;&#32447;&#24615;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Processing with Linear Optics. (arXiv:2307.08533v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#20302;&#20809;&#21151;&#29575;&#21516;&#26102;&#21512;&#25104;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#23454;&#29616;&#33021;&#37327;&#39640;&#25928;&#21644;&#39640;&#36895;&#30340;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#22810;&#23618;&#25968;&#25454;&#22788;&#29702;&#26469;&#25552;&#21462;&#38544;&#34255;&#30340;&#34920;&#24449;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#31361;&#30772;&#65292;&#20294;&#21364;&#20197;&#22823;&#30005;&#23376;&#35745;&#31639;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#20026;&#20102;&#25552;&#39640;&#33021;&#37327;&#25928;&#29575;&#21644;&#36895;&#24230;&#65292;&#20809;&#23398;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20809;&#23398;&#24102;&#23485;&#30340;&#20248;&#21183;&#21644;&#20809;&#23398;&#20114;&#36830;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#22312;&#32570;&#20047;&#20302;&#21151;&#29575;&#20809;&#23398;&#38750;&#32447;&#24615;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23454;&#29616;&#22810;&#23618;&#20809;&#23398;&#32593;&#32476;&#20013;&#30340;&#25361;&#25112;&#22312;&#20110;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#65292;&#32780;&#19981;&#20381;&#36182;&#30005;&#23376;&#20803;&#20214;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#27425;&#25955;&#23556;&#21487;&#20197;&#21516;&#26102;&#20197;&#20302;&#20809;&#21151;&#29575;&#21512;&#25104;&#21487;&#32534;&#31243;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36716;&#25442;&#65292;&#21033;&#29992;&#25955;&#23556;&#21183;&#33021;&#65288;&#30001;&#25968;&#25454;&#34920;&#31034;&#65289;&#19982;&#25955;&#23556;&#22330;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22810;&#27425;&#25955;&#23556;&#36827;&#34892;&#25968;&#25454;&#37325;&#22797;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#20809;&#23398;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable breakthroughs by leveraging multiple layers of data processing to extract hidden representations, albeit at the cost of large electronic computing power. To enhance energy efficiency and speed, the optical implementation of neural networks aims to harness the advantages of optical bandwidth and the energy efficiency of optical interconnections. In the absence of low-power optical nonlinearities, the challenge in the implementation of multilayer optical networks lies in realizing multiple optical layers without resorting to electronic components. In this study, we present a novel framework that uses multiple scattering that is capable of synthesizing programmable linear and nonlinear transformations concurrently at low optical power by leveraging the nonlinear relationship between the scattering potential, represented by data, and the scattered field. Theoretical and experimental investigations show that repeating the data by multiple scatte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#23548;&#20986;&#22270;&#21644;&#26080;&#22238;&#36335;&#23548;&#20986;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#36138;&#23146;&#26377;&#30028;&#23485;&#24230;&#38598;&#21512;&#21644;&#26080;&#22238;&#36335;&#23548;&#20986;&#22270;&#38598;&#21512;&#30456;&#31561;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#23384;&#22312;&#35268;&#21017;&#30340;&#20998;&#26512;&#35777;&#26126;&#35770;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.08481</link><description>&lt;p&gt;
&#22522;&#20110;&#23548;&#20986;&#22270;&#30340;&#21487;&#21028;&#23450;&#23384;&#22312;&#35268;&#21017;&#38598;&#30340;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Derivation-Graph-Based Characterizations of Decidable Existential Rule Sets. (arXiv:2307.08481v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#23548;&#20986;&#22270;&#21644;&#26080;&#22238;&#36335;&#23548;&#20986;&#22270;&#30340;&#27010;&#24565;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#36138;&#23146;&#26377;&#30028;&#23485;&#24230;&#38598;&#21512;&#21644;&#26080;&#22238;&#36335;&#23548;&#20986;&#22270;&#38598;&#21512;&#30456;&#31561;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#23384;&#22312;&#35268;&#21017;&#30340;&#20998;&#26512;&#35777;&#26126;&#35770;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#31181;&#38750;&#24120;&#34920;&#36798;&#24615;&#30340;&#23384;&#22312;&#35268;&#21017;&#38598;&#30340;&#26367;&#20195;&#29305;&#24449;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#35268;&#21017;&#38598;&#20855;&#26377;&#21487;&#21028;&#23450;&#30340;&#26597;&#35810;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#26174;&#33879;&#30340;&#36138;&#23146;&#26377;&#30028;&#23485;&#24230;&#38598;&#21512;(gbts)&#21450;&#20854;&#19968;&#31181;&#26032;&#30340;&#24191;&#20041;&#21464;&#31181;&#65292;&#31216;&#20026;&#24369;gbts(wgbts)&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#21644;&#26500;&#24314;&#23548;&#20986;&#22270;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#65288;&#24369;&#65289;&#26080;&#22238;&#36335;&#23548;&#20986;&#22270;&#38598;&#21512;&#65288;(w)cdgs&#65289;&#65292;&#24182;&#36816;&#29992;&#35814;&#32454;&#30340;&#35777;&#26126;&#35770;&#35777;&#26041;&#27861;&#24471;&#20986;&#32467;&#35770;&#65306;gbts&#21644;cdgs&#30456;&#31561;&#65292;wgbts&#21644;wcdgs&#20063;&#30456;&#31561;&#12290;&#36825;&#20123;&#26032;&#39062;&#30340;&#29305;&#24449;&#21270;&#26041;&#27861;&#25512;&#21160;&#20102;&#23545;&#23384;&#22312;&#35268;&#21017;&#30340;&#20998;&#26512;&#35777;&#26126;&#35770;&#30340;&#29702;&#35299;&#65292;&#24182;&#26377;&#26395;&#22312;&#23454;&#36341;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper establishes alternative characterizations of very expressive classes of existential rule sets with decidable query entailment. We consider the notable class of greedy bounded-treewidth sets (gbts) and a new, generalized variant, called weakly gbts (wgbts). Revisiting and building on the notion of derivation graphs, we define (weakly) cycle-free derivation graph sets ((w)cdgs) and employ elaborate proof-theoretic arguments to obtain that gbts and cdgs coincide, as do wgbts and wcdgs. These novel characterizations advance our analytic proof-theoretic understanding of existential rules and will likely be instrumental in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#20165;&#40657;&#30418;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.08424</link><description>&lt;p&gt;
&#26080;&#27861;&#38459;&#27490;&#30340;&#25915;&#20987;: &#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26631;&#31614;&#20165;&#27169;&#22411;&#36870;&#25512;
&lt;/p&gt;
&lt;p&gt;
Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion Model. (arXiv:2307.08424v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#20165;&#40657;&#30418;&#22330;&#26223;&#19979;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;(MIAs)&#26088;&#22312;&#20174;&#30446;&#26631;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#24674;&#22797;&#31169;&#23494;&#25968;&#25454;&#65292;&#36825;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#26500;&#25104;&#23041;&#32961;&#12290;MIAs&#20027;&#35201;&#20851;&#27880;&#30333;&#30418;&#24773;&#26223;&#65292;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23436;&#20840;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#26159;&#40657;&#30418;&#24773;&#26223;&#65292;&#23545;&#25163;&#24456;&#38590;&#33719;&#21462;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#35768;&#22810;&#27169;&#22411;&#20165;&#36755;&#20986;&#39044;&#27979;&#26631;&#31614;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;MIAs&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#20248;&#21270;&#31574;&#30053;&#19978;&#65292;&#32780;&#29983;&#25104;&#27169;&#22411;&#21482;&#26159;&#20174;&#30333;&#30418;MIA&#20013;&#20351;&#29992;&#30340;GAN&#36801;&#31227;&#32780;&#26469;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#26631;&#31614;&#20165;&#40657;&#30418;&#24773;&#26223;&#19979;&#21487;&#34892;&#25915;&#20987;&#27169;&#22411;&#30340;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#30446;&#26631;&#27169;&#22411;&#36755;&#20986;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20248;&#21270;&#26469;&#24674;&#22797;&#30446;&#26631;&#30340;&#31934;&#30830;&#26679;&#26412;&#12290;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Model inversion attacks (MIAs) are aimed at recovering private data from a target model's training set, which poses a threat to the privacy of deep learning models. MIAs primarily focus on the white-box scenario where the attacker has full access to the structure and parameters of the target model. However, practical applications are black-box, it is not easy for adversaries to obtain model-related parameters, and various models only output predicted labels. Existing black-box MIAs primarily focused on designing the optimization strategy, and the generative model is only migrated from the GAN used in white-box MIA. Our research is the pioneering study of feasible attack models in label-only black-box scenarios, to the best of our knowledge.  In this paper, we develop a novel method of MIA using the conditional diffusion model to recover the precise sample of the target without any extra optimization, as long as the target model outputs the label. Two primary techniques are introduced t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bateson&#30340;&#21551;&#21457;&#65292;&#20174;&#22797;&#26434;&#30340;&#31354;&#38388;&#24863;&#30693;&#25968;&#25454;&#20013;&#29983;&#25104;&#23618;&#27425;&#27010;&#24565;&#32467;&#26500;&#30340;&#31526;&#21495;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08087</link><description>&lt;p&gt;
&#20174;&#31354;&#38388;&#24863;&#30693;&#25968;&#25454;&#20013;&#29983;&#25104;&#35821;&#20041;&#24418;&#24335;&#27010;&#24565;&#30340;&#36882;&#24402;Bateson&#21551;&#21457;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Recursive Bateson-Inspired Model for the Generation of Semantic Formal Concepts from Spatial Sensory Data. (arXiv:2307.08087v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bateson&#30340;&#21551;&#21457;&#65292;&#20174;&#22797;&#26434;&#30340;&#31354;&#38388;&#24863;&#30693;&#25968;&#25454;&#20013;&#29983;&#25104;&#23618;&#27425;&#27010;&#24565;&#32467;&#26500;&#30340;&#31526;&#21495;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#20102;&#36830;&#25509;&#20027;&#20041;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#31532;&#19968;&#27169;&#22359;&#26469;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#31526;&#21495;&#24341;&#25806;&#22788;&#29702;&#20026;&#31526;&#21495;&#65292;&#25552;&#20379;&#25512;&#29702;&#12289;&#27010;&#24565;&#32467;&#26500;&#12289;&#32452;&#21512;&#24615;&#12289;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36229;&#20986;&#20998;&#24067;&#23398;&#20064;&#31561;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31070;&#32463;&#26041;&#27861;&#22312;&#24863;&#30693;&#25968;&#25454;&#20013;&#23545;&#31526;&#21495;&#30340;&#22522;&#30784;&#24615;&#24037;&#20316;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#32321;&#29712;&#30340;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#20174;&#22797;&#26434;&#30340;&#31354;&#38388;&#24863;&#30693;&#25968;&#25454;&#20013;&#29983;&#25104;&#23618;&#27425;&#27010;&#24565;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Bateson&#20851;&#20110;&#24046;&#24322;&#20316;&#20026;&#29983;&#25104;&#24605;&#24819;&#25110;&#27010;&#24565;&#30340;&#20851;&#38190;&#30340;&#27010;&#24565;&#12290;&#25353;&#29031;&#20182;&#30340;&#24314;&#35758;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#22810;&#21464;&#37327;&#25968;&#23383;&#24207;&#21015;&#20013;&#35745;&#31639;&#20803;&#32032;&#39034;&#24207;&#27604;&#36739;&#26469;&#25552;&#21462;&#21407;&#23376;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic approaches to machine learning incorporate the advantages from both connectionist and symbolic methods. Typically, these models employ a first module based on a neural architecture to extract features from complex data. Then, these features are processed as symbols by a symbolic engine that provides reasoning, concept structures, composability, better generalization and out-of-distribution learning among other possibilities. However, neural approaches to the grounding of symbols in sensory data, albeit powerful, still require heavy training and tedious labeling for the most part. This paper presents a new symbolic-only method for the generation of hierarchical concept structures from complex spatial sensory data. The approach is based on Bateson's notion of difference as the key to the genesis of an idea or a concept. Following his suggestion, the model extracts atomic features from raw data by computing elemental sequential comparisons in a stream of multivariate numer
&lt;/p&gt;</description></item><item><title>DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.07909</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#30340;&#21482;&#26159;&#27169;&#20223;&#21527;&#65311;&#20855;&#26377;&#21452;&#38454;&#27573;&#35757;&#32451;&#30340;&#27867;&#21270;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Is Imitation All You Need? Generalized Decision-Making with Dual-Phase Training. (arXiv:2307.07909v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07909
&lt;/p&gt;
&lt;p&gt;
DualMind&#20351;&#29992;&#21452;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#25511;&#21046;&#20219;&#21153;&#20013;&#23398;&#20064;&#20849;&#21516;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27169;&#20223;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;DualMind&#22312;MetaWorld&#21644;Habitat&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#20855;&#26377;&#36229;&#36807;50%&#21644;70%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DualMind&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#24615;&#20195;&#29702;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#36807;&#24230;&#25311;&#21512;&#34892;&#20026;&#21644;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;DualMind&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21452;&#38454;&#27573;&#8221;&#35757;&#32451;&#31574;&#30053;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#22312;&#19990;&#30028;&#20013;&#34892;&#21160;&#30340;&#26041;&#24335;&#12290;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#38024;&#23545;&#25511;&#21046;&#20219;&#21153;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#26469;&#23398;&#20064;&#22522;&#26412;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#22522;&#20110;&#32473;&#23450;&#25552;&#31034;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20570;&#20986;&#20915;&#31574;&#12290; DualMind&#21487;&#20197;&#22788;&#29702;&#36328;&#22495;&#12289;&#22330;&#26223;&#21644;&#20855;&#20307;&#38382;&#39064;&#65292;&#24182;&#20165;&#20351;&#29992;&#21333;&#32452;&#27169;&#22411;&#26435;&#37325;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#25552;&#31034;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#32454;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;MetaWorld&#21644;Habitat&#19978;&#35780;&#20272;&#20102;DualMind&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;Habitat&#21644;MetaWorld&#19978;&#30340;&#34920;&#29616;&#20998;&#21035;&#36229;&#36807;&#20102;&#20854;&#20182;&#36890;&#29992;&#24615;&#20195;&#29702;&#30340;50%&#21644;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DualMind, a generalist agent designed to tackle various decision-making tasks that addresses challenges posed by current methods, such as overfitting behaviors and dependence on task-specific fine-tuning. DualMind uses a novel "Dual-phase" training strategy that emulates how humans learn to act in the world. The model first learns fundamental common knowledge through a self-supervised objective tailored for control tasks and then learns how to make decisions based on different contexts through imitating behaviors conditioned on given prompts. DualMind can handle tasks across domains, scenes, and embodiments using just a single set of model weights and can execute zero-shot prompting without requiring task-specific fine-tuning. We evaluate DualMind on MetaWorld and Habitat through extensive experiments and demonstrate its superior generalizability compared to previous techniques, outperforming other generalist agents by over 50$\%$ and 70$\%$ on Habitat and MetaWorld, respe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#12290;&#36890;&#36807;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26381;&#35013;&#19978;&#30340;&#32467;&#26500;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.07754</link><description>&lt;p&gt;
&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer. (arXiv:2307.07754v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07754
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#12290;&#36890;&#36807;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#26381;&#35013;&#19978;&#30340;&#32467;&#26500;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#39057;&#30340;&#20154;&#20307;&#23039;&#24577;&#36716;&#31227;&#26159;&#19968;&#20010;&#23558;&#26222;&#36890;&#28304;&#20154;&#20307;&#22270;&#20687;&#26681;&#25454;&#19968;&#31995;&#21015;&#30446;&#26631;&#20154;&#29289;&#23039;&#24577;&#36827;&#34892;&#21160;&#30011;&#21270;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#37492;&#20110;&#22312;&#26381;&#35013;&#30340;&#39640;&#24230;&#32467;&#26500;&#24615;&#22270;&#26696;&#21644;&#19981;&#36830;&#32493;&#30340;&#23039;&#21183;&#36716;&#31227;&#19978;&#23384;&#22312;&#30340;&#22256;&#38590;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#19981;&#29702;&#24819;&#30340;&#32467;&#26524;&#65292;&#22914;&#25197;&#26354;&#30340;&#32441;&#29702;&#21644;&#38378;&#28865;&#30340;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21464;&#24418;&#36816;&#21160;&#35843;&#21046;&#65288;DMM&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20960;&#20309;&#26680;&#20559;&#31227;&#21644;&#33258;&#36866;&#24212;&#26435;&#37325;&#35843;&#21046;&#26469;&#21516;&#26102;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;&#19982;&#22312;&#39118;&#26684;&#36716;&#31227;&#20013;&#20351;&#29992;&#30340;&#26222;&#36890;&#39118;&#26684;&#35843;&#21046;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#35843;&#21046;&#26426;&#21046;&#36890;&#36807;&#19968;&#31181;&#38750;&#35268;&#21017;&#24863;&#21463;&#37326;&#26681;&#25454;&#23545;&#35937;&#24418;&#29366;&#33258;&#36866;&#24212;&#37325;&#26500;&#24179;&#28369;&#24103;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#36716;&#31227;&#12290;&#20026;&#22686;&#24378;&#26102;&#31354;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21452;&#21521;&#20256;&#25773;&#20174;&#30001;&#22122;&#22768;&#23039;&#21183;&#29983;&#25104;&#30340;&#30072;&#21464;&#22270;&#20687;&#24207;&#21015;&#20013;&#25552;&#21462;&#38544;&#34255;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The prop
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07250</link><description>&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#30340;&#22240;&#26524;&#21442;&#25968;&#20272;&#35745;&#26469;&#32531;&#35299;&#25932;&#23545;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#37327;&#21270;&#21644;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#25932;&#23545;&#36755;&#20837;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#34893;&#29983;&#20986;&#30340;&#25932;&#23545;&#20363;&#23376;&#21487;&#20197;&#36731;&#26494;&#22320;&#25439;&#23475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#38450;&#27490;&#28508;&#22312;&#30340;&#23041;&#32961;&#65292;&#21508;&#31181;&#22522;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#38450;&#24481;&#26041;&#27861;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#25104;&#20026;&#31283;&#20581;&#24615;&#30340;&#20107;&#23454;&#19978;&#26631;&#20934;&#26041;&#27861;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25104;&#23601;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25932;&#23545;&#33030;&#24369;&#24615;&#22312;&#19981;&#21516;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#26576;&#20123;&#33030;&#24369;&#24615;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#28145;&#23618;&#27425;&#30340;&#26550;&#26500;&#21644;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36825;&#31181;&#22855;&#29305;&#30340;&#29616;&#35937;&#20173;&#28982;&#26080;&#27861;&#32531;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#25932;&#23545;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;ADML&#65289;&#30340;&#22240;&#26524;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#37327;&#21270;&#32593;&#32476;&#39044;&#27979;&#30340;&#25932;&#23545;&#33030;&#24369;&#24615;&#31243;&#24230;&#65292;&#24182;&#25429;&#25417;&#23545;&#32467;&#26524;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;ADML&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#25932;&#23545;&#25200;&#21160;&#26412;&#36523;&#30340;&#22240;&#26524;&#21442;&#25968;&#65292;&#24182;&#20943;&#36731;&#21487;&#33021;&#25439;&#23475;&#31283;&#20581;&#24615;&#30340;&#36127;&#38754;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04964</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494; &#31532;&#19968;&#37096;&#20998;&#65306;PPO
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;RLHF&#30340;&#31192;&#23494;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#22870;&#21169;&#27169;&#22411;&#12289;PPO&#21644;&#36827;&#31243;&#30417;&#30563;&#31561;&#25216;&#26415;&#36335;&#24452;&#65292;&#25506;&#32034;&#22914;&#20309;&#35299;&#20915;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25512;&#21160;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#34013;&#22270;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25104;&#20026;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#65288;&#26377;&#30410;&#12289;&#35802;&#23454;&#21644;&#26080;&#23475;&#65289;&#21161;&#25163;&#12290;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#25104;&#20026;&#25903;&#25745;&#36825;&#19968;&#36861;&#27714;&#30340;&#20851;&#38190;&#25216;&#26415;&#33539;&#24335;&#12290;&#24403;&#21069;&#30340;&#25216;&#26415;&#36335;&#32447;&#36890;&#24120;&#21253;&#25324;&#29992;&#20110;&#34913;&#37327;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12289;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#27169;&#22411;&#36755;&#20986;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20197;&#21450;&#29992;&#20110;&#25913;&#21892;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#36827;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22870;&#21169;&#35774;&#35745;&#12289;&#29615;&#22659;&#20132;&#20114;&#21644;&#20195;&#29702;&#35757;&#32451;&#30340;&#25361;&#25112;&#65292;&#20877;&#21152;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35797;&#39564;&#25104;&#26412;&#24040;&#22823;&#65292;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#28608;&#21169;&#25216;&#26415;&#23545;&#40784;&#21644;LLMs&#30340;&#23433;&#20840;&#30528;&#38470;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#12290;RLHF&#30340;&#31283;&#23450;&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbf{reward models} to measure human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize policy model outputs, and \textbf{process supervision} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#26469;&#28040;&#38500;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.04550</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gradient Surgery for One-shot Unlearning on Generative Model. (arXiv:2307.04550v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#23398;&#20064;&#26799;&#24230;&#25163;&#26415;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#26799;&#24230;&#26469;&#28040;&#38500;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#8220;&#36951;&#24536;&#26435;&#8221;&#30340;&#30417;&#31649;&#24341;&#21457;&#20102;&#23545;&#21462;&#28040;&#39044;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#36817;&#26399;&#30340;&#26426;&#22120;&#23398;&#20064;&#21462;&#28040;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#26435;&#37325;&#26469;&#28040;&#38500;&#26679;&#26412;&#23545;&#26435;&#37325;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#20197;&#36924;&#36817;&#19968;&#31181;&#30452;&#25509;&#20294;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#28040;&#38500;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#21463;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#25805;&#20316;&#26469;&#35843;&#25972;&#26679;&#26412;&#20043;&#38388;&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26799;&#24230;&#25237;&#24433;&#21040;&#20445;&#30041;&#26799;&#24230;&#30340;&#27861;&#24179;&#38754;&#19978;&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#31227;&#38500;&#26679;&#26412;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#39318;&#27425;&#22312;&#21462;&#28040;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02839</link><description>&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35843;&#20248;&#22686;&#24378;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26159;&#24773;&#25253;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#22411;&#26412;&#36523;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#25991;&#26412;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#20934;&#30830;&#29983;&#25104;&#21487;&#38752;&#20449;&#24687;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#20174;&#26032;&#38395;&#27573;&#33853;&#20013;&#25552;&#21462;&#22810;&#20010;&#32467;&#26500;&#21270;&#20107;&#20214;&#27169;&#24335;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#24212;&#30340;&#20107;&#20214;&#27169;&#24335;&#36755;&#20837;LLM&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#22120;(NSG)&#26469;&#36873;&#25321;&#21644;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#36807;&#21435;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00497</link><description>&lt;p&gt;
&#19981;&#35201;&#32972;&#35829;&#65292;&#27169;&#20223;&#36807;&#21435;&#65306;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory. (arXiv:2307.00497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#20351;&#29992;&#35760;&#24518;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#36807;&#21435;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#32531;&#35299;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#23481;&#26131;&#24536;&#35760;&#36807;&#21435;&#23398;&#20064;&#30340;&#20449;&#24687;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#20998;&#25955;&#30340;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#20250;&#29420;&#31435;&#22320;&#36827;&#34892;&#26356;&#25913;&#12290;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20027;&#35201;&#22312;&#20013;&#24515;&#21270;&#30340;&#29615;&#22659;&#20013;&#30740;&#31350;&#36825;&#31181;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#30452;&#25509;&#35775;&#38382;&#23436;&#25972;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#23558;CL&#25216;&#26415;&#24212;&#29992;&#20110;FL&#24182;&#19981;&#30452;&#25509;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#38544;&#31169;&#38382;&#39064;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20174;&#36807;&#21435;&#30340;&#20998;&#24067;&#20013;&#21512;&#25104;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#23384;&#20648;&#37096;&#20998;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#22312;&#26412;&#22320;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#32467;&#26463;&#26102;&#20351;&#29992;&#26080;&#25968;&#25454;&#26041;&#27861;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#35831;&#27714;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are prone to forgetting information learned in the past when trained on new data. This problem becomes even more pronounced in the context of federated learning (FL), where data is decentralized and subject to independent changes for each user. Continual Learning (CL) studies this so-called \textit{catastrophic forgetting} phenomenon primarily in centralized settings, where the learner has direct access to the complete training dataset. However, applying CL techniques to FL is not straightforward due to privacy concerns and resource limitations. This paper presents a framework for federated class incremental learning that utilizes a generative model to synthesize samples from past distributions instead of storing part of past data. Then, clients can leverage the generative model to mitigate catastrophic forgetting locally. The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Therefore, i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;</title><link>http://arxiv.org/abs/2306.15788</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#35780;&#20272;GPT-3.5&#21644;GPT-4
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese. (arXiv:2306.15788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#34429;&#28982;GPT-4&#30340;&#21484;&#22238;&#29575;&#36739;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#24230;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;GPT-3.5&#21644;GPT-4&#36825;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;Microsoft Word&#21644;Google Docs&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;GEC&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#31867;&#21035;&#65306;&#35821;&#27861;&#12289;&#25340;&#20889;&#12289;&#20114;&#32852;&#32593;&#21644;&#24555;&#36895;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4&#30340;&#21484;&#22238;&#29575;&#27604;&#20854;&#20182;&#26041;&#27861;&#39640;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#20855;&#26377;&#36739;&#20302;&#30340;&#31934;&#30830;&#24230;&#65292;&#23548;&#33268;&#36807;&#24230;&#20462;&#27491;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#23454;&#38469;GEC&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#21644;&#20854;&#20182;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the effectiveness of GPT-3.5 and GPT-4, two large language models, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese and compare their performance against Microsoft Word and Google Docs. We introduce a GEC dataset for Brazilian Portuguese with four categories: Grammar, Spelling, Internet, and Fast typing. Our results show that while GPT-4 has higher recall than other methods, LLMs tend to have lower precision, leading to overcorrection. This study demonstrates the potential of LLMs as practical GEC tools for Brazilian Portuguese and encourages further exploration of LLMs for non-English languages and other educational settings.
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10224</link><description>&lt;p&gt;
&#33192;&#32960;&#30340;&#25259;&#38706;&#65306;ChatGPT&#26159;&#21542;&#33021;&#24110;&#21161;&#25237;&#36164;&#32773;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?. (arXiv:2306.10224v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10224
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#24037;&#20855;&#65288;&#22914; ChatGPT&#65289;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#25237;&#36164;&#32773;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#32929;&#31080;&#24066;&#22330;&#20316;&#20026;&#23454;&#39564;&#23460;&#65292;&#25506;&#31350;&#36825;&#20123;&#24037;&#20855;&#22312;&#24635;&#32467;&#22797;&#26434;&#30340;&#20844;&#21496;&#25259;&#38706;&#20449;&#24687;&#26102;&#30340;&#32463;&#27982;&#25928;&#29992;&#12290;&#24635;&#32467;&#25688;&#35201;&#26126;&#26174;&#26356;&#30701;&#65292;&#36890;&#24120;&#27604;&#21407;&#22987;&#25991;&#26412;&#32553;&#30701;&#36229;&#36807; 70%&#65292;&#32780;&#20449;&#24687;&#20869;&#23481;&#24471;&#21040;&#22686;&#24378;&#12290;&#24403;&#19968;&#20221;&#25991;&#20214;&#20855;&#26377;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#24773;&#24863;&#26102;&#65292;&#20854;&#24635;&#32467;&#21464;&#24471;&#26356;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24635;&#32467;&#23545;&#35299;&#37322;&#32929;&#24066;&#23545;&#25259;&#38706;&#20449;&#24687;&#30340;&#21453;&#24212;&#26356;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#8220;&#33192;&#32960;&#8221;&#25351;&#26631;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#33192;&#32960;&#30340;&#25259;&#38706;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#20363;&#22914;&#26356;&#20302;&#30340;&#20215;&#26684;&#26377;&#25928;&#24615;&#21644;&#26356;&#39640;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#30830;&#23450;&#20844;&#21496;&#30340;&#65288;&#38750;&#65289;&#36130;&#21153;&#34920;&#29616;&#21644;&#39118;&#38505;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#29983;&#25104;&#24335; AI &#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#25237;&#36164;&#32773;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are dramatically shorter, often by more than 70% compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). More importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information "bloat." We show that bloated disclosure is associated with adverse capital markets consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance and risks. Collectively, our results indi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09662</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Objective Reinforcement Learning for Traffic Signal Control and Carbon Emission Reduction. (arXiv:2306.09662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;MOMA-DDPG&#65292;&#29992;&#20110;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#21644;&#30899;&#20943;&#25490;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20381;&#36182;&#20110;&#36807;&#20110;&#31616;&#21270;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20063;&#32463;&#24120;&#26159;&#27425;&#20248;&#30340;&#21644;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#30340;&#22810;&#30446;&#26631;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MOMA-DDPG&#65289;&#65292;&#20351;&#29992;&#34928;&#20943;&#26435;&#37325;&#26469;&#20272;&#35745;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20248;&#21270;&#30340;&#22810;&#20010;&#22870;&#21169;&#39033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20004;&#31181;&#31867;&#22411;&#30340;&#26234;&#33021;&#20307;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20248;&#21270;&#27599;&#20010;&#36335;&#21475;&#30340;&#26412;&#22320;&#20132;&#36890;&#65292;&#32780;&#21478;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#20840;&#23616;&#20132;&#36890;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#19968;&#20010;&#20122;&#27954;&#22269;&#23478;&#30340;&#20132;&#36890;&#25668;&#20687;&#22836;&#25910;&#38598;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#21253;&#21547;&#20102;&#19968;&#20010;&#20840;&#23616;&#26234;&#33021;&#20307;&#65292;&#20294;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#26159;&#20998;&#25955;&#30340;&#65292;&#22240;&#20026;&#36825;&#20010;&#26234;&#33021;&#20307;&#22312;&#25512;&#29702;&#38454;&#27573;&#19981;&#20877;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;MOMA-DDPG&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#26368;&#23567;&#21270;&#20102;&#31561;&#24453;&#26102;&#38388;&#21644;&#30899;&#25490;&#25918;&#37327;&#20004;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing traffic signal control systems rely on oversimplified rule-based methods, and even RL-based methods are often suboptimal and unstable. To address this, we propose a cooperative multi-objective architecture called Multi-Objective Multi-Agent Deep Deterministic Policy Gradient (MOMA-DDPG), which estimates multiple reward terms for traffic signal control optimization using age-decaying weights. Our approach involves two types of agents: one focuses on optimizing local traffic at each intersection, while the other aims to optimize global traffic throughput. We evaluate our method using real-world traffic data collected from an Asian country's traffic cameras. Despite the inclusion of a global agent, our solution remains decentralized as this agent is no longer necessary during the inference stage. Our results demonstrate the effectiveness of MOMA-DDPG, outperforming state-of-the-art methods across all performance metrics. Additionally, our proposed system minimizes both waiting ti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05439</link><description>&lt;p&gt;
CLC: &#22522;&#20110;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#32858;&#31867;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLC: Cluster Assignment via Contrastive Representation Learning. (arXiv:2306.05439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#26679;&#26412;&#20998;&#32452;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#24471;&#21040;&#30340;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21253;&#21547;&#22823;&#37327;&#32858;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20173;&#28982;&#26080;&#27861;&#23454;&#29616;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#32858;&#31867;&#26041;&#27861;&#65288;CLC&#65289;&#65292;&#23427;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30452;&#25509;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#12290;&#25105;&#20204;&#23558;&#34920;&#31034;&#20998;&#35299;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#23545;&#31867;&#21035;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#37319;&#29992;&#31561;&#20998;&#32422;&#26463;&#65292;&#21478;&#19968;&#37096;&#20998;&#25429;&#25417;&#23454;&#20363;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#25439;&#22833;&#65292;&#20351;&#29992;&#34920;&#31034;&#30340;&#20004;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#24182;&#25581;&#31034;&#20102;CLC&#22312;&#23398;&#20064;&#32858;&#31867;&#20998;&#37197;&#26102;&#20026;&#36127;&#26679;&#26412;&#35774;&#32622;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;&#36827;&#19968;&#27493;&#30340;&#26799;&#24230;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;CLC&#26102;&#65292;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering remains an important and challenging task of grouping samples into clusters without manual annotations. Recent works have achieved excellent results on small datasets by performing clustering on feature representations learned from self-supervised learning. However, for datasets with a large number of clusters, such as ImageNet, current methods still can not achieve high clustering performance. In this paper, we propose Contrastive Learning-based Clustering (CLC), which uses contrastive learning to directly learn cluster assignment. We decompose the representation into two parts: one encodes the categorical information under an equipartition constraint, and the other captures the instance-wise factors. We propose a contrastive loss using both parts of the representation. We theoretically analyze the proposed contrastive loss and reveal that CLC sets different weights for the negative samples while learning cluster assignments. Further gradient analysis shows that the larger 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16044</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23558;&#22122;&#22768;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Exploiting Noise as a Resource for Computation and Learning in Spiking Neural Networks. (arXiv:2305.16044v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#23637;&#31034;&#20102;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#26159;&#22823;&#33041;&#38750;&#20961;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24182;&#24050;&#25104;&#20026;&#31070;&#32463;&#24418;&#24577;&#26234;&#33021;&#30340;&#25903;&#26609;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22122;&#22768;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#65288;NSNN&#65289;&#21644;&#22122;&#22768;&#39537;&#21160;&#23398;&#20064;&#35268;&#21017;&#65288;NDL&#65289;&#65292;&#37319;&#29992;&#24102;&#26377;&#22122;&#22768;&#31070;&#32463;&#20803;&#21160;&#21147;&#23398;&#30340;&#33033;&#20914;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#26174;&#31034;&#22122;&#22768;&#21487;&#20197;&#20316;&#20026;&#35745;&#31639;&#21644;&#23398;&#20064;&#30340;&#36164;&#28304;&#65292;&#24182;&#29702;&#35770;&#19978;&#20026;&#19968;&#33324;&#33033;&#20914;&#31070;&#32463;&#20803;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;NDL&#20026;&#20195;&#29702;&#26799;&#24230;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#12290;&#36890;&#36807;&#23558;&#21508;&#31181;SNN&#26550;&#26500;&#21644;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#30830;&#23450;&#24615;SNNs&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;NSNNs&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#26159;&#26410;&#26469;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#30340;&#28508;&#22312;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Networks of spiking neurons underpin the extraordinary information-processing capabilities of the brain and have emerged as pillar models in neuromorphic intelligence. Despite extensive research on spiking neural networks (SNNs), most are established on deterministic models. Integrating noise into SNNs leads to biophysically more realistic neural dynamics and may benefit model performance. This work presents the noisy spiking neural network (NSNN) and the noise-driven learning rule (NDL) by introducing a spiking neuron model incorporating noisy neuronal dynamics. Our approach shows how noise may act as a resource for computation and learning and theoretically provides a framework for general SNNs. Moreover, NDL provides an insightful biological rationale for surrogate gradients. By incorporating various SNN architectures and algorithms, we show that our approach exhibits competitive performance and improved robustness against challenging perturbations than deterministic SNNs. Additiona
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13115</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30417;&#30563;&#27880;&#24847;&#21147;&#65306;&#26356;&#22909;&#21644;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#24378;&#22823;&#30340;&#20851;&#27880;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal-Based Supervision of Attention in Graph Neural Network: A Better and Simpler Choice towards Powerful Attention. (arXiv:2305.13115v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13115
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#20351;&#24471;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#20013;&#26356;&#21152;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#27491;&#22312;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#35774;&#23450;&#26032;&#30340;&#22522;&#20934;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#30001;&#20110;&#32570;&#20047;&#30452;&#25509;&#30417;&#30563;&#65292;&#23427;&#20204;&#25152;&#20135;&#29983;&#30340;&#20851;&#27880;&#21147;&#23545;&#20110;&#22024;&#26434;&#30340;&#22270;&#34920;&#36798;&#19981;&#22815;&#31283;&#20581;&#21644;&#20855;&#26377;&#19968;&#33324;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24615;&#24037;&#20855;&#20026;&#27880;&#24847;&#21147;&#20989;&#25968;&#30340;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#27880;&#24847;&#21147;&#23545;&#20110;&#26368;&#32456;&#39044;&#27979;&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#35813;&#25928;&#24212;&#65292;&#24341;&#23548;&#27880;&#24847;&#21147;&#20851;&#27880;&#26356;&#26377;&#24847;&#20041;&#30340;&#37051;&#23621;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#32463;&#20856;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#20351;&#29992;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;&#36890;&#36807;&#30452;&#25509;&#30417;&#30563;&#27880;&#24847;&#21147;&#20989;&#25968;&#65292;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#20135;&#29983;&#26356;&#28165;&#26224;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great potential of attention mechanism in graph representation learning. However, while variants of attention-based GNNs are setting new benchmarks for numerous real-world datasets, recent works have pointed out that their induced attentions are less robust and generalizable against noisy graphs due to lack of direct supervision. In this paper, we present a new framework which utilizes the tool of causality to provide a powerful supervision signal for the learning process of attention functions. Specifically, we estimate the direct causal effect of attention to the final prediction, and then maximize such effect to guide attention attending to more meaningful neighbors. Our method can serve as a plug-and-play module for any canonical attention-based GNNs in an end-to-end fashion. Extensive experiments on a wide range of benchmark datasets illustrated that, by directly supervising attention functions, the model is able to converge faster with a clearer de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12421</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#23454;&#24615;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#20154;&#24037;&#35780;&#20272;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;QA&#35780;&#20272;&#65288;QA-Eval&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#65292;&#26088;&#22312;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;Open-QA&#20013;&#30340;&#26631;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37027;&#20123;&#19982;&#20154;&#24037;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#35748;&#20026;&#23427;&#20204;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#20197;&#21450;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#26032;&#30340;QA-Eval&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#23558;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11997</link><description>&lt;p&gt;
&#20855;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#40065;&#26834;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees. (arXiv:2305.11997v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#31070;&#32463;&#32593;&#32476;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20559;&#31227;&#65292;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#21487;&#33021;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#21453;&#20107;&#23454;&#35299;&#37322;&#20248;&#21270;&#20013;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#26469;&#23558;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#38752;&#36817;&#25968;&#25454;&#27969;&#24418;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#21457;&#29983;&#30340;&#27169;&#22411;&#21464;&#21270;&#30340;&#39640;&#27010;&#29575;&#40065;&#26834;&#24615;&#12290;&#26032;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model $m$ and the new model $M$ are bounded in the parameter space, i.e., $\|\text{Params}(M){-}\text{Params}(m)\|{&lt;}\Delta$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \emph{naturally-occurring} model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \emph{Stability} -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counter
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09378</link><description>&lt;p&gt;
&#22312;Lenia&#20013;&#25429;&#33719;&#26032;&#20852;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09378
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#25506;&#35752;&#20102;Lenia&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25968;&#23383;&#29983;&#29289;&#31995;&#32479;&#30340;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;&#12290;Lenia&#30340;&#29983;&#24577;&#31995;&#32479;&#30001;&#31616;&#21333;&#30340;&#20154;&#24037;&#29983;&#29289;&#32452;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#31227;&#21160;&#12289;&#28040;&#32791;&#12289;&#29983;&#38271;&#21644;&#32321;&#27542;&#12290;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#21644;&#36827;&#21270;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#21270;&#29983;&#29289;&#12290;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#22312;Lenia&#20013;&#27979;&#37327;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#27979;&#37327;&#35268;&#21017;&#30340;&#38271;&#26399;&#22797;&#26434;&#24615;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#36827;&#21270;&#20986;&#23578;&#26410;&#21457;&#29616;&#30340;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#37051;&#21306;&#22495;&#25110;&#26680;&#20316;&#20026;&#22522;&#22240;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;Lenia&#30340;&#20854;&#20182;&#21442;&#25968;&#65288;&#20363;&#22914;&#29983;&#38271;&#20989;&#25968;&#65289;&#19981;&#21464;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#20154;&#21475;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#28982;&#21518;&#27979;&#37327;&#36866;&#24212;&#24230;&#20540;&#20197;&#20915;&#23450;&#25152;&#24471;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#20316;&#20026;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07617</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#36923;&#36753;&#25512;&#29702;&#30340;&#21487;&#25193;&#23637;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Scalable Coupling of Deep Learning with Logical Reasoning. (arXiv:2305.07617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;NP-hard&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#31163;&#25955;&#22270;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#21516;&#26102;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#23545;&#39044;&#27979;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#31163;&#25955;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#28151;&#21512;&#30340;&#19981;&#26029;&#25506;&#32034;&#20013;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#31070;&#32463;&#32467;&#26500;&#20855;&#22791;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#31163;&#25955;&#25512;&#29702;&#25110;&#20248;&#21270;&#38382;&#39064;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#32467;&#26500;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#23398;&#20064;&#34987;&#34920;&#31034;&#20026;&#31163;&#25955;&#22270;&#27169;&#22411;&#30340; NP-hard &#25512;&#29702;&#38382;&#39064;&#30340;&#32422;&#26463;&#21644;&#26631;&#20934;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#35299;&#20915;&#20102; Besag &#30340;&#20266;&#23545;&#25968;&#20284;&#28982;&#30340;&#20027;&#35201;&#38480;&#21046;&#20043;&#19968;&#65292;&#33021;&#22815;&#23398;&#20064;&#39640;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#33258;&#28982;&#36755;&#20837;&#20013;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915; NP-hard &#25512;&#29702;&#38382;&#39064;&#65292;&#22914;&#31526;&#21495;&#12289;&#35270;&#35273;&#25110;&#22810;&#35299;&#25968;&#25968;&#29420;&#38382;&#39064;&#65292;&#20197;&#21450;&#34507;&#30333;&#36136;&#35774;&#35745;&#38382;&#39064;&#30340;&#33021;&#37327;&#20248;&#21270;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#39044;&#27979;&#30340; \textit{a posteriori} &#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ongoing quest for hybridizing discrete reasoning with neural nets, there is an increasing interest in neural architectures that can learn how to solve discrete reasoning or optimization problems from natural inputs. In this paper, we introduce a scalable neural architecture and loss function dedicated to learning the constraints and criteria of NP-hard reasoning problems expressed as discrete Graphical Models. Our loss function solves one of the main limitations of Besag's pseudo-loglikelihood, enabling learning of high energies. We empirically show it is able to efficiently learn how to solve NP-hard reasoning problems from natural inputs as the symbolic, visual or many-solutions Sudoku problems as well as the energy optimization formulation of the protein design problem, providing data efficiency, interpretability, and \textit{a posteriori} control over predictions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05560</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Distributional Multi-Objective Decision Making. (arXiv:2305.05560v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21487;&#20197;&#21521;&#20915;&#31574;&#32773;&#21576;&#29616;&#19968;&#32452;&#21487;&#33021;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25506;&#35752;&#36825;&#20123;&#35299;&#24212;&#35813;&#21253;&#21547;&#21738;&#20123;&#31574;&#30053;&#20197;&#21450;&#22914;&#20309;&#39640;&#25928;&#22320;&#35745;&#31639;&#36825;&#20123;&#35299;&#12290;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21183;&#20934;&#21017;&#65292;&#30452;&#25509;&#20851;&#32852;&#31574;&#30053;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20010;&#20934;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#20854;&#20013;&#21253;&#21547;&#20102;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#24573;&#30053;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#23427;&#21253;&#25324;&#25152;&#26377;&#22312;&#22810;&#32500;&#39118;&#38505;&#35268;&#36991;&#20915;&#31574;&#32773;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#25928;&#29992;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#36129;&#29486;&#20102;&#21098;&#26525;&#31639;&#23376;&#26469;&#23558;&#20854;&#20943;&#23569;&#21040;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For effective decision support in scenarios with conflicting objectives, sets of potentially optimal solutions can be presented to the decision maker. We explore both what policies these sets should contain and how such sets can be computed efficiently. With this in mind, we take a distributional approach and introduce a novel dominance criterion relating return distributions of policies directly. Based on this criterion, we present the distributional undominated set and show that it contains optimal policies otherwise ignored by the Pareto front. In addition, we propose the convex distributional undominated set and prove that it comprises all policies that maximise expected utility for multivariate risk-averse decision makers. We propose a novel algorithm to learn the distributional undominated set and further contribute pruning operators to reduce the set to the convex distributional undominated set. Through experiments, we demonstrate the feasibility and effectiveness of these metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03209</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#19987;&#23478;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38544;&#24615;&#35299;&#21078;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts. (arXiv:2304.03209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORSE&#30340;&#22522;&#20110;&#38544;&#24335;&#35299;&#21078;&#28210;&#26579;&#30340;&#36890;&#29992;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24110;&#21161;&#34701;&#21512;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#32423;&#35821;&#20041;&#30456;&#20851;&#20869;&#23481;&#21644;&#20302;&#32423;&#35299;&#21078;&#29305;&#24449;&#38598;&#25104;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#36817;&#26399;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#20998;&#21106;&#26041;&#27861;&#22312;&#26356;&#22909;&#22320;&#24314;&#27169;&#36825;&#20123;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#20998;&#21106;&#30340;&#21367;&#31215;&#25805;&#20316;&#36890;&#24120;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#36816;&#34892;&#65292;&#36825;&#22312;&#39640;&#39057;&#21306;&#22495;&#21363;&#36793;&#30028;&#21306;&#22495;&#20013;&#22825;&#29983;&#27169;&#31946;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MORSE&#30340;&#36890;&#29992;&#38544;&#24335;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#35299;&#21078;&#23618;&#38754;&#19978;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20107;&#23454;&#65306;&#30456;&#36739;&#20110;&#31163;&#25955;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34920;&#31034;&#26041;&#24335;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#22312;&#25311;&#21512;&#22797;&#26434;&#20449;&#21495;&#21644;&#35299;&#20915;&#35745;&#31639;&#26426;&#22270;&#24418;&#38382;&#39064;&#26102;&#34920;&#29616;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#23558;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#35270;&#20026;&#28210;&#26579;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25345;&#32493;&#22320;&#23545;&#40784;&#31895;&#30053;&#30340;&#20998;&#21106;p&#24182;&#21033;&#29992;&#38543;&#26426;&#19987;&#23478;&#26469;&#29983;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.02689</link><description>&lt;p&gt;
ACTION++&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#24230;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast. (arXiv:2304.02689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;ACTION++&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#35299;&#21078;&#23545;&#27604;&#26469;&#25913;&#21892;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20026;&#38271;&#23614;&#20998;&#24067;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#33258;&#28982;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#65288;&#21363;&#36793;&#30028;&#21306;&#22495;&#25110;&#32597;&#35265;&#29289;&#20307;&#65289;&#30340;&#20998;&#31867;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#37197;&#22791;&#26080;&#30417;&#30563;&#23545;&#27604;&#26631;&#20934;&#65292;&#22312;&#38271;&#23614;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#36827;&#20102;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#21035;&#20998;&#24067;&#20063;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#25968;&#25454;&#37096;&#20998;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION++&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35299;&#21078;&#23545;&#27604;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical data often exhibits long-tail distributions with heavy class imbalance, which naturally leads to difficulty in classifying the minority classes (i.e., boundary regions or rare objects). Recent work has significantly improved semi-supervised medical image segmentation in long-tailed scenarios by equipping them with unsupervised contrastive criteria. However, it remains unclear how well they will perform in the labeled portion of data where class distribution is also highly imbalanced. In this work, we present ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Specifically, we propose an adaptive supervised contrastive loss, where we first compute the optimal locations of class centers uniformly distributed on the embedding space (i.e., off-line), and then perform online contrastive matching training by encouraging different class features to adaptively match these distinct and uniformly distributed cla
&lt;/p&gt;</description></item><item><title>SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05118</link><description>&lt;p&gt;
SLCA: &#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05118
&lt;/p&gt;
&lt;p&gt;
SLCA&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24930;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#23545;&#40784;&#26469;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#20915;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#22312;&#23398;&#20064;&#39034;&#24207;&#21040;&#36798;&#30340;&#25968;&#25454;&#20013;&#25552;&#39640;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22823;&#37096;&#20998;&#29616;&#26377;&#24037;&#20316;&#37117;&#24314;&#31435;&#22312;&#20174;&#22836;&#23398;&#20064;&#30340;&#21069;&#25552;&#19979;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#21162;&#21147;&#24050;&#32463;&#33268;&#21147;&#20110;&#34701;&#20837;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#27599;&#20010;&#22686;&#37327;&#20219;&#21153;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#24182;&#23558;&#20851;&#38190;&#25361;&#25112;&#24402;&#22240;&#20110;&#28176;&#36827;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#35266;&#23519;&#21040;&#22312;&#34920;&#24449;&#23618;&#27425;&#19978;&#36873;&#25321;&#24615;&#38477;&#20302;&#23398;&#20064;&#29575;&#20960;&#20046;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#24930;&#23398;&#20064;&#32773;&#19982;&#20998;&#31867;&#22120;&#23545;&#40784;&#65288;SLCA&#65289;&#65292;&#36890;&#36807;&#24314;&#27169;&#31867;&#21035;&#20998;&#24067;&#24182;&#22312;&#20107;&#21518;&#23545;&#40784;&#20998;&#31867;&#23618;&#27425;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20998;&#31867;&#23618;&#27425;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SLCA&#22312;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continual learning on a pre-trained model (CLPM), and attribute the key challenge to a progressive overfitting problem. Observing that selectively reducing the learning rate can almost resolve this issue in the representation layer, we propose a simple but extremely effective approach named Slow Learner with Classifier Alignment (SLCA), which further improves the classification layer by modeling the class-wise distributions and aligning the classification layers in a post-hoc fashion. Across a variety o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02401</link><description>&lt;p&gt;
&#22312;3D&#28857;&#20113;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;3D&#28857;&#20113;&#20013;&#36827;&#34892;&#26080;&#38480;&#25968;&#37327;&#25903;&#25745;&#26816;&#27979;&#30340;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#26469;&#21033;&#29992;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#35774;&#32622;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25745;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25903;&#25745;&#26816;&#27979;&#26041;&#27861;&#23616;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#25903;&#25745;&#26631;&#31614;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#25903;&#25745;&#26816;&#27979;&#65288;OpenAD&#65289;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#28857;&#20113;&#20013;&#26816;&#27979;&#26080;&#38480;&#25968;&#37327;&#30340;&#25903;&#25745;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#25903;&#25745;&#25991;&#26412;&#21644;&#28857;&#29305;&#24449;&#65292;OpenAD&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25903;&#25745;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#38646;-shot&#26816;&#27979;&#65292;&#24182;&#33021;&#22815;&#22312;&#27809;&#26377;&#20219;&#20309;&#27880;&#37322;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25903;&#25745;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OpenAD&#22312;&#21508;&#31181;&#25903;&#25745;&#26816;&#27979;&#35774;&#32622;&#19978;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;OpenAD&#22312;&#23454;&#38469;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordance detection is a challenging problem with a wide variety of robotic applications. Traditional affordance detection methods are limited to a predefined set of affordance labels, hence potentially restricting the adaptability of intelligent robots in complex and dynamic environments. In this paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method, which is capable of detecting an unbounded number of affordances in 3D point clouds. By simultaneously learning the affordance text and the point feature, OpenAD successfully exploits the semantic relationships between affordances. Therefore, our proposed method enables zero-shot detection and can be able to detect previously unseen affordances without a single annotation example. Intensive experimental results show that OpenAD works effectively on a wide range of affordance detection setups and outperforms other baselines by a large margin. Additionally, we demonstrate the practicality of the proposed OpenAD in real
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#27874;&#26031;&#35821;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35789;&#27719;&#20851;&#32852;&#21644;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#29983;&#25104;&#22270;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#25552;&#21462;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.09775</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#21644;&#22270;&#23884;&#20837;&#30340;&#27874;&#26031;&#35821;&#35805;&#39064;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Persian topic detection based on Human Word association and graph embedding. (arXiv:2302.09775v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#27874;&#26031;&#35821;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35789;&#27719;&#20851;&#32852;&#21644;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#29983;&#25104;&#22270;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#25552;&#21462;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#35782;&#21035;&#35752;&#35770;&#30340;&#35805;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#36825;&#20010;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#26159;&#22312;&#33521;&#35821;&#26041;&#38754;&#30340;&#65292;&#20294;&#26159;&#22312;&#27874;&#26031;&#35821;&#26041;&#38754;&#20063;&#20570;&#20102;&#24456;&#22810;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#27874;&#26031;&#35821;&#30340;&#24494;&#21338;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#26356;&#22810;&#22320;&#20851;&#27880;&#25506;&#32034;&#39057;&#32321;&#27169;&#24335;&#25110;&#35821;&#20041;&#20851;&#31995;&#65292;&#24573;&#35270;&#20102;&#35821;&#35328;&#30340;&#32467;&#26500;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#35789;&#27719;&#20851;&#32852;&#30340;&#35805;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#27169;&#20223;&#24515;&#29702;&#33021;&#21147;&#36827;&#34892;&#35789;&#27719;&#20851;&#32852;&#12290;&#35813;&#26041;&#27861;&#36824;&#35745;&#31639;&#20986;&#20102;&#20851;&#32852;&#24341;&#21147;&#21147;&#37327;&#26469;&#26174;&#31034;&#35789;&#35821;&#30340;&#20851;&#32852;&#31243;&#24230;&#12290;&#21033;&#29992;&#36825;&#20010;&#21442;&#25968;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#22270;&#65292;&#36890;&#36807;&#23884;&#20837;&#36825;&#20010;&#22270;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#20986;&#35805;&#39064;&#12290;&#36825;&#20010;&#26041;&#27861;&#24050;&#32463;&#24212;&#29992;&#20110;&#20174;Telegram&#25910;&#38598;&#30340;&#27874;&#26031;&#35821;&#25968;&#25454;&#38598;&#19978;&#12290;&#36827;&#34892;&#20102;&#22810;&#20010;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a framework to detect topics in social media based on Human Word Association. Identifying topics discussed in these media has become a critical and significant challenge. Most of the work done in this area is in English, but much has been done in the Persian language, especially microblogs written in Persian. Also, the existing works focused more on exploring frequent patterns or semantic relationships and ignored the structural methods of language. In this paper, a topic detection framework using HWA, a method for Human Word Association, is proposed. This method uses the concept of imitation of mental ability for word association. This method also calculates the Associative Gravity Force that shows how words are related. Using this parameter, a graph can be generated. The topics can be extracted by embedding this graph and using clustering methods. This approach has been applied to a Persian language dataset collected from Telegram. Several experimental studi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00270</link><description>&lt;p&gt;
&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19968;&#20010;&#19982;&#31574;&#30053;&#30456;&#20851;&#19988;&#19982;&#31574;&#30053;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#12290;&#31574;&#30053;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#23548;&#33268;&#20102;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#26469;&#33258;&#19981;&#25104;&#29087;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#22024;&#26434;&#30340;&#65292;&#38459;&#30861;&#20102;&#31574;&#30053;&#30340;&#23398;&#20064;&#65307;&#21453;&#36807;&#26469;&#65292;&#26410;&#32463;&#20248;&#21270;&#30340;&#31574;&#30053;&#20063;&#20250;&#38459;&#30861;&#21028;&#21035;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23398;&#20064;&#35774;&#32622;&#31216;&#20026;&#8220;&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;IRRL&#65289;&#65292;&#22240;&#20026;&#22870;&#21169;&#19981;&#26159;&#30452;&#25509;&#26469;&#33258;&#29615;&#22659;&#65292;&#32780;&#26159;&#30001;&#21028;&#21035;&#22120;&#8220;&#20869;&#37096;&#8221;&#25552;&#20379;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#22320;&#34920;&#36848;&#20102;IRRL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#23646;&#20110;IRRL&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#24182;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;IRRL&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#25552;&#20986;&#20102;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25345;&#32493;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.13816</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#25191;&#34892;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-based Code Generation using Deep Reinforcement Learning. (arXiv:2301.13816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13816
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;PPOCoder&#26694;&#26550;&#23558;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#21644;Proximal Policy Optimization&#25216;&#26415;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22312;&#22823;&#35268;&#27169;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;PL&#65289;&#27169;&#22411;&#65292;&#20316;&#20026;&#33258;&#21160;&#21270;&#36719;&#20214;&#24037;&#31243;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#22312;&#20195;&#30721;&#23436;&#25104;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#31243;&#24207;&#21512;&#25104;&#31561;&#21508;&#31181;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#25991;&#26412;&#29983;&#25104;&#20013;&#20511;&#29992;&#30340;&#30417;&#30563;&#24494;&#35843;&#30446;&#26631;&#65292;&#24573;&#35270;&#20102;&#20195;&#30721;&#30340;&#29420;&#29305;&#24207;&#21015;&#32423;&#29305;&#24449;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21487;&#32534;&#35793;&#24615;&#20197;&#21450;&#35821;&#27861;&#21644;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPOCoder&#65292;&#19968;&#31181;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;PL&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#30456;&#32467;&#21512;&#65292;PPO&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#21644;&#32467;&#26500;&#23545;&#40784;&#30340;&#38750;&#21487;&#24494;&#21453;&#39304;&#65292;PPOCoder&#23558;&#22806;&#37096;&#20195;&#30721;&#29305;&#23450;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#36825;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13066</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Human Word Association based model for topic detection in social networks. (arXiv:2301.13066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#24182;&#35774;&#35745;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#65292;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#26816;&#27979;&#36825;&#20123;&#32593;&#32476;&#20013;&#35752;&#35770;&#30340;&#20027;&#39064;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#39057;&#32321;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#35821;&#35328;&#32467;&#26500;&#12290;&#35821;&#35328;&#32467;&#26500;&#26041;&#27861;&#30340;&#24847;&#20041;&#22312;&#20110;&#21457;&#29616;&#35789;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20154;&#31867;&#22914;&#20309;&#29702;&#35299;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#21033;&#29992;&#35789;&#27719;&#32852;&#24819;&#30340;&#24515;&#29702;&#33021;&#21147;&#27169;&#25311;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#30340;&#31038;&#20132;&#32593;&#32476;&#20027;&#39064;&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20154;&#31867;&#35789;&#27719;&#32852;&#24819;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#25277;&#21462;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;FA-CUP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#21484;&#22238;&#29575;&#21644;&#20851;&#38190;&#35789;F1&#20540;&#19978;&#26377;&#36739;&#22909;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#20027;&#39064;&#26816;&#27979;&#39046;&#22495;&#20013;&#30340;&#22823;&#22810;&#25968;&#20808;&#21069;&#24037;&#20316;&#20027;&#35201;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#25110;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. A special extraction algorithm has also been designed for this purpose. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.03181</link><description>&lt;p&gt;
&#22522;&#20110;&#28431;&#26007;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#20219;&#21153;&#30340;&#22870;&#21169;&#22609;&#24418;
&lt;/p&gt;
&lt;p&gt;
Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28431;&#26007;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#40065;&#26834;&#28385;&#36275;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#65288;STL&#65289;&#26159;&#25551;&#36848;&#21160;&#24577;&#31995;&#32479;&#22797;&#26434;&#26102;&#24577;&#21644;&#36923;&#36753;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#24378;&#21046;&#25191;&#34892;STL&#35268;&#33539;&#30340;&#25511;&#21046;&#22120;&#65292;&#28982;&#32780;&#65292;&#20182;&#20204;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#30830;&#20445;&#40065;&#26834;&#28385;&#36275;&#21644;&#20445;&#25345;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20511;&#21161;&#28431;&#26007;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;STL&#35268;&#33539;&#30340;&#40065;&#26834;&#28385;&#36275;&#30340;&#26102;&#38388;&#20381;&#36182;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#20102;Cube-and-Conquer&#26041;&#27861;&#23558;MD4&#21644;MD5&#30340;&#27493;&#39588;&#32553;&#20943;&#29256;&#26412;&#36827;&#34892;&#21453;&#36716;&#12290;&#36890;&#36807;&#36880;&#27493;&#20462;&#25913;Dobbertin&#32422;&#26463;&#26469;&#29983;&#25104;MD4&#30340;&#21453;&#36716;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#31435;&#26041;&#38454;&#27573;&#30340;&#31435;&#26041;&#21644;&#24449;&#26381;&#26041;&#27861;&#36827;&#34892;&#21453;&#36716;&#12290;</title><link>http://arxiv.org/abs/2212.02405</link><description>&lt;p&gt;
&#36890;&#36807;&#31435;&#26041;&#21644;&#24449;&#26381;&#27861;&#21453;&#36716;&#23494;&#30721;&#21704;&#24076;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Inverting Cryptographic Hash Functions via Cube-and-Conquer. (arXiv:2212.02405v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#20102;Cube-and-Conquer&#26041;&#27861;&#23558;MD4&#21644;MD5&#30340;&#27493;&#39588;&#32553;&#20943;&#29256;&#26412;&#36827;&#34892;&#21453;&#36716;&#12290;&#36890;&#36807;&#36880;&#27493;&#20462;&#25913;Dobbertin&#32422;&#26463;&#26469;&#29983;&#25104;MD4&#30340;&#21453;&#36716;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#31435;&#26041;&#38454;&#27573;&#30340;&#31435;&#26041;&#21644;&#24449;&#26381;&#26041;&#27861;&#36827;&#34892;&#21453;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MD4&#21644;MD5&#26159;&#22312;1990&#24180;&#20195;&#21021;&#25552;&#20986;&#30340;&#20855;&#26377;&#37324;&#31243;&#30865;&#24847;&#20041;&#30340;&#23494;&#30721;&#21704;&#24076;&#20989;&#25968;&#12290;MD4&#30001;48&#20010;&#27493;&#39588;&#32452;&#25104;&#65292;&#32473;&#23450;&#20219;&#24847;&#26377;&#38480;&#22823;&#23567;&#30340;&#28040;&#24687;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;128&#20301;&#30340;&#21704;&#24076;&#20540;&#12290;MD5&#26159;MD4&#30340;&#26356;&#23433;&#20840;&#30340;64&#27493;&#25193;&#23637;&#12290;&#23613;&#31649;MD4&#21644;MD5&#37117;&#23481;&#26131;&#21463;&#21040;&#30896;&#25758;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#32763;&#36716;&#23427;&#20204;&#65292;&#21363;&#36890;&#36807;&#21704;&#24076;&#20540;&#25214;&#21040;&#21407;&#22987;&#28040;&#24687;&#20173;&#28982;&#19981;&#29616;&#23454;&#12290;&#22312;2007&#24180;&#65292;MD4&#30340;39&#27493;&#29256;&#26412;&#36890;&#36807;&#21270;&#31616;&#20026;SAT&#24182;&#24212;&#29992;CDCL&#27714;&#35299;&#22120;&#20197;&#21450;&#25152;&#35859;&#30340;Dobbertin&#32422;&#26463;&#34987;&#21453;&#36716;&#12290;&#33267;&#20110;MD5&#65292;&#22312;2012&#24180;&#65292;&#23427;&#30340;28&#27493;&#29256;&#26412;&#36890;&#36807;CDCL&#27714;&#35299;&#22120;&#20165;&#38024;&#23545;&#19968;&#20010;&#29305;&#23450;&#30340;&#21704;&#24076;&#20540;&#34987;&#21453;&#36716;&#65292;&#32780;&#19981;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#26412;&#30740;&#31350;&#23558;&#31435;&#26041;&#21644;&#24449;&#26381;&#65288;CDCL&#19982;&#20808;&#34892;&#25628;&#32034;&#30340;&#32452;&#21512;&#65289;&#24212;&#29992;&#20110;&#27493;&#39588;&#32553;&#20943;&#29256;&#26412;&#30340;MD4&#21644;MD5&#30340;&#21453;&#36716;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861;&#36890;&#36807;&#36880;&#27493;&#20462;&#25913;Dobbertin&#32422;&#26463;&#26469;&#29983;&#25104;MD4&#30340;&#21453;&#36716;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#31639;&#27861;&#23581;&#35797;&#20351;&#29992;&#31435;&#26041;&#38454;&#27573;&#30340;&#31435;&#26041;&#21644;&#24449;&#26381;&#26041;&#27861;&#36827;&#34892;&#21453;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;
MD4 and MD5 are seminal cryptographic hash functions proposed in early 1990s. MD4 consists of 48 steps and produces a 128-bit hash given a message of arbitrary finite size. MD5 is a more secure 64-step extension of MD4. Both MD4 and MD5 are vulnerable to practical collision attacks, yet it is still not realistic to invert them, i.e. to find a message given a hash. In 2007, the 39-step version of MD4 was inverted via reducing to SAT and applying a CDCL solver along with the so-called Dobbertin's constraints. As for MD5, in 2012 its 28-step version was inverted via a CDCL solver for one specified hash without adding any additional constraints. In this study, Cube-and-Conquer (a combination of CDCL and lookahead) is applied to invert step-reduced versions of MD4 and MD5. For this purpose, two algorithms are proposed. The first one generates inversion problems for MD4 by gradually modifying the Dobbertin's constraints. The second algorithm tries the cubing phase of Cube-and-Conquer with di
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2210.01426</link><description>&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01426
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#32447;&#35268;&#21010;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#22312;&#32447;&#35268;&#21010;&#65292;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#26469;&#26435;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;MCTS&#22312;&#35768;&#22810;&#31163;&#25955;&#20915;&#31574;&#39046;&#22495;&#65288;&#22914;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#21644;&#23558;&#26827;&#65289;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#32780;&#38024;&#23545;&#36830;&#32493;&#39046;&#22495;&#30340;MCTS&#25193;&#23637;&#20063;&#24050;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#20998;&#25903;&#22240;&#23376;&#21644;&#23548;&#33268;&#25628;&#32034;&#26641;&#22823;&#23567;&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;MCTS&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#23558;&#30456;&#20284;&#29366;&#24577;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#21487;&#20197;&#24471;&#21040;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;CMCGS&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#38543;&#26426;&#21160;&#20316;&#36172;&#21338;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.09920</link><description>&lt;p&gt;
DESCN: &#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#29992;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation. (arXiv:2207.09920v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DESCN&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;&#30340;&#26041;&#24335;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#35282;&#24230;&#24314;&#27169;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#31934;&#20934;&#21307;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;(ITE)&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20998;&#21035;&#22312;&#21508;&#33258;&#30340;&#26679;&#26412;&#31354;&#38388;&#20013;&#24314;&#27169;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#30340;&#21709;&#24212;&#20989;&#25968;&#26469;&#39044;&#27979;ITE&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#36935;&#21040;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#27835;&#30103;&#20559;&#24046;&#32780;&#23548;&#33268;&#30340;&#21463;&#27835;&#30103;&#32452;&#21644;&#23545;&#29031;&#32452;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31163;&#65292;&#20197;&#21450;&#20854;&#20154;&#21475;&#35268;&#27169;&#20043;&#38388;&#30340;&#26174;&#33879;&#26679;&#26412;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#25972;&#20307;&#31354;&#38388;&#20132;&#21449;&#32593;&#32476;(DESCN)&#65292;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#24314;&#27169;&#27835;&#30103;&#25928;&#26524;&#12290;DESCN&#36890;&#36807;&#19968;&#20010;&#36328;&#32593;&#32476;&#20197;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#24335;&#25429;&#25417;&#27835;&#30103;&#20542;&#21521;&#12289;&#21709;&#24212;&#21644;&#38544;&#34255;&#27835;&#30103;&#25928;&#26524;&#30340;&#32508;&#21512;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#20013;&#32852;&#21512;&#23398;&#20064;&#27835;&#30103;&#21644;&#21709;&#24212;&#20989;&#25968;&#65292;&#20197;&#36991;&#20813;&#27835;&#30103;&#20559;&#24046;&#65292;&#24182;&#37319;&#29992;&#20013;&#38388;&#20266;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2206.07082</link><description>&lt;p&gt;
&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#20013;&#38543;&#26426;&#20248;&#21270;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Stability and Generalization of Stochastic Optimization with Nonconvex and Nonsmooth Problems. (arXiv:2206.07082v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#38750;&#24120;&#24191;&#27867;&#65292;&#36825;&#20419;&#20351;&#20102;&#35768;&#22810;&#29702;&#35770;&#30740;&#31350;&#65292;&#20197;&#29702;&#35299;&#20854;&#23454;&#38469;&#25104;&#21151;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20248;&#21270;&#35823;&#24046;&#30340;&#25910;&#25947;&#24615;&#19978;&#65292;&#32780;&#38543;&#26426;&#20248;&#21270;&#30340;&#27867;&#21270;&#20998;&#26512;&#36828;&#36828;&#28382;&#21518;&#12290;&#36825;&#23588;&#20854;&#36866;&#29992;&#20110;&#23454;&#36341;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21551;&#21160;&#20102;&#23545;&#38750;&#20984;&#38750;&#20809;&#28369;&#38382;&#39064;&#19978;&#38543;&#26426;&#20248;&#21270;&#30340;&#31995;&#32479;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#20998;&#26512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#31283;&#23450;&#24615;&#24230;&#37327;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#19982;&#31181;&#32676;&#26799;&#24230;&#21644;&#32463;&#39564;&#26799;&#24230;&#20043;&#38388;&#30340;&#23450;&#37327;&#36830;&#25509;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#23558;&#20854;&#25193;&#23637;&#21040;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#21644;&#32676;&#20307;&#39118;&#38505;&#30340;Moreau&#21253;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20123;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#23450;&#37327;&#32852;&#31995;&#65292;&#26080;&#35770;&#26159;&#22312;&#26799;&#24230;&#36824;&#26159;Moreau&#21253;&#32476;&#26041;&#38754;&#65292;&#37117;&#26159;&#25991;&#29486;&#20013;&#39318;&#27425;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#27867;&#21270;&#29702;&#35770;&#24212;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;(DLNN)&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#24037;&#20316;&#25552;&#20379;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization has found wide applications in minimizing objective functions in machine learning, which motivates a lot of theoretical studies to understand its practical success. Most of existing studies focus on the convergence of optimization errors, while the generalization analysis of stochastic optimization is much lagging behind. This is especially the case for nonconvex and nonsmooth problems often encountered in practice. In this paper, we initialize a systematic stability and generalization analysis of stochastic optimization on nonconvex and nonsmooth problems. We introduce novel algorithmic stability measures and establish their quantitative connection on the gap between population gradients and empirical gradients, which is then further extended to study the gap between the Moreau envelope of the empirical risk and that of the population risk. To our knowledge, these quantitative connection between stability and generalization in terms of either gradients or Morea
&lt;/p&gt;</description></item><item><title>FedFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#22238;&#25253;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2205.13697</link><description>&lt;p&gt;
FedFormer&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19978;&#19979;&#25991;&#32852;&#37030;&#23398;&#20064;&#19982;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13697
&lt;/p&gt;
&lt;p&gt;
FedFormer&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#22238;&#25253;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#35265;&#35299;&#12290;&#36890;&#24120;&#37319;&#29992;&#23558;&#27599;&#20010;&#21442;&#19982;&#26234;&#33021;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#21462;&#24179;&#22343;&#24471;&#21040;&#19968;&#20010;&#20849;&#21516;&#27169;&#22411;&#65288;FedAvg&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FedFormer&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#31574;&#30053;&#65292;&#21033;&#29992;Transformer Attention&#26469;&#19978;&#19979;&#25991;&#22320;&#27719;&#24635;&#26469;&#33258;&#19981;&#21516;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#26681;&#25454;&#24403;&#21069;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#21644;&#23398;&#24471;&#20851;&#31995;&#26377;&#36873;&#25321;&#22320;&#34913;&#37327;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#36129;&#29486;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;Meta-World&#29615;&#22659;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FedAvg&#21644;&#38750;&#32852;&#37030;Soft Actor-Critic&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#19982;Soft Actor-Critic&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;FedFormer&#22312;&#20173;&#36981;&#23432;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#32422;&#26463;&#30340;&#21516;&#26102;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20998;&#38598;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Noisier2Noise&#26694;&#26550;&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;SSDU&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20462;&#25913;&#12290;</title><link>http://arxiv.org/abs/2205.10278</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21464;&#23494;&#24230;Noisier2Noise&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#30340;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A theoretical framework for self-supervised MR image reconstruction using sub-sampling via variable density Noisier2Noise. (arXiv:2205.10278v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Noisier2Noise&#26694;&#26550;&#30340;&#33258;&#30417;&#30563;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;SSDU&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#24314;&#27169;&#33021;&#21147;&#37325;&#24314;&#23376;&#37319;&#26679;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#20195;&#34920;&#24615;&#30340;&#23436;&#20840;&#37319;&#26679;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#23436;&#20840;&#30563;&#23548;&#24335;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#23436;&#20840;&#37319;&#26679;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#32780;&#19988;&#21487;&#33021;&#38750;&#24120;&#38590;&#20197;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#21457;&#23637;&#21644;&#29702;&#35299;&#20165;&#20351;&#29992;&#23376;&#37319;&#26679;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#23558;&#26368;&#21021;&#29992;&#20110;&#33258;&#30417;&#30563;&#21435;&#22122;&#20219;&#21153;&#30340;Noisier2Noise&#26694;&#26550;&#25193;&#23637;&#21040;&#21464;&#23494;&#24230;&#23376;&#37319;&#26679;MRI&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;Noisier2Noise&#26694;&#26550;&#26469;&#35299;&#37322;&#36817;&#26399;&#25552;&#20986;&#30340;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#20294;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064; via &#25968;&#25454;&#27424;&#37319;&#26679;&#65288;SSDU&#65289;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20004;&#31181;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been attention on leveraging the statistical modeling capabilities of neural networks for reconstructing sub-sampled Magnetic Resonance Imaging (MRI) data. Most proposed methods assume the existence of a representative fully-sampled dataset and use fully-supervised training. However, for many applications, fully sampled training data is not available, and may be highly impractical to acquire. The development and understanding of self-supervised methods, which use only sub-sampled data for training, are therefore highly desirable. This work extends the Noisier2Noise framework, which was originally constructed for self-supervised denoising tasks, to variable density sub-sampled MRI data. We use the Noisier2Noise framework to analytically explain the performance of Self-Supervised Learning via Data Undersampling (SSDU), a recently proposed method that performs well in practice but until now lacked theoretical justification. Further, we propose two modifications 
&lt;/p&gt;</description></item></channel></rss>