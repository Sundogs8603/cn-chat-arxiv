<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01335</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#26080;&#20223;&#30495;&#22120;&#35270;&#35273;&#39046;&#22495;&#38543;&#26426;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulator-Free Visual Domain Randomization via Video Games
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BehAVE&#30340;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#26080;&#38656;&#20223;&#30495;&#22120;&#30340;&#25903;&#25345;&#12290;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#22810;&#26679;&#24615;&#36827;&#34892;&#38543;&#26426;&#21270;&#65292;&#20197;&#21450;&#36890;&#36807;&#29609;&#23478;&#34892;&#20026;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25351;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#65292;BehAVE&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#23637;&#29616;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#29992;&#20110;&#25552;&#39640;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#19978;&#25130;&#28982;&#19981;&#21516;&#20294;&#20869;&#23481;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#30340;&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#37327;&#20381;&#36182;&#20110;&#35843;&#25972;&#22797;&#26434;&#21644;&#19987;&#38376;&#30340;&#20223;&#30495;&#24341;&#25806;&#65292;&#36825;&#20123;&#24341;&#25806;&#30340;&#26500;&#24314;&#24456;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#21487;&#34892;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BehAVE&#65292;&#19968;&#31181;&#35270;&#39057;&#29702;&#35299;&#26694;&#26550;&#65292;&#23427;&#29420;&#29305;&#22320;&#21033;&#29992;&#29616;&#26377;&#30340;&#21830;&#19994;&#35270;&#39057;&#28216;&#25103;&#26469;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#23427;&#20204;&#30340;&#20223;&#30495;&#24341;&#25806;&#12290;&#22312;BehAVE&#19979;&#65292;(1) &#35270;&#39057;&#28216;&#25103;&#22266;&#26377;&#30340;&#20016;&#23500;&#35270;&#35273;&#22810;&#26679;&#24615;&#25104;&#20026;&#38543;&#26426;&#21270;&#30340;&#26469;&#28304;&#65292;(2) &#29609;&#23478;&#34892;&#20026; - &#36890;&#36807;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034; - &#24341;&#23548;&#20855;&#26377;&#30456;&#20284;&#20869;&#23481;&#30340;&#35270;&#39057;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35270;&#39057;&#21644;&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#19978;&#27979;&#35797;&#20102;BehAVE&#65292;&#24182;&#25253;&#21578;&#20102;&#23427;&#22312;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain randomization is an effective computer vision technique for improving transferability of vision models across visually distinct domains exhibiting similar content. Existing approaches, however, rely extensively on tweaking complex and specialized simulation engines that are difficult to construct, subsequently affecting their feasibility and scalability. This paper introduces BehAVE, a video understanding framework that uniquely leverages the plethora of existing commercial video games for domain randomization, without requiring access to their simulation engines. Under BehAVE (1) the inherent rich visual diversity of video games acts as the source of randomization and (2) player behavior -- represented semantically via textual descriptions of actions -- guides the *alignment* of videos with similar content. We test BehAVE on 25 games of the first-person shooter (FPS) genre across various video and text foundation models and we report its robustness for domain randomization. Beh
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20216</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#20998;&#24067;&#24335;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Distributed agency in second language learning and teaching through generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20216
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20026;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#22823;&#26426;&#20250;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#20070;&#38754;&#25110;&#21475;&#22836;&#24418;&#24335;&#30340;&#23545;&#35805;&#20026;&#31532;&#20108;&#35821;&#35328;&#25552;&#20379;&#38750;&#27491;&#24335;&#32451;&#20064;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25552;&#31034;&#25351;&#23450;&#23545;&#35805;&#21442;&#25968;&#65292;&#22914;&#29087;&#32451;&#31243;&#24230;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#35752;&#35770;&#20027;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34987;&#25351;&#23548;&#32473;&#20104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#21019;&#24314;&#32451;&#20064;&#39064;&#65292;&#25110;&#21046;&#23450;&#25193;&#23637;&#23398;&#20064;&#35745;&#21010;&#12290;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#21508;&#31181;&#23186;&#20307;&#30340;&#23398;&#20064;&#21644;&#35780;&#20272;&#26448;&#26009;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#20250;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#65292;&#25670;&#33073;&#33050;&#26412;&#21270;&#30340;&#20114;&#21160;&#12290;&#23545;&#20110;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#32780;&#35328;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#23616;&#38480;&#24615;&#26469;&#33258;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#30340;&#32431;&#32479;&#35745;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#35821;&#35328;&#20351;&#29992;&#20013;&#24494;&#22937;&#31038;&#20250;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#26041;&#24335;&#23384;&#22312;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 Announce Type: cross  Abstract: Generative AI offers significant opportunities for language learning. Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are crea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.10144</link><description>&lt;p&gt;
NLP&#39564;&#35777;&#65306;&#36208;&#21521;&#19968;&#31181;&#36890;&#29992;&#30340;&#29992;&#20110;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#35770;
&lt;/p&gt;
&lt;p&gt;
NLP Verification: Towards a General Methodology for Certifying Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#24635;&#32467;&#21644;&#35780;&#20272;&#30001;&#35813;&#39046;&#22495;&#36804;&#20170;&#36827;&#23637;&#32780;&#24418;&#25104;&#30340;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#23545;&#21464;&#21270;&#25110;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#23545;&#20854;&#36755;&#20986;&#32473;&#20986;&#20445;&#35777;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#19981;&#21516;&#65292;NLP&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26041;&#27861;&#35770;&#65292;&#23613;&#31649;&#36817;&#24180;&#26469;&#25991;&#29486;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NLP&#39564;&#35777;&#30340;&#23454;&#29992;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#19981;&#28145;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#25552;&#28860;&#21644;&#35780;&#20272;&#19968;&#20010;NLP&#39564;&#35777;&#27969;&#31243;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#65292;&#35813;&#27969;&#31243;&#26469;&#28304;&#20110;&#36804;&#20170;&#20026;&#27490;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23558;&#21477;&#23376;&#23884;&#20837;&#36830;&#32493;&#31354;&#38388;&#24471;&#21040;&#30340;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#19968;&#33324;&#25551;&#36848;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21487;&#39564;&#35777;&#23376;&#31354;&#38388;&#30340;&#35821;&#20041;&#27867;&#21270;&#25216;&#26415;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10144v1 Announce Type: cross  Abstract: Deep neural networks have exhibited substantial success in the field of Natural Language Processing (NLP) and ensuring their safety and reliability is crucial: there are safety critical contexts where such models must be robust to variability or attack, and give guarantees over their output. Unlike Computer Vision, NLP lacks a unified verification methodology and, despite recent advancements in literature, they are often light on the pragmatical issues of NLP verification. In this paper, we make an attempt to distil and evaluate general components of an NLP verification pipeline, that emerges from the progress in the field to date. Our contributions are two-fold. Firstly, we give a general characterisation of verifiable subspaces that result from embedding sentences into continuous spaces. We identify, and give an effective method to deal with, the technical challenge of semantic generalisability of verified subspaces; and propose it a
&lt;/p&gt;</description></item><item><title>QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.09930</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#36890;&#36807;&#20540;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity Actor-Critic: Learning High-Performing and Diverse Behaviors via Value and Successor Features Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09930
&lt;/p&gt;
&lt;p&gt;
QDAC&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#34920;&#29616;&#20986;&#36866;&#24212;&#24847;&#22806;&#24773;&#20917;&#30340;&#24191;&#27867;&#34892;&#20026;&#35889;&#12290;&#36807;&#21435;&#21313;&#24180;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#27493;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#36820;&#22238;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;QDAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#35780;&#35770;&#23478;&#21644;&#32487;&#25215;&#29305;&#24449;&#35780;&#35770;&#23478;&#23398;&#20064;&#39640;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#34892;&#20026;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#28436;&#21592;&#36890;&#36807;&#21463;&#38480;&#20248;&#21270;&#26469;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#25191;&#34892;&#22810;&#26679;&#24615;&#25216;&#33021;&#30340;&#23458;&#35266;&#20989;&#25968;&#65292;&#26080;&#32541;&#32479;&#19968;&#20102;&#20004;&#20010;&#35780;&#35770;&#23478;&#12290;&#19982;&#20854;&#20182;&#36136;&#37327;&#22810;&#26679;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;QDAC&#22312;&#20845;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#36816;&#21160;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#22810;&#26679;&#24615;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09930v1 Announce Type: cross  Abstract: A key aspect of intelligence is the ability to demonstrate a broad spectrum of behaviors for adapting to unexpected situations. Over the past decade, advancements in deep reinforcement learning have led to groundbreaking achievements to solve complex continuous control tasks. However, most approaches return only one solution specialized for a specific problem. We introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic deep reinforcement learning algorithm that leverages a value function critic and a successor features critic to learn high-performing and diverse behaviors. In this framework, the actor optimizes an objective that seamlessly unifies both critics using constrained optimization to (1) maximize return, while (2) executing diverse skills. Compared with other Quality-Diversity methods, QDAC achieves significantly higher performance and more diverse behaviors on six challenging continuous control locomotion 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17810</link><description>&lt;p&gt;
BioT5+: &#36890;&#36807;IUPAC&#38598;&#25104;&#21644;&#22810;&#20219;&#21153;&#35843;&#25972;&#23454;&#29616;&#24191;&#20041;&#29983;&#29289;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17810
&lt;/p&gt;
&lt;p&gt;
BioT5+&#26159;BioT5&#26694;&#26550;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#25972;&#21512;IUPAC&#21517;&#31216;&#12289;&#21253;&#21547;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#12289;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#21450;&#26032;&#39062;&#30340;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#20998;&#23376;&#34920;&#31034;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#30740;&#31350;&#36235;&#21183;&#36234;&#26469;&#36234;&#38598;&#20013;&#20110;&#25972;&#21512;&#25991;&#26412;&#21644;&#29983;&#29289;&#23454;&#20307;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;BioT5&#30340;&#20808;&#21069;&#24037;&#20316;&#22312;&#36328;&#36234;&#22810;&#26679;&#21270;&#20219;&#21153;&#21644;&#32570;&#20047;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#32454;&#33268;&#29702;&#35299;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30340;&#25991;&#26412;&#34920;&#31034;&#65288;&#20363;&#22914;IUPAC&#65289;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioT5+&#65292;&#36825;&#26159;BioT5&#26694;&#26550;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#26088;&#22312;&#22686;&#24378;&#29983;&#29289;&#30740;&#31350;&#21644;&#33647;&#29289;&#21457;&#29616;&#12290; BioT5+&#21253;&#21547;&#20960;&#20010;&#26032;&#39062;&#30340;&#29305;&#24615;&#65306;&#25972;&#21512;IUPAC&#21517;&#31216;&#20197;&#21152;&#28145;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#65292;&#21253;&#25324;&#26469;&#33258;bioRxiv&#21644;PubChem&#31561;&#28304;&#30340;&#24191;&#27867;&#29983;&#29289;&#25991;&#26412;&#21644;&#20998;&#23376;&#25968;&#25454;&#65292;&#22810;&#20219;&#21153;&#25351;&#20196;&#35843;&#25972;&#20197;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#25968;&#23383;&#25968;&#25454;&#22788;&#29702;&#30340;&#26032;&#39062;&#25968;&#20540;&#26631;&#35760;&#25216;&#26415;&#12290; &#36825;&#20123;&#22686;&#24378;&#21151;&#33021;&#20351;BioT5+&#33021;&#22815;&#24357;&#21512;&#20998;&#23376;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17810v1 Announce Type: cross  Abstract: Recent research trends in computational biology have increasingly focused on integrating text and bio-entity modeling, especially in the context of molecules and proteins. However, previous efforts like BioT5 faced challenges in generalizing across diverse tasks and lacked a nuanced understanding of molecular structures, particularly in their textual representations (e.g., IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework, tailored to enhance biological research and drug discovery. BioT5+ incorporates several novel features: integration of IUPAC names for molecular understanding, inclusion of extensive bio-text and molecule data from sources like bioRxiv and PubChem, the multi-task instruction tuning for generality across tasks, and a novel numerical tokenization technique for improved processing of numerical data. These enhancements allow BioT5+ to bridge the gap between molecular representations and their text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.16714</link><description>&lt;p&gt;
&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26159;Transformer&#26550;&#26500;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Quantum linear algebra is all you need for Transformer architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#19979;Transformer&#26550;&#26500;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#32447;&#24615;&#20195;&#25968;&#26500;&#24314;Transformer&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21019;&#20316;&#12290;&#26412;&#25991;&#36890;&#36807;&#23481;&#38169;&#24615;&#37327;&#23376;&#35745;&#31639;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;Transformer&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20934;&#22791;self-attention&#30697;&#38453;&#30340;&#22359;&#32534;&#30721;&#65292;&#24182;&#32467;&#21512;&#37327;&#23376;&#23376;&#31243;&#24207;&#26500;&#24314;&#20102;Transformer&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16714v1 Announce Type: cross  Abstract: Generative machine learning methods such as large-language models are revolutionizing the creation of text and images. While these models are powerful they also harness a large amount of computational resources. The transformer is a key component in large language models that aims to generate a suitable completion of a given partial sequence. In this work, we investigate transformer architectures under the lens of fault-tolerant quantum computing. The input model is one where pre-trained weight matrices are given as block encodings to construct the query, key, and value matrices for the transformer. As a first step, we show how to prepare a block encoding of the self-attention matrix, with a row-wise application of the softmax function using the Hadamard product. In addition, we combine quantum subroutines to construct important building blocks in the transformer, the residual connection, layer normalization, and the feed-forward neura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>AutoSAT&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#25552;&#21319;&#27714;&#35299;&#22120;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#25554;&#21363;&#29992;&#25805;&#20316;&#65292;&#20445;&#35777;&#20102;&#23481;&#38169;&#24615;&#65292;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10705</link><description>&lt;p&gt;
AutoSAT:&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoSAT: Automatically Optimize SAT Solvers via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10705
&lt;/p&gt;
&lt;p&gt;
AutoSAT&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#25552;&#21319;&#27714;&#35299;&#22120;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#21363;&#25554;&#21363;&#29992;&#25805;&#20316;&#65292;&#20445;&#35777;&#20102;&#23481;&#38169;&#24615;&#65292;&#22312;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#22312;SAT&#27714;&#35299;&#22120;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#36866;&#29992;&#20110;&#25152;&#26377;&#38382;&#39064;&#23454;&#20363;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#38656;&#35201;&#20026;&#29305;&#23450;&#38382;&#39064;&#23454;&#20363;&#20248;&#21270;&#29305;&#23450;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoSAT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#20248;&#21270;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#12290;AutoSAT&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#65292;&#36827;&#34892;&#35780;&#20272;&#65292;&#28982;&#21518;&#21033;&#29992;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#21551;&#21457;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#22686;&#24378;&#27714;&#35299;&#22120;&#33021;&#21147;&#12290;AutoSAT&#22522;&#20110;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#28040;&#38500;&#20102;&#23545;&#24191;&#27867;&#30340;&#21021;&#27493;&#35774;&#32622;&#21644;&#27169;&#22411;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#24102;&#26377;&#23481;&#38169;&#33021;&#21147;&#30340;&#24605;&#32500;&#38142;&#21327;&#20316;&#36807;&#31243;&#65292;&#30830;&#20445;&#21551;&#21457;&#24335;&#20248;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;&#23545;&#20351;&#29992;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;AutoSAT&#30340;&#25972;&#20307;&#24615;&#33021;&#20248;&#36234;&#65292;&#29305;&#21035;&#22312;&#35299;&#20915;&#26576;&#20123;&#29305;&#23450;&#30340;SAT&#38382;&#39064;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10705v1 Announce Type: new  Abstract: Heuristics are crucial in SAT solvers, while no heuristic rules are suitable for all problem instances. Therefore, it typically requires to refine specific solvers for specific problem instances. In this context, we present AutoSAT, a novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT is based on Large Large Models (LLMs) which is able to autonomously generate code, conduct evaluation, then utilize the feedback to further optimize heuristics, thereby reducing human intervention and enhancing solver capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need for extensive preliminary setup and model training, and fosters a Chain of Thought collaborative process with fault-tolerance, ensuring robust heuristic optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL) solver demonstrates the overall superior performance of AutoSAT, especially in solving some specific SAT pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;</title><link>https://arxiv.org/abs/2402.09894</link><description>&lt;p&gt;
&#19981;&#20165;&#20165;&#26159;&#26032;&#39062;&#24615;&#65306;&#20851;&#20110;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#25928;&#29992;&#21644;&#23450;&#21046;&#21270;&#30340;&#32437;&#21521;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not Just Novelty: A Longitudinal Study on Utility and Customization of AI Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32437;&#21521;&#30740;&#31350;&#35843;&#26597;&#20102;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#29087;&#24713;&#21270;&#38454;&#27573;&#21518;&#65292;&#29992;&#25143;&#24863;&#30693;&#21040;&#30340;&#31995;&#32479;&#25928;&#29992;&#25552;&#39640;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#20026;&#20154;&#20204;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#26032;&#39062;&#32780;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#26377;&#35768;&#22810;AI&#24037;&#20316;&#27969;&#31243;&#36890;&#36807;&#23558;AI&#36755;&#20986;&#19982;&#20154;&#31867;&#20114;&#21160;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#30495;&#23454;&#32780;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;AI&#20855;&#26377;&#26080;&#21487;&#21542;&#35748;&#30340;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#26032;&#40092;&#24863;&#28040;&#22833;&#21518;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20316;&#27969;&#31243;&#30340;&#23454;&#29992;&#24615;&#22914;&#20309;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#26500;&#24314;&#30340;&#24037;&#20855;&#20855;&#26377;&#20010;&#24615;&#21270;&#21644;&#24555;&#36895;&#36866;&#24212;&#30340;&#28508;&#21147;&#65292;&#20294;&#29992;&#25143;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#20010;&#24615;&#21270;&#30340;&#21487;&#33021;&#24615;&#21602;&#65311;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20026;&#26399;&#19977;&#21608;&#30340;&#32437;&#21521;&#30740;&#31350;&#65292;&#20849;&#26377;12&#20010;&#29992;&#25143;&#65292;&#26088;&#22312;&#20102;&#35299;&#31185;&#23398;&#20256;&#25773;&#20013;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#29087;&#24713;&#24230;&#21644;&#23450;&#21046;&#21270;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29087;&#24713;&#21270;&#38454;&#27573;&#25345;&#32493;&#20102;4.3&#20010;&#20250;&#35805;&#65292;&#29992;&#25143;&#22312;&#36825;&#20010;&#38454;&#27573;&#25506;&#32034;&#24037;&#20316;&#27969;&#31243;&#30340;&#21151;&#33021;&#20197;&#21450;&#20182;&#20204;&#21457;&#29616;&#21738;&#20123;&#26041;&#38754;&#26377;&#29992;&#12290;&#22312;&#29087;&#24713;&#21270;&#21518;&#65292;&#31995;&#32479;&#30340;&#24863;&#30693;&#25928;&#29992;&#35780;&#20998;&#39640;&#20110;&#20043;&#21069;&#65292;&#34920;&#26126;&#20102;&#24863;&#30693;&#25928;&#29992;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09894v1 Announce Type: cross  Abstract: Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it's uncertain how useful generative AI workflows are after the novelty wears off. Additionally, tools built with generative AI have the potential to be personalized and adapted quickly and easily, but do users take advantage of the potential to customize? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that the familiarization phase lasts for 4.3 sessions, where users explore the capabilities of the workflow and which aspects they find useful. After familiarization, the perceived utility of the system is rated higher than before, indicating that the perceived
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09723</link><description>&lt;p&gt;
&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#36805;&#36895;&#23398;&#20064;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification for Prompt Learning under a Limited Budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09723
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#32771;&#34385;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65292;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#23454;&#29616;&#20102;&#20004;&#20010;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#24341;&#21457;&#20102;&#23545;&#33258;&#21160;&#23398;&#20064;&#21512;&#36866;&#25552;&#31034;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25104;&#26412;&#65288;&#20363;&#22914;&#35775;&#38382;LLM&#21644;&#35780;&#20272;&#21709;&#24212;&#65289;&#23578;&#26410;&#24471;&#21040;&#32771;&#34385;&#12290;&#20026;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#26126;&#30830;&#24341;&#20837;&#20102;&#26377;&#38480;&#39044;&#31639;&#32422;&#26463;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#30740;&#31350;&#22312;&#25552;&#31034;&#23398;&#20064;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI-FB&#65289;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#22522;&#20110;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;TRIPLE&#65288;&#29992;&#20110;&#25552;&#31034;&#23398;&#20064;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65289;&#65292;&#20197;&#31995;&#32479;&#22320;&#21033;&#29992;BAI-FB&#22312;&#25552;&#31034;&#23398;&#20064;&#20013;&#30340;&#21147;&#37327;&#12290;&#25552;&#31034;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#28857;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#32858;&#31867;&#21644;&#23884;&#20837;&#24605;&#24819;&#25552;&#20986;&#20102;TRIPLE&#30340;&#20004;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09723v1 Announce Type: cross  Abstract: The remarkable instruction-following capability of large language models (LLMs) has sparked a growing interest in automatically learning suitable prompts. However, while many effective methods have been proposed, the cost incurred during the learning process (e.g., accessing LLM and evaluating the responses) has not been considered. To overcome this limitation, this work explicitly incorporates a finite budget constraint into prompt learning. Towards developing principled solutions, a novel connection is established between prompt learning and fixed-budget best arm identification (BAI-FB) in multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE (besT aRm Identification for Prompt LEarning) is proposed to harness the power of BAI-FB in prompt learning systematically. Unique characteristics of prompt learning further lead to two embedding-based enhancements of TRIPLE by exploiting the ideas of clustering and fun
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.07043</link><description>&lt;p&gt;
&#23614;&#24052;&#30340;&#25925;&#20107;&#65306;&#20316;&#20026;&#23610;&#24230;&#24459;&#21464;&#21270;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
A Tale of Tails: Model Collapse as a Change of Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#31070;&#32463;&#23610;&#24230;&#24459;&#24050;&#25104;&#20026;&#39044;&#27979;&#22823;&#27169;&#22411;&#22312;&#25193;&#23481;&#21644;&#21407;&#22987;&#65288;&#20154;&#31867;&#25110;&#33258;&#28982;&#65289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#22686;&#21152;&#26102;&#25913;&#21892;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24847;&#21619;&#30528;&#22312;&#32447;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#29983;&#24577;&#31995;&#32479;&#23558;&#36880;&#28176;&#21253;&#21547;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#24403;&#21512;&#25104;&#25968;&#25454;&#36827;&#20837;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#65292;&#23610;&#24230;&#24459;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#26410;&#26469;&#30340;&#27169;&#22411;&#20173;&#20250;&#25913;&#21892;&#65292;&#36824;&#26159;&#27880;&#23450;&#20250;&#23436;&#20840;&#23849;&#28291;&#65288;&#27169;&#22411;&#23849;&#28291;&#65289;&#65311;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#23849;&#28291;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24191;&#27867;&#30340;&#34928;&#20943;&#29616;&#35937;&#65292;&#20998;&#26512;&#20102;&#23610;&#24230;&#30340;&#20007;&#22833;&#12289;&#19982;&#20195;&#25968;&#30340;&#21464;&#21270;&#23610;&#24230;&#12289;&#25216;&#33021;&#30340;"&#36951;&#24536;"&#20197;&#21450;&#28151;&#21512;&#20154;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#26102;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36890;&#36807;&#23545;&#19968;&#20010;&#31639;&#26415;&#20219;&#21153;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06958</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#21270;&#31354;&#38388;-&#26102;&#38388;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#24402;&#19968;&#21270;&#27969;&#33021;&#22815;&#24314;&#27169;&#22810;&#27169;&#24577;&#31354;&#38388;&#20998;&#24067;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#21487;&#36870;&#24615;&#20197;&#21450;&#22312;&#37319;&#26679;&#21644;&#25512;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#39033;&#22909;&#22788;&#12290;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#38382;&#39064;&#30340;&#21512;&#36866;&#20505;&#36873;&#32773;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#22320;&#29699;&#31185;&#23398;&#12289;&#22825;&#20307;&#29289;&#29702;&#23398;&#25110;&#20998;&#23376;&#31185;&#23398;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#30340;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#12290;&#35813;&#26041;&#27861;&#22312;&#20174;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26085;&#28201;&#24230;&#21644;&#23567;&#26102;&#31561;&#21387;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20043;&#22806;&#36827;&#34892;&#33391;&#22909;&#30340;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05764</link><description>&lt;p&gt;
&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#30340;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#34507;&#30333;&#36136;&#21151;&#33021;&#38656;&#35201;&#19982;&#23567;&#20998;&#23376;&#32467;&#21512;&#65292;&#21253;&#25324;&#37238;&#20652;&#21270;&#12290;&#22240;&#27492;&#65292;&#20026;&#23567;&#20998;&#23376;&#35774;&#35745;&#32467;&#21512;&#21475;&#34955;&#20855;&#26377;&#20174;&#33647;&#29289;&#21512;&#25104;&#21040;&#33021;&#37327;&#23384;&#20648;&#31561;&#22810;&#31181;&#24433;&#21709;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;HarmonicFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20110;&#33258;&#35843;&#27969;&#21305;&#37197;&#30446;&#26631;&#30340;3D&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#12290;FlowSite&#23558;&#36825;&#31181;&#27969;&#27169;&#22411;&#25193;&#23637;&#21040;&#32852;&#21512;&#29983;&#25104;&#34507;&#30333;&#36136;&#21475;&#34955;&#30340;&#31163;&#25955;&#27531;&#22522;&#31867;&#22411;&#21644;&#20998;&#23376;&#30340;&#32467;&#21512;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HarmonicFlow&#22312;&#21475;&#34955;&#32423;&#23545;&#25509;&#20013;&#22312;&#31616;&#21333;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#24179;&#22343;&#26679;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20511;&#21161;&#20110;&#36825;&#31181;&#32467;&#26500;&#24314;&#27169;&#65292;FlowSite&#35774;&#35745;&#30340;&#32467;&#21512;&#20301;&#28857;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.17045</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#35299;&#37322;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explaining Explanations in Probabilistic Logic Programming. (arXiv:2401.17045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#20063;&#23548;&#33268;&#20102;&#20135;&#29983;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#19968;&#20123;&#26041;&#27861;&#20013;&#65292;&#31995;&#32479;&#26159;&#19981;&#36879;&#26126;&#30340;&#65288;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#36866;&#24403;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36923;&#36753;&#32534;&#31243;&#65288;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#65289;&#21644;&#27010;&#29575;&#65288;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#32467;&#21512;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#35828;&#27169;&#22411;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#65292;&#36825;&#26041;&#20415;&#20102;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#26597;&#35810;&#65292;&#36890;&#24120;&#30340;&#8220;&#35299;&#37322;&#8221;&#30340;&#27010;&#24565;&#26159;&#19982;&#27169;&#22411;&#30340;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#36873;&#25321;&#38598;&#30456;&#20851;&#32852;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#38598;&#21512;&#27809;&#26377;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#38469;&#19978;&#65292;&#19968;&#20123;&#36873;&#25321;&#23454;&#38469;&#19978;&#19982;&#25152;&#32771;&#34385;&#30340;&#26597;&#35810;&#26080;&#20851;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#25512;&#29702;&#23450;&#20041;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In some approaches, the system is not transparent (often referred to as a "black box"), making it difficult to generate appropriate explanations. In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty). In this setting, one can say that models are interpretable, which eases its understanding. However, given a particular query, the usual notion of "explanation" is associated with a set of choices, one for each random variable of the model. Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query. In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#36981;&#24490;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.12459</link><description>&lt;p&gt;
&#26397;&#30528;&#20855;&#26377;&#31038;&#20250;&#21644;&#36947;&#24503;&#24847;&#35782;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65306;&#20351;&#29992;LLM&#36827;&#34892;&#22870;&#21169;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Towards Socially and Morally Aware RL agent: Reward Design With LLM. (arXiv:2401.12459v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#36981;&#24490;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#24182;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#21644;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#26102;&#65292;&#22870;&#21169;&#20989;&#25968;&#28608;&#21169;&#26234;&#33021;&#20307;&#23454;&#29616;&#19968;&#20010;&#30446;&#26631;&#12290;&#30446;&#26631;&#30340;&#19981;&#27491;&#30830;&#25110;&#19981;&#23436;&#25972;&#30340;&#35268;&#33539;&#21487;&#33021;&#23548;&#33268;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#34892;&#20026;&#65292;&#19981;&#36981;&#23432;&#27169;&#31946;&#21644;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#31038;&#20250;&#21644;&#36947;&#24503;&#35268;&#33539;&#65292;&#24182;&#23548;&#33268;&#36127;&#38754;&#21103;&#20316;&#29992;&#21644;&#19981;&#23433;&#20840;&#30340;&#25506;&#32034;&#31561;&#19981;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#25163;&#21160;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#26469;&#36991;&#20813;&#36127;&#38754;&#21103;&#20316;&#29992;&#65292;&#20351;&#29992;&#20154;&#31867;&#30417;&#30563;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#25110;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#35268;&#21010;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#23433;&#20840;&#25506;&#32034;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#30452;&#25509;&#22870;&#21169;&#20449;&#21495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2312.09085</link><description>&lt;p&gt;
&#22320;&#29699;&#26159;&#25153;&#24179;&#30340;&#65292;&#22240;&#20026;......&#65306;&#36890;&#36807;&#35828;&#26381;&#24615;&#23545;&#35805;&#30740;&#31350;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#20449;&#20208;
&lt;/p&gt;
&lt;p&gt;
The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;LLMs&#23545;&#35823;&#23548;&#20449;&#24687;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23553;&#35013;&#20102;&#22823;&#37327;&#30693;&#35782;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22806;&#37096;&#35823;&#23548;&#20449;&#24687;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22312;&#21333;&#36718;&#23545;&#35805;&#20013;&#30740;&#31350;&#20102;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#29305;&#21035;&#26159;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#65292;&#20449;&#20208;&#21487;&#20197;&#21457;&#29983;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#23545;&#35828;&#26381;&#24615;&#23545;&#35805;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22238;&#31572;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;Farm&#65288;&#21363;&#20107;&#23454;&#21040;&#35823;&#23548;&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#31995;&#32479;&#29983;&#25104;&#30340;&#35828;&#26381;&#24615;&#35823;&#23548;&#20449;&#24687;&#30456;&#21305;&#37197;&#30340;&#20107;&#23454;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#36861;&#36394;LLMs&#22312;&#35828;&#26381;&#24615;&#23545;&#35805;&#20013;&#30340;&#20449;&#20208;&#21464;&#21270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#20107;&#23454;&#30693;&#35782;&#19978;&#30340;&#27491;&#30830;&#20449;&#24565;&#24456;&#23481;&#26131;&#34987;&#21508;&#31181;&#35828;&#26381;&#31574;&#30053;&#25152;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;</title><link>http://arxiv.org/abs/2312.06717</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Privacy Issues in Large Language Models: A Survey. (arXiv:2312.06717v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20027;&#35201;&#24635;&#32467;&#20102;&#32418;&#38431;&#27169;&#22411;&#25581;&#31034;&#38544;&#31169;&#39118;&#38505;&#12289;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12289;&#39640;&#25928;&#21024;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20197;&#31526;&#21512;&#38544;&#31169;&#27861;&#35268;&#12289;&#20197;&#21450;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#31561;&#25216;&#26415;&#30740;&#31350;&#12290;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#34429;&#28982;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#65292;&#20294;&#19981;&#26159;&#26412;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#39318;&#20010;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;AI&#30740;&#31350;&#39046;&#22495;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#32418;&#38431;&#27169;&#22411;&#20197;&#31361;&#20986;&#38544;&#31169;&#39118;&#38505;&#30340;&#24037;&#20316;&#65292;&#23581;&#35797;&#23558;&#38544;&#31169;&#32435;&#20837;&#35757;&#32451;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24037;&#20316;&#65292;&#20351;&#24471;&#25968;&#25454;&#21487;&#20197;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#39640;&#25928;&#21024;&#38500;&#20197;&#31526;&#21512;&#29616;&#26377;&#30340;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#35797;&#22270;&#20943;&#36731;&#29256;&#26435;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#24635;&#32467;&#24320;&#21457;&#31639;&#27861;&#12289;&#35777;&#26126;&#23450;&#29702;&#21644;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#30340;&#25216;&#26415;&#30740;&#31350;&#12290;&#34429;&#28982;&#26377;&#22823;&#37327;&#30340;&#27861;&#24459;&#21644;&#25919;&#31574;&#30740;&#31350;&#20174;&#19981;&#21516;&#35282;&#24230;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#36825;&#19981;&#26159;&#25105;&#20204;&#35843;&#30740;&#30340;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20316;&#21697;&#20197;&#21450;&#26368;&#36817;&#30340;&#27861;&#24459;&#36827;&#23637;&#30830;&#23454;&#24433;&#21709;&#20102;&#36825;&#20123;&#25216;&#26415;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#22788;&#29702;&#26041;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#31532;&#19968;&#33410;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#31616;&#35201;&#35752;&#35770;&#12290;&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#23613;&#21147;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#28431;&#25481;&#19968;&#20123;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we h
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2312.04234</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#22312;Transformer&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#36215;&#21040;&#20102;&#25913;&#36827;&#30340;&#20316;&#29992;&#65281;&#65288;arXiv&#65306;2312.04234v2 [cs.LG]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutions Enrich the Self-Attention in Transformers!. (arXiv:2312.04234v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04234
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#21367;&#31215;&#26469;&#25913;&#36827;Transformer&#27169;&#22411;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22240;&#20854;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#32780;&#38395;&#21517;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;Transformer&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#21363;&#34920;&#31034;&#22312;&#21508;&#20010;&#23618;&#20043;&#38388;&#36235;&#20110;&#26080;&#27861;&#21306;&#20998;&#30340;&#20540;&#65292;&#23548;&#33268;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35299;&#37322;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#22270;&#28388;&#27874;&#22120;&#65292;&#24182;&#20174;&#22270;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#30340;&#35282;&#24230;&#37325;&#26032;&#35774;&#35745;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#28388;&#27874;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GFSA&#65289;&#65292;&#20197;&#23398;&#20064;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#20854;&#22797;&#26434;&#24230;&#30053;&#39640;&#20110;&#21407;&#22987;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GFSA&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#27169;&#24335;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#20195;&#30721;&#20998;&#31867;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#25913;&#36827;&#20102;Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, renowned for their self-attention mechanism, have achieved state-of-the-art performance across various tasks in natural language processing, computer vision, time-series modeling, etc. However, one of the challenges with deep Transformer models is the oversmoothing problem, where representations across layers converge to indistinguishable values, leading to significant performance degradation. We interpret the original self-attention as a simple graph filter and redesign it from a graph signal processing (GSP) perspective. We propose graph-filter-based self-attention (GFSA) to learn a general yet effective one, whose complexity, however, is slightly larger than that of the original self-attention mechanism. We demonstrate that GFSA improves the performance of Transformers in various fields, including computer vision, natural language processing, graph pattern classification, speech recognition, and code classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00154</link><description>&lt;p&gt;
&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#65306;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#23454;&#29616;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual Continual Learning: Stability and Plasticity through Lagrange Multipliers. (arXiv:2310.00154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21407;&#22987;-&#23545;&#20598;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#35299;&#20915;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#20998;&#26512;&#20219;&#21153;&#23618;&#38754;&#21644;&#26679;&#26412;&#23618;&#38754;&#30340;&#32422;&#26463;&#65292;&#22312;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#20013;&#20998;&#37197;&#36164;&#28304;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22266;&#26377;&#22320;&#26159;&#19968;&#20010;&#21463;&#38480;&#23398;&#20064;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#22312;&#8220;&#26080;&#36951;&#24536;&#8221;&#35201;&#27714;&#19979;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22120;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#26159;&#21487;&#34892;&#19988;&#26377;&#30410;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#38480;&#21046;&#24615;&#23398;&#20064;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21487;&#20197;&#23558;&#20808;&#21069;&#20219;&#21153;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#23384;&#20648;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#22312;&#20219;&#21153;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31895;&#31961;&#26041;&#27861;&#21644;&#19968;&#20010;&#22312;&#26679;&#26412;&#23618;&#38754;&#19978;&#26377;&#32422;&#26463;&#30340;&#31934;&#32454;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20598;&#21464;&#37327;&#25351;&#31034;&#20102;&#26368;&#20248;&#20540;&#23545;&#20110;&#32422;&#26463;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#22312;&#31895;&#31961;&#26041;&#27861;&#20013;&#23545;&#32531;&#20914;&#21306;&#36827;&#34892;&#20102;&#21010;&#20998;&#65292;&#23558;&#26356;&#22810;&#36164;&#28304;&#20998;&#37197;&#32473;&#26356;&#38590;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is inherently a constrained learning problem. The goal is to learn a predictor under a \emph{no-forgetting} requirement. Although several prior studies formulate it as such, they do not solve the constrained problem explicitly. In this work, we show that it is both possible and beneficial to undertake the constrained optimization problem directly. To do this, we leverage recent results in constrained learning through Lagrangian duality. We focus on memory-based methods, where a small subset of samples from previous tasks can be stored in a replay buffer. In this setting, we analyze two versions of the continual learning problem: a coarse approach with constraints at the task level and a fine approach with constraints at the sample level. We show that dual variables indicate the sensitivity of the optimal value with respect to constraint perturbations. We then leverage this result to partition the buffer in the coarse approach, allocating more resources to harder task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12985</link><description>&lt;p&gt;
&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#19979;&#30340;&#21608;&#30028;&#25511;&#21046;&#65306;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control with Heterogeneous Cordon Signal Behaviors: A Semi-Model Dependent Reinforcement Learning Approach. (arXiv:2308.12985v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#30028;&#25511;&#21046;&#65288;PC&#65289;&#31574;&#30053;&#26088;&#22312;&#36890;&#36807;&#30417;&#27979;&#21463;&#20445;&#25252;&#32593;&#32476;&#65288;PN&#65289;&#30340;&#36716;&#31227;&#27969;&#37327;&#26469;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#22312;&#36807;&#39281;&#21644;&#24773;&#20917;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#35686;&#25106;&#20449;&#21495;&#30340;&#22343;&#21248;&#27979;&#37327;&#29575;&#24573;&#35270;&#20102;&#20132;&#21449;&#21475;&#32423;&#21035;&#30340;&#20132;&#36890;&#29366;&#24577;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#23616;&#37096;&#20132;&#36890;&#25317;&#22581;&#21644;&#30772;&#22351;&#32593;&#32476;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#21322;&#27169;&#22411;&#20381;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#26469;&#23454;&#29616;&#20855;&#26377;&#24322;&#26500;&#35686;&#25106;&#20449;&#21495;&#34892;&#20026;&#30340;&#21608;&#30028;&#25511;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#23558;&#22522;&#20110;MARL&#30340;&#20449;&#21495;&#25511;&#21046;&#26041;&#27861;&#19982;&#38598;&#20013;&#24335;&#21453;&#39304;PC&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#24182;&#24212;&#29992;&#20110;PN&#30340;&#35686;&#25106;&#20449;&#21495;&#12290;&#23427;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31995;&#32479;&#65292;&#21453;&#39304;PC&#31574;&#30053;&#26816;&#27979;PN&#30340;&#25972;&#20307;&#20132;&#36890;&#29366;&#24577;&#65292;&#28982;&#21518;&#23558;&#26412;&#22320;&#25351;&#20196;&#20998;&#21457;&#32473;&#30001;MARL&#26694;&#26550;&#20013;&#30340;&#26234;&#33021;&#20307;&#25511;&#21046;&#30340;&#35686;&#25106;&#20449;&#21495;&#12290;&#27599;&#20010;&#35686;&#25106;&#20449;&#21495;&#37117;&#29420;&#31435;&#32780;&#19981;&#21516;&#65292;&#21019;&#24314;&#20102;&#24377;&#24615;&#21644;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control (PC) strategies have been proposed to address urban road network control in oversaturated situations by monitoring transfer flows of the Protected Network (PN). The uniform metering rate for cordon signals in existing studies ignores the variety of local traffic states at the intersection level, which may cause severe local traffic congestion and ruin the network stability. This paper introduces a semi-model dependent Multi-Agent Reinforcement Learning (MARL) framework to conduct PC with heterogeneous cordon signal behaviors. The proposed strategy integrates the MARL-based signal control method with centralized feedback PC policy and is applied to cordon signals of the PN. It operates as a two-stage system, with the feedback PC strategy detecting the overall traffic state within the PN and then distributing local instructions to cordon signals controlled by agents in the MARL framework. Each cordon signal acts independently and differently, creating a slack and distri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.01399</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Model the World with Language. (arXiv:2308.01399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35821;&#35328;&#23398;&#20064;&#23545;&#19990;&#30028;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#24182;&#36827;&#34892;&#34892;&#21160;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#65292;&#20195;&#29702;&#22120;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#27169;&#22411;&#22238;&#28378;&#20013;&#36827;&#34892;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#19982;&#20154;&#31867;&#22312;&#19990;&#30028;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#20195;&#29702;&#22120;&#38656;&#35201;&#29702;&#35299;&#20154;&#20204;&#20351;&#29992;&#30340;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#35270;&#35273;&#19990;&#30028;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#22522;&#20110;&#35821;&#35328;&#34892;&#21160;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#20195;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#20219;&#21153;&#22870;&#21169;&#23398;&#20064;&#25191;&#34892;&#31616;&#21333;&#30340;&#35821;&#35328;&#25351;&#20196;&#65292;&#20294;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#21487;&#20197;&#21033;&#29992;&#20256;&#36798;&#19968;&#33324;&#30693;&#35782;&#12289;&#25551;&#36848;&#19990;&#30028;&#29366;&#24577;&#12289;&#25552;&#20379;&#20114;&#21160;&#21453;&#39304;&#31561;&#22810;&#26679;&#21270;&#35821;&#35328;&#30340;&#20195;&#29702;&#22120;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#35821;&#35328;&#24110;&#21161;&#20195;&#29702;&#22120;&#39044;&#27979;&#26410;&#26469;&#65306;&#23558;&#20250;&#34987;&#35266;&#23519;&#21040;&#20160;&#20040;&#12289;&#19990;&#30028;&#23558;&#22914;&#20309;&#36816;&#34892;&#20197;&#21450;&#21738;&#20123;&#24773;&#20917;&#23558;&#33719;&#24471;&#22870;&#21169;&#12290;&#36825;&#20010;&#35266;&#28857;&#23558;&#35821;&#35328;&#29702;&#35299;&#19982;&#26410;&#26469;&#39044;&#27979;&#32479;&#19968;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Dynalang&#65292;&#19968;&#31181;&#23398;&#20064;&#22810;&#27169;&#24577;&#19990;&#30028;&#27169;&#22411;&#30340;&#20195;&#29702;&#22120;&#65292;&#23427;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#22312;&#24819;&#20687;&#30340;&#27169;&#22411;&#22238;&#28378;&#20013;&#23398;&#20064;&#34892;&#21160;&#12290;&#19982;&#21482;&#20351;&#29992;&#35821;&#35328;&#39044;&#27979;&#21160;&#20316;&#30340;&#20256;&#32479;&#20195;&#29702;&#22120;&#19981;&#21516;&#65292;Dynalang&#36890;&#36807;&#36807;&#21435;&#30340;&#35821;&#35328;&#36824;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08116</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#65292;&#20197;&#21450;&#20854;&#34920;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
The Structure and Dynamics of Knowledge Graphs, with Superficiality. (arXiv:2305.08116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#34920;&#23618;&#24615;&#26469;&#31616;&#21333;&#24314;&#27169;&#22797;&#26434;&#29305;&#24449;&#65292;&#20174;&#32780;&#25484;&#25569;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#32508;&#21512;&#20102;&#20174;&#23398;&#26415;&#26426;&#26500;&#21644;&#20225;&#19994;&#21040;&#22823;&#20247;&#38598;&#36164;&#31561;&#39033;&#30446;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#30693;&#35782;&#65292;&#27599;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#20195;&#34920;&#36825;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#22522;&#26412;&#20107;&#23454;&#12290;&#20851;&#31995;&#35821;&#20041;&#30340;&#22810;&#26679;&#24615;&#32452;&#25104;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#20016;&#23500;&#24615;&#65292;&#23548;&#33268;&#20986;&#29616;&#26377;&#26102;&#28151;&#20081;&#30340;&#22855;&#24322;&#25299;&#25169;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#34920;&#23618;&#24615;&#30340;&#27010;&#24565;&#26469;&#31616;&#21333;&#24314;&#27169;&#65292;&#34920;&#23618;&#24615;&#25511;&#21046;&#30528;&#29420;&#31435;&#29983;&#25104;&#20107;&#23454;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#37325;&#21472;&#24773;&#20917;&#65292;&#20063;&#36890;&#36807;&#30830;&#23450;&#38169;&#35823;&#25551;&#36848;&#23454;&#20307;&#30340;&#27604;&#20363;&#26469;&#25511;&#21046;&#20840;&#23616;&#30693;&#35782;&#20998;&#24067;&#30340;&#24179;&#34913;&#12290;&#36825;&#26159;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#21160;&#24577;&#26041;&#38754;&#30340;&#39318;&#20010;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#27491;&#24335;&#30693;&#35782;&#33719;&#21462;&#21644;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01461</link><description>&lt;p&gt;
&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#37327;&#31649;&#29702;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mixed-Integer Optimal Control via Reinforcement Learning: A Case Study on Hybrid Vehicle Energy Management. (arXiv:2305.01461v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#65292; &#24182;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;&#33021;&#28304;&#31649;&#29702;&#38382;&#39064;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#36755;&#20986;&#36830;&#32493;&#21644;&#31163;&#25955;&#25511;&#21046;&#21464;&#37327;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#34987;&#21046;&#23450;&#20026;&#28151;&#21512;&#25972;&#25968;&#26368;&#20248;&#25511;&#21046;(MIOC)&#38382;&#39064;&#65292;&#30001;&#20110;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#35299;&#20915;&#12290;&#25968;&#20540;&#26041;&#27861;&#22914;&#20998;&#25903;&#23450;&#30028;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#19981;&#36866;&#21512;&#23454;&#26102;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#31163;&#25955;&#24378;&#21270;&#23398;&#20064;(CDRL)&#31639;&#27861;&#65292;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#28436;&#21592;- Q(TD3AQ)&#65292;&#29992;&#20110;MIOC&#38382;&#39064;&#12290;TD3AQ&#32467;&#21512;&#20102;&#28436;&#21592;-&#25209;&#35780;&#23478;&#21644;Q&#23398;&#20064;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#21487;&#20197;&#22788;&#29702;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#28151;&#21512;&#21160;&#21147;&#36710;&#36742;(HEV)&#33021;&#37327;&#31649;&#29702;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#20013;&#36830;&#32493;&#21464;&#37327;&#21457;&#21160;&#26426;&#36716;&#30697;&#21644;&#31163;&#25955;&#21464;&#37327;&#40831;&#36718;&#27604;&#30340;&#23454;&#26102;&#25511;&#21046;&#23545;&#20110;&#26368;&#22823;&#21270;&#29123;&#27833;&#32463;&#27982;&#24615;&#24182;&#28385;&#36275;&#39550;&#39542;&#32422;&#26463;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#21516;&#39537;&#21160;&#24490;&#29615;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CDRL&#31639;&#27861;&#22312;&#35299;&#20915;&#36136;&#37327;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many optimal control problems require the simultaneous output of continuous and discrete control variables. Such problems are usually formulated as mixed-integer optimal control (MIOC) problems, which are challenging to solve due to the complexity of the solution space. Numerical methods such as branch-and-bound are computationally expensive and unsuitable for real-time control. This paper proposes a novel continuous-discrete reinforcement learning (CDRL) algorithm, twin delayed deep deterministic actor-Q (TD3AQ), for MIOC problems. TD3AQ combines the advantages of both actor-critic and Q-learning methods, and can handle the continuous and discrete action spaces simultaneously. The proposed algorithm is evaluated on a hybrid electric vehicle (HEV) energy management problem, where real-time control of the continuous variable engine torque and discrete variable gear ratio is essential to maximize fuel economy while satisfying driving constraints. Simulation results on different drive cyc
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.07946</link><description>&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#32479;&#19968;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23436;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24320;&#21457;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#30001;&#22806;&#37096;&#30417;&#30563;&#21592;&#25351;&#23450;&#30340;&#22870;&#21169;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#28041;&#21450;&#37096;&#20998;&#35266;&#27979;&#65292;&#24418;&#24335;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#34892;&#21160;&#21644;&#35266;&#27979;&#35760;&#24518;&#25110;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#29615;&#22659;&#30340;&#30495;&#23454;&#29366;&#24577;&#26469;&#35299;&#20915;POMDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#38543;&#26102;&#38388;&#32858;&#21512;&#35266;&#27979;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#26679;&#26412;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20851;&#27880;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#24573;&#35270;&#20102;&#25512;&#26029;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;&#22312;POMDP&#20013;&#21046;&#23450;&#30340;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31216;&#20026;&#26399;&#26395;&#33258;&#30001;&#33021;&#65288;EFE&#65289;&#30340;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#12290;&#36825;&#25552;&#20379;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#65288;&#23500;&#26377;&#24320;&#21457;&#24615;&#65289;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#38500;OCV&#65288;Aliasing&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22797;&#25968;&#21367;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Gabor&#26679;&#24335;&#30340;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.00394</link><description>&lt;p&gt;
&#20174;CNN&#21040;&#22522;&#20110;&#22797;&#23567;&#27874;&#30340;&#24179;&#31227;&#19981;&#21464;&#21452;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From CNNs to Shift-Invariant Twin Models Based on Complex Wavelets. (arXiv:2212.00394v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00394
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#38500;OCV&#65288;Aliasing&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22797;&#25968;&#21367;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Gabor&#26679;&#24335;&#30340;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#28151;&#21472;&#26041;&#27861;&#26469;&#22686;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#29992;&#8220;&#22797;&#20540;&#21367;&#31215;+&#27169;&#36816;&#31639;&#8221;&#65288;$\mathbb{C}$Mod&#65289;&#20195;&#26367;&#31532;&#19968;&#23618;&#30340;&#8220;&#23454;&#20540;&#21367;&#31215;+&#26368;&#22823;&#27744;&#21270;&#8221;&#65288;$\mathbb{R}$Max&#65289;&#65292;&#22240;&#20026;&#23427;&#31283;&#23450;&#20110;&#24179;&#31227;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22768;&#31216;&#24403;&#21367;&#31215;&#26680;&#26159;&#24102;&#36890;&#21644;&#23450;&#21521;&#30340;&#65288;&#31867;&#20284;&#20110;Gabor&#28388;&#27874;&#22120;&#65289;&#26102;&#65292;$\mathbb{C}$Mod&#21644;$\mathbb{R}$Max&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;$\mathbb{C}$Mod&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;$\mathbb{R}$Max&#30340;&#31283;&#23450;&#26367;&#20195;&#21697;&#12290;&#22240;&#27492;&#65292;&#22312;&#25239;&#28151;&#21472;&#20043;&#21069;&#65292;&#25105;&#20204;&#24378;&#21046;&#21367;&#31215;&#26680;&#37319;&#29992;&#36825;&#31181;Gabor&#26679;&#24335;&#30340;&#32467;&#26500;&#12290;&#30456;&#24212;&#30340;&#26550;&#26500;&#31216;&#20026;&#25968;&#23398;&#21452;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20351;&#29992;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#25968;&#23398;&#36816;&#31639;&#31526;&#26469;&#27169;&#25311;&#21407;&#22987;&#30340;&#33258;&#30001;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#25239;&#28151;&#21472;&#26041;&#27861;&#22312;Imagenet&#21644;CIFAR-10&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel antialiasing method to increase shift invariance and prediction accuracy in convolutional neural networks. Specifically, we replace the first-layer combination "real-valued convolutions + max pooling" ($\mathbb{R}$Max) by "complex-valued convolutions + modulus" ($\mathbb{C}$Mod), which is stable to translations. To justify our approach, we claim that $\mathbb{C}$Mod and $\mathbb{R}$Max produce comparable outputs when the convolution kernel is band-pass and oriented (Gabor-like filter). In this context, $\mathbb{C}$Mod can be considered as a stable alternative to $\mathbb{R}$Max. Thus, prior to antialiasing, we force the convolution kernels to adopt such a Gabor-like structure. The corresponding architecture is called mathematical twin, because it employs a well-defined mathematical operator to mimic the behavior of the original, freely-trained model. Our antialiasing approach achieves superior accuracy on ImageNet and CIFAR-10 classification tasks, compared to prior 
&lt;/p&gt;</description></item></channel></rss>