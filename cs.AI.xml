<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#24635;&#32467;&#20102;NeurIPS 2023&#20250;&#35758;&#19978;&#20851;&#20110;&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01580</link><description>&lt;p&gt;
&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#65306;&#36827;&#23637;&#12289;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Education (GAIED): Advances, Opportunities, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#24635;&#32467;&#20102;NeurIPS 2023&#20250;&#35758;&#19978;&#20851;&#20110;&#25945;&#32946;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAIED&#65289;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25991;&#31456;&#26159;&#30001;&#20316;&#32773;&#22312;NeurIPS 2023&#20250;&#35758;&#19978;&#32452;&#32455;&#30340;GAIED&#30740;&#35752;&#20250;&#21457;&#23637;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#32452;&#32455;GAIED&#30740;&#35752;&#20250;&#26159;&#20026;&#20102;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#12289;&#25945;&#32946;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#20132;&#27969;&#65292;&#25506;&#32034;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#27010;&#35201;&#20171;&#32461;&#30740;&#35752;&#20250;&#30340;&#27963;&#21160;&#65292;&#24182;&#31361;&#20986;&#25351;&#20986;&#22312;GAIED&#39046;&#22495;&#20013;&#30340;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey article has grown out of the GAIED (pronounced "guide") workshop organized by the authors at the NeurIPS 2023 conference. We organized the GAIED workshop as part of a community-building effort to bring together researchers, educators, and practitioners to explore the potential of generative AI for enhancing education. This article aims to provide an overview of the workshop activities and highlight several future research directions in the area of GAIED.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>Edu-ConvoKit&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#35299;&#20915;&#20102;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25991;&#26723;&#21644;&#38468;&#21152;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.05111</link><description>&lt;p&gt;
Edu-ConvoKit:&#19968;&#31181;&#29992;&#20110;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
Edu-ConvoKit: An Open-Source Library for Education Conversation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05111
&lt;/p&gt;
&lt;p&gt;
Edu-ConvoKit&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#35299;&#20915;&#20102;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25991;&#26723;&#21644;&#38468;&#21152;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Edu-ConvoKit&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#12289;&#27880;&#37322;&#21644;&#20998;&#26512;&#30340;&#24320;&#28304;&#24211;&#12290;&#25945;&#32946;&#20250;&#35805;&#25968;&#25454;&#30340;&#20998;&#26512;&#36164;&#28304;&#31232;&#32570;&#65292;&#20351;&#24471;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#27492;&#38590;&#20197;&#33719;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;Edu-ConvoKit&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;Edu-ConvoKit&#26159;&#24320;&#28304;&#30340;&#65288;https://github.com/stanfordnlp/edu-convokit&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;pip&#36827;&#34892;&#23433;&#35013;&#65288;https://pypi.org/project/edu-convokit/&#65289;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#25991;&#26723;&#65288;https://edu-convokit.readthedocs.io/en/latest/&#65289;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#35266;&#30475;&#65306;https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8-&#12290;&#25105;&#20204;&#36824;&#22312;GitHub&#20179;&#24211;&#20013;&#25552;&#20379;&#20102;&#38468;&#21152;&#36164;&#28304;&#65292;&#20363;&#22914;Edu-ConvoKit&#22312;&#19977;&#20010;&#19981;&#21516;&#25945;&#32946;&#25968;&#25454;&#38598;&#19978;&#30340;Colab&#24212;&#29992;&#31243;&#24207;&#20197;&#21450;Edu-ConvoKit&#30456;&#20851;&#35770;&#25991;&#30340;&#23384;&#20648;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Edu-ConvoKit, an open-source library designed to handle pre-processing, annotation and analysis of conversation data in education. Resources for analyzing education conversation data are scarce, making the research challenging to perform and therefore hard to access. We address these challenges with Edu-ConvoKit. Edu-ConvoKit is open-source (https://github.com/stanfordnlp/edu-convokit ), pip-installable (https://pypi.org/project/edu-convokit/ ), with comprehensive documentation (https://edu-convokit.readthedocs.io/en/latest/ ). Our demo video is available at: https://youtu.be/zdcI839vAko?si=h9qlnl76ucSuXb8- . We include additional resources, such as Colab applications of Edu-ConvoKit to three diverse education datasets and a repository of Edu-ConvoKit related papers, that can be found in our GitHub repository.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26089;&#26399;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;GRIT&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#20197;&#36866;&#24212;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05106</link><description>&lt;p&gt;
&#20351;&#29992;GRIT&#27169;&#22411;&#36827;&#34892;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Image captioning for Brazilian Portuguese using GRIT model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26089;&#26399;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;GRIT&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#20197;&#36866;&#24212;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#30340;&#26089;&#26399;&#24320;&#21457;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;GRIT&#65288;&#22522;&#20110;&#32593;&#26684;&#21644;&#21306;&#22495;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;Transformer&#65289;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#24037;&#20316;&#12290;GRIT&#26159;&#19968;&#20010;&#20165;&#20351;&#29992;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#20004;&#20010;&#35270;&#35273;&#29305;&#24449;&#26469;&#29983;&#25104;&#26356;&#22909;&#30340;&#23383;&#24149;&#12290;GRIT&#26041;&#27861;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;GRIT&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20197;&#22312;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Portuguese dataset to have an image captioning method for the Brazilian Portuguese Language.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05070</link><description>&lt;p&gt;
&#36890;&#24448;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
A Roadmap to Pluralistic Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05070
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20197;&#35299;&#20915;&#35774;&#35745;AI&#31995;&#32479;&#33021;&#22815;&#26381;&#21153;&#20110;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#38656;&#27714;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#40784;&#23450;&#20041;&#21644;&#23454;&#29616;&#22810;&#20803;&#20027;&#20041;&#30340;&#19977;&#31181;&#26041;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#26469;&#35780;&#20272;&#21644;&#27979;&#35797;&#22810;&#20803;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26435;&#21147;&#21644;&#26222;&#21450;&#31243;&#24230;&#30340;&#22686;&#21152;&#65292;&#35774;&#35745;&#33021;&#22815;&#20026;&#19981;&#21516;&#20215;&#20540;&#35266;&#21644;&#35266;&#28857;&#30340;&#20154;&#26381;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#27169;&#22411;&#23545;&#40784;&#20197;&#26381;&#21153;&#22810;&#20803;&#20154;&#31867;&#20215;&#20540;&#35266;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36890;&#21521;&#22810;&#20803;&#23545;&#40784;&#30340;&#36335;&#32447;&#22270;&#65292;&#20855;&#20307;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30830;&#23450;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#22810;&#20803;&#20027;&#20041;&#65306;1&#65289;Overton&#22810;&#20803;&#27169;&#22411;&#65292;&#23637;&#31034;&#21512;&#29702;&#21453;&#24212;&#30340;&#20809;&#35889;&#65307;2&#65289;&#21487;&#25805;&#25511;&#30340;&#22810;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#35843;&#25972;&#20197;&#21453;&#26144;&#29305;&#23450;&#30340;&#35266;&#28857;&#65307;3&#65289;&#20998;&#24067;&#22810;&#20803;&#27169;&#22411;&#65292;&#22312;&#20998;&#24067;&#20013;&#24456;&#22909;&#22320;&#26657;&#20934;&#32473;&#23450;&#20154;&#32676;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#21644;&#24418;&#24335;&#21270;&#20102;&#19977;&#31181;&#21487;&#33021;&#30340;&#22810;&#20803;&#22522;&#20934;&#31867;&#21035;&#65306;1&#65289;&#22810;&#30446;&#26631;&#22522;&#20934;&#65307;2&#65289;&#26435;&#34913;&#21487;&#25805;&#25511;&#22522;&#20934;&#65292;&#40723;&#21169;&#27169;&#22411;&#23545;&#20219;&#24847;&#26435;&#34913;&#36827;&#34892;&#35843;&#25972;&#65307;3&#65289;&#38506;&#23457;&#22242;&#22810;&#20803;&#22522;&#20934;&#65292;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#19981;&#21516;&#38506;&#23457;&#22242;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#30740;&#31350;&#26088;&#22312;&#25490;&#38500;&#36890;&#36807;AI&#30417;&#31649;&#25552;&#26696;&#25152;&#37319;&#29992;&#30340;&#19981;&#24688;&#24403;&#30340;AI&#23450;&#20041;&#23545;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#38750;AI&#20316;&#21697;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#24402;&#21040;&#20197;&#21069;&#22312;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.05048</link><description>&lt;p&gt;
&#20320;&#30340;AI&#26377;&#22810;&#20040;VADER&#65311;&#38754;&#21521;&#36866;&#29992;&#20110;&#30417;&#31649;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#30740;&#31350;&#26088;&#22312;&#25490;&#38500;&#36890;&#36807;AI&#30417;&#31649;&#25552;&#26696;&#25152;&#37319;&#29992;&#30340;&#19981;&#24688;&#24403;&#30340;AI&#23450;&#20041;&#23545;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#38750;AI&#20316;&#21697;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#24402;&#21040;&#20197;&#21069;&#22312;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25512;&#21160;&#20102;&#35768;&#22810;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#65288;ICT&#65289;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#33258;&#22270;&#28789;&#27979;&#35797;&#25552;&#35758;&#20197;&#26469;&#65292;ICT&#31995;&#32479;&#30340;&#33539;&#22260;&#24050;&#36828;&#36828;&#36229;&#20986;&#20102;AI&#12290;&#20851;&#38190;&#26159;&#65292;&#26368;&#36817;&#30340;AI&#30417;&#31649;&#25552;&#26696;&#37319;&#29992;&#20102;&#24433;&#21709;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#31995;&#32479;&#30340;AI&#23450;&#20041;&#65292;&#32780;&#36825;&#20123;&#24182;&#19981;&#26159;AI&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#21253;&#25324;&#25968;&#23398;&#12289;&#32479;&#35745;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#20316;&#21697;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#20174;&#35199;&#26041;&#31038;&#20250;&#21040;&#20840;&#29699;&#21335;&#26041;&#65292;&#37117;&#21457;&#29616;&#20102;AI&#23450;&#20041;&#19978;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;AI&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;VADER&#26694;&#26550;&#35780;&#20998;&#20102;&#24212;&#35813;&#20316;&#20026;AI&#23450;&#20041;&#30340;&#21069;&#25552;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#36825;&#20123;&#23450;&#20041;&#26088;&#22312;&#65288;i&#65289;&#37325;&#29616;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21253;&#25324;&#25152;&#26377;&#30340;AI&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21516;&#26102;&#25490;&#38500;&#38750;AI&#30340;&#20316;&#21697;&#12290;&#20851;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#30340;&#35780;&#20998;&#22522;&#20110;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal. Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI. In some cases, even works from mathematics, statistics, and engineering would be affected. Worryingly, AI misdefinitions are observed from Western societies to the Global South. In this paper, we propose a framework to score how \textit{validated as appropriately-defined for regulation} (VADER) an AI definition is. Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works. Regarding the latter, our score is based on a 
&lt;/p&gt;</description></item><item><title>SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2402.05044</link><description>&lt;p&gt;
SALAD-Bench: &#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#27425;&#21270;&#21644;&#20840;&#38754;&#24615;&#23433;&#20840;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05044
&lt;/p&gt;
&lt;p&gt;
SALAD-Bench&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#23433;&#20840;&#22522;&#20934;&#65292;&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#30340;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20197;&#21450;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#23545;LLMs&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#24378;&#22823;&#30340;&#23433;&#20840;&#25514;&#26045;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#20851;&#38190;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;LLMs&#12289;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#30340;&#23433;&#20840;&#22522;&#20934;&#65292;&#31216;&#20026;SALAD-Bench&#12290;SALAD-Bench&#36890;&#36807;&#20854;&#22823;&#35268;&#27169;&#12289;&#20016;&#23500;&#22810;&#26679;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#36328;&#19977;&#20010;&#23618;&#27425;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#22522;&#20934;&#12290;SALAD-Bench&#36890;&#36807;&#23545;&#26631;&#20934;&#26597;&#35810;&#21644;&#22797;&#26434;&#26597;&#35810;&#65288;&#21253;&#25324;&#25915;&#20987;&#12289;&#38450;&#24481;&#20462;&#25913;&#21644;&#22810;&#39033;&#36873;&#25321;&#65289;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#26377;&#25928;&#31649;&#29702;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#30830;&#20445;&#26080;&#32541;&#21487;&#38752;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#22120;&#65306;&#22522;&#20110;LLM&#30340;MD-Judge&#65292;&#19987;&#27880;&#20110;&#25915;&#20987;&#22686;&#24378;&#26597;&#35810;&#30340;&#38382;&#31572;&#23545;&#35780;&#20272;&#12290;&#20197;&#19978;&#32452;&#20214;&#23558;SALAD-Bench&#20174;&#26631;&#20934;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#25193;&#23637;&#21040;&#20102;LLM&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#35780;&#20272;&#65292;&#30830;&#20445;&#20102;&#32852;&#21512;&#30446;&#26631;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05027</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#29615;&#22659;&#32473;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#20998;&#25955;&#24335;&#26041;&#27861;&#20013;&#65292;Agent&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25805;&#20316;&#65292;&#24182;&#26681;&#25454;&#37096;&#20998;&#25110;&#36807;&#26102;&#30340;&#35266;&#23519;&#20570;&#20986;&#20915;&#31574;&#12290;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#30340;&#22823;&#23567;&#38480;&#21046;&#20102;&#22312;&#19981;&#21516;&#22270;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24433;&#21709;&#21040;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#30340;&#21160;&#20316;&#36136;&#37327;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#20449;&#24687;&#27969;&#35299;&#20915;&#20102;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#22823;&#23567;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#19982;&#29615;&#22659;&#30340;&#27493;&#39588;&#36845;&#20195;&#65292;&#24182;&#20801;&#35768;&#33410;&#28857;&#36890;&#36807;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#28040;&#24687;&#26469;&#21019;&#24314;&#22270;&#30340;&#20840;&#23616;&#34920;&#31034;&#12290;&#26681;&#25454;Agent&#22312;&#22270;&#20013;&#30340;&#20301;&#32622;&#65292;Agent&#25509;&#25910;&#21040;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#22270;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#19982;&#36873;&#25321;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05008</link><description>&lt;p&gt;
&#39640;&#25928;ViT-SAM: &#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05008
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;ViT-SAM&#26159;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#30340;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#24182;&#26367;&#25442;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;48.9&#20493;&#30340;&#21152;&#36895;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#8212;&#8212;&#39640;&#25928;ViT-SAM&#12290;&#25105;&#20204;&#20445;&#30041;&#20102;SAM&#30340;&#36731;&#37327;&#32423;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#25513;&#30721;&#35299;&#30721;&#22120;&#65292;&#21516;&#26102;&#29992;&#39640;&#25928;ViT&#26367;&#25442;&#20102;&#27785;&#37325;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#22312;&#35757;&#32451;&#26041;&#38754;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;SAM-ViT-H&#22270;&#20687;&#32534;&#30721;&#22120;&#21040;&#39640;&#25928;ViT&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;SA-1B&#25968;&#25454;&#38598;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#30001;&#20110;&#39640;&#25928;ViT&#30340;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#39640;&#25928;ViT-SAM&#22312;A100 GPU&#19978;&#30456;&#27604;SAM-ViT-H&#23454;&#29616;&#20102;48.9&#20493;&#30340;TensorRT&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#22312;https://github.com/mit-han-lab/efficientvit&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
&lt;/p&gt;</description></item><item><title>FairDebugger&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#30340;&#31995;&#32479;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#25214;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.05007</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Example-based Explanations for Random Forests using Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05007
&lt;/p&gt;
&lt;p&gt;
FairDebugger&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#30340;&#31995;&#32479;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#25214;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20363;&#22914;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#26131;&#20110;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#23427;&#20204;&#21463;&#21040;&#24191;&#27867;&#30340;&#27426;&#36814;&#21644;&#35748;&#21487;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20063;&#34987;&#21457;&#29616;&#20250;&#20135;&#29983;&#24847;&#22806;&#25110;&#20855;&#26377;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#37492;&#20110;&#23427;&#20204;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#26377;&#24517;&#35201;&#25214;&#20986;&#23427;&#20204;&#24847;&#22806;&#21644;&#20855;&#26377;&#27495;&#35270;&#24615;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#24179;&#24615;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#21644;&#35843;&#35797;&#22522;&#20110;&#26641;&#30340;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#24037;&#20316;&#36824;&#19981;&#22810;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FairDebugger&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#36776;&#35782;&#23548;&#33268;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#26524;&#20013;&#20844;&#24179;&#24615;&#36829;&#35268;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#30340;&#26032;&#22411;&#30740;&#31350;&#25104;&#26524;&#30340;&#31995;&#32479;&#12290;FairDebugger&#29983;&#25104;&#21069;-k&#20010;&#35299;&#37322;&#65288;&#20197;&#19968;&#33268;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#30340;&#24418;&#24335;&#65289;&#26469;&#35299;&#37322;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;FairDebugger&#39318;&#20808;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21435;&#36776;&#35782;&#20986;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree-based machine learning models, such as decision trees and random forests, have been hugely successful in classification tasks primarily because of their predictive power in supervised learning tasks and ease of interpretation. Despite their popularity and power, these models have been found to produce unexpected or discriminatory outcomes. Given their overwhelming success for most tasks, it is of interest to identify sources of their unexpected and discriminatory behavior. However, there has not been much work on understanding and debugging tree-based classifiers in the context of fairness.   We introduce FairDebugger, a system that utilizes recent advances in machine unlearning research to identify training data subsets responsible for instances of fairness violations in the outcomes of a random forest classifier. FairDebugger generates top-$k$ explanations (in the form of coherent training data subsets) for model unfairness. Toward this goal, FairDebugger first utilizes machine 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#30340;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;Edge&#35774;&#22791;&#19978;&#23454;&#29616;&#23545;&#24179;&#38754;&#32441;&#29702;&#32570;&#22833;&#30340;&#24037;&#19994;&#29289;&#20307;&#36827;&#34892;&#26368;&#20808;&#36827;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04979</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#22312;HoloLens&#19978;&#26816;&#27979;&#21644;&#23039;&#24577;&#20272;&#35745;&#24179;&#38754;&#32441;&#29702;&#32570;&#22833;&#30340;&#24037;&#19994;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
Detection and Pose Estimation of flat, Texture-less Industry Objects on HoloLens using synthetic Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#30340;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#33021;&#22815;&#22312;Edge&#35774;&#22791;&#19978;&#23454;&#29616;&#23545;&#24179;&#38754;&#32441;&#29702;&#32570;&#22833;&#30340;&#24037;&#19994;&#29289;&#20307;&#36827;&#34892;&#26368;&#20808;&#36827;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#23545;&#20110;Edge&#35774;&#22791;&#65292;&#22914;Microsoft HoloLens&#65288;2&#65289;&#25110;Apple iPad&#26469;&#35828;&#35745;&#31639;&#23494;&#38598;&#24230;&#22826;&#39640;&#65292;&#26080;&#27861;&#37096;&#32626;&#22312;&#19978;&#38754;&#12290;&#22686;&#24378;&#29616;&#23454;&#30340;&#36136;&#37327;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20854;&#26816;&#27979;&#21644;&#21472;&#21152;&#22330;&#26223;&#20013;&#30340;&#20960;&#20309;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23458;&#25143;&#31471;&#26381;&#21153;&#22120;&#30340;&#21512;&#25104;&#35757;&#32451;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#65292;&#22312;Edge&#35774;&#22791;&#19978;&#23637;&#31034;&#20102;&#23545;&#37329;&#23646;&#21644;&#32441;&#29702;&#32570;&#22833;&#30340;&#24037;&#19994;&#29289;&#20307;&#30340;&#26368;&#20808;&#36827;&#23039;&#24577;&#20272;&#35745;&#12290;&#21512;&#25104;&#25968;&#25454;&#20351;&#24471;&#21487;&#20197;&#23545;&#23578;&#26410;&#21046;&#36896;&#30340;&#29289;&#20307;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#30495;&#23454;&#29031;&#29255;&#12290;&#25105;&#20204;&#22312;AR&#36741;&#21161;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#24182;&#22312;&#28210;&#26579;&#21644;&#22312;HoloLens 2&#19978;&#35760;&#24405;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art 6d pose estimation is too compute intensive to be deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both used for an increasing number of augmented reality applications. The quality of AR is greatly dependent on its capabilities to detect and overlay geometry within the scene. We propose a synthetically trained client-server-based augmented reality application, demonstrating state-of-the-art object pose estimation of metallic and texture-less industry objects on edge devices. Synthetic data enables training without real photographs, i.e. for yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted sorting task, and quantitative evaluation on both renderings, as well as real-world data recorded on HoloLens 2, sheds light on its real-world applicability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;</title><link>https://arxiv.org/abs/2402.04978</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21327;&#20316;&#30340;&#22686;&#24378;&#22411;&#22522;&#20110;&#25552;&#31034;&#30340;LLM&#25512;&#29702;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#21512;&#20316;&#35757;&#32451;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#31561;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#30693;&#35782;&#25512;&#29702;&#21644;&#25512;&#29702;&#32467;&#26524;&#36861;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#26500;&#38382;&#39064;&#12289;&#30693;&#35782;&#26356;&#26032;&#19981;&#36275;&#20197;&#21450;&#25512;&#29702;&#36807;&#31243;&#30340;&#36879;&#26126;&#24230;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#21019;&#26032;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#33258;&#30001;&#30340;&#25512;&#29702;&#26041;&#26696;&#65292;&#20854;&#20013;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;LLMs&#20043;&#38388;&#23494;&#20999;&#21512;&#20316;&#12290;&#35813;&#26041;&#26696;&#39318;&#20808;&#20351;&#29992;LLMs&#36845;&#20195;&#22320;&#25506;&#32034;KG&#65292;&#36873;&#25321;&#24615;&#22320;&#26816;&#32034;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#30693;&#35782;&#23376;&#22270;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;&#28982;&#21518;&#24341;&#23548;LLMs&#36827;&#19968;&#27493;&#32452;&#21512;&#20869;&#22312;&#30340;&#38544;&#24335;&#30693;&#35782;&#65292;&#22312;&#23376;&#22270;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26126;&#30830;&#38416;&#36848;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#36825;&#31181;&#21327;&#20316;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#26356;&#21487;&#38752;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#65292;&#24182;&#20415;&#20110;&#36861;&#36394;&#25512;&#29702;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across
&lt;/p&gt;</description></item><item><title>ChatScratch&#26159;&#19968;&#31181;AI&#22686;&#24378;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;6-12&#23681;&#20799;&#31461;&#33258;&#20027;&#23398;&#20064;&#35270;&#35273;&#32534;&#31243;&#12290;&#23427;&#36890;&#36807;&#32467;&#26500;&#21270;&#20114;&#21160;&#25925;&#20107;&#26495;&#12289;&#25968;&#23383;&#32472;&#30011;&#21644;&#19987;&#19994;&#32534;&#30721;&#25351;&#23548;&#31561;&#26041;&#24335;&#35299;&#20915;&#20102;&#20799;&#31461;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21019;&#20316;&#22256;&#38590;&#21644;&#32534;&#30721;&#25351;&#23548;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04975</link><description>&lt;p&gt;
ChatScratch: &#19968;&#31181;&#38754;&#21521;6-12&#23681;&#20799;&#31461;&#30340;&#33258;&#20027;&#35270;&#35273;&#32534;&#31243;&#23398;&#20064;&#30340;AI&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04975
&lt;/p&gt;
&lt;p&gt;
ChatScratch&#26159;&#19968;&#31181;AI&#22686;&#24378;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;6-12&#23681;&#20799;&#31461;&#33258;&#20027;&#23398;&#20064;&#35270;&#35273;&#32534;&#31243;&#12290;&#23427;&#36890;&#36807;&#32467;&#26500;&#21270;&#20114;&#21160;&#25925;&#20107;&#26495;&#12289;&#25968;&#23383;&#32472;&#30011;&#21644;&#19987;&#19994;&#32534;&#30721;&#25351;&#23548;&#31561;&#26041;&#24335;&#35299;&#20915;&#20102;&#20799;&#31461;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21019;&#20316;&#22256;&#38590;&#21644;&#32534;&#30721;&#25351;&#23548;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#24605;&#32500;(CT)&#22312;K-12&#25945;&#32946;&#20013;&#32487;&#32493;&#26222;&#21450;&#65292;&#20687;Scratch&#36825;&#26679;&#30340;&#24050;&#24314;&#31435;&#30340;CT&#24179;&#21488;&#38754;&#20020;&#30528;&#36814;&#21512;&#36825;&#20123;&#24180;&#24188;&#23398;&#20064;&#32773;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23567;&#23398;&#29983;(6-12&#23681;)&#12290;&#36890;&#36807;&#19982;Scratch&#19987;&#23478;&#30340;&#24418;&#25104;&#24615;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#20799;&#31461;&#22312;&#33258;&#20027;&#23398;&#20064;Scratch&#36807;&#31243;&#20013;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#38556;&#30861;&#65306;&#39033;&#30446;&#35268;&#21010;&#20013;&#30340;&#21019;&#20316;&#38590;&#39064;&#12289;&#36164;&#28304;&#21019;&#24314;&#20013;&#30340;&#21019;&#36896;&#21147;&#21463;&#38480;&#20197;&#21450;&#23454;&#29616;&#36807;&#31243;&#20013;&#32570;&#20047;&#32534;&#30721;&#25351;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatScratch&#65292;&#19968;&#31181;AI&#22686;&#24378;&#31995;&#32479;&#65292;&#29992;&#20110;&#20419;&#36827;&#24180;&#24188;&#20799;&#31461;&#30340;&#33258;&#20027;&#32534;&#31243;&#23398;&#20064;&#12290;ChatScratch&#37319;&#29992;&#20102;&#32467;&#26500;&#21270;&#20114;&#21160;&#25925;&#20107;&#26495;&#21644;&#35270;&#35273;&#25552;&#31034;&#26469;&#20811;&#26381;&#21019;&#20316;&#38556;&#30861;&#65292;&#38598;&#25104;&#20102;&#25968;&#23383;&#32472;&#30011;&#21644;&#39640;&#32423;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#26469;&#25552;&#21319;&#21019;&#36896;&#21147;&#65292;&#24182;&#21033;&#29992;&#19987;&#38376;&#38024;&#23545;Scratch&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#25552;&#20379;&#19987;&#19994;&#30340;&#32534;&#30721;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;Scratch&#30456;&#27604;&#65292;ChatScratch&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20799;&#31461;&#30340;&#32534;&#31243;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children's autonomous Scratch learning: artist's block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist's block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.04971</link><description>&lt;p&gt;
&#22810;&#21457;&#20449;&#32773;&#35828;&#26381; - &#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-Sender Persuasion -- A Computational Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#20010;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#24433;&#21709;&#20854;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36890;&#36807;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#24050;&#26377;&#26041;&#27861;&#30340;&#23616;&#37096;&#22343;&#34913;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21040;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#22810;&#20010;&#21457;&#20449;&#32773;&#21521;&#21333;&#20010;&#33258;&#31169;&#34892;&#20026;&#32773;&#20256;&#36882;&#20449;&#21495;&#20197;&#20351;&#20854;&#37319;&#21462;&#26576;&#20123;&#34892;&#21160;&#12290;&#36825;&#20123;&#35774;&#32622;&#26159;&#35745;&#31639;&#32463;&#27982;&#23398;&#65292;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#21644;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26680;&#24515;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#26159;&#21457;&#20449;&#32773;&#20449;&#21495;&#31574;&#30053;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#25214;&#21040;&#19968;&#20010;&#22343;&#34913;&#26159;PPAD-Hard&#30340;;&#23454;&#38469;&#19978;&#65292;&#35745;&#31639;&#19968;&#20010;&#21457;&#20449;&#32773;&#30340;&#26368;&#20339;&#21709;&#24212;&#29978;&#33267;&#26159;NP-Hard&#30340;&#12290;&#37492;&#20110;&#36825;&#20123;&#22266;&#26377;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36716;&#32780;&#23547;&#25214;&#23616;&#37096;&#32435;&#20160;&#22343;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#35813;&#28216;&#25103;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#25928;&#29992;&#12290;&#32467;&#21512;&#39069;&#22806;&#26799;&#24230;&#31639;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36229;&#36234;&#23436;&#20840;&#23637;&#31034;&#22343;&#34913;&#21644;&#29616;&#26377;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#30340;&#23616;&#37096;&#22343;&#34913;&#12290;&#24191;&#20041;&#19978;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#36129;&#29486;&#23545;&#24191;&#27867;&#30340;&#31867;&#21035;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider multiple senders with informational advantage signaling to convince a single self-interested actor towards certain actions. Generalizing the seminal Bayesian Persuasion framework, such settings are ubiquitous in computational economics, multi-agent learning, and machine learning with multiple objectives. The core solution concept here is the Nash equilibrium of senders' signaling policies. Theoretically, we prove that finding an equilibrium in general is PPAD-Hard; in fact, even computing a sender's best response is NP-Hard. Given these intrinsic difficulties, we turn to finding local Nash equilibria. We propose a novel differentiable neural network to approximate this game's non-linear and discontinuous utilities. Complementing this with the extra-gradient algorithm, we discover local equilibria that Pareto dominates full-revelation equilibria and those found by existing neural networks. Broadly, our theoretical and empirical contributions are of interest to a large class 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20167;&#24680;&#36855;&#22240;&#30340;&#25991;&#26412;&#37096;&#20998;&#23545;&#20110;&#27867;&#21270;&#33267;&#19981;&#21516;&#39046;&#22495;&#30340;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22270;&#20687;&#37096;&#20998;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#25935;&#24863;&#12290;&#23454;&#39564;&#35777;&#26126;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#12290;&#40657;&#30418;&#35299;&#37322;&#34920;&#26126;&#25991;&#26412;&#27169;&#24577;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#36739;&#22823;&#65292;&#20294;&#24341;&#20837;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#12290;&#26032;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#35780;&#20272;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#34920;&#29616;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04967</link><description>&lt;p&gt;
&#25991;&#26412;&#36824;&#26159;&#22270;&#20687;&#65311;&#23545;&#20110;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#30340;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#27169;&#22411;&#26469;&#35828;&#65292;&#20160;&#20040;&#26356;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;&#20167;&#24680;&#36855;&#22240;&#30340;&#25991;&#26412;&#37096;&#20998;&#23545;&#20110;&#27867;&#21270;&#33267;&#19981;&#21516;&#39046;&#22495;&#30340;&#20998;&#31867;&#22120;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22270;&#20687;&#37096;&#20998;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#25935;&#24863;&#12290;&#23454;&#39564;&#35777;&#26126;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#12290;&#40657;&#30418;&#35299;&#37322;&#34920;&#26126;&#25991;&#26412;&#27169;&#24577;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36129;&#29486;&#36739;&#22823;&#65292;&#20294;&#24341;&#20837;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#12290;&#26032;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#35780;&#20272;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24335;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20013;&#36328;&#39046;&#22495;&#27867;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#35777;&#25454;&#25903;&#25345;&#20551;&#35774;&#65306;&#21482;&#26377;&#20167;&#24680;&#36855;&#22240;&#20013;&#30340;&#25991;&#26412;&#25104;&#20998;&#20351;&#24471;&#29616;&#26377;&#30340;&#22810;&#27169;&#24335;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#65292;&#32780;&#22270;&#20687;&#25104;&#20998;&#21017;&#23545;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38598;&#38750;&#24120;&#25935;&#24863;&#12290;&#35777;&#25454;&#21253;&#25324;&#28436;&#31034;&#65292;&#35777;&#26126;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#20167;&#24680;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#34920;&#29616;&#19982;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#23558;&#30001;&#36855;&#22240;&#22270;&#20687;&#29983;&#25104;&#30340;&#26631;&#39064;&#24341;&#20837;&#21040;&#20167;&#24680;&#36855;&#22240;&#20998;&#31867;&#22120;&#20013;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#24179;&#22343;F1&#25351;&#26631;&#32422;&#20026;0.02&#12290;&#36890;&#36807;&#40657;&#30418;&#35299;&#37322;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#26412;&#27169;&#24577;&#30340;&#37325;&#35201;&#36129;&#29486;&#65288;&#24179;&#22343;83%&#65289;&#65292;&#20294;&#24341;&#20837;&#36855;&#22240;&#22270;&#20687;&#26631;&#39064;&#21518;&#36129;&#29486;&#38477;&#20302;&#33267;52%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#26032;&#21019;&#24314;&#30340;&#28151;&#28102;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#25991;&#26412;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into the formidable challenge of cross-domain generalization in multimodal hate meme detection, presenting compelling findings. We provide enough pieces of evidence supporting the hypothesis that only the textual component of hateful memes enables the existing multimodal classifier to generalize across different domains, while the image component proves highly sensitive to a specific training dataset. The evidence includes demonstrations showing that hate-text classifiers perform similarly to hate-meme classifiers in a zero-shot setting. Simultaneously, the introduction of captions generated from images of memes to the hate-meme classifier worsens performance by an average F1 of 0.02. Through blackbox explanations, we identify a substantial contribution of the text modality (average of 83%), which diminishes with the introduction of meme's image captions (52%). Additionally, our evaluation on a newly created confounder dataset reveals higher performance on text confou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04955</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65306;&#27604;&#36739;&#24847;&#22270;&#21644;&#22522;&#20110;LLM&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Chatbots in Knowledge-Intensive Contexts: Comparing Intent and LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04955
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#21161;&#25163;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#20026;&#20154;&#31867;&#24037;&#20316;&#32773;&#25552;&#20379;&#19978;&#19979;&#25991;&#24863;&#30693;&#25903;&#25345;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#20256;&#32479;&#19978;&#65292;&#35748;&#30693;&#21161;&#25163;&#20197;&#29305;&#23450;&#26041;&#24335;&#22238;&#24212;&#39044;&#23450;&#20041;&#30340;&#29992;&#25143;&#24847;&#22270;&#21644;&#23545;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21018;&#24615;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#22788;&#29702;&#19981;&#22909;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#22914;GPT-4&#12289;Llama2&#21644;Gemini&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21487;&#33021;&#20351;&#35748;&#30693;&#21161;&#25163;&#33021;&#22815;&#20197;&#26356;&#28789;&#27963;&#12289;&#26356;&#20687;&#20154;&#31867;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39069;&#22806;&#30340;&#33258;&#30001;&#24230;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#29615;&#22659;&#20013;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;LLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#21644;&#22522;&#20110;&#24847;&#22270;&#30340;&#31995;&#32479;&#22312;&#20132;&#20114;&#25928;&#29575;&#12289;&#29992;&#25143;&#20307;&#39564;&#12289;&#24037;&#20316;&#37327;&#21644;&#21487;&#29992;&#24615;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#35748;&#30693;&#21161;&#25163;&#22312;&#29992;&#25143;&#20307;&#39564;&#12289;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#21487;&#29992;&#24615;&#21644;&#20010;&#20154;&#25928;&#33021;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive assistants (CA) are chatbots that provide context-aware support to human workers in knowledge-intensive tasks. Traditionally, cognitive assistants respond in specific ways to predefined user intents and conversation patterns. However, this rigidness does not handle the diversity of natural language well. Recent advances in natural language processing (NLP), powering large language models (LLM) such as GPT-4, Llama2, and Gemini, could enable CAs to converse in a more flexible, human-like manner. However, the additional degrees of freedom may have unforeseen consequences, especially in knowledge-intensive contexts where accuracy is crucial. As a preliminary step to assessing the potential of using LLMs in these contexts, we conducted a user study comparing an LLM-based CA to an intent-based system regarding interaction efficiency, user experience, workload, and usability. This revealed that LLM-based CAs exhibited better user experience, task completion rate, usability, and per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35270;&#39057;&#28216;&#25103;Beta&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#35270;&#39057;&#28216;&#25103;&#30340;&#29305;&#27530;&#24615;&#65292;&#20351;&#24471;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.04938</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#35270;&#39057;&#28216;&#25103;Beta&#27979;&#35797;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An approach to automated videogame beta testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35270;&#39057;&#28216;&#25103;Beta&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#35270;&#39057;&#28216;&#25103;&#30340;&#29305;&#27530;&#24615;&#65292;&#20351;&#24471;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#33021;&#22815;&#33258;&#21160;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1970&#24180;&#20195;&#21644;1980&#24180;&#20195;&#24320;&#21457;&#30340;&#35270;&#39057;&#28216;&#25103;&#26159;&#19968;&#20123;&#31616;&#21333;&#30340;&#31243;&#24207;&#65292;&#30001;&#19968;&#20010;&#20154;&#22312;&#20960;&#20010;&#26376;&#20869;&#23436;&#25104;&#65292;&#25198;&#28436;&#35774;&#35745;&#24072;&#12289;&#33402;&#26415;&#23478;&#21644;&#31243;&#24207;&#21592;&#30340;&#22810;&#37325;&#35282;&#33394;&#12290;&#20174;&#37027;&#26102;&#36215;&#65292;&#35270;&#39057;&#28216;&#25103;&#21457;&#23637;&#25104;&#20102;&#19968;&#20010;&#20215;&#20540;&#25968;&#30334;&#19975;&#32654;&#20803;&#30340;&#34892;&#19994;&#12290;&#22914;&#20170;&#65292;AAA&#28216;&#25103;&#30340;&#24320;&#21457;&#38656;&#35201;&#25968;&#30334;&#20154;&#22312;&#25968;&#24180;&#30340;&#26102;&#38388;&#20869;&#20849;&#21516;&#21512;&#20316;&#12290;&#31649;&#29702;&#21644;&#24037;&#31243;&#38656;&#27714;&#20063;&#38543;&#20043;&#25913;&#21464;&#12290;&#23613;&#31649;&#35768;&#22810;&#36807;&#31243;&#24050;&#32463;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36827;&#34892;&#20102;&#35843;&#25972;&#65292;&#20294;&#23545;&#20110;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#26469;&#35828;&#65292;&#24773;&#20917;&#24182;&#27809;&#26377;&#23436;&#20840;&#25913;&#21464;&#65292;&#36825;&#20123;&#20219;&#21153;&#20027;&#35201;&#20173;&#28982;&#30001;&#20154;&#24037;Beta&#27979;&#35797;&#20154;&#21592;&#25163;&#21160;&#23436;&#25104;&#65292;&#36825;&#26159;&#30001;&#20110;&#35270;&#39057;&#28216;&#25103;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;Beta&#27979;&#35797;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Videogames developed in the 1970s and 1980s were modest programs created in a couple of months by a single person, who played the roles of designer, artist and programmer. Since then, videogames have evolved to become a multi-million dollar industry. Today, AAA game development involves hundreds of people working together over several years. Management and engineering requirements have changed at the same pace. Although many of the processes have been adapted over time, this is not quite true for quality assurance tasks, which are still done mainly manually by human beta testers due to the specific peculiarities of videogames. This paper presents an approach to automate this beta testing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;ChatGPT&#23545;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#65292;&#23581;&#35797;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#36827;&#34892;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20173;&#38590;&#20197;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.04918</link><description>&lt;p&gt;
&#20419;&#20351;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#27880;&#37322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Prompting Implicit Discourse Relation Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;ChatGPT&#23545;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#65292;&#23581;&#35797;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#36827;&#34892;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20173;&#38590;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#34987;&#21457;&#29616;&#36229;&#36807;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;ChatGPT&#22312;&#36890;&#36807;&#26631;&#20934;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#24341;&#23548;&#30340;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#36828;&#36828;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#20960;&#31181;&#32463;&#36807;&#39564;&#35777;&#30340;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25913;&#21892;ChatGPT&#23545;&#35805;&#35821;&#20851;&#31995;&#30340;&#35782;&#21035;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#28041;&#21450;&#22823;&#37327;&#25277;&#35937;&#26631;&#31614;&#30340;&#20998;&#31867;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25512;&#29702;&#20934;&#30830;&#29575;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#38544;&#24335;&#35805;&#35821;&#20851;&#31995;&#20998;&#31867;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#23578;&#19981;&#21487;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#65292;&#22312;&#36275;&#29699;&#36187;&#23395;&#20013;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#65292;&#20943;&#23569;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#21644;&#26080;&#25928;&#33457;&#36153;&#65292;&#20026;&#30495;&#23454;&#36275;&#29699;&#22242;&#38431;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#29699;&#21592;&#31119;&#21033;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04898</link><description>&lt;p&gt;
&#25104;&#21151;&#30340;&#21387;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#36275;&#29699;&#36816;&#21160;&#21592;&#21463;&#20260;&#39118;&#38505;&#21644;&#25552;&#39640;&#29699;&#38431;&#25104;&#21151;&#29575;&#30340;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Strain of Success: A Predictive Model for Injury Risk Mitigation and Team Success in Soccer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#65292;&#22312;&#36275;&#29699;&#36187;&#23395;&#20013;&#20248;&#21270;&#22242;&#38431;&#34920;&#29616;&#65292;&#20943;&#23569;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#21644;&#26080;&#25928;&#33457;&#36153;&#65292;&#20026;&#30495;&#23454;&#36275;&#29699;&#22242;&#38431;&#38477;&#20302;&#25104;&#26412;&#24182;&#25552;&#39640;&#29699;&#21592;&#31119;&#21033;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36275;&#29699;&#39034;&#24207;&#22242;&#38431;&#36873;&#25321;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#30495;&#23454;&#36275;&#29699;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#29699;&#21592;&#29305;&#23450;&#20449;&#24687;&#23545;&#29699;&#21592;&#21463;&#20260;&#21644;&#19981;&#21487;&#29992;&#24615;&#30340;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#12290;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#34987;&#29992;&#20110;&#20026;&#28216;&#25103;&#36873;&#25321;&#22242;&#38431;&#65292;&#36890;&#36807;&#23545;&#29699;&#21592;&#21463;&#20260;&#27010;&#29575;&#36827;&#34892;&#25512;&#29702;&#65292;&#20248;&#21270;&#25972;&#20010;&#36187;&#23395;&#30340;&#22242;&#38431;&#32489;&#25928;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;2018/19&#33521;&#36229;&#36187;&#23395;&#30340;&#22522;&#20934;&#35299;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38477;&#20302;&#19968;&#32447;&#38431;&#21463;&#20260;&#20154;&#25968;&#32422;13%&#21644;&#26080;&#25928;&#33457;&#36153;&#22312;&#21463;&#20260;&#29699;&#21592;&#36523;&#19978;&#30340;&#36164;&#37329;&#32422;11%&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#22522;&#20934;&#30456;&#20284;&#30340;&#36187;&#23395;&#39044;&#26399;&#31215;&#20998; - &#36825;&#34920;&#26126;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36275;&#29699;&#22242;&#38431;&#20013;&#38477;&#20302;&#25104;&#26412;&#21644;&#25913;&#21892;&#29699;&#21592;&#31119;&#21033;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel sequential team selection model in soccer. Specifically, we model the stochastic process of player injury and unavailability using player-specific information learned from real-world soccer data. Monte-Carlo Tree Search is used to select teams for games that optimise long-term team performance across a soccer season by reasoning over player injury probability. We validate our approach compared to benchmark solutions for the 2018/19 English Premier League season. Our model achieves similar season expected points to the benchmark whilst reducing first-team injuries by ~13% and the money inefficiently spent on injured players by ~11% - demonstrating the potential to reduce costs and improve player welfare in real-world soccer teams.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04892</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Probabilistic Verification of AI Systems via Weighted Model Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#29575;&#39564;&#35777;AI&#31995;&#32479;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#24378;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24418;&#24335;&#39564;&#35777;&#65288;PFV) AI&#31995;&#32479;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23545;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#27169;&#22411;&#21644;/&#25110;&#23646;&#24615;&#65292;&#26041;&#27861;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31639;&#27861;&#32780;&#24050;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21152;&#26435;&#27169;&#22411;&#38598;&#25104;&#65288;WMI&#65289;&#30340;AI&#31995;&#32479;PFV&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#24120;&#36890;&#29992;&#22320;&#23450;&#20041;&#38382;&#39064;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#32422;&#31616;&#21487;&#20197;&#22312;&#19981;&#20570;&#36807;&#24378;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39564;&#35777;&#35768;&#22810;&#26377;&#36259;&#30340;&#23646;&#24615;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#21333;&#35843;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;WMI&#27714;&#35299;&#22120;&#35299;&#20915;&#22810;&#20010;&#39564;&#35777;&#20219;&#21153;&#26469;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#28982;&#21518;&#35752;&#35770;&#19982;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#30456;&#20851;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probabilistic formal verification (PFV) of AI systems is in its infancy. So far, approaches have been limited to ad-hoc algorithms for specific classes of models and/or properties.   We propose a unifying framework for the PFV of AI systems based onWeighted Model Integration (WMI), which allows to frame the problem in very general terms.   Crucially, this reduction enables the verification of many properties of interest, like fairness, robustness or monotonicity, over a wide range of machine learning models, without making strong distributional assumptions.   We support the generality of the approach by solving multiple verification tasks with a single, off-the-shelf WMI solver, then discuss the scalability challenges and research directions related to this promising framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.04888</link><description>&lt;p&gt;
RSCNet&#65306;&#20113;&#22522;WiFi&#24863;&#30693;&#30340;&#21160;&#24577;CSI&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
RSCNet: Dynamic CSI Compression for Cloud-based WiFi Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSCNet&#30340;&#21160;&#24577;CSI&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#26469;&#20943;&#23569;&#29289;&#32852;&#32593;&#35774;&#22791;&#21521;&#20113;&#26381;&#21153;&#22120;&#20256;&#36755;CSI&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#21644;&#20248;&#21270;&#30340;CSI&#31383;&#21475;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#24863;&#30693;&#21644;CSI&#37325;&#24314;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#30340;&#20113;&#22522;WiFi&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WiFi&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#27491;&#22312;&#20174;&#32431;&#31929;&#30340;&#36890;&#20449;&#35774;&#22791;&#21457;&#23637;&#20026;&#21033;&#29992;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#25552;&#21462;&#33021;&#21147;&#30340;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#26377;&#38480;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#35201;&#27714;&#23558;CSI&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#23613;&#31649;&#21487;&#34892;&#65292;&#20294;&#36825;&#20250;&#23548;&#33268;&#22823;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#26102;&#24863;&#30693;&#21644;&#21387;&#32553;&#32593;&#32476;&#65288;RSCNet&#65289;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21387;&#32553;CSI&#26469;&#23454;&#29616;&#24863;&#30693;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;RSCNet&#22312;&#30001;&#23569;&#37327;CSI&#24103;&#32452;&#25104;&#30340;CSI&#31383;&#21475;&#20043;&#38388;&#36827;&#34892;&#20248;&#21270;&#12290;&#19968;&#26086;&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#65292;&#23427;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21333;&#20803;&#20174;&#20808;&#21069;&#30340;&#31383;&#21475;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20174;&#32780;&#22686;&#24378;&#24863;&#30693;&#20934;&#30830;&#24615;&#21644;CSI&#37325;&#24314;&#12290;RSCNet&#24039;&#22937;&#22320;&#24179;&#34913;&#20102;CSI&#21387;&#32553;&#21644;&#24863;&#30693;&#31934;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23454;&#26102;&#20113;&#22522;WiFi&#24863;&#30693;&#65292;&#24182;&#20943;&#23569;&#20102;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
WiFi-enabled Internet-of-Things (IoT) devices are evolving from mere communication devices to sensing instruments, leveraging Channel State Information (CSI) extraction capabilities. Nevertheless, resource-constrained IoT devices and the intricacies of deep neural networks necessitate transmitting CSI to cloud servers for sensing. Although feasible, this leads to considerable communication overhead. In this context, this paper develops a novel Real-time Sensing and Compression Network (RSCNet) which enables sensing with compressed CSI; thereby reducing the communication overheads. RSCNet facilitates optimization across CSI windows composed of a few CSI frames. Once transmitted to cloud servers, it employs Long Short-Term Memory (LSTM) units to harness data from prior windows, thus bolstering both the sensing accuracy and CSI reconstruction. RSCNet adeptly balances the trade-off between CSI compression and sensing precision, thus streamlining real-time cloud-based WiFi sensing with redu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.04885</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#39640;&#26031;&#36807;&#31243;&#29992;&#20110;&#20998;&#25903;&#21644;&#23884;&#22871;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25104;&#21151;&#20013;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#36229;&#21442;&#25968;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#30452;&#25509;&#25511;&#21046;&#35757;&#32451;&#31639;&#27861;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#33719;&#24471;&#39640;&#25928;&#30340;&#35843;&#21442;&#65292;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#22522;&#20110;&#19968;&#20010;&#26041;&#20415;&#20294;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#65292;&#21363;&#35843;&#21442;&#21442;&#25968;&#24444;&#27492;&#29420;&#31435;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26465;&#20214;&#20381;&#36182;&#30340;&#35843;&#21442;&#21442;&#25968;&#26159;&#24120;&#35265;&#30340;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#35843;&#21442;&#21442;&#25968;&#65306;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#12290;&#23884;&#22871;&#21442;&#25968;&#25351;&#30340;&#26159;&#37027;&#20123;&#20165;&#23384;&#22312;&#20110;&#21478;&#19968;&#20010;&#35843;&#21442;&#21442;&#25968;&#29305;&#23450;&#35774;&#32622;&#20013;&#30340;&#35843;&#21442;&#21442;&#25968;&#65292;&#32780;&#20854;&#23427;&#21442;&#25968;&#22312;&#20854;&#20013;&#23884;&#22871;&#30340;&#21442;&#25968;&#31216;&#20026;&#20998;&#25903;&#21442;&#25968;&#12290;&#20026;&#20102;&#25429;&#25417;&#20998;&#25903;&#21644;&#23884;&#22871;&#21442;&#25968;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33033;&#20914;&#27169;&#22411;LMUFormer&#65292;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04882</link><description>&lt;p&gt;
LMUFormer&#65306;&#20855;&#26377;Legendre&#35760;&#24518;&#21333;&#20803;&#30340;&#20302;&#22797;&#26434;&#24230;&#20294;&#24378;&#22823;&#30340;&#33033;&#20914;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LMUFormer: Low Complexity Yet Powerful Spiking Model With Legendre Memory Units
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33033;&#20914;&#27169;&#22411;LMUFormer&#65292;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#22797;&#26434;&#24230;&#39640;&#19988;&#32570;&#20047;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35768;&#22810;&#36793;&#32536;&#27969;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#23558;Transformer&#27169;&#22411;&#37325;&#26032;&#23450;&#20041;&#20026;&#20855;&#26377;&#26174;&#24335;&#29366;&#24577;&#30340;RNN&#27169;&#22359;&#65292;&#26469;&#20462;&#25913;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#27169;&#22411;&#65306;&#24182;&#34892;&#35757;&#32451;&#12289;&#27969;&#24335;&#22788;&#29702;&#21644;&#20302;&#25104;&#26412;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;SOTA&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#24490;&#29615;&#27169;&#22411;&#36827;&#34892;&#26550;&#26500;&#20462;&#25913;&#65292;&#23558;&#20854;&#24615;&#33021;&#25512;&#21521;Transformer&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#39034;&#24207;&#22788;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21463;&#21040;&#20102;Legendre&#35760;&#24518;&#21333;&#20803;&#65288;LMU&#65289;&#22312;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;LMUFormer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and SOTA performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LM
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.04880</link><description>&lt;p&gt;
&#32467;&#21512;&#20113;&#35745;&#31639;&#19982;&#31227;&#21160;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Combining Cloud and Mobile Computing for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#20026;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#30340;&#35745;&#31639;&#65292;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#36127;&#25285;&#65292;&#24182;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#27491;&#22312;&#22686;&#21152;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#20063;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#36235;&#21183;&#32473;&#31227;&#21160;&#35774;&#22791;&#24102;&#26469;&#20102;&#38382;&#39064;&#65292;&#22914;&#20869;&#23384;&#23481;&#37327;&#21644;&#30005;&#27744;&#23551;&#21629;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#35768;&#22810;&#26381;&#21153;&#65288;&#22914;ChatGPT&#21644;Midjourney&#65289;&#22312;&#20113;&#20013;&#36816;&#34892;&#25152;&#26377;&#30340;&#25512;&#29702;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#28789;&#27963;&#24615;&#21644;&#32454;&#31890;&#24230;&#30340;&#20219;&#21153;&#20998;&#37197;&#26356;&#21487;&#21462;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#20998;&#21106;&#35270;&#20026;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#35745;&#31639;&#20998;&#21106;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#20113;&#20043;&#38388;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#35745;&#31639;&#23494;&#38598;&#37096;&#20998;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#21106;&#19981;&#20165;&#20943;&#23569;&#20102;&#29992;&#25143;&#31561;&#24453;&#26102;&#38388;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#32454;&#31890;&#24230;&#35843;&#25972;&#26469;&#20248;&#21270;&#20113;&#31471;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35843;&#24230;&#22120;&#65292;&#25910;&#38598;&#32593;&#32476;&#36136;&#37327;&#12289;&#23458;&#25143;&#31471;&#35774;&#22791;&#33021;&#21147;&#21644;&#20316;&#19994;&#35201;&#27714;&#30340;&#20449;&#24687;&#65292;&#20570;&#20986;&#20915;&#31574;&#20197;&#23454;&#29616;&#22312;&#21508;&#31181;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the computing power of mobile devices is increasing, machine learning models are also growing in size. This trend creates problems for mobile devices due to limitations like their memory capacity and battery life. While many services, like ChatGPT and Midjourney, run all the inferences in the cloud, we believe a flexible and fine-grained task distribution is more desirable. In this work, we consider model segmentation as a solution to improving the user experience, dividing the computation between mobile devices and the cloud in a way that offloads the compute-heavy portion of the model while minimizing the data transfer required. We show that the division not only reduces the wait time for users but can also be fine-tuned to optimize the workloads of the cloud. To achieve that, we design a scheduler that collects information about network quality, client device capability, and job requirements, making decisions to achieve consistent performance across a range of devices while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26512;&#21462;&#20986;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#21478;&#19968;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04874</link><description>&lt;p&gt;
&#36873;&#25321;&#20855;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Choosing a Classical Planner with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26512;&#21462;&#20986;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#21478;&#19968;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#26159;&#22312;&#32473;&#23450;&#35268;&#21010;&#38382;&#39064;&#30340;&#39044;&#23450;&#20041;&#35299;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#31639;&#22120;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#35268;&#21010;&#35745;&#31639;&#22797;&#26434;&#65292;&#35299;&#31639;&#22120;&#22312;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#35299;&#31639;&#22120;&#22312;&#32473;&#23450;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#23398;&#20064;&#26041;&#27861;&#34987;&#37319;&#29992;&#65292;&#20294;&#22312;&#32463;&#20856;&#30340;&#26368;&#20248;&#20195;&#20215;&#35268;&#21010;&#20013;&#65292;&#20027;&#27969;&#26041;&#27861;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32487;&#32493;&#20351;&#29992;GNN&#36827;&#34892;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;GNN&#27169;&#22411;&#12289;&#22270;&#34920;&#31034;&#21644;&#33410;&#28857;&#29305;&#24449;&#20197;&#21450;&#39044;&#27979;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;GNN&#33719;&#24471;&#30340;&#22270;&#34920;&#31034;&#20316;&#20026;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#31181;&#26356;&#21152;&#36164;&#28304;&#39640;&#25928;&#20294;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#36873;&#25321;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24320;&#36767;&#20102;&#26032;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online planner selection is the task of choosing a solver out of a predefined set for a given planning problem. As planning is computationally hard, the performance of solvers varies greatly on planning problems. Thus, the ability to predict their performance on a given problem is of great importance. While a variety of learning methods have been employed, for classical cost-optimal planning the prevailing approach uses Graph Neural Networks (GNNs). In this work, we continue the line of work on using GNNs for online planner selection. We perform a thorough investigation of the impact of the chosen GNN model, graph representation and node features, as well as prediction task. Going further, we propose using the graph representation obtained by a GNN as an input to the Extreme Gradient Boosting (XGBoost) model, resulting in a more resource-efficient yet accurate approach. We show the effectiveness of a variety of GNN-based online planner selection methods, opening up new exciting avenues
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.04870</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;
&lt;/p&gt;
&lt;p&gt;
Embedding Knowledge Graphs in Degenerate Clifford Algebras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#36864;&#21270;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#20013;&#12290;&#36890;&#36807;&#32771;&#34385;&#20855;&#26377;&#38646;&#24130;&#25351;&#25968;&#20026;2&#30340;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#24182;&#25429;&#25417;&#23454;&#20307;&#23884;&#20837;&#20013;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#27169;&#24335;&#12290;&#30740;&#31350;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#20195;&#25968;&#30340;&#21442;&#25968;&#65292;&#24182;&#35777;&#26126;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#26159;&#23454;&#25968;&#12289;&#22797;&#25968;&#21644;&#22235;&#20803;&#25968;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22312;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#32972;&#26223;&#19979;&#65292;&#21482;&#26377;&#24418;&#24335;&#20026;$Cl_{p,q}$&#65288;&#21363;&#27809;&#26377;&#38646;&#24130;&#22522;&#21521;&#37327;&#30340;&#20195;&#25968;&#65289;&#30340;&#20811;&#21033;&#31119;&#24503;&#20195;&#25968;&#21463;&#21040;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#38646;&#24130;&#22522;&#21521;&#37327;&#65292;&#20854;&#24130;&#25351;&#25968;&#20026;2&#12290;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#65292;&#34987;&#31216;&#20026;$Cl_{p,q,r}$&#65292;&#21487;&#20197;&#27867;&#21270;&#22522;&#20110;&#20108;&#27425;&#25968;&#30340;&#26041;&#27861;&#65288;&#26080;&#27861;&#20351;&#29992;$Cl_{p,q}$&#36827;&#34892;&#24314;&#27169;&#65289;&#24182;&#25429;&#25417;&#28304;&#20110;&#23454;&#25968;&#21644;&#22797;&#25968;&#37096;&#20998;&#38388;&#32570;&#20047;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#23454;&#20307;&#23884;&#20837;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#27169;&#22411;&#26469;&#21457;&#29616;&#21442;&#25968;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#20248;&#21270;$p$&#65292;$q$&#21644;$r$&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#22522;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#30340;&#36755;&#20837;&#30693;&#35782;&#22270;&#35889;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;$(p, q, r)$&#12290;&#25105;&#20204;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#24130;&#21521;&#37327;&#26377;&#21161;&#20110;&#25429;&#25417;&#23454;&#20307;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clifford algebras are a natural generalization of the real numbers, the complex numbers, and the quaternions. So far, solely Clifford algebras of the form $Cl_{p,q}$ (i.e., algebras without nilpotent base vectors) have been studied in the context of knowledge graph embeddings. We propose to consider nilpotent base vectors with a nilpotency index of two. In these spaces, denoted $Cl_{p,q,r}$, allows generalizing over approaches based on dual numbers (which cannot be modelled using $Cl_{p,q}$) and capturing patterns that emanate from the absence of higher-order interactions between real and complex parts of entity embeddings. We design two new models for the discovery of the parameters $p$, $q$, and $r$. The first model uses a greedy search to optimize $p$, $q$, and $r$. The second predicts $(p, q,r)$ based on an embedding of the input knowledge graph computed using neural networks. The results of our evaluation on seven benchmark datasets suggest that nilpotent vectors can help capture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#65292;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.04869</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#23454;&#36341;&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#30340;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning by Doing: An Online Causal Reinforcement Learning Framework with Causal-Aware Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#65292;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#20915;&#31574;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#25506;&#32034;&#21644;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#30693;&#35782;&#20316;&#20026;&#20154;&#31867;&#26234;&#33021;&#20013;&#30452;&#35266;&#35748;&#30693;&#21644;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20915;&#31574;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#24110;&#21161;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#21644;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#36827;&#20837;RL&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#22240;&#26524;&#20851;&#31995;RL&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22240;&#26524;&#22270;&#27169;&#22411;&#26126;&#30830;&#22320;&#24314;&#27169;&#29366;&#24577;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#32467;&#26500;&#26356;&#26032;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#20027;&#21160;&#29615;&#22659;&#24178;&#39044;&#23398;&#20064;&#30340;RL&#20132;&#20114;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#34893;&#29983;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#30340;&#26694;&#26550;&#65292;&#20004;&#20010;&#27493;&#39588;&#20132;&#26367;&#36827;&#34892;&#65306;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#24178;&#39044;&#36827;&#34892;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#65292;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#31574;&#30053;&#24341;&#23548;&#12290;&#30001;&#20110;&#32570;&#23569;&#20844;&#20849;&#22522;&#20934;&#65292;&#29992;&#20110;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a key component to intuitive cognition and reasoning solutions in human intelligence, causal knowledge provides great potential for reinforcement learning (RL) agents' interpretability towards decision-making by helping reduce the searching space. However, there is still a considerable gap in discovering and incorporating causality into RL, which hinders the rapid development of causal RL. In this paper, we consider explicitly modeling the generation process of states with the causal graphical model, based on which we augment the policy. We formulate the causal structure updating into the RL interaction process with active intervention learning of the environment. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventions for causal structure learning during exploration and using the learned causal structure for policy guidance during exploitation. Due to the lack of public benchmarks that 
&lt;/p&gt;</description></item><item><title>CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04858</link><description>&lt;p&gt;
CodeIt&#65306;&#20855;&#26377;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04858
&lt;/p&gt;
&lt;p&gt;
CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#36890;&#24120;&#34987;&#35748;&#20026;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36890;&#29992;&#26234;&#33021;&#22522;&#20934;&#27979;&#35797;&#20363;&#22914;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#34920;&#29616;&#20173;&#28982;&#38750;&#24120;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ARC&#35270;&#20026;&#19968;&#20010;&#20197;&#32534;&#31243;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Code Iteration&#65288;CodeIt&#65289;&#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;1&#65289;&#31243;&#24207;&#25277;&#26679;&#21644;&#22238;&#39038;&#37325;&#26631;&#35760;&#20197;&#21450;2&#65289;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#32463;&#39564;&#22238;&#25918;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;episode&#30340;&#30446;&#26631;&#65288;&#21363;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#31243;&#24207;&#36755;&#20986;&#65289;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#20135;&#29983;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#26497;&#24230;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#24212;&#29992;CodeIt&#20110;ARC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#12289;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;CodeIt&#26159;&#31532;&#19968;&#20010;&#31070;&#32463;&#20803;-&#21512;&#25104;&#26426;&#21046;&#19968;&#20307;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>&#32467;&#26500;&#21270;&#30340;d-DNNF&#19981;&#25903;&#25345;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#21542;&#23450;&#12289;&#26512;&#21462;&#25110;&#23384;&#22312;&#37327;&#21270;&#25805;&#20316;&#65292;&#22238;&#31572;&#20102;OBDD&#26159;&#21542;&#25903;&#25345;&#26356;&#21487;&#22788;&#29702;&#30340;&#36716;&#25442;&#38382;&#39064;&#12290;&#32467;&#26500;&#21270;&#30340;d-DNNF&#27604;SDD&#26356;&#31616;&#27905;&#65292;&#19981;&#23384;&#22312;&#31561;&#20215;&#30340;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;SDD&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.04832</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#30340;d-DNNF&#19981;&#25903;&#25345;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
Structured d-DNNF Is Not Closed Under Negation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04832
&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30340;d-DNNF&#19981;&#25903;&#25345;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#21542;&#23450;&#12289;&#26512;&#21462;&#25110;&#23384;&#22312;&#37327;&#21270;&#25805;&#20316;&#65292;&#22238;&#31572;&#20102;OBDD&#26159;&#21542;&#25903;&#25345;&#26356;&#21487;&#22788;&#29702;&#30340;&#36716;&#25442;&#38382;&#39064;&#12290;&#32467;&#26500;&#21270;&#30340;d-DNNF&#27604;SDD&#26356;&#31616;&#27905;&#65292;&#19981;&#23384;&#22312;&#31561;&#20215;&#30340;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;SDD&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30340;d-DNNF&#21644;SDD&#27604;OBDD&#26356;&#31616;&#27905;&#12290;&#27492;&#22806;&#65292;SDD&#30340;&#21487;&#22788;&#29702;&#24615;&#19982;OBDD&#22522;&#26412;&#30456;&#21516;&#12290;&#20294;&#36825;&#30041;&#19979;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;OBDD&#26159;&#21542;&#25903;&#25345;&#27604;&#32467;&#26500;&#21270;&#30340;d-DNNF&#26356;&#21487;&#22788;&#29702;&#30340;&#36716;&#25442;&#65311;&#20854;&#27425;&#65292;&#32467;&#26500;&#21270;&#30340;d-DNNF&#26159;&#21542;&#27604;SDD&#26356;&#31616;&#27905;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#38382;&#39064;&#32473;&#20104;&#20102;&#32943;&#23450;&#30340;&#22238;&#31572;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;OBDD&#19981;&#21516;&#65292;&#32467;&#26500;&#21270;&#30340;d-DNNF&#19981;&#25903;&#25345;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#21542;&#23450;&#12289;&#26512;&#21462;&#25110;&#23384;&#22312;&#37327;&#21270;&#25805;&#20316;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#23384;&#22312;&#19968;&#20123;&#20989;&#25968;&#65292;&#20854;&#31561;&#20215;&#30340;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#32467;&#26500;&#21270;d-DNNF&#34920;&#31034;&#19981;&#23384;&#22312;SDD&#34920;&#31034;&#65292;&#20174;&#32780;&#22238;&#31572;&#20102;&#31532;&#20108;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#20010;&#31532;&#20108;&#20010;&#32467;&#26524;&#23545;&#31639;&#26415;&#30005;&#36335;&#65288;AC&#65289;&#36827;&#34892;&#20102;&#25512;&#24191;&#65292;&#23637;&#31034;&#20102;PSDD&#21644;&#32467;&#26500;&#21270;d-DNNF&#30340;&#21333;&#35843;AC&#23545;&#24212;&#20043;&#38388;&#30340;&#31616;&#27905;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both structured d-DNNF and SDD can be exponentially more succinct than OBDD. Moreover, SDD is essentially as tractable as OBDD. But this has left two important open questions. Firstly, does OBDD support more tractable transformations than structured d-DNNF? And secondly, is structured d-DNNF more succinct than SDD? In this paper, we answer both questions in the affirmative. For the first question we show that, unlike OBDD, structured d-DNNF does not support polytime negation, disjunction, or existential quantification operations. As a corollary, we deduce that there are functions with an equivalent polynomial-sized structured d-DNNF but with no such representation as an SDD, thus answering the second question. We also lift this second result to arithmetic circuits (AC) to show a succinctness gap between PSDD and the monotone AC analogue to structured d-DNNF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.04792</link><description>&lt;p&gt;
&#26469;&#33258;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#30340;&#30452;&#25509;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Language Model Alignment from Online AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OAIF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#21453;&#39304;&#26469;&#25913;&#21892;DAP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30452;&#25509;&#23545;&#40784;&#20559;&#22909;&#65288;DAP&#65289;&#26041;&#27861;&#22914;DPO&#24050;&#25104;&#20026;&#23545;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65292;&#19981;&#35201;&#27714;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DAP&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#36890;&#24120;&#22312;&#35757;&#32451;&#20043;&#21069;&#25910;&#38598;&#65292;&#24182;&#19988;&#20174;&#19981;&#26356;&#26032;&#65292;&#22240;&#27492;&#21453;&#39304;&#32431;&#31929;&#26159;&#31163;&#32447;&#30340;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#22238;&#24212;&#36890;&#24120;&#26159;&#20174;&#19968;&#20010;&#19982;&#34987;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#30001;&#20110;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20250;&#21464;&#21270;&#65292;&#23545;&#40784;&#38454;&#27573;&#24517;&#28982;&#26159;&#38750;&#31574;&#30053;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#21453;&#39304;&#26159;&#20851;&#38190;&#65292;&#21487;&#20197;&#25913;&#21892;DAP&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#32447;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#65288;OAIF&#65289;&#65292;&#20351;&#29992;LLM&#20316;&#20026;&#26631;&#27880;&#22120;&#65306;&#22312;&#27599;&#20010;&#35757;&#32451;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#20174;&#24403;&#21069;&#27169;&#22411;&#20013;&#37319;&#26679;&#20004;&#20010;&#22238;&#24212;&#65292;&#24182;&#25552;&#31034;LLM&#26631;&#27880;&#22120;&#36873;&#25321;&#21738;&#20010;&#26356;&#21463;&#27426;&#36814;&#65292;&#20174;&#32780;&#25552;&#20379;&#22312;&#32447;&#21453;&#39304;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#20294;&#36890;&#36807;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;OAIF&#20248;&#20110;&#31163;&#32447;DAP&#21644;RLHF
&lt;/p&gt;
&lt;p&gt;
Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04788</link><description>&lt;p&gt;
MLLM&#20316;&#20026;&#27861;&#23448;&#65306;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#20934;&#35780;&#20272;&#22810;&#27169;&#24577;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;MLLM&#20316;&#20026;&#27861;&#23448;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#36741;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#24120;&#31867;&#20154;&#30340;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20808;&#36827;&#27169;&#22411;&#22914;GPT-4V&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#31561;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#23454;&#29992;&#24615;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#31526;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#12290;&#26412;&#25991;&#21463;&#21040;LLM&#27169;&#22411;&#20013;LLM&#20316;&#20026;&#27861;&#23448;&#30340;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#34987;&#31216;&#20026;MLLM&#20316;&#20026;&#27861;&#23448;&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#22312;&#21327;&#21161;&#27861;&#23448;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#35780;&#20998;&#35780;&#20272;&#12289;&#23545;&#27604;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;MLLM&#22312;&#23545;&#27604;&#35780;&#20272;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#31867;&#20154;&#36776;&#21035;&#33021;&#21147;&#65292;&#20294;&#22312;&#35780;&#20998;&#35780;&#20272;&#21644;&#25209;&#37327;&#25490;&#24207;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#20559;&#22909;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#20687;GPT-4V&#36825;&#26679;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;MLLM&#20173;&#28982;&#38754;&#20020;&#30528;&#21028;&#26029;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22810;&#26679;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#24335;&#30340;&#22238;&#31572;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#23545;MLLM&#30340;&#25913;&#36827;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence multimodal benchmarks that align with human preferences. Inspired by LLM-as-a-Judge in LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges including three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. Furthermore, MLLMs still face challenges in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further rese
&lt;/p&gt;</description></item><item><title>StableMask&#26159;&#19968;&#31181;&#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26080;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.04779</link><description>&lt;p&gt;
StableMask: &#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StableMask: Refining Causal Masking in Decoder-only Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04779
&lt;/p&gt;
&lt;p&gt;
StableMask&#26159;&#19968;&#31181;&#22312;&#20165;&#35299;&#30721;Transformer&#20013;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#30340;&#26080;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#65292;&#20165;&#35299;&#30721;Transformer&#26550;&#26500;&#20013;&#37319;&#29992;&#22240;&#26524;&#23631;&#34109;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPE&#65289;&#24050;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#36873;&#25321;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#65306;&#39318;&#20808;&#65292;&#21363;&#20351;&#24403;&#21069;&#23884;&#20837;&#20855;&#26377;&#36275;&#22815;&#30340;&#33258;&#21253;&#21547;&#20449;&#24687;&#65292;&#23427;&#35201;&#27714;&#25152;&#26377;&#27880;&#24847;&#21147;&#20998;&#25968;&#37117;&#20026;&#38750;&#38646;&#19988;&#24635;&#21644;&#20026;1&#12290;&#36825;&#24378;&#36843;&#27169;&#22411;&#23545;&#29305;&#23450;&#30340;&#26631;&#35760;&#20998;&#37197;&#19981;&#25104;&#27604;&#20363;&#30340;&#36807;&#24230;&#20851;&#27880;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;RPE&#30340;Transformer&#22312;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#27492;&#22312;&#20301;&#32622;&#20851;&#38190;&#20219;&#21153;&#20013;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;StableMask&#65306;&#19968;&#31181;&#26080;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22240;&#26524;&#23631;&#34109;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#23427;&#24341;&#20837;&#20102;&#20266;&#27880;&#24847;&#21147;&#20540;&#26469;&#24179;&#34913;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#30340;&#23631;&#34109;&#27604;&#29575;&#26469;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#20449;&#24687;&#12290;StableMask&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both 
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.04763</link><description>&lt;p&gt;
&#36827;&#21270;&#24322;&#36136;&#32676;&#38598;&#20013;&#30340;&#19987;&#38376;&#21270;&#38598;&#20307;&#34892;&#20026;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of specialized Collective Behaviors in Evolving Heterogeneous Swarms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04763
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#21160;&#29289;&#32676;&#20307;&#65292;&#22914;&#31038;&#20250;&#26118;&#34411;&#32676;&#20307;&#65292;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#20219;&#21153;&#19987;&#19994;&#21270;&#31243;&#24230;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#24182;&#29983;&#23384;&#19979;&#26469;&#12290;&#36825;&#26159;&#36890;&#36807;&#34920;&#22411;&#21487;&#22609;&#24615;&#25903;&#25345;&#30340;&#65306;&#20849;&#20139;&#30456;&#21516;&#22522;&#22240;&#22411;&#20294;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#20010;&#20307;&#34920;&#36798;&#19981;&#21516;&#30340;&#22522;&#22240;&#22411;&#65292;&#27599;&#20010;&#19987;&#38376;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#34920;&#22411;&#21487;&#22609;&#24615;&#28436;&#21270;&#20102;&#19968;&#32676;&#27169;&#25311;&#26426;&#22120;&#20154;&#65292;&#30740;&#31350;&#22312;&#32039;&#24613;&#24863;&#30693;&#20219;&#21153;&#20013;&#19987;&#38376;&#21270;&#38598;&#20307;&#34892;&#20026;&#30340;&#20986;&#29616;&#12290;&#34920;&#22411;&#21487;&#22609;&#24615;&#36890;&#36807;&#34892;&#20026;&#30340;&#24322;&#36136;&#24615;&#23454;&#29616;&#65292;&#23558;&#22522;&#22240;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#19982;&#19968;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30456;&#20851;&#32852;&#12290;&#25972;&#20010;&#22522;&#22240;&#22411;&#36890;&#36807;&#36825;&#20004;&#20010;&#37096;&#20998;&#34920;&#36798;&#25972;&#20010;&#32676;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#21333;&#19968;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#36827;&#34892;&#28436;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#33719;&#24471;&#30340;&#34892;&#20026;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#30340;&#35265;&#35299;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#35843;&#33410;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural groups of animals, such as swarms of social insects, exhibit astonishing degrees of task specialization, useful to address complex tasks and to survive. This is supported by phenotypic plasticity: individuals sharing the same genotype that is expressed differently for different classes of individuals, each specializing in one task. In this work, we evolve a swarm of simulated robots with phenotypic plasticity to study the emergence of specialized collective behavior during an emergent perception task. Phenotypic plasticity is realized in the form of heterogeneity of behavior by dividing the genotype into two components, with one different neural network controller associated to each component. The whole genotype, expressing the behavior of the whole group through the two components, is subject to evolution with a single fitness function. We analyse the obtained behaviors and use the insights provided by these results to design an online regulatory mechanism. Our experiments sho
&lt;/p&gt;</description></item><item><title>EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.04699</link><description>&lt;p&gt;
EvoSeed&#65306;&#25581;&#31034;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#24187;&#35273;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04699
&lt;/p&gt;
&lt;p&gt;
EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#25152;&#21033;&#29992;&#65292;&#36825;&#20123;&#26679;&#26412;&#23545;&#20154;&#31867;&#24863;&#30693;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30333;&#30418;&#24615;&#36136;&#26469;&#29983;&#25104;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#65292;&#25110;&#32773;&#25913;&#21464;&#23545;&#25239;&#26679;&#26412;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#32531;&#35299;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoSeed&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;EvoSeed&#26694;&#26550;&#20351;&#29992;&#36741;&#21161;&#25193;&#25955;&#21644;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;CMA-ES&#26469;&#20248;&#21270;&#23545;&#23545;&#25239;&#31181;&#23376;&#21521;&#37327;&#30340;&#25628;&#32034;&#65292;&#35813;&#21521;&#37327;&#22312;&#32463;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22788;&#29702;&#21518;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#26080;&#38480;&#21046;&#30340;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#29983;&#25104;&#30340;&#23545;&#25239;&#22270;&#20687;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04644</link><description>&lt;p&gt;
LEVI:&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#22312;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#24494;&#35843;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24494;&#35843;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65288;&#21363;&#65292;&#36229;&#20986;&#20998;&#24067;&#65307;OOD&#65289;&#19978;&#30340;&#27867;&#21270;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;OOB&#27867;&#21270;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#35843;&#25972;&#24494;&#35843;&#20197;&#20445;&#30041;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#38480;&#21046;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#34920;&#31034;&#21487;&#33021;&#20250;&#38459;&#30861;&#24494;&#35843;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#30340;&#37325;&#35201;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;OOB&#27867;&#21270;&#12290;&#24403;&#26032;&#20219;&#21153;&#26469;&#33258;&#20110;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#65288;&#23376;&#65289;&#39046;&#22495;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33021;&#22815;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#32447;&#32034;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04627</link><description>&lt;p&gt;
SPARQL&#29983;&#25104;&#65306;&#23545;OpenLLaMA&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#24494;&#35843;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04627
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33021;&#22815;&#36827;&#34892;&#24494;&#35843;&#65292;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#32447;&#32034;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#25104;&#21151;&#20026;&#22522;&#20110;LLM&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#20854;&#23454;&#26045;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;SPARQL&#26597;&#35810;&#30340;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38024;&#23545;&#29983;&#21629;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#23545;OpenLlama LLM&#36827;&#34892;&#24494;&#35843;&#30340;&#20960;&#31181;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#23637;&#24050;&#26377;&#26597;&#35810;&#38598;&#21512;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#32452;&#26356;&#22823;&#30340;&#35821;&#20041;&#20016;&#23500;&#30340;&#38382;&#39064;-SPARQL&#26597;&#35810;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#22312;&#36825;&#20123;&#23545;&#31232;&#32570;&#30340;&#25968;&#25454;&#38598;&#20013;&#20063;&#33021;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#26597;&#35810;&#20013;&#35821;&#20041;&#8220;&#32447;&#32034;&#8221;&#30340;&#20316;&#29992;&#65292;&#20363;&#22914;&#26377;&#24847;&#20041;&#30340;&#21464;&#37327;&#21517;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The recent success of Large Language Models (LLM) in a wide range of Natural Language Processing applications opens the path towards novel Question Answering Systems over Knowledge Graphs leveraging LLMs. However, one of the main obstacles preventing their implementation is the scarcity of training data for the task of translating questions into corresponding SPARQL queries, particularly in the case of domain-specific KGs. To overcome this challenge, in this study, we evaluate several strategies for fine-tuning the OpenLlama LLM for question answering over life science knowledge graphs. In particular, we propose an end-to-end data augmentation approach for extending a set of existing queries over a given knowledge graph towards a larger dataset of semantically enriched question-to-SPARQL query pairs, enabling fine-tuning even for datasets where these pairs are scarce. In this context, we also investigate the role of semantic "clues" in the queries, such as meaningful variable names and
&lt;/p&gt;</description></item><item><title>InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.04617</link><description>&lt;p&gt;
InfLLM: &#25581;&#31034;LLMs&#23545;&#20110;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26080;&#38656;&#35757;&#32451;&#30340;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04617
&lt;/p&gt;
&lt;p&gt;
InfLLM&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25581;&#31034;LLMs&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#23384;&#20648;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35745;&#31639;&#26426;&#21046;&#65292;&#20801;&#35768;LLMs&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#27969;&#24335;&#36755;&#20837;&#30340;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20855;&#26377;&#28459;&#38271;&#20256;&#36755;&#36755;&#20837;&#30340;&#29616;&#23454;&#24212;&#29992;&#30340;&#22522;&#30707;&#65292;&#22914;LLM&#39537;&#21160;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#21463;&#38480;&#26368;&#22823;&#38271;&#24230;&#24207;&#21015;&#19978;&#39044;&#35757;&#32451;&#30340;LLMs&#26080;&#27861;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#22806;&#21644;&#20998;&#25955;&#27880;&#24847;&#21147;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#28369;&#21160;&#27880;&#24847;&#21147;&#31383;&#21475;&#21644;&#20002;&#24323;&#36828;&#36317;&#31163;&#26631;&#35760;&#65292;&#20197;&#22788;&#29702;&#36229;&#38271;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#33719;&#24207;&#21015;&#20869;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#35821;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;InfLLM&#65292;&#26469;&#25581;&#31034;LLMs&#22788;&#29702;&#27969;&#24335;&#38271;&#24207;&#21015;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;InfLLM&#23558;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#23384;&#20648;&#21040;&#38468;&#21152;&#30340;&#20869;&#23384;&#21333;&#20803;&#20013;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#21046;&#26469;&#26597;&#25214;&#19982;&#27880;&#24847;&#35745;&#31639;&#30456;&#20851;&#30340;&#26631;&#35760;&#21333;&#20803;&#12290;&#22240;&#27492;&#65292;InfLLM&#20801;&#35768;LLMs&#39640;&#25928;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23545;&#35821;&#20041;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04615</link><description>&lt;p&gt;
ScreenAI: &#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ScreenAI: A Vision-Language Model for UI and Infographics Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04615
&lt;/p&gt;
&lt;p&gt;
ScreenAI&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#20462;&#34917;&#31574;&#30053;&#21644;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#20197;&#21450;&#38024;&#23545;UI&#20803;&#32032;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#30340;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23631;&#24149;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#21644;&#20449;&#24687;&#22270;&#34920;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#20154;&#26426;&#20132;&#20114;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#20849;&#20139;&#30456;&#20284;&#30340;&#35270;&#35273;&#35821;&#35328;&#21644;&#35774;&#35745;&#21407;&#21017;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ScreenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#29702;&#35299;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25913;&#36827;&#20102;PaLI&#26550;&#26500;&#65292;&#37319;&#29992;&#20102;pix2struct&#30340;&#28789;&#27963;&#20462;&#34917;&#31574;&#30053;&#65292;&#24182;&#32463;&#36807;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#26680;&#24515;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#23631;&#24149;&#27880;&#35299;&#20219;&#21153;&#65292;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;UI&#20803;&#32032;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25991;&#26412;&#27880;&#35299;&#26469;&#25551;&#36848;&#23631;&#24149;&#65292;&#24182;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#38382;&#31572;&#65288;QA&#65289;&#65292;UI&#23548;&#33322;&#21644;&#25688;&#35201;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#20197;&#23637;&#31034;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#22312;&#20165;&#26377;5B&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;ScreenAI&#22312;&#22522;&#20110;UI&#21644;&#20449;&#24687;&#22270;&#34920;&#30340;&#20219;&#21153;&#65288;&#22810;&#39029;&#25991;&#26723;VQA&#65292;WebSRC&#65292;MoTIF&#21644;Widget&#23383;&#24149;&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#24182;&#19988;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Screen user interfaces (UIs) and infographics, sharing similar visual language and design principles, play important roles in human communication and human-machine interaction. We introduce ScreenAI, a vision-language model that specializes in UI and infographics understanding. Our model improves upon the PaLI architecture with the flexible patching strategy of pix2struct and is trained on a unique mixture of datasets. At the heart of this mixture is a novel screen annotation task in which the model has to identify the type and location of UI elements. We use these text annotations to describe screens to Large Language Models and automatically generate question-answering (QA), UI navigation, and summarization training datasets at scale. We run ablation studies to demonstrate the impact of these design choices. At only 5B parameters, ScreenAI achieves new state-of-the-artresults on UI- and infographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget Captioning), and new best-in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04601</link><description>&lt;p&gt;
Alirector: &#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#20462;&#27491;&#65292;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;CGEC&#65289;&#22312;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26102;&#38754;&#20020;&#20005;&#37325;&#30340;&#36807;&#24230;&#20462;&#27491;&#25361;&#25112;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#23545;&#40784;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#21644;&#20165;&#35299;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#20915;&#36807;&#24230;&#20462;&#27491;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#20462;&#27491;&#27169;&#22411;&#65292;&#29983;&#25104;&#28304;&#21477;&#23376;&#30340;&#21021;&#22987;&#20462;&#27491;&#12290;&#28982;&#21518;&#65292;&#23558;&#28304;&#21477;&#23376;&#19982;&#21021;&#22987;&#20462;&#27491;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#21478;&#19968;&#36718;&#20462;&#27491;&#65292;&#20197;&#20419;&#20351;&#23545;&#40784;&#27169;&#22411;&#19987;&#27880;&#20110;&#28508;&#22312;&#30340;&#36807;&#24230;&#20462;&#27491;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#35782;&#21035;&#32454;&#24494;&#24046;&#21035;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#28304;&#21477;&#23376;&#21644;&#21021;&#22987;&#20462;&#27491;&#30340;&#36870;&#21521;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#40784;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;CGEC&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#20462;&#27491;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese grammatical error correction (CGEC) faces serious overcorrection challenges when employing autoregressive generative models such as sequence-to-sequence (Seq2Seq) models and decoder-only large language models (LLMs). While previous methods aim to address overcorrection in Seq2Seq models, they are difficult to adapt to decoder-only LLMs. In this paper, we propose an alignment-enhanced corrector for the overcorrection problem that applies to both Seq2Seq models and decoder-only LLMs. Our method first trains a correction model to generate an initial correction of the source sentence. Then, we combine the source sentence with the initial correction and feed it through an alignment model for another round of correction, aiming to enforce the alignment model to focus on potential overcorrection. Moreover, to enhance the model's ability to identify nuances, we further explore the reverse alignment of the source sentence and the initial correction. Finally, we transfer the alignment kn
&lt;/p&gt;</description></item><item><title>JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04599</link><description>&lt;p&gt;
&#35265; JEANIE&#65306;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30456;&#20284;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04599
&lt;/p&gt;
&lt;p&gt;
JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24207;&#21015;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24178;&#25200;&#24615;&#21464;&#21270;&#65292;&#21253;&#25324;&#21160;&#20316;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#20027;&#20307;&#23039;&#21183;&#65292;&#23548;&#33268;&#22312;&#27604;&#36739;&#20004;&#32452;&#24103;&#25110;&#35780;&#20272;&#20004;&#20010;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#26102;&#20135;&#29983;&#26102;&#38388;-&#35270;&#35282;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#23545;&#27604;&#30340;&#32852;&#21512;&#26102;&#38388;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#23545;&#40784;&#26041;&#27861;&#65288;JEANIE&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#33021;&#22815;&#22312;&#19977;&#32500;&#20013;&#36731;&#26494;&#25805;&#20316;&#25668;&#20687;&#26426;&#21644;&#20027;&#20307;&#23039;&#21183;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#65288;FSAR&#65289;&#19978;&#35780;&#20272;&#20102;JEANIE&#65292;&#20854;&#20013;&#30001;&#20110;&#26032;&#31867;&#21035;&#26679;&#26412;&#26377;&#38480;&#65292;&#36890;&#36807;&#21305;&#37197;&#22909;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#65288;&#32452;&#25104;&#24207;&#21015;&#30340;&#26102;&#38388;&#22359;&#65289;&#26469;&#25490;&#38500;&#24178;&#25200;&#21464;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#26597;&#35810;&#24207;&#21015;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#25668;&#20687;&#26426;&#20301;&#32622;&#21019;&#24314;&#22810;&#20010;&#35270;&#35282;&#12290;&#23545;&#20110;&#25903;&#25345;&#24207;&#21015;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27169;&#25311;&#20986;&#30340;&#26597;&#35810;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#25903;&#25345;&#26102;&#38388;&#22359;&#21487;&#20197;&#19982;&#35270;&#35282;&#27169;&#25311;&#30340;&#26597;&#35810;&#24207;&#21015;&#21305;&#37197;&#65292;&#22914;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CMSA&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36719;&#20214;&#20135;&#21697;&#32447;&#20013;&#30340;&#20248;&#20808;&#32423;&#37197;&#23545;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#23454;&#20363;&#35268;&#27169;&#36739;&#22823;&#21644;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35745;&#31639;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04597</link><description>&lt;p&gt;
&#22312;&#36719;&#20214;&#20135;&#21697;&#32447;&#20013;&#27714;&#35299;&#20248;&#20808;&#32423;&#37197;&#23545;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#30340;CMSA&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CMSA algorithm for solving the prioritized pairwise test data generation problem in software product lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CMSA&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36719;&#20214;&#20135;&#21697;&#32447;&#20013;&#30340;&#20248;&#20808;&#32423;&#37197;&#23545;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#23454;&#20363;&#35268;&#27169;&#36739;&#22823;&#21644;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35745;&#31639;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#20135;&#21697;&#32447;(SPLs)&#20013;&#65292;&#30001;&#20110;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26377;&#25928;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#27979;&#35797;&#25972;&#20010;&#20135;&#21697;&#26063;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19968;&#20010;&#26368;&#23567;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#27979;&#35797;&#25152;&#26377;&#36825;&#20123;&#21487;&#33021;&#30340;&#32452;&#21512;(&#37197;&#23545;)&#12290;&#27492;&#22806;&#65292;&#24403;&#27979;&#35797;&#21333;&#20010;&#20135;&#21697;&#38656;&#35201;&#24456;&#22823;&#30340;&#24037;&#20316;&#37327;&#26102;&#65292;&#39318;&#20808;&#27979;&#35797;&#30001;&#19968;&#32452;&#20248;&#20808;&#32423;&#29305;&#24449;&#32452;&#25104;&#30340;&#20135;&#21697;&#26159;&#21487;&#21462;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#20248;&#20808;&#32423;&#37197;&#23545;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#23545;&#20110;&#23567;&#21644;&#20013;&#31561;&#35268;&#27169;&#30340;&#23454;&#20363;&#26469;&#35828;&#36275;&#22815;&#24555;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20505;&#36873;&#35299;&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#23384;&#22312;&#19968;&#20123;&#22826;&#22823;&#26080;&#27861;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35745;&#31639;&#36825;&#20123;&#31639;&#27861;&#30340;&#23454;&#38469;&#23454;&#20363;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#24182;&#19981;&#24635;&#26159;&#33021;&#24102;&#25105;&#20204;&#25214;&#21040;&#26368;&#22909;&#30340;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;CMSA&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Software Product Lines (SPLs) it may be difficult or even impossible to test all the products of the family because of the large number of valid feature combinations that may exist. Thus, we want to find a minimal subset of the product family that allows us to test all these possible combinations (pairwise). Furthermore, when testing a single product is a great effort, it is desirable to first test products composed of a set of priority features. This problem is called Prioritized Pairwise Test Data Generation Problem.   State-of-the-art algorithms based on Integer Linear Programming for this problema are faster enough for small and medium instances. However, there exists some real instances that are too large to be computed with these algorithms in a reasonable time because of the exponential growth of the number of candidate solutions. Also, these heuristics not always lead us to the best solutions. In this work we propose a new approach based on a hybrid metaheuristic algorithm c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#35299;&#20915;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04596</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#30340;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#25913;&#36827;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Imbalance Robustness in Continual Multi-Label Learning with Dual Output Spiking Architecture (DOSA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#35299;&#20915;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20856;&#22411;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#21482;&#33021;&#20174;&#22266;&#23450;&#30340;&#26679;&#26412;&#21644;&#26631;&#31614;&#38598;&#20013;&#23398;&#20064;&#65292;&#36825;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#20013;&#25968;&#25454;&#20197;&#26679;&#26412;&#27969;&#30340;&#24418;&#24335;&#21040;&#36798;&#65292;&#24182;&#19988;&#24448;&#24448;&#38543;&#26102;&#38388;&#20851;&#32852;&#30528;&#22810;&#20010;&#26631;&#31614;&#12290;&#36825;&#20419;&#20351;&#30740;&#31350;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20855;&#26377;&#36739;&#22823;&#30340;&#35745;&#31639;&#37327;&#12290;&#34429;&#28982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#29616;&#26377;&#25991;&#29486;&#23578;&#26410;&#23558;&#20854;&#29992;&#20110;&#36830;&#32493;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#20934;&#30830;&#22320;&#30830;&#23450;SNNs&#30340;&#22810;&#20010;&#26631;&#31614;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#36755;&#20986;&#33033;&#20914;&#26550;&#26500;&#65288;DOSA&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#30740;&#31350;&#24046;&#36317;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#26631;&#35760;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms designed for addressing typical supervised classification problems can only learn from a fixed set of samples and labels, making them unsuitable for the real world, where data arrives as a stream of samples often associated with multiple labels over time. This motivates the study of task-agnostic continual multi-label learning problems. While algorithms using deep learning approaches for continual multi-label learning have been proposed in the recent literature, they tend to be computationally heavy. Although spiking neural networks (SNNs) offer a computationally efficient alternative to artificial neural networks, existing literature has not used SNNs for continual multi-label learning. Also, accurately determining multiple labels with SNNs is still an open research problem. This work proposes a dual output spiking architecture (DOSA) to bridge these research gaps. A novel imbalance-aware loss function is also proposed, improving the multi-label classification performance o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04580</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04580
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#20154;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20174;&#30446;&#26631;&#39046;&#22495;&#37319;&#38598;&#26080;&#20559;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#20174;&#28304;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#12290;&#21516;&#26102;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#38382;&#39064;&#35774;&#32622;&#19979;&#30340;&#35774;&#35745;&#32771;&#34385;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#34028;&#21187;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#21644;&#20005;&#26684;&#30340;&#23433;&#20840;&#35201;&#27714;&#65292;&#20174;&#30446;&#26631;&#39046;&#22495;&#25910;&#38598;&#36275;&#22815;&#30340;&#26080;&#20559;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#37319;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#29615;&#22659;&#65289;&#65292;&#20197;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#24555;&#36895;&#27169;&#22411;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28304;&#39046;&#22495;&#30340;&#29615;&#22659;&#21644;&#20855;&#36523;&#26041;&#24335;&#21487;&#33021;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#29305;&#24449;&#30456;&#24046;&#24456;&#22823;&#65292;&#24378;&#35843;&#20102;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#36328;&#39046;&#22495;&#31574;&#30053;&#36716;&#31227;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#36890;&#36807;&#23545;&#39046;&#22495;&#24046;&#36317;&#30340;&#31934;&#32454;&#20998;&#31867;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#20010;&#38382;&#39064;&#35774;&#32622;&#30340;&#24635;&#20307;&#35265;&#35299;&#21644;&#35774;&#35745;&#32771;&#34385;&#12290;&#25105;&#20204;&#36824;&#23601;&#20351;&#29992;&#30340;&#20851;&#38190;&#26041;&#27861;&#36827;&#34892;&#20102;&#39640;&#23618;&#27425;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used
&lt;/p&gt;</description></item><item><title>S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.04578</link><description>&lt;p&gt;
S-Agents: &#33258;&#32452;&#32455;&#20195;&#29702;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
S-Agents: self-organizing agents in open-ended environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04578
&lt;/p&gt;
&lt;p&gt;
S-Agents&#26159;&#19968;&#20010;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#29702;&#26641;&#32467;&#26500;&#12289;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#21644;&#38750;&#38459;&#22622;&#21327;&#20316;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#39640;&#25928;&#21327;&#35843;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#20248;&#21270;&#21327;&#20316;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#33258;&#20027;&#20195;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#65292;&#20855;&#22791;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26080;&#38480;&#29615;&#22659;&#20013;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#20248;&#21270;&#21327;&#20316;&#38656;&#35201;&#28789;&#27963;&#30340;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#24378;&#35843;&#22266;&#23450;&#30340;&#12289;&#20219;&#21153;&#23548;&#21521;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#24573;&#35270;&#20102;&#20197;&#20195;&#29702;&#20026;&#20013;&#24515;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#21463;&#20154;&#31867;&#32452;&#32455;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#20195;&#29702;&#31995;&#32479;&#65288;S-Agents&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#24037;&#20316;&#27969;&#31243;&#30340;&#8220;&#20195;&#29702;&#26641;&#8221;&#32467;&#26500;&#12289;&#24179;&#34913;&#20449;&#24687;&#20248;&#20808;&#32423;&#30340;&#8220;&#27801;&#28431;&#20195;&#29702;&#26550;&#26500;&#8221;&#20197;&#21450;&#20801;&#35768;&#20195;&#29702;&#20043;&#38388;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#30340;&#8220;&#38750;&#38459;&#22622;&#21327;&#20316;&#8221;&#26041;&#27861;&#12290;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#33258;&#20027;&#21327;&#35843;&#19968;&#32452;&#20195;&#29702;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#26080;&#38480;&#19988;&#21160;&#24577;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;S-Agents&#33021;&#22815;&#29087;&#32451;&#22320;&#25191;&#34892;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#21644;&#36164;&#28304;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging large language models (LLMs), autonomous agents have significantly improved, gaining the ability to handle a variety of tasks. In open-ended settings, optimizing collaboration for efficiency and effectiveness demands flexible adjustments. Despite this, current research mainly emphasizes fixed, task-oriented workflows and overlooks agent-centric organizational structures. Drawing inspiration from human organizational behavior, we introduce a self-organizing agent system (S-Agents) with a "tree of agents" structure for dynamic workflow, an "hourglass agent architecture" for balancing information priorities, and a "non-obstructive collaboration" method to allow asynchronous task execution among agents. This structure can autonomously coordinate a group of agents, efficiently addressing the challenges of an open and dynamic environment without human intervention. Our experiments demonstrate that S-Agents proficiently execute collaborative building tasks and resource collection i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#34892;&#20026;&#29305;&#24449;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.04567</link><description>&lt;p&gt;
OIL-AD: &#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OIL-AD: An Anomaly Detection Framework for Sequential Decision Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#34892;&#20026;&#29305;&#24449;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#27491;&#24120;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#21644;&#20219;&#21153;&#30340;&#39034;&#24207;&#24615;&#36136;&#12290;&#22823;&#37096;&#20998;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#23545;&#29615;&#22659;&#21160;&#24577;&#12289;&#22870;&#21169;&#20449;&#21495;&#21644;&#19982;&#29615;&#22659;&#30340;&#22312;&#32447;&#20132;&#20114;&#31561;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#65288;OIL-AD&#65289;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20004;&#20010;&#25552;&#21462;&#30340;&#34892;&#20026;&#29305;&#24449;&#65288;&#21160;&#20316;&#20248;&#21270;&#21644;&#39034;&#24207;&#20851;&#32852;&#65289;&#26469;&#26816;&#27979;&#20915;&#31574;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#12290;&#25105;&#20204;&#30340;&#31163;&#32447;&#23398;&#20064;&#27169;&#22411;&#26159;&#22522;&#20110;&#21464;&#21387;&#22120;&#31574;&#30053;&#32593;&#32476;&#30340;&#34892;&#20026;&#20811;&#38534;&#30340;&#36866;&#24212;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#20174;&#27491;&#24120;&#36712;&#36857;&#20013;&#23398;&#20064;Q&#20989;&#25968;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;Q&#20989;&#25968;&#21644;&#29366;&#24577;&#20540;&#20989;&#25968;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24322;&#24120;&#30340;&#36275;&#22815;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in decision-making sequences is a challenging problem due to the complexity of normality representation learning and the sequential nature of the task. Most existing methods based on Reinforcement Learning (RL) are difficult to implement in the real world due to unrealistic assumptions, such as having access to environment dynamics, reward signals, and online interactions with the environment. To address these limitations, we propose an unsupervised method named Offline Imitation Learning based Anomaly Detection (OIL-AD), which detects anomalies in decision-making sequences using two extracted behaviour features: action optimality and sequential association. Our offline learning model is an adaptation of behavioural cloning with a transformer policy network, where we modify the training process to learn a Q function and a state value function from normal trajectories. We propose that the Q function and the state value function can provide sufficient information about 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25552;&#20379;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#65292;&#20197;&#20415;&#20805;&#20998;&#21033;&#29992;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04563</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;CAM&#65306;&#33258;&#27880;&#24847;&#21147;&#25351;&#23548;&#19979;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25552;&#20379;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#65292;&#20197;&#20415;&#20805;&#20998;&#21033;&#29992;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;ViT&#26550;&#26500;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#38656;&#35201;&#36866;&#24403;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#23450;&#20301;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#20854;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#30446;&#21069;&#22312;CNN&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#36825;&#20123;&#26041;&#27861;&#22312;ViT&#20013;&#20173;&#28982;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;ViT&#30340;&#27880;&#24847;&#21147;&#24341;&#23548;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20026;&#20854;&#20915;&#31574;&#25552;&#20379;&#39640;&#32423;&#35821;&#20041;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36873;&#25321;&#24615;&#22320;&#32858;&#21512;&#20174;&#20998;&#31867;&#36755;&#20986;&#30452;&#25509;&#20256;&#25773;&#21040;&#27599;&#20010;&#33258;&#27880;&#24847;&#21147;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#25910;&#38598;&#20174;&#36755;&#20837;&#22270;&#20687;&#30340;&#27599;&#20010;&#20301;&#32622;&#25552;&#21462;&#30340;&#22270;&#20687;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#36825;&#20123;&#26799;&#24230;&#36824;&#21463;&#21040;&#26631;&#20934;&#21270;&#30340;&#33258;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#25351;&#23548;&#65292;&#36825;&#20123;&#24471;&#20998;&#26159;&#25104;&#23545;&#30340;&#34917;&#19969;&#30456;&#20851;&#24615;&#24471;&#20998;&#12290;&#23427;&#20204;&#29992;&#20110;&#26377;&#25928;&#26816;&#27979;&#34917;&#19969;&#32423;&#19978;&#19979;&#25991;&#20449;&#24687;&#19978;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.04559</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Model Agents Simulate Human Trust Behaviors?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04559
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#34920;&#29616;&#20986;&#22312;&#20449;&#20219;&#28216;&#25103;&#20013;&#30340;&#20449;&#20219;&#34892;&#20026;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#34892;&#20026;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#20559;&#35265;&#21644;&#23545;&#20195;&#29702;&#19982;&#20154;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#20316;&#20026;&#27169;&#25311;&#24037;&#20855;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#22312;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;LLM&#20195;&#29702;&#26159;&#21542;&#30495;&#30340;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20154;&#31867;&#20114;&#21160;&#20013;&#26368;&#20851;&#38190;&#30340;&#34892;&#20026;&#20043;&#19968;&#65292;&#20449;&#20219;&#65292;&#26088;&#22312;&#35843;&#26597;LLM&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#65292;&#22312;&#34987;&#34892;&#20026;&#32463;&#27982;&#23398;&#24191;&#27867;&#25509;&#21463;&#30340;&#20449;&#20219;&#28216;&#25103;&#26694;&#26550;&#19979;&#65292;LLM&#20195;&#29702;&#36890;&#24120;&#34920;&#29616;&#20986;&#20449;&#20219;&#34892;&#20026;&#65292;&#31216;&#20026;&#20195;&#29702;&#20449;&#20219;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#20195;&#29702;&#22312;&#20449;&#20219;&#34892;&#20026;&#26041;&#38754;&#19982;&#20154;&#31867;&#20855;&#26377;&#36739;&#39640;&#30340;&#34892;&#20026;&#19968;&#33268;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#30340;&#20449;&#20219;&#34892;&#20026;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20195;&#29702;&#20449;&#20219;&#20013;&#30340;&#20559;&#35265;&#20197;&#21450;&#20195;&#29702;&#20449;&#20219;&#22312;&#23545;&#20195;&#29702;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21253;&#25324;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#22312;&#20869;&#30340;&#26465;&#20214;&#19979;&#20195;&#29702;&#20449;&#20219;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#20316;&#20026;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#65292;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#21516;&#26102;&#20173;&#20445;&#25345;</title><link>https://arxiv.org/abs/2402.04539</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#20214;&#33258;&#29983;&#25104;&#30340;&#24341;&#23548;&#23398;&#20064;&#22810;&#26679;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Policies with Soft Self-Generated Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#20316;&#20026;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#65292;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#21516;&#26102;&#20173;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#31232;&#30095;&#21644;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#22870;&#21169;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#20960;&#20046;&#24456;&#23569;&#33021;&#22815;&#33719;&#24471;&#38750;&#38646;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#35745;&#31639;&#30340;&#26799;&#24230;&#21487;&#33021;&#26159;&#38543;&#26426;&#30340;&#19988;&#32570;&#20047;&#26377;&#25928;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#32463;&#39564;&#30340;&#20869;&#23384;&#32531;&#20914;&#21306;&#21487;&#20197;&#20351;&#23398;&#20064;&#36807;&#31243;&#26356;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#36825;&#20123;&#32463;&#39564;&#24517;&#39035;&#25104;&#21151;&#65292;&#24182;&#21487;&#33021;&#36807;&#24230;&#21033;&#29992;&#23427;&#20204;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#37319;&#21462;&#27425;&#20248;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#36827;&#34892;&#26356;&#24555;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#20351;&#36825;&#20123;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#25110;&#27809;&#26377;&#39640;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#31574;&#30053;&#25913;&#36827;&#27493;&#39588;&#21644;&#20351;&#29992;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#30340;&#39069;&#22806;&#25506;&#32034;&#27493;&#39588;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65292;&#23558;&#22810;&#26679;&#21270;&#30340;&#36807;&#21435;&#36712;&#36857;&#35270;&#20026;&#24341;&#23548;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#31574;&#30053;&#36319;&#38543;&#21644;&#25193;&#23637;&#36807;&#21435;&#30340;&#36712;&#36857;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.04536</link><description>&lt;p&gt;
&#22522;&#20110;&#35302;&#35273;&#30340;&#20174;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#29289;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tactile-based Object Retrieval From Granular Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GEOTACT&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#39063;&#31890;&#20171;&#36136;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;&#20165;&#20381;&#38752;&#35302;&#35273;&#21453;&#39304;&#26469;&#23436;&#25104;&#65292;&#22240;&#20026;&#19968;&#20010;&#22475;&#34255;&#30340;&#29289;&#20307;&#21487;&#33021;&#23436;&#20840;&#34987;&#35270;&#35273;&#38544;&#34255;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#35302;&#35273;&#21453;&#39304;&#26412;&#36523;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#21608;&#22260;&#20171;&#36136;&#36827;&#34892;&#26222;&#36941;&#25509;&#35302;&#65292;&#24182;&#19988;&#30001;&#35302;&#35273;&#35835;&#25968;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#34920;&#36848;&#23548;&#33268;&#20102;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#25805;&#20316;&#22120;&#20351;&#29992;&#36825;&#20123;&#34892;&#20026;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#29289;&#20307;&#24341;&#23548;&#21040;&#31283;&#23450;&#30340;&#25235;&#21462;&#20301;&#32622;&#65292;&#23613;&#31649;&#23384;&#22312;&#20551;&#30340;&#21644;&#22122;&#22768;&#30340;&#35302;&#35273;&#35835;&#25968;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#36825;&#20123;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GEOTACT&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04527</link><description>&lt;p&gt;
RA-Rec:&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;RA-Rec&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#65292;&#24182;&#35774;&#35745;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20026;LLM&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#32467;&#21512;&#24102;&#26469;&#20102;&#26032;&#30340;&#28526;&#27969;&#65292;&#31216;&#20026;LLM-based RS&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#33539;&#20363;&#65292;&#21363;ID&#30452;&#25509;&#20351;&#29992;&#33539;&#20363;&#21644;ID&#32763;&#35793;&#33539;&#20363;&#65292;&#25351;&#20986;&#23427;&#20204;&#30340;&#26680;&#24515;&#24369;&#28857;&#22312;&#20110;&#32570;&#20047;&#25512;&#33616;&#30693;&#35782;&#21644;&#29420;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21363;ID&#34920;&#31034;&#65292;&#23427;&#20197;&#19968;&#31181;&#20114;&#34917;&#30340;&#26041;&#24335;&#23558;&#39044;&#35757;&#32451;&#30340;ID&#23884;&#20837;&#21040;LLMs&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RA-Rec&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#39640;&#25928;ID&#34920;&#31034;&#23545;&#40784;&#26694;&#26550;&#65292;&#19982;&#22810;&#31181;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#21644;LLM&#26550;&#26500;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ID&#23884;&#20837;&#35270;&#20026;&#36719;&#25552;&#31034;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#40784;&#27169;&#22359;&#21644;&#19968;&#31181;&#29992;&#20110;&#23545;&#40784;&#30340;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#21450;&#20026;&#23545;&#40784;&#23450;&#21046;&#30340;&#25968;&#25454;&#26500;&#24314;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RA-Rec&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) have recently emerged as a powerful tool for a variety of natural language processing tasks, bringing a new surge of combining LLM with recommendation systems, termed as LLM-based RS. Current approaches generally fall into two main paradigms, the ID direct usage paradigm and the ID translation paradigm, noting their core weakness stems from lacking recommendation knowledge and uniqueness. To address this limitation, we propose a new paradigm, ID representation, which incorporates pre-trained ID embeddings into LLMs in a complementary manner. In this work, we present RA-Rec, an efficient ID representation alignment framework for LLM-based recommendation, which is compatible with multiple ID-based methods and LLM architectures. Specifically, we treat ID embeddings as soft prompts and design an innovative alignment module and an efficient tuning method with tailored data construction for alignment. Extensive experiments demonstrate RA-Rec substantially outperfo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04520</link><description>&lt;p&gt;
&#20851;&#20110;&#29616;&#20195;Hopfield&#27169;&#22411;&#35745;&#31639;&#38480;&#21046;&#30340;&#19968;&#20010;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On Computational Limits of Modern Hopfield Models: A Fine-Grained Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#35745;&#31639;&#38480;&#21046;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#33539;&#25968;&#30340;&#30456;&#21464;&#34892;&#20026;&#65292;&#24182;&#19988;&#24314;&#31435;&#20102;&#26377;&#25928;&#21464;&#20307;&#30340;&#19978;&#30028;&#26465;&#20214;&#12290;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#26500;&#36896;&#30340;&#31034;&#20363;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#35745;&#31639;&#26102;&#38388;&#19979;&#30028;&#12289;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#32454;&#31890;&#24230;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#35760;&#24518;&#26816;&#32034;&#21160;&#21147;&#23398;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#27169;&#24335;&#30340;&#33539;&#25968;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#30340;&#25928;&#29575;&#36827;&#34892;&#30456;&#21464;&#34892;&#20026;&#30340;&#21051;&#30011;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#36755;&#20837;&#26597;&#35810;&#27169;&#24335;&#21644;&#35760;&#24518;&#27169;&#24335;&#30340;&#33539;&#25968;&#30340;&#19978;&#30028;&#26631;&#20934;&#12290;&#20165;&#22312;&#36825;&#20010;&#26631;&#20934;&#20043;&#19979;&#65292;&#20551;&#35774;&#28385;&#36275;Strong Exponential Time Hypothesis (SETH)&#65292;&#23384;&#22312;&#23376;&#20108;&#27425;&#65288;&#39640;&#25928;&#65289;&#21464;&#20307;&#30340;&#29616;&#20195;Hopfield&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24403;&#26377;&#25928;&#26631;&#20934;&#25104;&#31435;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#20195;Hopfield&#27169;&#22411;&#20351;&#29992;&#20302;&#31209;&#36924;&#36817;&#30340;&#26377;&#25928;&#26500;&#36896;&#30340;&#27491;&#24335;&#31034;&#20363;&#12290;&#36825;&#21253;&#25324;&#19968;&#20010;&#35745;&#31639;&#26102;&#38388;&#30340;&#19979;&#30028;&#23548;&#20986;&#65292;&#19982;$\Max\{$&#23384;&#20648;&#30340;&#35760;&#24518;&#27169;&#24335;&#25968;&#37327;&#65292;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#30340;&#38271;&#24230;$\}$&#32447;&#24615;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#26816;&#32034;&#35823;&#24046;&#30028;&#21644;&#25351;&#25968;&#35760;&#24518;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the computational limits of the memory retrieval dynamics of modern Hopfield models from the fine-grained complexity analysis. Our key contribution is the characterization of a phase transition behavior in the efficiency of all possible modern Hopfield models based on the norm of patterns. Specifically, we establish an upper bound criterion for the norm of input query patterns and memory patterns. Only below this criterion, sub-quadratic (efficient) variants of the modern Hopfield model exist, assuming the Strong Exponential Time Hypothesis (SETH). To showcase our theory, we provide a formal example of efficient constructions of modern Hopfield models using low-rank approximation when the efficient criterion holds. This includes a derivation of a lower bound on the computational time, scaling linearly with $\Max\{$# of stored memory patterns, length of input query sequence$\}$. In addition, we prove its memory retrieval error bound and exponential memory capacity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20132;&#36890;&#36335;&#30001;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#28145;&#24230;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#37319;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#32593;&#32476;&#25299;&#25169;&#21644;&#38142;&#36335;&#33410;&#28857;&#23646;&#24615;&#20013;&#23398;&#20064;&#27969;&#37327;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.04515</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20132;&#36890;&#36335;&#30001;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Deep Reinforcement Learning Approach for Adaptive Traffic Routing in Next-gen Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#19979;&#19968;&#20195;&#32593;&#32476;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#20132;&#36890;&#36335;&#30001;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#28145;&#24230;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#37319;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#32593;&#32476;&#25299;&#25169;&#21644;&#38142;&#36335;&#33410;&#28857;&#23646;&#24615;&#20013;&#23398;&#20064;&#27969;&#37327;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20195;&#32593;&#32476;&#38656;&#35201;&#22312;&#31649;&#29702;&#26041;&#38754;&#36827;&#34892;&#37325;&#22823;&#25913;&#36827;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#24182;&#26681;&#25454;&#27969;&#37327;&#21160;&#24577;&#35843;&#25972;&#32593;&#32476;&#37197;&#32622;&#12290;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#21644;&#21487;&#32534;&#31243;&#20132;&#25442;&#26426;&#30340;&#20986;&#29616;&#20351;&#24471;&#32593;&#32476;&#20855;&#22791;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#32534;&#31243;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20915;&#31574;&#27969;&#37327;&#31574;&#30053;&#30340;&#25216;&#26415;&#36890;&#24120;&#22522;&#20110;&#25163;&#24037;&#20248;&#21270;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#23613;&#31649;&#20026;&#19979;&#19968;&#20195;&#32593;&#32476;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21364;&#22522;&#20110;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#20363;&#22914;&#38745;&#24577;&#32593;&#32476;&#36127;&#36733;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#20132;&#36890;&#36335;&#30001;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28145;&#24230;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DGCNN&#65289;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;DRL&#26694;&#26550;&#20013;&#65292;&#20197;&#20174;&#32593;&#32476;&#25299;&#25169;&#21644;&#38142;&#36335;&#33410;&#28857;&#23646;&#24615;&#20013;&#23398;&#20064;&#27969;&#37327;&#34892;&#20026;&#12290;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;DRL&#26694;&#26550;&#20013;&#35757;&#32451;DGCNN&#27169;&#22411;&#65292;&#26080;&#38656;&#25163;&#24037;&#32534;&#31243;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next-gen networks require significant evolution of management to enable automation and adaptively adjust network configuration based on traffic dynamics. The advent of software-defined networking (SDN) and programmable switches enables flexibility and programmability. However, traditional techniques that decide traffic policies are usually based on hand-crafted programming optimization and heuristic algorithms. These techniques make non-realistic assumptions, e.g., considering static network load and topology, to obtain tractable solutions, which are inadequate for next-gen networks. In this paper, we design and develop a deep reinforcement learning (DRL) approach for adaptive traffic routing. We design a deep graph convolutional neural network (DGCNN) integrated into the DRL framework to learn the traffic behavior from not only the network topology but also link and node attributes. We adopt the Deep Q-Learning technique to train the DGCNN model in the DRL framework without the need f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.04494</link><description>&lt;p&gt;
&#19981;&#38656;&#25628;&#32034;&#21363;&#21487;&#23454;&#29616;&#22823;&#24072;&#32423;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;
&lt;/p&gt;
&lt;p&gt;
Grandmaster-Level Chess Without Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#24222;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;270M&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#25110;&#26174;&#24335;&#25628;&#32034;&#65292;&#21462;&#24471;&#20102;&#22823;&#24072;&#32423;&#27700;&#24179;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25104;&#21151;&#12290;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;&#35780;&#20998;&#19978;&#36798;&#21040;&#20102;2895&#65292;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#20248;&#20110;AlphaZero&#21644;GPT-3.5-turbo-instruct&#12290;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#23545;&#20110;&#23454;&#29616;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#25928;&#26524;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#31361;&#30772;&#24615;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#35268;&#27169;&#21270;&#65292;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#21644;&#31354;&#21069;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22269;&#38469;&#35937;&#26827;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#30340;&#20381;&#36182;&#22797;&#26434;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#26174;&#24335;&#25628;&#32034;&#25110;&#20108;&#32773;&#32467;&#21512;&#30340;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;1000&#19975;&#23616;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#20102;&#19968;&#20010;&#25317;&#26377;2.7&#20159;&#21442;&#25968;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#29992;&#24378;&#22823;&#30340;Stockfish 16&#24341;&#25806;&#25552;&#20379;&#30340;&#21160;&#20316;&#20540;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#26827;&#23616;&#65292;&#20135;&#29983;&#22823;&#32422;150&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#26368;&#22823;&#30340;&#27169;&#22411;&#22312;Lichess&#38378;&#30005;&#25112;Elo&#19978;&#36798;&#21040;&#20102;2895&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22269;&#38469;&#35937;&#26827;&#35868;&#39064;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#39046;&#22495;&#30340;&#35843;&#25972;&#25110;&#26174;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;AlphaZero&#30340;&#31574;&#30053;&#21644;&#20215;&#20540;&#32593;&#32476;&#65288;&#26080;MCTS&#65289;&#20197;&#21450;GPT-3.5-turbo-instruct&#12290;&#23545;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#31995;&#32479;&#30740;&#31350;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#22269;&#38469;&#35937;&#26827;&#23545;&#23616;&#21487;&#20197;&#22312;&#35268;&#27169;&#19978;&#21462;&#24471;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent breakthrough successes in machine learning are mainly attributed to scale: namely large-scale attention-based architectures and datasets of unprecedented scale. This paper investigates the impact of training at scale for chess. Unlike traditional chess engines that rely on complex heuristics, explicit search, or a combination of both, we train a 270M parameter transformer model with supervised learning on a dataset of 10 million chess games. We annotate each board in the dataset with action-values provided by the powerful Stockfish 16 engine, leading to roughly 15 billion data points. Our largest model reaches a Lichess blitz Elo of 2895 against humans, and successfully solves a series of challenging chess puzzles, without any domain-specific tweaks or explicit search algorithms. We also show that our model outperforms AlphaZero's policy and value networks (without MCTS) and GPT-3.5-turbo-instruct. A systematic investigation of model and dataset size shows that strong chess 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#20107;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#36880;&#28176;&#20986;&#29616;&#20102;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.04477</link><description>&lt;p&gt;
&#36890;&#36807;&#21465;&#36848;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27169;&#24335;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Detecting Mode Collapse in Language Models via Narration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;&#25925;&#20107;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#36880;&#28176;&#20986;&#29616;&#20102;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#20004;&#20010;&#20316;&#32773;&#20889;&#20316;&#26041;&#24335;&#30456;&#21516;&#12290;&#20174;&#35789;&#27719;&#21040;&#20462;&#36766;&#25163;&#27861;&#65292;&#22312;&#20070;&#38754;&#21465;&#36848;&#20013;&#21576;&#29616;&#20986;&#30340;&#20010;&#20154;&#29305;&#33394;&#65292;&#26263;&#31034;&#20102;&#19968;&#20301;&#29305;&#23450;&#30340;&#20316;&#32773;&#65292;&#25991;&#23398;&#29702;&#35770;&#23478;&#23558;&#20854;&#31216;&#20026;&#38544;&#21547;&#25110;&#34394;&#25311;&#20316;&#32773;&#65292;&#19982;&#25991;&#26412;&#30340;&#23454;&#38469;&#20316;&#32773;&#25110;&#21465;&#36848;&#32773;&#19981;&#21516;&#12290;&#26089;&#26399;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#19981;&#21327;&#35843;&#26469;&#28304;&#30340;&#26410;&#32463;&#36807;&#28388;&#30340;&#35757;&#32451;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#20102;&#19981;&#36830;&#36143;&#30340;&#20010;&#24615;&#65292;&#36825;&#23545;&#20110;&#23545;&#35805;&#20219;&#21153;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#20294;&#23545;&#20110;&#20174;&#22810;&#20010;&#35266;&#28857;&#37319;&#26679;&#25991;&#23398;&#21364;&#26159;&#26377;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#23545;&#40784;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#21644;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23545;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#20027;&#35266;&#19968;&#33268;&#30340;&#20154;&#29289;&#24418;&#35937;&#65292;&#20294;&#23545;&#40784;&#27169;&#22411;&#26159;&#21542;&#20445;&#30041;&#20102;&#23545;&#27169;&#25311;&#20219;&#24847;&#34394;&#25311;&#20316;&#32773;&#30340;&#33021;&#21147;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#23457;&#26597;&#12290;&#36890;&#36807;&#30740;&#31350;&#26469;&#33258;&#19977;&#20010;OpenAI&#35821;&#35328;&#27169;&#22411;&#30340;4,374&#20010;&#25925;&#20107;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;GPT-3&#30340;&#26356;&#26032;&#29256;&#26412;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#8220;&#27169;&#24335;&#23849;&#28291;&#8221;&#29616;&#35937;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of "mode collapse" whereby overf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>https://arxiv.org/abs/2402.04476</link><description>&lt;p&gt;
&#21452;&#35270;&#22270;&#35270;&#35273;&#32972;&#26223;&#21270;&#30340;&#32593;&#39029;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dual-View Visual Contextualization for Web Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20351;&#24471;HTML&#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32593;&#39029;&#23548;&#33322;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#22312;&#23454;&#38469;&#32593;&#31449;&#19978;&#25191;&#34892;&#22797;&#26434;&#21644;&#22810;&#26679;&#20219;&#21153;&#30340;&#32593;&#32476;&#20195;&#29702;&#12290;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#26159;&#20197; HTML &#25991;&#26723;&#20316;&#20026;&#36755;&#20837;&#65292;HTML &#25991;&#26723;&#23450;&#20041;&#20102;&#32593;&#39029;&#30340;&#20869;&#23481;&#21644;&#25805;&#20316;&#31354;&#38388;&#65288;&#21363;&#21487;&#25805;&#20316;&#20803;&#32032;&#21644;&#25805;&#20316;&#65289;&#12290;&#28982;&#32780;&#65292;HTML &#25991;&#26723;&#21487;&#33021;&#26080;&#27861;&#20026;&#27599;&#20010;&#20803;&#32032;&#25552;&#20379;&#28165;&#26224;&#30340;&#20219;&#21153;&#30456;&#20851;&#32972;&#26223;&#65292;&#20351;&#24471;&#36873;&#25321;&#27491;&#30830;&#30340;&#65288;&#19968;&#31995;&#21015;&#30340;&#65289;&#25805;&#20316;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#32593;&#39029;&#25130;&#22270;&#20013;&#20803;&#32032;&#30340;&#8220;&#21452;&#35270;&#22270;&#8221;&#26469;&#36827;&#34892; HTML &#20803;&#32032;&#30340;&#32972;&#26223;&#21270;&#65306;&#27599;&#20010; HTML &#20803;&#32032;&#22312;&#25130;&#22270;&#20013;&#26377;&#20854;&#23545;&#24212;&#30340;&#36793;&#30028;&#26694;&#21644;&#35270;&#35273;&#20869;&#23481;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#27934;&#23519;&#21147;&#8212;&#8212;&#32593;&#39029;&#24320;&#21457;&#32773;&#20542;&#21521;&#20110;&#22312;&#32593;&#39029;&#19978;&#23558;&#20219;&#21153;&#30456;&#20851;&#20803;&#32032;&#25918;&#32622;&#22312;&#38468;&#36817;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#25552;&#20986;&#23558;&#27599;&#20010;&#20803;&#32032;&#19982;&#20854;&#37051;&#23621;&#20803;&#32032;&#36827;&#34892;&#32972;&#26223;&#21270;&#65292;&#20351;&#29992;&#25991;&#26412;&#21644;&#35270;&#35273;&#29305;&#24449;&#12290;HTML &#20803;&#32032;&#30340;&#32467;&#26524;&#34920;&#31034;&#23545;&#20110;&#20195;&#29702;&#25191;&#34892;&#25805;&#20316;&#26356;&#21152;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic web navigation aims to build a web agent that can follow language instructions to execute complex and diverse tasks on real-world websites. Existing work primarily takes HTML documents as input, which define the contents and action spaces (i.e., actionable elements and operations) of webpages. Nevertheless, HTML documents may not provide a clear task-related context for each element, making it hard to select the right (sequence of) actions. In this paper, we propose to contextualize HTML elements through their "dual views" in webpage screenshots: each HTML element has its corresponding bounding box and visual content in the screenshot. We build upon the insight -- web developers tend to arrange task-related elements nearby on webpages to enhance user experiences -- and propose to contextualize each element with its neighbor elements, using both textual and visual features. The resulting representations of HTML elements are more informative for the agent to take action. We val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;NVIDIA Holoscan&#24179;&#21488;&#20013;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#30340;&#24310;&#36831;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.04466</link><description>&lt;p&gt;
NVIDIA Holoscan&#20013;&#38754;&#21521;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Deterministic End-to-end Latency for Medical AI Systems in NVIDIA Holoscan
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;NVIDIA Holoscan&#24179;&#21488;&#20013;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#30340;&#24310;&#36831;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;ML&#25216;&#26415;&#30340;&#24341;&#20837;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21307;&#30103;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#21307;&#30103;&#35774;&#22791;&#21046;&#36896;&#21830;&#28212;&#26395;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;AI&#21644;ML&#30340;&#20248;&#21183;&#65292;&#23558;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#25972;&#21512;&#21040;&#19968;&#20010;&#24179;&#21488;&#19978;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;AI&#24212;&#29992;&#31243;&#24207;&#65292;&#27599;&#20010;&#24212;&#29992;&#31243;&#24207;&#37117;&#26377;&#33258;&#24049;&#30340;&#21487;&#35270;&#21270;&#32452;&#20214;&#65292;&#20250;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#36164;&#28304;&#20105;&#29992;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21046;&#36896;&#21830;&#36890;&#24120;&#20250;&#20026;&#19981;&#21516;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#21333;&#29420;&#30340;&#24037;&#20316;&#31449;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#36130;&#21153;&#12289;&#33021;&#28304;&#21644;&#32500;&#25252;&#25104;&#26412;&#12290;&#26412;&#25991;&#38024;&#23545;NVIDIA&#30340;Holoscan&#24179;&#21488;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;Holoscan&#26159;&#19968;&#20010;&#29992;&#20110;&#27969;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#22270;&#20687;&#30340;&#23454;&#26102;AI&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24322;&#26500;GPU&#24037;&#20316;&#36127;&#36733;&#20248;&#21270;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#22270;&#24418;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#21033;&#29992;CUDA MPS&#23545;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#36827;&#34892;&#31354;&#38388;&#21010;&#20998;&#65292;&#24182;&#20998;&#38548;&#35745;&#31639;&#21644;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of AI and ML technologies into medical devices has revolutionized healthcare diagnostics and treatments. Medical device manufacturers are keen to maximize the advantages afforded by AI and ML by consolidating multiple applications onto a single platform. However, concurrent execution of several AI applications, each with its own visualization components, leads to unpredictable end-to-end latency, primarily due to GPU resource contentions. To mitigate this, manufacturers typically deploy separate workstations for distinct AI applications, thereby increasing financial, energy, and maintenance costs. This paper addresses these challenges within the context of NVIDIA's Holoscan platform, a real-time AI system for streaming sensor data and images. We propose a system design optimized for heterogeneous GPU workloads, encompassing both compute and graphics tasks. Our design leverages CUDA MPS for spatial partitioning of compute workloads and isolates compute and graphics proc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#36890;&#29992;&#33021;&#21147;&#30340;&#21457;&#23637;&#12289;&#24615;&#33021;&#20445;&#38556;&#12289;&#30446;&#26631;&#23545;&#40784;&#12289;&#24191;&#27867;&#24212;&#29992;&#12289;&#32463;&#27982;&#21464;&#38761;&#12289;&#20840;&#27665;&#21442;&#19982;&#12289;&#31038;&#20250;&#36127;&#36131;&#20219;&#37096;&#32626;&#12289;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#12289;&#25216;&#26415;&#27835;&#29702;&#21644;&#21746;&#23398;&#21464;&#38761;&#31649;&#29702;&#12290;&#35201;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04464</link><description>&lt;p&gt;
&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ten Hard Problems in Artificial Intelligence We Must Get Right
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#36890;&#29992;&#33021;&#21147;&#30340;&#21457;&#23637;&#12289;&#24615;&#33021;&#20445;&#38556;&#12289;&#30446;&#26631;&#23545;&#40784;&#12289;&#24191;&#27867;&#24212;&#29992;&#12289;&#32463;&#27982;&#21464;&#38761;&#12289;&#20840;&#27665;&#21442;&#19982;&#12289;&#31038;&#20250;&#36127;&#36131;&#20219;&#37096;&#32626;&#12289;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#12289;&#25216;&#26415;&#27835;&#29702;&#21644;&#21746;&#23398;&#21464;&#38761;&#31649;&#29702;&#12290;&#35201;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;AI2050&#20013;&#38459;&#30861;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#24341;&#21457;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#30340;"&#38590;&#39064;"&#65306;&#65288;1&#65289;&#21457;&#23637;&#31995;&#32479;&#30340;&#36890;&#29992;&#33021;&#21147;&#65307;&#65288;2&#65289;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20854;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#23558;&#31995;&#32479;&#30446;&#26631;&#19982;&#20154;&#31867;&#30446;&#26631;&#23545;&#40784;&#65307;&#65288;4&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65307;&#65288;5&#65289;&#24212;&#23545;&#32463;&#27982;&#21464;&#38761;&#65307;&#65288;6&#65289;&#30830;&#20445;&#20840;&#27665;&#21442;&#19982;&#65307;&#65288;7&#65289;&#21516;&#26102;&#30830;&#20445;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65307;&#65288;8&#65289;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#24341;&#21457;&#30340;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#65307;&#65288;9&#65289;&#25512;&#21160;&#23545;&#25216;&#26415;&#30340;&#20581;&#20840;&#27835;&#29702;&#65307;&#20197;&#21450;&#65288;10&#65289;&#31649;&#29702;&#29983;&#27963;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20013;&#30340;&#21746;&#23398;&#21464;&#38761;&#12290;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#30456;&#20851;&#39046;&#22495;&#65292;&#25351;&#20986;&#20102;&#37325;&#35201;&#30340;&#26368;&#36817;&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#25512;&#36827;&#30340;&#26041;&#24335;&#12290;[&#27880;&#65306;&#26412;&#35770;&#25991;&#22238;&#39038;&#30340;&#25991;&#29486;&#26102;&#38388;&#25130;&#33267;2023&#24180;1&#26376;&#12290;]
&lt;/p&gt;
&lt;p&gt;
We explore the AI2050 "hard problems" that block the promise of AI and cause AI risks: (1) developing general capabilities of the systems; (2) assuring the performance of AI systems and their training processes; (3) aligning system goals with human goals; (4) enabling great applications of AI in real life; (5) addressing economic disruptions; (6) ensuring the participation of all; (7) at the same time ensuring socially responsible deployment; (8) addressing any geopolitical disruptions that AI causes; (9) promoting sound governance of the technology; and (10) managing the philosophical disruptions for humans living in the age of AI. For each problem, we outline the area, identify significant recent work, and suggest ways forward. [Note: this paper reviews literature through January 2023.]
&lt;/p&gt;</description></item><item><title>PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.04435</link><description>&lt;p&gt;
PreGIP: &#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04435
&lt;/p&gt;
&lt;p&gt;
PreGIP&#26159;&#38024;&#23545;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#30340;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#35757;&#32451;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#28155;&#21152;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#65292;&#24182;&#37319;&#29992;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#39044;&#35757;&#32451;&#22312;&#20419;&#36827;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#39044;&#35757;&#32451;&#30340;GNNs&#25104;&#20026;&#21512;&#27861;&#25317;&#26377;&#32773;&#30340;&#39640;&#20215;&#20540;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#25163;&#21487;&#33021;&#20250;&#38750;&#27861;&#22797;&#21046;&#21644;&#37096;&#32626;&#39044;&#35757;&#32451;&#30340;GNN&#27169;&#22411;&#29992;&#20110;&#20854;&#19979;&#28216;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#22987;&#23581;&#35797;&#20026;IP&#20445;&#25252;&#28155;&#21152;GNN&#20998;&#31867;&#22120;&#30340;&#27700;&#21360;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#25165;&#33021;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;GNN&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;PreGIP&#65292;&#29992;&#20110;&#22312;&#20445;&#25345;&#23884;&#20837;&#31354;&#38388;&#39640;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#32473;GNN&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#28155;&#21152;&#27700;&#21360;&#20197;&#36827;&#34892;IP&#20445;&#25252;&#12290;PreGIP&#24341;&#20837;&#20102;&#26080;&#20219;&#21153;&#30340;&#27700;&#21360;&#25439;&#22833;&#26469;&#32473;&#39044;&#35757;&#32451;&#30340;GNN&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#28155;&#21152;&#27700;&#21360;&#12290;&#21516;&#26102;&#37319;&#29992;&#20102;&#25239;&#24494;&#35843;&#30340;&#27700;&#21360;&#27880;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#25193;&#23637;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and exte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;Code-PLMs&#22312;R&#20013;&#30340;&#20195;&#30721;&#23454;&#20307;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;R&#25968;&#25454;&#38598;&#21644;CodeAttack&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#22914;&#20309;&#25915;&#20987;&#19981;&#21516;&#30340;&#23454;&#20307;&#12290;&#36825;&#26159;&#20102;&#35299;R&#20196;&#29260;&#31867;&#22411;&#37325;&#35201;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.04421</link><description>&lt;p&gt;
&#22312;R&#20013;&#30740;&#31350;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#23454;&#20307;
&lt;/p&gt;
&lt;p&gt;
Studying Vulnerable Code Entities in R
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;Code-PLMs&#22312;R&#20013;&#30340;&#20195;&#30721;&#23454;&#20307;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;R&#25968;&#25454;&#38598;&#21644;CodeAttack&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#22914;&#20309;&#25915;&#20987;&#19981;&#21516;&#30340;&#23454;&#20307;&#12290;&#36825;&#26159;&#20102;&#35299;R&#20196;&#29260;&#31867;&#22411;&#37325;&#35201;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;Code-PLMs&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#23637;&#31034;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#24182;&#22312;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22914;Java&#21644;Python&#65292;&#32780;&#25490;&#38500;&#20102;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#65292;&#22914;R&#12290;&#23613;&#31649;R&#25317;&#26377;&#24191;&#27867;&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#31038;&#21306;&#65292;&#20294;&#23545;Code-PLMs&#22312;R&#20013;&#30340;&#36866;&#29992;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;Code-PLMs&#22312;R&#20013;&#30340;&#20195;&#30721;&#23454;&#20307;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#30340;R&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#24212;&#29992;CodeAttack&#65292;&#19968;&#20010;&#21033;&#29992;&#20195;&#30721;&#32467;&#26500;&#29983;&#25104;&#23545;&#25239;&#24615;&#20195;&#30721;&#26679;&#26412;&#30340;&#40657;&#30418;&#25915;&#20987;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#22914;&#20309;&#25915;&#20987;R&#20013;&#30340;&#19981;&#21516;&#23454;&#20307;&#12290;&#36825;&#26159;&#20102;&#35299;R&#20196;&#29260;&#31867;&#22411;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#65289;&#37325;&#35201;&#24615;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#33539;&#22260;&#38480;&#21046;&#22312;&#20195;&#30721;&#25688;&#35201;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#23454;&#20307;&#26159;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Code Language Models (Code-PLMs) have shown many advancements and achieved state-of-the-art results for many software engineering tasks in the past few years. These models are mainly targeted for popular programming languages such as Java and Python, leaving out many other ones like R. Though R has a wide community of developers and users, there is little known about the applicability of Code-PLMs for R. In this preliminary study, we aim to investigate the vulnerability of Code-PLMs for code entities in R. For this purpose, we use an R dataset of code and comment pairs and then apply CodeAttack, a black-box attack model that uses the structure of code to generate adversarial code samples. We investigate how the model can attack different entities in R. This is the first step towards understanding the importance of R token types, compared to popular programming languages (e.g., Java). We limit our study to code summarization. Our results show that the most vulnerable code en
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#26597;&#30740;&#31350;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#23545;&#20154;&#20204;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#32780;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#21457;&#21051;&#26495;&#21360;&#35937;&#21644;&#20260;&#23475;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04420</link><description>&lt;p&gt;
&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20260;&#23475;&#65306;&#38656;&#35201;&#20102;&#35299;&#35841;&#27491;&#22312;&#21463;&#21040;&#21738;&#20123;&#38169;&#35823;&#20197;&#21450;&#20197;&#20309;&#31181;&#26041;&#24335;&#21463;&#21040;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04420
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#26597;&#30740;&#31350;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#23545;&#20154;&#20204;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#32780;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#21457;&#21051;&#26495;&#21360;&#35937;&#21644;&#20260;&#23475;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#23427;&#20204;&#21487;&#33021;&#36896;&#25104;&#30340;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#24456;&#23569;&#22522;&#20110;&#20154;&#31867;&#23545;&#20260;&#23475;&#30340;&#24515;&#29702;&#20307;&#39564;&#12290;&#20511;&#37492;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#24515;&#29702;&#23398;&#65292;&#25105;&#20204;&#20197;&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#20154;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#30340;&#21453;&#24212;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35843;&#26597;&#30740;&#31350;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#37117;&#21453;&#26144;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#20063;&#27809;&#26377;&#21516;&#26679;&#30340;&#20260;&#23475;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#20351;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#24378;&#21270;&#12289;&#36829;&#21453;&#21644;&#20013;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#24341;&#36215;&#26356;&#22810;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#20307;&#39564;&#65292;&#20294;&#23545;&#35748;&#30693;&#20449;&#24565;&#12289;&#24577;&#24230;&#25110;&#34892;&#20026;&#30340;&#25913;&#21464;&#24456;&#23567;&#12290;&#36825;&#31181;&#20307;&#39564;&#19978;&#30340;&#20260;&#23475;&#23545;&#22899;&#24615;&#24433;&#21709;&#26356;&#22823;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#36829;&#21453;&#21051;&#26495;&#21360;&#35937;&#30340;&#38169;&#35823;&#23545;&#30007;&#24615;&#20135;&#29983;&#26356;&#22823;&#30340;&#20027;&#35266;&#19978;&#30340;&#20260;&#23475;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#23545;&#30007;&#24615;&#38451;&#21018;&#24615;&#30340;&#23041;&#32961;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.04412</link><description>&lt;p&gt;
VampPrior&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The VampPrior Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;DLVMs&#65289;&#30340;&#32858;&#31867;&#20808;&#39564;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36739;&#24046;&#30340;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#38598;&#25104;&#21644;&#32858;&#31867;&#30340;&#26041;&#24335;&#26497;&#22823;&#22320;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;scRNA-seq&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;VampPrior&#65288;Tomczak&#21644;Welling&#65292;2018&#65289;&#35843;&#25972;&#20026;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24471;&#21040;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#65292;&#20132;&#26367;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#65292;&#20197;&#28165;&#26970;&#22320;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#23558;VMM&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;scRNA-seq&#38598;&#25104;&#26041;&#27861;scVI&#65288;Lopez&#31561;&#65292;2018&#65289;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRECA&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;FedTruth&#26694;&#26550;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#25490;&#38500;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;FRECA&#23545;&#20110;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04409</link><description>&lt;p&gt;
&#36827;&#19968;&#27493;&#23454;&#29616;&#20844;&#24179;&#12289;&#40065;&#26834;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRECA&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;FedTruth&#26694;&#26550;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#25490;&#38500;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;FRECA&#23545;&#20110;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#23458;&#25143;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#35780;&#20272;&#27599;&#20010;&#23458;&#25143;&#30340;&#36129;&#29486;&#23545;&#20110;&#23458;&#25143;&#36873;&#25321;&#21644;&#34917;&#20607;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#36890;&#24120;&#20855;&#26377;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;non-iid&#65289;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#25110;&#21457;&#25955;&#30340;&#26356;&#26032;&#65292;&#22240;&#27492;&#35780;&#20272;&#23458;&#25143;&#36129;&#29486;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#26080;&#27861;&#35775;&#38382;&#23458;&#25143;&#30340;&#26412;&#22320;&#25968;&#25454;&#25110;&#22522;&#20934;&#26681;&#25968;&#25454;&#38598;&#26102;&#65292;&#24694;&#24847;&#23458;&#25143;&#30340;&#39118;&#38505;&#20250;&#36827;&#19968;&#27493;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20844;&#24179;&#12289;&#40065;&#26834;&#21644;&#39640;&#25928;&#23458;&#25143;&#35780;&#20272;&#65288;FRECA&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;FL&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#12290;FRECA&#37319;&#29992;&#19968;&#31181;&#21517;&#20026;FedTruth&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#20840;&#23616;&#27169;&#22411;&#30340;&#30495;&#23454;&#26356;&#26032;&#65292;&#24179;&#34913;&#26469;&#33258;&#25152;&#26377;&#23458;&#25143;&#30340;&#36129;&#29486;&#65292;&#24182;&#36807;&#28388;&#20986;&#24694;&#24847;&#23458;&#25143;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#23545;&#25308;&#21344;&#24237;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32858;&#21512;&#31639;&#27861;&#12290;FRECA&#36824;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#20165;&#20165;&#22312;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#19978;&#25805;&#20316;&#65292;&#19988;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20840;&#23616;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir
&lt;/p&gt;</description></item><item><title>CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04400</link><description>&lt;p&gt;
&#29983;&#25104;&#24102;&#26377;&#30149;&#20154;&#26102;&#38388;&#36724;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;CEHR-GPT
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04400
&lt;/p&gt;
&lt;p&gt;
CEHR-GPT&#26159;&#19968;&#31181;&#20351;&#29992;&#30149;&#20154;&#26102;&#38388;&#36724;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#12289;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#31561;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#25512;&#36827;&#21307;&#30103;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#30452;&#25509;&#35775;&#38382;&#21307;&#30103;&#25968;&#25454;&#30340;&#30740;&#31350;&#20154;&#21592;&#32780;&#35328;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;EHR&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#34920;&#26684;&#26684;&#24335;&#65292;&#24573;&#30053;&#20102;&#30149;&#20154;&#21382;&#21490;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#22797;&#21046;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26469;&#22788;&#29702;EHR&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#30142;&#30149;&#36827;&#23637;&#20998;&#26512;&#12289;&#20154;&#21475;&#20272;&#35745;&#12289;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#28304;&#33258;CEHR-BERT&#30340;&#29305;&#23450;&#30149;&#20154;&#34920;&#31034;&#35757;&#32451;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#33021;&#22815;&#29983;&#25104;&#21487;&#26080;&#32541;&#36716;&#25442;&#30340;&#30149;&#20154;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04398</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Learning from Time Series under Temporal Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20998;&#31867;&#20219;&#21153;&#21463;&#21040;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#22122;&#22768;&#21487;&#33021;&#20250;&#23548;&#33268;&#26631;&#31614;&#36136;&#37327;&#38543;&#26102;&#38388;&#25913;&#21892;&#12289;&#24694;&#21270;&#25110;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#21644;&#31995;&#32479;&#21270;&#20102;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#20998;&#31867;&#30340;&#19968;&#20010;&#26410;&#32463;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#22810;&#20010;&#26631;&#31614;&#36830;&#32493;&#35760;&#24405;&#65292;&#21516;&#26102;&#21463;&#21040;&#19968;&#20010;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#20989;&#25968;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24314;&#27169;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#30340;&#25345;&#32493;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#23545;&#22122;&#22768;&#20855;&#26377;&#23481;&#24525;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#65292;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
&lt;/p&gt;</description></item><item><title>QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.04396</link><description>&lt;p&gt;
QuIP#: &#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#36827;&#34892;&#26356;&#22909;&#30340;LLM&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04396
&lt;/p&gt;
&lt;p&gt;
QuIP#&#26159;&#19968;&#31181;&#20351;&#29992;&#21704;&#36798;&#29595;&#24503;&#38750;&#30456;&#24178;&#24615;&#21644;&#26684;&#20070;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;&#19979;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#36890;&#36807;&#23558;LLM&#30340;&#26435;&#37325;&#37327;&#21270;&#20026;&#20302;&#31934;&#24230;&#26469;&#20943;&#23569;&#20854;&#20869;&#23384;&#21344;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QuIP#&#65292;&#19968;&#31181;&#20165;&#22522;&#20110;&#26435;&#37325;&#30340;PTQ&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#22312;&#26497;&#38480;&#21387;&#32553;&#33539;&#22260;($\le$ 4&#27604;&#29305;&#27599;&#20010;&#26435;&#37325;)&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;QuIP#&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#24503;&#21464;&#25442;&#25913;&#36827;&#20102;QuIP&#20013;&#30340;&#38750;&#30456;&#24178;&#22788;&#29702;&#65292;&#35813;&#26041;&#27861;&#26356;&#24555;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#12290;&#20854;&#27425;&#65292;QuIP#&#20351;&#29992;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#21033;&#29992;&#20102;&#38750;&#30456;&#24178;&#26435;&#37325;&#20855;&#26377;&#30340;&#29699;&#24418;&#20122;&#39640;&#26031;&#20998;&#24067;&#29305;&#24615;&#65306;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#22522;&#20110;&#39640;&#24230;&#23545;&#31216;$E_8$&#26684;&#20070;&#30340;&#30828;&#20214;&#39640;&#25928;&#20195;&#30721;&#20070;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;8&#32500;&#21333;&#20301;&#29699;&#35013;&#22635;&#12290;&#31532;&#19977;&#65292;QuIP#&#20351;&#29992;&#24494;&#35843;&#26469;&#25552;&#39640;&#23545;&#21407;&#22987;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;QuIP#&#20248;&#20110;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#26032;&#30340;PTQ&#25193;&#23637;&#34892;&#20026;&#65292;&#24182;&#25903;&#25345;&#24555;&#36895;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves the incoherence processing from QuIP by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization techniques to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31572;&#26696;&#38598;&#32534;&#31243;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#29702;&#24615;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04382</link><description>&lt;p&gt;
&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generation with Answer Set Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#31572;&#26696;&#38598;&#32534;&#31243;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#30340;&#36879;&#26126;&#24230;&#21644;&#21512;&#29702;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36151;&#27454;&#23457;&#25209;&#12289;&#20445;&#37322;&#23457;&#25209;&#12289;&#25307;&#32856;&#31561;&#39046;&#22495;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#33258;&#21160;&#20915;&#31574;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#37117;&#26159;&#40657;&#21283;&#23376;&#65292;&#21363;&#26080;&#27861;&#25581;&#31034;&#20854;&#39044;&#27979;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#20986;&#20110;&#36879;&#26126;&#24230;&#30340;&#38656;&#27714;&#65292;&#23545;&#36825;&#20123;&#39044;&#27979;&#38656;&#35201;&#36827;&#34892;&#35299;&#37322;&#12290;&#21463;&#24433;&#21709;&#30340;&#20010;&#20307;&#21487;&#33021;&#36824;&#24076;&#26395;&#35299;&#37322;&#20026;&#20160;&#20040;&#20570;&#20986;&#20102;&#26576;&#20010;&#20915;&#31574;&#12290;&#36947;&#24503;&#21644;&#27861;&#24459;&#32771;&#34385;&#36827;&#19968;&#27493;&#35201;&#27714;&#21578;&#30693;&#20010;&#20307;&#21487;&#20197;&#36827;&#34892;&#21738;&#20123;&#36755;&#20837;&#23646;&#24615;&#30340;&#26356;&#25913;&#20197;&#20135;&#29983;&#26399;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21453;&#20107;&#23454;&#29983;&#25104;&#19982;s(CASP) (CFGS)&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;(ASP)&#21644;s(CASP)&#30446;&#26631;&#23548;&#21521;ASP&#31995;&#32479;&#65292;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#26426;&#22120;&#23398;&#20064;(RBML)&#31639;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#20013;&#33258;&#21160;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models that automate decision-making are increasingly being used in consequential areas such as loan approvals, pretrial bail approval, hiring, and many more. Unfortunately, most of these models are black-boxes, i.e., they are unable to reveal how they reach these prediction decisions. A need for transparency demands justification for such predictions. An affected individual might also desire explanations to understand why a decision was made. Ethical and legal considerations may further require informing the individual of changes in the input attribute that could be made to produce a desirable outcome. This paper focuses on the latter problem of automatically generating counterfactual explanations. We propose a framework Counterfactual Generation with s(CASP) (CFGS) that utilizes answer set programming (ASP) and the s(CASP) goal-directed ASP system to automatically generate counterfactual explanations from rules generated by rule-based machine learning (RBML) algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.04376</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for learning with real and surrogate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#25104;&#26412;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#33539;&#22260;&#20869;, &#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#30456;&#21453;&#22320;, &#21487;&#20197;&#23558;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#25110;&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;, &#20316;&#20026;&#26367;&#20195;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#23558;&#26367;&#20195;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;, &#24182;&#20351;&#29992;&#29702;&#35770;&#27169;&#22411;&#21644;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(i) &#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21407;&#22987;&#20998;&#24067;&#30340;&#27979;&#35797;&#35823;&#24046;&#65307;(ii) &#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#25928;&#30410;, &#20351;&#29992;&#26368;&#20248;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38750;&#24120;&#20851;&#38190;&#65307;(iii) &#22312;&#28151;&#21512;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#12290;&#36825;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#26426;&#26800;&#24335;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#20915;&#31574;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#37322;&#34892;&#20154;&#30340;&#20915;&#31574;&#34892;&#20026;&#20197;&#21450;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#38480;&#21046;&#12290;&#27169;&#22411;&#25104;&#21151;&#22320;&#22797;&#29616;&#20102;&#22810;&#20010;&#24050;&#30693;&#30340;&#23454;&#35777;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.04370</link><description>&lt;p&gt;
&#34892;&#20154;&#36807;&#39532;&#36335;&#20915;&#31574;&#21487;&#20197;&#36890;&#36807;&#21463;&#24178;&#25200;&#30340;&#35270;&#35273;&#30693;&#35273;&#19979;&#30340;&#26377;&#30028;&#26368;&#20248;&#20915;&#31574;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Pedestrian crossing decisions can be explained by bounded optimal decision-making under noisy visual perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#26426;&#26800;&#24335;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#34892;&#20154;&#36807;&#39532;&#36335;&#20915;&#31574;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#37322;&#34892;&#20154;&#30340;&#20915;&#31574;&#34892;&#20026;&#20197;&#21450;&#23545;&#35270;&#35273;&#30693;&#35273;&#30340;&#38480;&#21046;&#12290;&#27169;&#22411;&#25104;&#21151;&#22320;&#22797;&#29616;&#20102;&#22810;&#20010;&#24050;&#30693;&#30340;&#23454;&#35777;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34892;&#20154;&#36807;&#39532;&#36335;&#20915;&#31574;&#27169;&#22411;&#65292;&#22522;&#20110;&#35745;&#31639;&#21512;&#29702;&#24615;&#29702;&#35770;&#12290;&#20551;&#35774;&#36807;&#39532;&#36335;&#20915;&#31574;&#26159;&#26377;&#30028;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#20154;&#31867;&#35748;&#30693;&#38480;&#21046;&#32780;&#20135;&#29983;&#36825;&#31181;&#26377;&#30028;&#24615;&#12290;&#20043;&#21069;&#30340;&#34892;&#20154;&#34892;&#20026;&#27169;&#22411;&#35201;&#20040;&#26159;&#8220;&#40657;&#30418;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35201;&#20040;&#26159;&#20855;&#26377;&#26126;&#30830;&#23545;&#35748;&#30693;&#22240;&#32032;&#30340;&#20551;&#35774;&#30340;&#26426;&#26800;&#24335;&#27169;&#22411;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;&#26426;&#26800;&#26041;&#24335;&#27169;&#25311;&#20102;&#20154;&#31867;&#35270;&#35273;&#30693;&#35273;&#30340;&#22122;&#22768;&#21644;&#36807;&#39532;&#36335;&#26102;&#30340;&#22870;&#21169;&#65292;&#20294;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#26377;&#30028;&#26368;&#20248;&#34892;&#20026;&#31574;&#30053;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22797;&#29616;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#26356;&#22810;&#24050;&#30693;&#30340;&#32463;&#39564;&#35777;&#25454;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#65306;&#65288;1&#65289;&#25509;&#36817;&#36710;&#36742;&#21040;&#36798;&#30340;&#26102;&#38388;&#23545;&#34892;&#20154;&#26159;&#21542;&#25509;&#21463;&#32541;&#38553;&#30340;&#24433;&#21709;&#65292;&#36710;&#36742;&#36895;&#24230;&#23545;&#65288;2&#65289;&#32541;&#38553;&#25509;&#21463;&#21644;&#65288;3&#65289;&#34892;&#20154;&#36890;&#36807;&#35753;&#34892;&#36710;&#36742;&#30340;&#26102;&#38388;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#65288;4&#65289;&#36825;&#31181;&#36807;&#39532;&#36335;&#34892;&#26102;&#38388;&#36873;&#25321;&#23545;&#34892;&#20154;&#23433;&#20840;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a model of pedestrian crossing decisions, based on the theory of computational rationality. It is assumed that crossing decisions are boundedly optimal, with bounds on optimality arising from human cognitive limitations. While previous models of pedestrian behaviour have been either 'black-box' machine learning models or mechanistic models with explicit assumptions about cognitive factors, we combine both approaches. Specifically, we model mechanistically noisy human visual perception and assumed rewards in crossing, but we use reinforcement learning to learn bounded optimal behaviour policy. The model reproduces a larger number of known empirical phenomena than previous models, in particular: (1) the effect of the time to arrival of an approaching vehicle on whether the pedestrian accepts the gap, the effect of the vehicle's speed on both (2) gap acceptance and (3) pedestrian timing of crossing in front of yielding vehicles, and (4) the effect on this crossing timi
&lt;/p&gt;</description></item><item><title>PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04355</link><description>&lt;p&gt;
PQMass: &#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#27010;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04355
&lt;/p&gt;
&lt;p&gt;
PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20272;&#35745;&#20004;&#20010;&#26679;&#26412;&#38598;&#21512;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#27010;&#29575;&#65292;&#20026;&#35780;&#20272;&#21333;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#25110;&#27604;&#36739;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#19978;&#20005;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#27604;&#36739;&#21487;&#20197;&#36890;&#36807;&#23558;&#31354;&#38388;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#30340;&#21306;&#22495;&#24182;&#27604;&#36739;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#26469;&#36827;&#34892;&#12290;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#29983;&#25104;&#27169;&#22411;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#23427;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#26080;&#38656;&#38477;&#32500;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#30495;&#23454;&#20998;&#24067;&#23494;&#24230;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#25110;&#25311;&#21512;&#20219;&#20309;&#36741;&#21161;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#23427;&#30528;&#37325;&#20110;&#36817;&#20284;&#35745;&#31639;&#23494;&#24230;&#30340;&#31215;&#20998;&#65288;&#27010;&#29575;&#36136;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) acros
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#65292;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#23454;&#29616;&#36923;&#36753;&#36830;&#25509;&#65292;&#20197;&#35299;&#20915;&#29289;&#32852;&#32593;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04338</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#35782;&#21035;&#38382;&#39064;&#30340;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Logical recognition method for solving the problem of identification in the Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04338
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#65292;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#23454;&#29616;&#36923;&#36753;&#36830;&#25509;&#65292;&#20197;&#35299;&#20915;&#29289;&#32852;&#32593;&#20013;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#24212;&#29992;&#36923;&#36753;&#20195;&#25968;&#21644;&#20215;&#20540;&#36923;&#36753;&#26041;&#27861;&#30340;&#26032;&#39046;&#22495;&#65292;&#21363;&#35782;&#21035;&#21508;&#31181;&#29289;&#20307;&#21644;&#29616;&#35937;&#12289;&#21307;&#23398;&#25110;&#25216;&#26415;&#35786;&#26029;&#12289;&#26500;&#24314;&#29616;&#20195;&#26426;&#22120;&#12289;&#26816;&#26597;&#27979;&#35797;&#38382;&#39064;&#31561;&#38382;&#39064;&#65292;&#21487;&#20197;&#24402;&#32467;&#20026;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#26500;&#24314;&#36923;&#36753;&#20989;&#25968;&#30340;&#26368;&#20248;&#25193;&#23637;&#12290;&#20363;&#22914;&#65292;&#22312;&#36923;&#36753;&#35782;&#21035;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#20998;&#26512;&#21644;&#21629;&#39064;&#28436;&#31639;&#27861;&#30340;&#36923;&#36753;&#26041;&#27861;&#26469;&#26500;&#24314;&#33258;&#24049;&#30340;&#35782;&#21035;&#31639;&#27861;&#12290;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#30340;&#20351;&#29992;&#38656;&#35201;&#23384;&#22312;&#30001;k&#20540;&#20989;&#25968;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#24310;&#32493;&#25152;&#34920;&#31034;&#30340;&#36923;&#36753;&#36830;&#25509;&#65292;&#20854;&#20013;&#21464;&#37327;&#26159;&#27491;&#22312;&#35782;&#21035;&#30340;&#23545;&#35937;&#25110;&#29616;&#35937;&#30340;&#36923;&#36753;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#36923;&#36753;&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#20855;&#26377;&#36923;&#36753;&#29305;&#24449;&#21644;&#31867;&#21035;&#30340;&#21442;&#32771;&#34920;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new area of application of methods of algebra of logic and to valued logic, which has emerged recently, is the problem of recognizing a variety of objects and phenomena, medical or technical diagnostics, constructing modern machines, checking test problems, etc., which can be reduced to constructing an optimal extension of the logical function to the entire feature space. For example, in logical recognition systems, logical methods based on discrete analysis and propositional calculus based on it are used to build their own recognition algorithms. In the general case, the use of a logical recognition method provides for the presence of logical connections expressed by the optimal continuation of a k-valued function over the entire feature space, in which the variables are the logical features of the objects or phenomena being recognized. The goal of this work is to develop a logical method for object recognition consisting of a reference table with logical features and classes of non
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.04335</link><description>&lt;p&gt;
LegalLens: &#21033;&#29992;LLMs&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;LLMs&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#39564;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#19982;&#21463;&#23475;&#32773;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#35774;&#32622;&#36866;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#20998;&#21035;&#20026;62.69&#65285;&#21644;81.02&#65285;&#12290;&#30740;&#31350;&#26368;&#32456;&#20844;&#24320;&#21457;&#24067;&#20102;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#31532;&#19968;&#20010;&#26159;&#26816;&#27979;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#27861;&#24459;&#36829;&#35268;&#34892;&#20026;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#19982;&#21487;&#33021;&#21463;&#24433;&#21709;&#30340;&#20010;&#20154;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#30001;&#39046;&#22495;&#19987;&#23478;&#27880;&#37322;&#36827;&#34892;&#39564;&#35777;&#12290;&#20004;&#20010;&#20219;&#21153;&#37117;&#26159;&#20026;&#38598;&#20307;&#35785;&#35772;&#26696;&#24773;&#22659;&#29305;&#21035;&#35774;&#35745;&#30340;&#12290;&#23454;&#39564;&#35774;&#35745;&#37319;&#29992;&#20102;&#26469;&#33258;BERT&#31995;&#21015;&#21644;&#24320;&#28304;LLMs&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38381;&#28304;LLMs&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#35774;&#32622;&#21487;&#29992;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#20854;&#36829;&#35268;&#34892;&#20026;&#35782;&#21035;&#30340;F1&#20998;&#25968;&#20026;62.69&#65285;&#65292;&#19982;&#21463;&#23475;&#32773;&#30456;&#20851;&#30340;&#20998;&#25968;&#20026;81.02&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#29992;&#20110;&#23454;&#39564;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#65292;&#20197;&#25512;&#21160;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals. We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators. Both tasks were designed specifically for the context of class-action cases. The experimental design incorporated fine-tuning models from the BERT family and open-source LLMs, and conducting few-shot experiments using closed-source LLMs. Our results, with an F1-score of 62.69\% (violation identification) and 81.02\% (associating victims), show that our datasets and setups can be used for both tasks. Finally, we publicly release the datasets and the code used for the experiments in order to advance further research in the area of legal natural language processing (NLP).
&lt;/p&gt;</description></item><item><title>LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.04333</link><description>&lt;p&gt;
LESS&#65306;&#29992;&#20110;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#30340;&#36873;&#25321;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LESS: Selecting Influential Data for Targeted Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04333
&lt;/p&gt;
&lt;p&gt;
LESS&#26159;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#20197;&#24320;&#21457;&#29305;&#23450;&#33021;&#21147;&#65292;&#23427;&#37319;&#29992;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37322;&#25918;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#32452;&#21512;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#36890;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#24448;&#24448;&#38656;&#35201;&#19968;&#22871;&#19987;&#38376;&#30340;&#25216;&#33021;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#12290;&#25361;&#25112;&#22312;&#20110;&#20174;&#36825;&#20123;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#20986;&#26368;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#24320;&#21457;&#29305;&#23450;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#19968;&#31181;&#20248;&#21270;&#24863;&#30693;&#19988;&#23454;&#38469;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#24182;&#25191;&#34892;&#36866;&#29992;&#20110;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;&#30340;&#20302;&#31209;&#26799;&#24230;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#20851;&#38190;&#22312;&#20110;LESS&#23558;&#29616;&#26377;&#30340;&#24433;&#21709;&#20844;&#24335;&#35843;&#25972;&#20026;&#19982;Adam&#20248;&#21270;&#22120;&#21644;&#21487;&#21464;&#38271;&#24230;&#25351;&#20196;&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#12290;LESS&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#32500;&#26799;&#24230;&#29305;&#24449;&#30340;&#39640;&#24230;&#21487;&#37325;&#29992;&#21644;&#21487;&#20256;&#36882;&#30340;&#26799;&#24230;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20855;&#26377;&#29305;&#23450;&#33021;&#21147;&#30340;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#30456;&#20284;&#24230;&#36873;&#25321;&#31034;&#20363;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;t
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has unlocked powerful capabilities in large language models (LLMs), effectively using combined datasets to develop generalpurpose chatbots. However, real-world applications often require a specialized suite of skills (e.g., reasoning). The challenge lies in identifying the most relevant data from these extensive datasets to effectively develop specific capabilities, a setting we frame as targeted instruction tuning. We propose LESS, an optimizer-aware and practically efficient algorithm to effectively estimate data influences and perform Low-rank gradiEnt Similarity Search for instruction data selection. Crucially, LESS adapts existing influence formulations to work with the Adam optimizer and variable-length instruction data. LESS first constructs a highly reusable and transferable gradient datastore with low-dimensional gradient features and then selects examples based on their similarity to few-shot examples embodying a specific capability. Experiments show that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#26469;&#22686;&#24378;DNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;DNN&#23618;&#19978;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#26469;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04325</link><description>&lt;p&gt;
&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#22686;&#24378;DNN&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Enhance DNN Adversarial Robustness and Efficiency via Injecting Noise to Non-Essential Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#27880;&#20837;&#22122;&#38899;&#26469;&#22686;&#24378;DNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;DNN&#23618;&#19978;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#26469;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#22312;&#25968;&#25454;&#20998;&#26512;&#21644;&#20915;&#31574;&#26041;&#38754;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#65292;&#20174;&#21307;&#30103;&#21644;&#37329;&#34701;&#21040;&#27773;&#36710;&#12290;&#23613;&#31649;&#20854;&#20855;&#26377;&#30340;&#36716;&#22411;&#24433;&#21709;&#65292;DNN&#38754;&#20020;&#30528;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#23545;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21644;&#19982;&#26356;&#22797;&#26434;&#21644;&#26356;&#22823;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22686;&#24378;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;&#19982;&#20197;&#24448;&#36890;&#36807;&#22343;&#21248;&#27880;&#20837;&#22122;&#38899;&#20197;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38750;&#22343;&#21248;&#22122;&#38899;&#27880;&#20837;&#31639;&#27861;&#65292; strategically applied at&#27599;&#19968;&#20010;DNN layer&#65292;&#20197;&#24178;&#25200;&#25915;&#20987;&#20013;&#24341;&#20837;&#30340;&#23545;&#25239;&#25200;&#21160;&#12290;&#36890;&#36807;&#37319;&#29992;&#36817;&#20284;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35782;&#21035;&#24182;&#20445;&#25252;&#20851;&#38190;&#31070;&#32463;&#20803;&#65292;&#21516;&#26102;&#23545;&#38750;&#20851;&#38190;&#31070;&#32463;&#20803;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#22122;&#38899;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22686;&#24378;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robus
&lt;/p&gt;</description></item><item><title>AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04292</link><description>&lt;p&gt;
AdaFlow: &#21464;&#24322;&#33258;&#36866;&#24212;&#27969;&#31574;&#30053;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04292
&lt;/p&gt;
&lt;p&gt;
AdaFlow&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#20445;&#25345;&#22810;&#26679;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#20223;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#20915;&#31574;&#20013;&#25913;&#36827;&#20102;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#65292;&#20294;&#30001;&#20110;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#36882;&#24402;&#32780;&#23548;&#33268;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#39640;&#25928;&#30340;&#31574;&#30053;&#29983;&#25104;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22810;&#26679;&#21270;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;AdaFlow&#20351;&#29992;&#29366;&#24577;&#26465;&#20214;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#34920;&#31034;&#31574;&#30053;&#65292;&#36825;&#34987;&#31216;&#20026;&#27010;&#29575;&#27969;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#35757;&#32451;&#25439;&#22833;&#30340;&#26465;&#20214;&#26041;&#24046;&#19982;ODE&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26377;&#36259;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#24322;&#33258;&#36866;&#24212;ODE&#27714;&#35299;&#22120;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#20351;AdaFlow&#25104;&#20026;&#19968;&#20010;&#33258;&#36866;&#24212;&#20915;&#31574;&#32773;&#65292;&#33021;&#22815;&#24555;&#36895;&#25512;&#29702;&#32780;&#19981;&#29306;&#29298;&#22810;&#26679;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21160;&#20316;&#20998;&#24067;&#34987;&#38477;&#20302;&#21040;&#19968;&#27493;&#29983;&#25104;&#22120;&#26102;&#65292;&#23427;&#33258;&#21160;&#36864;&#21270;&#21040;&#19968;&#20010;&#19968;&#27493;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based imitation learning improves Behavioral Cloning (BC) on multi-modal decision-making, but comes at the cost of significantly slower inference due to the recursion in the diffusion process. It urges us to design efficient policy generators while keeping the ability to generate diverse actions. To address this challenge, we propose AdaFlow, an imitation learning framework based on flow-based generative modeling. AdaFlow represents the policy with state-conditioned ordinary differential equations (ODEs), which are known as probability flows. We reveal an intriguing connection between the conditional variance of their training loss and the discretization error of the ODEs. With this insight, we propose a variance-adaptive ODE solver that can adjust its step size in the inference stage, making AdaFlow an adaptive decision-maker, offering rapid inference without sacrificing diversity. Interestingly, it automatically reduces to a one-step generator when the action distribution i
&lt;/p&gt;</description></item><item><title>BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04291</link><description>&lt;p&gt;
BiLLM: &#25512;&#21160;LLMs&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04291
&lt;/p&gt;
&lt;p&gt;
BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#36890;&#29992;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20108;&#20540;&#21270;&#21487;&#20197;&#23558;&#27169;&#22411;&#26435;&#37325;&#26497;&#22823;&#22320;&#20943;&#23569;&#21040;&#20165;1&#20301;&#65292;&#38477;&#20302;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#25216;&#26415;&#22312;&#36229;&#20302;&#20301;&#23485;&#19979;&#26080;&#27861;&#20445;&#25345;LLM&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLM&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#12290;&#22522;&#20110;LLMs&#30340;&#26435;&#37325;&#20998;&#24067;&#65292;BiLLM&#39318;&#20808;&#35782;&#21035;&#21644;&#32467;&#26500;&#36873;&#25321;&#37325;&#35201;&#30340;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#20108;&#20540;&#21270;&#27531;&#24046;&#36924;&#36817;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#21387;&#32553;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#38750;&#37325;&#35201;&#26435;&#37325;&#30340;&#38047;&#24418;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#20998;&#21106;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#20998;&#32452;&#21644;&#20108;&#20540;&#21270;&#12290;BiLLM&#39318;&#27425;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
&lt;/p&gt;</description></item><item><title>CasCast&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#29087;&#32451;&#30340;&#32423;&#32852;&#24314;&#27169;&#35299;&#20915;&#20102;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21270;&#21644;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04290</link><description>&lt;p&gt;
CasCast: &#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#30340;&#29087;&#32451;&#32423;&#32852;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
CasCast: Skillful High-resolution Precipitation Nowcasting via Cascaded Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04290
&lt;/p&gt;
&lt;p&gt;
CasCast&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#21363;&#26102;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#29087;&#32451;&#30340;&#32423;&#32852;&#24314;&#27169;&#35299;&#20915;&#20102;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21270;&#21644;&#26497;&#31471;&#38477;&#27700;&#39044;&#27979;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38647;&#36798;&#25968;&#25454;&#30340;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#22312;&#26497;&#31471;&#22825;&#27668;&#39044;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#38477;&#27700;&#21363;&#26102;&#39044;&#25253;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#35299;&#20915;&#65306;&#65288;i&#65289;&#23545;&#20855;&#26377;&#19981;&#21516;&#23610;&#24230;&#30340;&#22797;&#26434;&#38477;&#27700;&#31995;&#32479;&#28436;&#21464;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23545;&#26497;&#31471;&#38477;&#27700;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CasCast&#65292;&#19968;&#20010;&#30001;&#30830;&#23450;&#24615;&#37096;&#20998;&#21644;&#27010;&#29575;&#24615;&#37096;&#20998;&#32452;&#25104;&#30340;&#32423;&#32852;&#26694;&#26550;&#65292;&#23558;&#20013;&#23610;&#24230;&#38477;&#27700;&#20998;&#24067;&#21644;&#23567;&#23610;&#24230;&#27169;&#24335;&#30340;&#39044;&#27979;&#35299;&#32806;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#22312;&#39640;&#20998;&#36776;&#29575;&#19979;&#35757;&#32451;&#32423;&#32852;&#26694;&#26550;&#65292;&#24182;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#27010;&#29575;&#24615;&#24314;&#27169;&#65292;&#21033;&#29992;&#38754;&#21521;&#24103;&#30340;&#24341;&#23548;&#25193;&#25955;&#21464;&#25442;&#22120;&#22686;&#24378;&#26497;&#31471;&#20107;&#20214;&#30340;&#20248;&#21270;&#65292;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#38647;&#36798;&#38477;&#27700;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;CasCast&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precipitation nowcasting based on radar data plays a crucial role in extreme weather prediction and has broad implications for disaster management. Despite progresses have been made based on deep learning, two key challenges of precipitation nowcasting are not well-solved: (i) the modeling of complex precipitation system evolutions with different scales, and (ii) accurate forecasts for extreme precipitation. In this work, we propose CasCast, a cascaded framework composed of a deterministic and a probabilistic part to decouple the predictions for mesoscale precipitation distributions and small-scale patterns. Then, we explore training the cascaded framework at the high resolution and conducting the probabilistic modeling in a low dimensional latent space with a frame-wise-guided diffusion transformer for enhancing the optimization of extreme events while reducing computational costs. Extensive experiments on three benchmark radar precipitation datasets show that CasCast achieves competi
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#21644;&#25968;&#25454;&#22122;&#22768;&#31561;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#25104;&#26524;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#26102;&#20195;&#12290;</title><link>https://arxiv.org/abs/2402.04286</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Progress and Opportunities of Foundation Models in Bioinformatics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04286
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#21644;&#25968;&#25454;&#22122;&#22768;&#31561;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#25928;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#25104;&#26524;&#65292;&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#39046;&#22495;&#24320;&#21551;&#20102;&#26032;&#26102;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26085;&#30410;&#25972;&#21512;&#19979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#37319;&#29992;&#65292;&#32463;&#21382;&#20102;&#19968;&#20010;&#33539;&#24335;&#36716;&#21464;&#12290;&#36825;&#20123;AI&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#35299;&#20915;&#20102;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#21382;&#21490;&#24615;&#30340;&#25361;&#25112;&#65292;&#22914;&#27880;&#37322;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#22122;&#22768;&#30340;&#23384;&#22312;&#12290;FMs&#29305;&#21035;&#25797;&#38271;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#29983;&#29289;&#23398;&#32972;&#26223;&#19979;&#36825;&#26159;&#24120;&#35265;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#23454;&#39564;&#30830;&#23450;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#31243;&#36153;&#26102;&#36153;&#21147;&#12290;&#36825;&#19968;&#29305;&#24615;&#20351;FMs&#22312;&#21508;&#31181;&#19979;&#28216;&#39564;&#35777;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#20986;&#30340;&#25104;&#26524;&#65292;&#23637;&#29616;&#20102;&#23427;&#20204;&#26377;&#25928;&#22320;&#34920;&#31034;&#22810;&#26679;&#21270;&#29983;&#29289;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#27627;&#26080;&#30097;&#38382;&#65292;FMs&#22312;&#35745;&#31639;&#29983;&#29289;&#23398;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24320;&#21551;&#20102;&#19968;&#20010;&#26032;&#26102;&#20195;&#12290;&#26412;&#32508;&#36848;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;FMs&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#36861;&#28335;&#20854;&#28436;&#21464;&#21644;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bioinformatics has witnessed a paradigm shift with the increasing integration of artificial intelligence (AI), particularly through the adoption of foundation models (FMs). These AI techniques have rapidly advanced, addressing historical challenges in bioinformatics such as the scarcity of annotated data and the presence of data noise. FMs are particularly adept at handling large-scale, unlabeled data, a common scenario in biological contexts due to the time-consuming and costly nature of experimentally determining labeled data. This characteristic has allowed FMs to excel and achieve notable results in various downstream validation tasks, demonstrating their ability to represent diverse biological entities effectively. Undoubtedly, FMs have ushered in a new era in computational biology, especially in the realm of deep learning. The primary goal of this survey is to conduct a systematic investigation and summary of FMs in bioinformatics, tracing their evolution, current research status
&lt;/p&gt;</description></item><item><title>&#36816;&#21160;&#26144;&#23556;&#35748;&#30693;&#26159;&#20154;&#31867;&#35270;&#35273;&#20013;&#19981;&#21487;&#20998;&#35299;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#35299;&#37322;&#22823;&#22810;&#25968;&#35270;&#35273;&#21151;&#33021;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35270;&#35273;&#22788;&#29702;&#26041;&#24335;&#24314;&#27169;&#12290;&#36890;&#36807;&#37327;&#21270;&#30340;&#25299;&#25169;&#21305;&#37197;&#21407;&#21017;&#21487;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26377;&#36259;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#26356;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#35270;&#35273;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.04275</link><description>&lt;p&gt;
&#36816;&#21160;&#26144;&#23556;&#35748;&#30693;&#65306;&#20154;&#31867;&#35270;&#35273;&#20013;&#30340;&#19981;&#21487;&#20998;&#35299;&#20027;&#35201;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Motion Mapping Cognition: A Nondecomposable Primary Process in Human Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04275
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#26144;&#23556;&#35748;&#30693;&#26159;&#20154;&#31867;&#35270;&#35273;&#20013;&#19981;&#21487;&#20998;&#35299;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#35299;&#37322;&#22823;&#22810;&#25968;&#35270;&#35273;&#21151;&#33021;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35270;&#35273;&#22788;&#29702;&#26041;&#24335;&#24314;&#27169;&#12290;&#36890;&#36807;&#37327;&#21270;&#30340;&#25299;&#25169;&#21305;&#37197;&#21407;&#21017;&#21487;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26377;&#36259;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#26356;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#35270;&#35273;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#33021;&#20284;&#20046;&#22914;&#27492;&#31070;&#31192;&#65292;&#20197;&#33267;&#20110;&#25105;&#20204;&#30452;&#21040;&#29616;&#22312;&#37117;&#27809;&#26377;&#25104;&#21151;&#29702;&#35299;&#23427;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#24819;&#20171;&#32461;&#19968;&#31181;&#22522;&#26412;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#21363;&#36816;&#21160;&#26144;&#23556;&#35748;&#30693;&#65288;MMC&#65289;&#65292;&#23427;&#24212;&#35813;&#26159;&#20154;&#31867;&#35270;&#35273;&#20013;&#19981;&#21487;&#20998;&#35299;&#30340;&#20027;&#35201;&#21151;&#33021;&#12290;&#22312;&#20854;&#20013;&#65292;&#25105;&#25351;&#20986;&#65292;MMC&#36807;&#31243;&#21487;&#20197;&#29992;&#26469;&#35299;&#37322;&#22823;&#22810;&#25968;&#20154;&#31867;&#35270;&#35273;&#21151;&#33021;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#20294;&#26080;&#27861;&#36890;&#36807;&#20256;&#32479;&#30340;&#35270;&#35273;&#22788;&#29702;&#26041;&#24335;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#21106;&#12289;&#29289;&#20307;&#35782;&#21035;&#21644;&#29289;&#20307;&#36319;&#36394;&#31561;&#65289;&#26377;&#25928;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#36824;&#25552;&#20986;MMC&#21487;&#20197;&#35270;&#20026;&#38472;&#30340;&#25299;&#25169;&#30693;&#35273;&#29702;&#35770;&#22312;&#20154;&#31867;&#35270;&#35273;&#19978;&#30340;&#24310;&#20280;&#65292;&#24182;&#19988;&#20284;&#20046;&#26080;&#27861;&#36890;&#36807;&#29616;&#26377;&#30340;&#26234;&#33021;&#31639;&#27861;&#25216;&#24039;&#26469;&#35299;&#20915;&#12290;&#26368;&#21518;&#65292;&#32467;&#21512;&#23545;MMC&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#21457;&#23637;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#30340;&#24605;&#24819;&#26469;&#25512;&#23548;&#20986;&#19968;&#20010;&#26377;&#36259;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#21363;&#37327;&#21270;&#30340;&#25299;&#25169;&#21305;&#37197;&#21407;&#21017;&#12290;&#20197;&#19978;&#32467;&#26524;&#21487;&#33021;&#32473;&#25105;&#20204;&#24320;&#21457;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#35270;&#35273;&#26041;&#27861;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence seems so mysterious that we have not successfully understood its foundation until now. Here, I want to present a basic cognitive process, motion mapping cognition (MMC), which should be a nondecomposable primary function in human vision. Wherein, I point out that, MMC process can be used to explain most of human visual functions in fundamental, but can not be effectively modelled by traditional visual processing ways including image segmentation, object recognition, object tracking etc. Furthermore, I state that MMC may be looked as an extension of Chen's theory of topological perception on human vision, and seems to be unsolvable using existing intelligent algorithm skills. Finally, along with the requirements of MMC problem, an interesting computational model, quantized topological matching principle can be derived by developing the idea of optimal transport theory. Above results may give us huge inspiration to develop more robust and interpretable machine vision m
&lt;/p&gt;</description></item><item><title>ProtAgents&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04268</link><description>&lt;p&gt;
ProtAgents: &#36890;&#36807;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#36827;&#34892;&#34507;&#30333;&#36136;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04268
&lt;/p&gt;
&lt;p&gt;
ProtAgents&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24179;&#21488;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#65292;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#36229;&#36234;&#33258;&#28982;&#30028;&#20013;&#24050;&#26377;&#30340;&#34507;&#30333;&#36136;&#23545;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#30340;&#36827;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#30446;&#21069;&#34507;&#30333;&#36136;&#35774;&#35745;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27169;&#22411;&#65292;&#22914;&#23558;&#34507;&#30333;&#36136;&#32467;&#26500;&#19982;&#26448;&#26009;&#24615;&#36136;&#30456;&#20114;&#20851;&#32852;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#35299;&#20915;&#31471;&#21040;&#31471;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#38598;&#20013;&#20110;&#29305;&#23450;&#30340;&#26448;&#26009;&#30446;&#26631;&#25110;&#32467;&#26500;&#29305;&#24615;&#65292;&#24403;&#38656;&#35201;&#23558;&#39046;&#22495;&#22806;&#30340;&#30693;&#35782;&#32435;&#20837;&#21040;&#35774;&#35745;&#36807;&#31243;&#25110;&#36827;&#34892;&#32508;&#21512;&#25968;&#25454;&#20998;&#26512;&#26102;&#65292;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;ProtAgents&#24179;&#21488;&#65292;&#29992;&#20110;&#26032;&#22411;&#34507;&#30333;&#36136;&#35774;&#35745;&#65292;&#20854;&#20013;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#30340;AI&#26234;&#33021;&#20307;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#21327;&#20316;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#36890;&#36807;&#26234;&#33021;&#20307;&#30340;&#22810;&#26679;&#21457;&#23637;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25317;&#26377;&#19987;&#38271;&#65292;&#21253;&#25324;&#30693;&#35782;&#26816;&#32034;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#20998;&#26512;&#12289;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#27169;&#25311;&#21644;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#34746;&#26059;CT&#25195;&#25551;&#22312;&#26089;&#26399;&#32954;&#30284;&#31579;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#23454;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#34746;&#26059;CT&#25195;&#25551;&#22312;&#26089;&#26399;&#32954;&#30284;&#31579;&#26597;&#20013;&#30340;&#24212;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Application analysis of ai technology combined with spiral CT scanning in early lung cancer screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#34746;&#26059;CT&#25195;&#25551;&#22312;&#26089;&#26399;&#32954;&#30284;&#31579;&#26597;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#35777;&#23454;&#26089;&#26399;&#35786;&#26029;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20013;&#22269;&#32954;&#30284;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#22312;&#25152;&#26377;&#24694;&#24615;&#32959;&#30244;&#20013;&#21517;&#21015;&#21069;&#33541;&#12290;&#23613;&#31649;&#20013;&#22269;&#21307;&#30103;&#27700;&#24179;&#19981;&#26029;&#21457;&#23637;&#21644;&#25552;&#39640;&#65292;&#20294;&#32954;&#30284;&#24739;&#32773;&#30340;&#24635;&#20307;5&#24180;&#29983;&#23384;&#29575;&#20173;&#28982;&#20302;&#20110;20&#65285;&#65292;&#24182;&#19988;&#20998;&#26399;&#36827;&#34892;&#12290;&#22823;&#37327;&#30740;&#31350;&#35777;&#23454;&#65292;&#26089;&#26399;&#35786;&#26029;&#21644;&#27835;&#30103;&#26089;&#26399;&#32954;&#30284;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#39044;&#21518;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36880;&#28176;&#24320;&#22987;&#24212;&#29992;&#20110;&#32959;&#30244;&#23398;&#20013;&#12290;ai&#22312;&#32959;&#30244;&#31579;&#26597;&#65292;&#20020;&#24202;&#35786;&#26029;&#65292;&#25918;&#30103;&#65288;&#22270;&#20687;&#33719;&#21462;&#65292;&#39118;&#38505;&#22120;&#23448;&#20998;&#21106;&#65292;&#22270;&#20687;&#26657;&#20934;&#21644;&#20256;&#36882;&#65289;&#31561;&#26041;&#38754;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;ai&#33021;&#21542;&#23454;&#29616;&#31038;&#20250;&#21270;&#65292;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20844;&#20247;&#30340;&#24577;&#24230;&#21644;&#25509;&#21463;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#32467;&#21512;AI&#25216;&#26415;&#21644;SCT&#25195;&#25551;&#36827;&#34892;&#26089;&#26399;&#32954;&#30284;&#35786;&#26029;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#34746;&#26059;CT&#25195;&#25551;&#32852;&#21512;&#36827;&#34892;&#32954;&#30284;&#26089;&#26399;&#35786;&#26029;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#21644;&#24212;&#29992;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the incidence and fatality rate of lung cancer in China rank first among all malignant tumors. Despite the continuous development and improvement of China's medical level, the overall 5-year survival rate of lung cancer patients is still lower than 20% and is staged. A number of studies have confirmed that early diagnosis and treatment of early stage lung cancer is of great significance to improve the prognosis of patients. In recent years, artificial intelligence technology has gradually begun to be applied in oncology. ai is used in cancer screening, clinical diagnosis, radiation therapy (image acquisition, at-risk organ segmentation, image calibration and delivery) and other aspects of rapid development. However, whether medical ai can be socialized depends on the public's attitude and acceptance to a certain extent. However, at present, there are few studies on the diagnosis of early lung cancer by AI technology combined with SCT scanning. In view of this, this study ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.04232</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#24773;&#24863;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Agents Predict Emotion?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#35768;&#22810;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#20849;&#24773;&#29702;&#35299;&#21644;&#24773;&#32490;&#29366;&#24577;&#23578;&#26410;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;LLM&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24773;&#22659;&#20013;&#29702;&#35299;&#26032;&#32463;&#39564;&#65292;&#26681;&#25454;&#24773;&#32490;&#35780;&#20272;&#29702;&#35770;&#65292;&#36825;&#23545;&#20110;&#24773;&#32490;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#26234;&#33021;&#20307;&#23558;&#26032;&#32463;&#39564;&#24863;&#30693;&#20026;&#26102;&#38388;&#24207;&#21015;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#24863;&#30693;&#27599;&#20010;&#26032;&#36755;&#20837;&#21518;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#36807;&#21435;&#30456;&#20851;&#35760;&#24518;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#8220;&#35268;&#33539;&#8221;&#65292;&#24182;&#23558;&#26032;&#32463;&#39564;&#19982;&#27492;&#35268;&#33539;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#26234;&#33021;&#20307;&#22914;&#20309;&#22312;&#24773;&#22659;&#20013;&#23545;&#26032;&#32463;&#39564;&#20570;&#20986;&#21453;&#24212;&#12290;&#20351;&#29992;&#24773;&#24863;&#27979;&#35797;PANAS&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#27979;&#35797;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#22312;&#26032;&#32463;&#39564;&#21518;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.04140</link><description>&lt;p&gt;
&#25512;&#36827;&#27861;&#24459;&#25512;&#29702;&#65306;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#22788;&#29702;&#20840;&#29699;&#27861;&#29702;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20559;&#35265;&#30340;&#21322;&#33258;&#21160;&#21270;&#20210;&#35009;&#27969;&#31243;&#65288;SAAPs&#65289;
&lt;/p&gt;
&lt;p&gt;
Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#21517;&#20026;SHIRLEY&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#35782;&#21035;&#27861;&#24459;&#21028;&#20915;&#20013;&#30340;&#20559;&#35265;&#21644;&#36923;&#36753;&#19981;&#19968;&#33268;&#65292;&#24182;&#20419;&#36827;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#22810;&#26041;&#35770;&#35777;&#65292;&#20197;&#20445;&#35777;&#27861;&#24459;&#22312;&#19981;&#21516;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23545;&#21253;&#25324;&#32654;&#22269;&#12289;&#33521;&#22269;&#12289;&#21346;&#26106;&#36798;&#12289;&#29790;&#20856;&#21644;&#39321;&#28207;&#22312;&#20869;&#30340;&#20116;&#20010;&#22269;&#23478;&#30340;&#27861;&#38498;&#21028;&#20915;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#27861;&#24459;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20132;&#21449;&#39046;&#22495;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#35782;&#21035;&#20154;&#31867;&#20559;&#35265;&#21644;&#20419;&#36827;&#27861;&#38498;&#21028;&#20915;&#30340;&#22810;&#26041;&#35770;&#35777;&#30340;&#33258;&#21160;&#21270;&#12289;&#26377;&#25928;&#21644;&#19968;&#33268;&#30340;&#35282;&#33394;&#65292;&#20174;&#32780;&#30830;&#20445;&#27861;&#24459;&#22312;&#21508;&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#20869;&#21644;&#36328;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;&#19968;&#33268;&#24212;&#29992;&#12290;&#36890;&#36807;&#32467;&#21512;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#21644;&#26032;&#24341;&#20837;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#26694;&#26550;&#65292;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#20197;ALMs&#20026;&#22522;&#30784;&#30340;&#22522;&#20110;Grounded Theory&#30340;&#27861;&#23398;&#23454;&#36341;&#30740;&#31350;&#35774;&#35745;&#12290;SHIRLEY&#26159;&#22522;&#20110;OpenAI&#30340;GPT&#25216;&#26415;&#26500;&#24314;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20027;&#35201;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;&#27861;&#24459;&#20915;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03784</link><description>&lt;p&gt;
AirPhyNet: &#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03784
&lt;/p&gt;
&lt;p&gt;
AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#21644;&#24314;&#27169;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#29615;&#22659;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24110;&#21161;&#20010;&#20154;&#21644;&#24403;&#23616;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#31934;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#32570;&#20047;&#22362;&#23454;&#29289;&#29702;&#22522;&#30784;&#30340;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Physics guided Neural Network for Air Quality Prediction&#65288;AirPhyNet&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#20004;&#20010;&#25104;&#29087;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#25193;&#25955;&#21644;&#24179;&#27969;&#65289;&#23558;&#20854;&#34920;&#31034;&#20026;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#23558;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#25429;&#25417;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03583</link><description>&lt;p&gt;
MQuinE:&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20013;&#8220;Z-&#24726;&#35770;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03583
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;KGE&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#27969;&#34892;&#30340;&#29616;&#26377;KGE&#27169;&#22411;&#23384;&#22312;&#34920;&#36798;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;Z-&#24726;&#35770;&#8221;&#12290;&#21463;&#21040;Z-&#24726;&#35770;&#30340;&#23384;&#22312;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KGE&#27169;&#22411;&#65292;&#31216;&#20026;MQuinE&#65292;&#22312;&#19981;&#21463;Z-&#24726;&#35770;&#30340;&#22256;&#25200;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;/&#38750;&#23545;&#31216;&#65292;&#36870;&#21521;&#65292;1-N/N-1/N-N&#21644;&#32452;&#21512;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#23545;&#23454;&#38469;&#30693;&#35782;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Z-&#24726;&#35770;&#30830;&#23454;&#38477;&#20302;&#20102;&#29616;&#26377;KGE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#36229;&#36807;20&#65285;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;MQuinE&#21487;&#20197;&#20943;&#36731;Z-&#24726;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20197;&#26126;&#26174;&#20248;&#21183;&#36229;&#36234;&#29616;&#26377;&#30340;KGE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
&lt;/p&gt;</description></item><item><title>VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03561</link><description>&lt;p&gt;
VLN-Video: &#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#36827;&#34892;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03561
&lt;/p&gt;
&lt;p&gt;
VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#36924;&#30495;&#30340;&#19977;&#32500;&#23460;&#22806;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#29616;&#26377;&#30340;VLN&#26041;&#27861;&#22312;&#23548;&#33322;&#29615;&#22659;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VLN-Video&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#32654;&#22269;&#22810;&#20010;&#22478;&#24066;&#30340;&#34892;&#36710;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#23460;&#22806;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#23548;&#33322;&#25351;&#20196;&#21644;&#21160;&#20316;&#26469;&#25552;&#39640;&#23460;&#22806;VLN&#24615;&#33021;&#12290;VLN-Video&#32467;&#21512;&#20102;&#30452;&#35266;&#32463;&#20856;&#26041;&#27861;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#27169;&#26495;&#22635;&#20805;&#29983;&#25104;&#26377;&#23454;&#38469;&#22522;&#30784;&#30340;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22270;&#20687;&#26059;&#36716;&#30456;&#20284;&#24230;&#30340;&#23548;&#33322;&#21160;&#20316;&#39044;&#27979;&#22120;&#20174;&#34892;&#36710;&#35270;&#39057;&#20013;&#33719;&#21462;VLN&#39118;&#26684;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;VLN&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Touchdown&#25968;&#25454;&#38598;&#21644;&#30001;&#34892;&#36710;&#35270;&#39057;&#21019;&#24314;&#30340;&#35270;&#39057;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.02910</link><description>&lt;p&gt;
DS-MS-TCN: &#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage Temporal Convolutional Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#30340;Otago&#20307;&#25805;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;IMU&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#29616;&#20934;&#30830;&#19988;&#31283;&#23450;&#30340;&#35782;&#21035;&#65292;&#20026;&#24247;&#22797;&#20030;&#25514;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Otago&#36816;&#21160;&#35745;&#21010;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#30340;&#37325;&#35201;&#24247;&#22797;&#20030;&#25514;&#65292;&#26088;&#22312;&#22686;&#24378;&#24179;&#34913;&#21644;&#21147;&#37327;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21333;&#20010;&#33136;&#37096;&#20329;&#25140;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMU)&#65292;&#22312;&#32769;&#24180;&#20154;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#35782;&#21035;Otago&#20307;&#25805;&#21160;&#20316;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#30740;&#31350;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#30740;&#31350;&#22312;&#23454;&#39564;&#23460;&#35774;&#32622;&#20013;&#25307;&#21215;&#20102;36&#21517;&#32769;&#24180;&#20154;&#65292;&#24182;&#23545;&#39069;&#22806;&#25307;&#21215;&#30340;7&#21517;&#32769;&#24180;&#20154;&#36827;&#34892;&#20102;&#23478;&#24237;&#35780;&#20272;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23610;&#24230;&#22810;&#38454;&#27573;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;(DS-MS-TCN)&#65292;&#29992;&#20110;&#20004;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#20998;&#31867;&#65292;&#23558;&#20854;&#32435;&#20837;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27169;&#22411;&#19987;&#27880;&#20110;&#35782;&#21035;&#27599;&#20010;&#20307;&#25805;&#21160;&#20316;&#30340;&#37325;&#22797;&#27425;&#25968;(&#24494;&#26631;&#31614;)&#12290;&#38543;&#21518;&#30340;&#38454;&#27573;&#25193;&#23637;&#20102;&#35782;&#21035;&#33539;&#22260;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#36816;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete ra
&lt;/p&gt;</description></item><item><title>LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02544</link><description>&lt;p&gt;
LHRS-Bot&#65306;&#21033;&#29992;VGI&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#36965;&#24863;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02544
&lt;/p&gt;
&lt;p&gt;
LHRS-Bot &#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;(VGI)&#22686;&#24378;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#36817;&#26399;MLLM&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#26410;&#23545;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#36827;&#34892;&#20805;&#20998;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38761;&#21629;&#24615;&#33021;&#21147;&#24320;&#21019;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24182;&#20419;&#36827;&#20102;&#22312;&#21508;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36965;&#24863;&#65288;RS&#65289;&#39046;&#22495;&#20013;&#65292;&#36817;&#26399;&#30340;MLLM&#21162;&#21147;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#36965;&#24863;&#22270;&#20687;&#20013;&#22810;&#26679;&#30340;&#22320;&#29702;&#26223;&#35266;&#21644;&#29289;&#20307;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;LHRS-Align&#65292;&#20197;&#21450;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;RS&#29305;&#23450;&#25351;&#23548;&#25968;&#25454;&#38598;LHRS-Instruct&#65292;&#21033;&#29992;&#20016;&#23500;&#30340;&#33258;&#24895;&#22320;&#29702;&#20449;&#24687;&#65288;VGI&#65289;&#21644;&#20840;&#29699;&#21487;&#29992;&#30340;RS&#22270;&#20687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LHRS-Bot&#65292;&#19968;&#31181;&#38024;&#23545;RS&#22270;&#20687;&#29702;&#35299;&#30340;MLLM&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23618;&#27425;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#31574;&#30053;&#21644;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LHRS-Bot&#23637;&#29616;&#20986;&#23545;RS&#22270;&#20687;&#30340;&#28145;&#21051;&#29702;&#35299;&#20197;&#21450;&#22312;RS&#39046;&#22495;&#20869;&#36827;&#34892;&#32454;&#33268;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The revolutionary capabilities of large language models (LLMs) have paved the way for multimodal large language models (MLLMs) and fostered diverse applications across various specialized domains. In the remote sensing (RS) field, however, the diverse geographical landscapes and varied objects in RS imagery are not adequately considered in recent MLLM endeavors. To bridge this gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an informative RS-specific instruction dataset, LHRS-Instruct, leveraging the extensive volunteered geographic information (VGI) and globally available RS images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored for RS image understanding through a novel multi-level vision-language alignment strategy and a curriculum learning method. Comprehensive experiments demonstrate that LHRS-Bot exhibits a profound understanding of RS images and the ability to perform nuanced reasoning within the RS domain.
&lt;/p&gt;</description></item><item><title>XAI-CF&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#21462;&#35777;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#35774;&#22791;&#21644;&#22823;&#37327;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#20445;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#27807;&#36890;&#21644;&#27861;&#24459;&#26631;&#20934;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02452</link><description>&lt;p&gt;
XAI-CF -- &#25506;&#35752;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#21462;&#35777;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
XAI-CF -- Examining the Role of Explainable Artificial Intelligence in Cyber Forensics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02452
&lt;/p&gt;
&lt;p&gt;
XAI-CF&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#32593;&#32476;&#21462;&#35777;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#32593;&#32476;&#35774;&#22791;&#21644;&#22823;&#37327;&#25968;&#25454;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#20445;&#19982;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#27807;&#36890;&#21644;&#27861;&#24459;&#26631;&#20934;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22797;&#26434;&#32593;&#32476;&#35774;&#22791;&#30340;&#20852;&#36215;&#65292;&#32593;&#32476;&#21462;&#35777;&#65288;CF&#65289;&#38754;&#20020;&#30528;&#35768;&#22810;&#26032;&#30340;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#25163;&#26426;&#19978;&#36816;&#34892;&#30528;&#25968;&#21313;&#20010;&#31995;&#32479;&#65292;&#27599;&#20010;&#31995;&#32479;&#25317;&#26377;&#19978;&#30334;&#19975;&#20010;&#21487;&#19979;&#36733;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#20040;&#24222;&#22823;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#31579;&#26597;&#24182;&#29702;&#35299;&#38656;&#35201;&#26032;&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#35201;&#25104;&#21151;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#32593;&#32476;&#21462;&#35777;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#21521;&#32593;&#32476;&#21462;&#35777;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#22914;&#21462;&#35777;&#20998;&#26512;&#24072;&#21644;&#27861;&#38498;&#25104;&#21592;&#65289;&#35777;&#26126;&#21644;&#35299;&#37322;&#32467;&#26524;&#65292;&#20197;&#20415;&#20182;&#20204;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#25104;&#21151;&#22320;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#32593;&#32476;&#21462;&#35777;&#20013;&#65292;&#23601;&#38656;&#35201;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24314;&#31435;&#20449;&#20219;&#12290;&#25509;&#21463;&#22312;&#32593;&#32476;&#21462;&#35777;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20123;&#22240;&#32032;&#26159;&#20351;&#20154;&#24037;&#26234;&#33021;&#30495;&#23454;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#29702;&#35299;&#21644;&#20132;&#20114;&#12290;&#36825;&#26679;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23558;&#26356;&#23481;&#26131;&#34987;&#20844;&#20247;&#25509;&#21463;&#65292;&#24182;&#30830;&#20445;&#19982;&#27861;&#24459;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#12290;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31995;&#32479;&#21487;&#20197;&#22312;&#32593;&#32476;&#21462;&#35777;&#20013;&#25285;&#24403;&#36825;&#20010;&#35282;&#33394;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#31995;&#32479;&#31216;&#20026;XAI-CF&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of complex cyber devices Cyber Forensics (CF) is facing many new challenges. For example, there are dozens of systems running on smartphones, each with more than millions of downloadable applications. Sifting through this large amount of data and making sense requires new techniques, such as from the field of Artificial Intelligence (AI). To apply these techniques successfully in CF, we need to justify and explain the results to the stakeholders of CF, such as forensic analysts and members of the court, for them to make an informed decision. If we want to apply AI successfully in CF, there is a need to develop trust in AI systems. Some other factors in accepting the use of AI in CF are to make AI authentic, interpretable, understandable, and interactive. This way, AI systems will be more acceptable to the public and ensure alignment with legal standards. An explainable AI (XAI) system can play this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and 
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.01748</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#20316;&#20026;AI&#21407;&#29983;&#26080;&#32447;&#31995;&#32479;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#12289;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20174;&#32780;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#30784;&#27169;&#22411;&#34987;&#23459;&#31216;&#20026;6G&#31995;&#32479;&#30340;&#25913;&#21464;&#32773;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#26080;&#32447;&#32593;&#32476;&#30340;LLMs&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#20197;&#26080;&#32447;&#20026;&#20013;&#24515;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#37326;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;(AI)&#21407;&#29983;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#22522;&#20110;NLP&#30340;&#22522;&#30784;&#27169;&#22411;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#33021;&#21147;&#20419;&#36827;&#20102;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#30340;&#35774;&#35745;&#65306;1) &#22788;&#29702;&#22810;&#27169;&#24577;&#24863;&#30693;&#25968;&#25454;&#65292;2) &#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#23558;&#29289;&#29702;&#31526;&#21495;&#34920;&#31034;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#26080;&#32447;&#31995;&#32479;&#32852;&#31995;&#36215;&#26469;&#65292;3) &#36890;&#36807;&#26080;&#32447;&#29615;&#22659;&#21453;&#39304;&#23454;&#29616;&#21487;&#25945;&#23548;&#24615;&#65292;&#20197;&#20419;&#36827;&#21160;&#24577;&#32593;&#32476;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and foundation models have been recently touted as a game-changer for 6G systems. However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language mod- els that were designed for natural language processing (NLP) applications. To address this challenge and create wireless-centric foundation models, this paper presents a comprehensive vision on how to design universal foundation models that are tailored towards the deployment of artificial intelligence (AI)-native networks. Diverging from NLP-based foundation models, the proposed framework promotes the design of large multi-modal models (LMMs) fostered by three key capabilities: 1) processing of multi-modal sensing data, 2) grounding of physical symbol representations in real-world wireless systems using causal reasoning and retrieval-augmented generation (RAG), and 3) enabling instructibility from the wireless environment feedback to facilitate dynamic network a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01729</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Contextualization Distillation from Large Language Model for Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#33021;&#20811;&#26381;&#29616;&#26377;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25991;&#26412;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20013;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#35821;&#26009;&#24211;&#20174;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#25110;&#21516;&#20041;&#35789;&#23450;&#20041;&#20013;&#25910;&#38598;&#30340;&#38745;&#24577;&#21644;&#22122;&#22768;&#24615;&#36136;&#24120;&#24120;&#38480;&#21046;&#20102;&#22522;&#20110;PLM&#30340;KGC&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#33976;&#39311;&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21487;&#25554;&#20837;&#21644;&#21487;&#25773;&#25918;&#30340;&#26041;&#27861;&#65292;&#19982;&#21028;&#21035;&#21644;&#29983;&#25104;&#30340;KGC&#26694;&#26550;&#20860;&#23481;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#32039;&#20945;&#30340;&#32467;&#26500;&#21270;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#27573;&#33853;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#23450;&#21046;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#37325;&#24314;&#21644;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#36739;&#23567;&#30340;KGC&#27169;&#22411;&#33021;&#22815;&#21560;&#25910;&#36825;&#20123;&#20016;&#23500;&#30340;&#19977;&#20803;&#32452;&#20013;&#30340;&#35265;&#35299;&#12290;&#23545;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;KGC&#25216;&#26415;&#30340;&#20840;&#38754;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#21644;&#36866;&#24212;&#24615;&#65292;&#25581;&#31034;&#20102;&#26080;&#35770;&#22522;&#30784;&#31649;&#36947;&#22914;&#20309;&#65292;&#22987;&#32456;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01726</link><description>&lt;p&gt;
AI&#20013;&#20171;&#20132;&#27969;&#30340;&#25351;&#23548;&#65306;AI&#19981;&#25913;&#21464;&#23545;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of Text Messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01726
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#21487;&#33021;&#23545;&#24863;&#30693;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#20154;&#26469;&#35828;&#65292;&#28966;&#34385;&#12289;&#25233;&#37057;&#21644;&#20854;&#20182;&#31038;&#20132;&#21644;&#24515;&#29702;&#22240;&#32032;&#21487;&#33021;&#20351;&#25776;&#20889;&#25991;&#26412;&#28040;&#24687;&#25104;&#20026;&#19968;&#39033;&#31215;&#26497;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#26159;&#24110;&#21161;&#37027;&#20123;&#26412;&#26469;&#20250;&#35273;&#24471;&#21457;&#36865;&#30701;&#20449;&#22256;&#38590;&#25110;&#26377;&#21387;&#21147;&#30340;&#29992;&#25143;&#30340;&#23436;&#32654;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20351;&#29992;&#24555;&#36895;&#26222;&#21450;&#65292;&#20294;&#23545;&#20854;&#22312;&#25991;&#26412;&#28040;&#24687;&#25776;&#20889;&#20013;&#30340;&#36741;&#21161;&#20351;&#29992;&#30340;&#32771;&#34385;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#65292;AI&#30340;&#20844;&#20247;&#24773;&#32490;&#36739;&#24046;&#21487;&#33021;&#23548;&#33268;&#20854;&#36741;&#21161;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#20351;&#29992;&#23545;&#24863;&#30693;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#20351;&#29992;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20154;&#20204;&#26159;&#21542;&#35748;&#20026;&#19968;&#26465;&#25991;&#26412;&#28040;&#24687;&#26159;&#21542;&#22312;&#25776;&#20889;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;AI&#30340;&#36741;&#21161;&#65292;&#20250;&#25913;&#21464;&#20854;&#24863;&#30693;&#30340;&#35821;&#35843;&#12289;&#28165;&#26224;&#24230;&#21644;&#34920;&#36798;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;26&#21517;&#21442;&#19982;&#32773;&#23545;18&#26465;&#38543;&#26426;&#26631;&#35760;&#30340;&#39044;&#20808;&#25776;&#20889;&#30340;&#25991;&#26412;&#28040;&#24687;&#30340;&#24863;&#30693;&#12290;&#36890;&#36807;&#20998;&#26512;&#21442;&#19982;&#32773;&#23545;&#28040;&#24687;&#35821;&#35843;&#30340;&#35780;&#20998;&#65292;&#25105;&#20204;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00350</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Based Fuzzing Techniques: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36719;&#20214;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#26102;&#20195;&#65292;&#36719;&#20214;&#23433;&#20840;&#21644;&#28431;&#27934;&#20998;&#26512;&#23545;&#20110;&#36719;&#20214;&#24320;&#21457;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#65292;&#27169;&#31946;&#27979;&#35797;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#24182;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#29616;&#26377;&#30340;&#27169;&#31946;&#27979;&#35797;&#25216;&#26415;&#24182;&#19981;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#36719;&#20214;&#28431;&#27934;&#19981;&#26029;&#28436;&#21270;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#36235;&#21183;&#26159;&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#27169;&#31946;&#27979;&#35797;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;&#34701;&#21512;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#30340;&#36719;&#20214;&#27979;&#35797;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#36890;&#36807;&#24635;&#32467;2024&#24180;&#20043;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#23545;LLMs&#12289;&#27169;&#31946;&#27979;&#35797;&#21644;&#22522;&#20110;LLMs&#30340;&#27169;&#31946;&#27979;&#35797;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#36824;&#30740;&#31350;&#20102;LLMs&#21644;&#27169;&#31946;&#27979;&#35797;&#22312;&#36719;&#20214;&#27979;&#35797;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also invest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2401.16123</link><description>&lt;p&gt;
&#23547;&#27714;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#65311;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#22686;&#37327;&#23398;&#20064;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#36710;&#34892;&#19994;&#36805;&#36895;&#21521;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#36710;&#36742;&#21457;&#23637;&#65292;&#20256;&#32479;&#30340;&#36710;&#36742;&#20132;&#20114;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#35302;&#25720;&#21644;&#35821;&#38899;&#21629;&#20196;&#30340;&#31995;&#32479;&#65289;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#38750;&#39550;&#39542;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#24341;&#29992;&#36710;&#36742;&#22806;&#37096;&#29289;&#20307;&#65289;&#20013;&#24050;&#32463;&#21464;&#24471;&#19981;&#21512;&#36866;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36716;&#21521;&#23039;&#21183;&#36755;&#20837;&#65288;&#22914;&#25163;&#21183;&#12289;&#35270;&#32447;&#21644;&#22836;&#37096;&#23039;&#21183;&#25163;&#21183;&#65289;&#20316;&#20026;&#39550;&#39542;&#36807;&#31243;&#20013;&#26356;&#21512;&#36866;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39550;&#39542;&#30340;&#21160;&#24577;&#29305;&#24615;&#21644;&#20010;&#20307;&#24046;&#24322;&#65292;&#39550;&#39542;&#21592;&#30340;&#23039;&#21183;&#36755;&#20837;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#34429;&#28982;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#22266;&#26377;&#30340;&#21487;&#21464;&#24615;&#21487;&#20197;&#36890;&#36807;&#22823;&#35268;&#27169;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#65292;&#20294;&#26222;&#36941;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#38024;&#23545;&#30446;&#26631;&#24341;&#29992;&#20351;&#29992;&#32422;&#26463;&#30340;&#21333;&#23454;&#20363;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#25345;&#32493;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#30446;&#26631;&#24341;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#36866;&#24212;&#20010;&#20307;&#39550;&#39542;&#21592;&#30340;&#21464;&#21270;&#34892;&#20026;&#21644;&#21508;&#31181;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#25361;&#25112;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#65292;&#26377;&#25928;&#23454;&#29616;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2401.15753</link><description>&lt;p&gt;
&#36890;&#36807;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#23545;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
An objective comparison of methods for augmented reality in laparoscopic liver resection by preoperative-to-intraoperative image fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#22686;&#24378;&#29616;&#23454;&#26041;&#27861;&#30340;&#23458;&#35266;&#20248;&#21155;&#65292;&#25552;&#20986;&#20102;&#26415;&#21069;&#21040;&#26415;&#20013;&#22270;&#20687;&#34701;&#21512;&#25361;&#25112;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#65292;&#26377;&#25928;&#23454;&#29616;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33145;&#33108;&#38236;&#32925;&#20999;&#38500;&#20013;&#30340;&#22686;&#24378;&#29616;&#23454;&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#27169;&#24335;&#65292;&#23427;&#20801;&#35768;&#22806;&#31185;&#21307;&#29983;&#22312;&#33145;&#33108;&#38236;&#22270;&#20687;&#19978;&#25237;&#23556;&#32959;&#30244;&#21644;&#23884;&#20837;&#22312;&#32925;&#33039;&#20869;&#37096;&#30340;&#34880;&#31649;&#65292;&#20197;&#24110;&#21161;&#23450;&#20301;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#26415;&#21069;&#20174;CT&#25110;MRI&#25968;&#25454;&#25552;&#21462;&#30340;3D&#27169;&#22411;&#34987;&#27880;&#20876;&#21040;&#26415;&#20013;&#30340;&#33145;&#33108;&#38236;&#22270;&#20687;&#20013;&#12290;&#20174;3D-2D&#34701;&#21512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#22823;&#22810;&#25968;&#31639;&#27861;&#21033;&#29992;&#35299;&#21078;&#26631;&#24535;&#29289;&#26469;&#25351;&#23548;&#27880;&#20876;&#65292;&#36825;&#20123;&#26631;&#24535;&#29289;&#21253;&#25324;&#32925;&#33039;&#30340;&#19979;&#32447;&#12289;&#38203;&#38256;&#38887;&#24102;&#21644;&#38381;&#38145;&#36718;&#24275;&#12290;&#22312;&#33145;&#33108;&#38236;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#20013;&#25163;&#24037;&#26631;&#35760;&#36825;&#20123;&#26631;&#24535;&#29289;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#32463;&#39564;&#20016;&#23500;&#30340;&#29992;&#25143;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#20351;&#22686;&#24378;&#29616;&#23454;&#22312;&#25163;&#26415;&#23460;&#20013;&#33021;&#22815;&#26377;&#25928;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#21307;&#23398;&#24433;&#20687;&#21644;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#65288;MICCAI 2022&#65289;&#30340;"&#26415;&#21069;&#21040;&#26415;&#20013;&#33145;&#33108;&#38236;&#34701;&#21512;&#25361;&#25112;"&#65288;P2ILF&#65289;&#20013;&#25552;&#20986;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmented reality for laparoscopic liver resection is a visualisation mode that allows a surgeon to localise tumours and vessels embedded within the liver by projecting them on top of a laparoscopic image. Preoperative 3D models extracted from CT or MRI data are registered to the intraoperative laparoscopic images during this process. In terms of 3D-2D fusion, most of the algorithms make use of anatomical landmarks to guide registration. These landmarks include the liver's inferior ridge, the falciform ligament, and the occluding contours. They are usually marked by hand in both the laparoscopic image and the 3D model, which is time-consuming and may contain errors if done by a non-experienced user. Therefore, there is a need to automate this process so that augmented reality can be used effectively in the operating room. We present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge (P2ILF), held during the Medical Imaging and Computer Assisted Interventions (MICCAI 2022)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2401.07836</link><description>&lt;p&gt;
&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#39118;&#38505;&#65306;&#20915;&#23450;&#24615;&#21644;&#32047;&#31215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Two Types of AI Existential Risk: Decisive and Accumulative
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#65292;&#25351;&#20986;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#28781;&#32477;&#24615;&#28798;&#38590;&#26377;&#20004;&#31181;&#21487;&#33021;&#36335;&#24452;&#65306;&#19968;&#31181;&#26159;&#31361;&#28982;&#21457;&#29983;&#30340;AI&#25509;&#31649;&#65292;&#21478;&#19968;&#31181;&#26159;&#36880;&#28176;&#31215;&#32047;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#23545;&#20154;&#24037;&#26234;&#33021;(AI)&#24341;&#36215;&#30340;&#23384;&#22312;&#39118;&#38505;(x-risks)&#30340;&#35752;&#35770;&#36890;&#24120;&#38598;&#20013;&#22312;&#30001;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#24341;&#36215;&#30340;&#31361;&#28982;&#12289;&#20005;&#37325;&#20107;&#20214;&#19978;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#21487;&#33021;&#36798;&#21040;&#25110;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#31995;&#32479;&#12290;&#36825;&#20123;&#20107;&#20214;&#23558;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#65292;&#35201;&#20040;&#23548;&#33268;&#20154;&#31867;&#28781;&#32477;&#65292;&#35201;&#20040;&#26080;&#27861;&#36870;&#36716;&#22320;&#20351;&#20154;&#31867;&#25991;&#26126;&#38519;&#20837;&#26080;&#27861;&#24674;&#22797;&#30340;&#29366;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35752;&#35770;&#32463;&#24120;&#24573;&#35270;AI x-risk&#36880;&#28176;&#36890;&#36807;&#19968;&#31995;&#21015;&#36739;&#23567;&#20294;&#30456;&#20114;&#20851;&#32852;&#30340;&#20013;&#26029;&#36880;&#28176;&#26174;&#29616;&#20986;&#26469;&#30340;&#20005;&#37325;&#21487;&#33021;&#24615;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#36328;&#36234;&#20851;&#38190;&#38408;&#20540;&#12290;&#35813;&#35770;&#25991;&#23558;&#20256;&#32479;&#30340;&#8220;&#20915;&#23450;&#24615;AI x-risk&#20551;&#35774;&#8221;&#19982;&#8220;&#32047;&#31215;&#24615;AI x-risk&#20551;&#35774;&#8221;&#36827;&#34892;&#23545;&#27604;&#12290;&#21069;&#32773;&#25551;&#32472;&#20102;&#19968;&#31181;&#26126;&#26174;&#30340;AI&#25509;&#31649;&#36335;&#24452;&#65292;&#20854;&#29305;&#24449;&#26159;&#26080;&#27861;&#25511;&#21046;&#30340;&#36229;&#32423;&#26234;&#33021;&#31561;&#24773;&#26223;&#65292;&#32780;&#21518;&#32773;&#21017;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#23548;&#33268;&#28781;&#32477;&#24615;&#28798;&#38590;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#36825;&#28041;&#21450;&#21040;&#30001;AI&#24341;&#36215;&#30340;&#20005;&#37325;&#23041;&#32961;&#30340;&#36880;&#28176;&#32047;&#31215;&#65292;&#20363;&#22914;&#20005;&#37325;&#30340;&#28431;&#27934;&#21644;&#31995;&#32479;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The conventional discourse on existential risks (x-risks) from AI typically focuses on abrupt, dire events caused by advanced AI systems, particularly those that might achieve or surpass human-level intelligence. These events have severe consequences that either lead to human extinction or irreversibly cripple human civilization to a point beyond recovery. This discourse, however, often neglects the serious possibility of AI x-risks manifesting incrementally through a series of smaller yet interconnected disruptions, gradually crossing critical thresholds over time. This paper contrasts the conventional "decisive AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the former envisions an overt AI takeover pathway, characterized by scenarios like uncontrollable superintelligence, the latter suggests a different causal pathway to existential catastrophes. This involves a gradual accumulation of critical AI-induced threats such as severe vulnerabilities and systemic e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#65292;&#36890;&#36807;&#28151;&#21512;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#25913;&#36827;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25351;&#20196;&#34701;&#21512;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20195;&#30721;LLM&#22312;&#22810;&#20010;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.15692</link><description>&lt;p&gt;
&#25351;&#20196;&#34701;&#21512;&#65306;&#36890;&#36807;&#28151;&#21512;&#21270;&#25512;&#36827;&#25552;&#31034;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instruction Fusion: Advancing Prompt Evolution through Hybridization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#65292;&#36890;&#36807;&#28151;&#21512;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#25913;&#36827;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25351;&#20196;&#34701;&#21512;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20195;&#30721;LLM&#22312;&#22810;&#20010;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24320;&#25918;&#22495;&#32534;&#30721;&#26597;&#35810;&#65292;&#32454;&#35843;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;Evol-Instruct&#22312;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#25552;&#31034;&#28436;&#21270;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#25351;&#20196;&#34701;&#21512;&#65288;IF&#65289;&#12290;IF&#36890;&#36807;&#28151;&#21512;&#21270;&#36807;&#31243;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29992;&#20110;&#20195;&#30721;LLM&#30340;&#35757;&#32451;&#25552;&#31034;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25552;&#20986;&#30340;&#26032;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;LLM&#22312;&#20154;&#24037;&#35780;&#20272;&#12289;&#20154;&#24037;&#35780;&#20272;+&#12289;MBPP&#12289;MBPP+&#21644;MultiPL-E&#31561;&#20116;&#20010;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#20984;&#26174;&#20102;&#25351;&#20196;&#34701;&#21512;&#22312;&#25512;&#36827;LLM&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#36716;&#25442;&#20026;&#26032;&#30340;&#34920;&#31034;&#65292;&#36827;&#32780;&#32553;&#23567;&#19982;&#26080;&#30417;&#30563;&#20154;&#33080;&#39564;&#35777;&#25216;&#26415;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;Labeled Faces in the Wild (LFW)&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#22312;EER&#26041;&#38754;&#23454;&#29616;&#20102;56%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.14395</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#39564;&#35777;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Learning Image Verification Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#36716;&#25442;&#20026;&#26032;&#30340;&#34920;&#31034;&#65292;&#36827;&#32780;&#32553;&#23567;&#19982;&#26080;&#30417;&#30563;&#20154;&#33080;&#39564;&#35777;&#25216;&#26415;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;Labeled Faces in the Wild (LFW)&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#22312;EER&#26041;&#38754;&#23454;&#29616;&#20102;56%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#22270;&#20687;&#35782;&#21035;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24448;&#24448;&#38590;&#20197;&#33719;&#24471;&#12290;&#36825;&#23548;&#33268;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#20154;&#33080;&#39564;&#35777;&#25216;&#26415;&#30456;&#27604;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#36716;&#25442;&#20026;&#26032;&#30340;&#34920;&#31034;&#26469;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#33258;&#21160;&#32534;&#30721;&#22120;&#35757;&#32451;&#30340;&#26159;&#37325;&#26500;&#30456;&#37051;&#30340;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#32780;&#19981;&#26159;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#21521;&#37327;&#12290;&#36825;&#20123;&#37051;&#23621;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#26159;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#19982;&#35757;&#32451;&#20154;&#33080;&#22270;&#20687;&#21521;&#37327;&#30340;&#26368;&#39640;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#26080;&#30417;&#30563;&#36807;&#31243;&#36873;&#25321;&#30340;&#12290;&#35813;&#26041;&#27861;&#22312;Labeled Faces in the Wild (LFW)&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#22312;EER&#26041;&#38754;&#21462;&#24471;&#20102;56%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#36825;&#25104;&#21151;&#22320;&#32553;&#23567;&#20102;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;PLDA&#35780;&#20998;&#31995;&#32479;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning are commonly employed for image recognition, usually huge amount of labeled training data is required, which may not always be readily available. This leads to a noticeable performance disparity when compared to state-of-the-art unsupervised face verification techniques. In this work, we propose a method to narrow this gap by leveraging an autoencoder to convert the face image vector into a novel representation. Notably, the autoencoder is trained to reconstruct neighboring face image vectors rather than the original input image vectors. These neighbor face image vectors are chosen through an unsupervised process based on the highest cosine scores with the training face image vectors. The proposed method achieves a relative improvement of 56\% in terms of EER over the baseline system on Labeled Faces in the Wild (LFW) dataset. This has successfully narrowed down the performance gap between cosine and PLDA scoring systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.12568</link><description>&lt;p&gt;
&#23545;&#39640;&#32500;&#24230;&#28216;&#25103;&#30340;&#23545;&#25163;&#22609;&#24418;&#36827;&#34892;&#20102;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Opponent Shaping to High Dimensional Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#21160;&#26426;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#20026;&#38646;&#21644;&#28216;&#25103;&#24320;&#21457;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#25163;&#22609;&#24418;&#65288;OS&#65289;&#26041;&#27861;&#26126;&#30830;&#22320;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#21512;&#20316;&#29609;&#23478;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20272;&#35745;&#26356;&#39640;&#38454;&#23548;&#25968;&#25110;&#25193;&#23637;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;OS&#26041;&#27861;&#21482;&#22312;&#20302;&#32500;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#33021;&#22815;&#25193;&#23637;&#21040;&#22797;&#26434;&#29615;&#22659;&#30340;&#26367;&#20195;&#26041;&#27861;&#35201;&#20040;&#25910;&#25947;&#20110;&#19981;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#23545;&#29615;&#22659;&#25110;&#21512;&#20316;&#29609;&#23478;&#36827;&#34892;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31532;&#19968;&#27425;&#25104;&#21151;&#22320;&#23558;&#22522;&#20110;OS&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#25805;&#20316;&#21644;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#24191;&#20041;&#28216;&#25103;&#20013;&#12290;&#32463;&#36807;&#23545;&#20808;&#21069;&#31639;&#27861;&#20351;&#29992;&#30340;&#20803;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#34920;&#31034;&#36827;&#34892;&#20998;&#26512;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26041;&#27861;&#31216;&#20026;Shaper&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25913;&#21892;&#20010;&#20307;&#21644;&#38598;&#20307;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent settings with mixed incentives, methods developed for zero-sum games have been shown to lead to detrimental outcomes. To address this issue, opponent shaping (OS) methods explicitly learn to influence the learning dynamics of co-players and empirically lead to improved individual and collective outcomes. However, OS methods have only been evaluated in low-dimensional environments due to the challenges associated with estimating higher-order derivatives or scaling model-free meta-learning. Alternative methods that scale to more complex settings either converge to undesirable solutions or rely on unrealistic assumptions about the environment or co-players. In this paper, we successfully scale an OS-based approach to general-sum games with temporally-extended actions and long-time horizons for the first time. After analysing the representations of the meta-state and history used by previous algorithms, we propose a simplified version called Shaper. We show empirically that 
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.10396</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#33021;&#22815;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#24110;&#21161;&#20174;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#20013;&#24674;&#22797;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can Fairness Constraints Help Recover From Biased Data?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10396
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#32422;&#26463;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#33021;&#22815;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#35748;&#20026;&#65292;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#20844;&#24179;&#24615;&#32422;&#26463;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#20943;&#23569;&#65292;&#32780;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#21152;&#21095;&#36825;&#31181;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26497;&#24230;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#19978;&#65292;&#21363;&#20351;&#37319;&#29992;&#24179;&#31561;&#26426;&#20250;&#32422;&#26463;&#65292;&#20063;&#21487;&#20197;&#24674;&#22797;&#21040;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#19978;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#20182;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#23427;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#21487;&#20197;&#38544;&#24335;&#20462;&#27491;&#25968;&#25454;&#20559;&#24046;&#65292;&#21516;&#26102;&#20811;&#26381;&#20102;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;&#20182;&#20204;&#30340;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#27169;&#25311;&#20102;&#21463;&#21387;&#36843;&#20154;&#32676;&#30340;&#34920;&#24449;&#21644;&#26631;&#31614;&#20559;&#35265;&#65292;&#24182;&#22312;&#20855;&#26377;&#29420;&#31435;&#26631;&#31614;&#22122;&#22768;&#30340;&#31616;&#21333;&#26465;&#20214;&#19979;&#65292;&#38024;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#23637;&#31034;&#20102;&#19978;&#36848;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;Blum&#65286;Stangl&#65288;2019&#65289;&#30340;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#12289;&#25968;&#25454;&#20559;&#24046;&#27169;&#22411;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#20551;&#35774;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
A general belief in fair classification is that fairness constraints incur a trade-off with accuracy, which biased data may worsen. Contrary to this belief, Blum &amp; Stangl (2019) show that fair classification with equal opportunity constraints even on extremely biased data can recover optimally accurate and fair classifiers on the original data distribution. Their result is interesting because it demonstrates that fairness constraints can implicitly rectify data bias and simultaneously overcome a perceived fairness-accuracy trade-off. Their data bias model simulates under-representation and label bias in underprivileged population, and they show the above result on a stylized data distribution with i.i.d. label noise, under simple conditions on the data distribution and bias parameters. We propose a general approach to extend the result of Blum &amp; Stangl (2019) to different fairness constraints, data bias models, data distributions, and hypothesis classes. We strengthen their result, and
&lt;/p&gt;</description></item><item><title>DiSK&#26159;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2312.05253</link><description>&lt;p&gt;
DiSK: &#19968;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiSK: A Diffusion Model for Structured Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05253
&lt;/p&gt;
&lt;p&gt;
DiSK&#26159;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20855;&#26377;&#22312;&#22810;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#65288;&#31867;&#20284;&#23383;&#20856;&#30340;&#65289;&#25968;&#25454;&#23545;&#20110;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22240;&#20026;&#26684;&#24335;&#21644;&#23646;&#24615;&#21576;&#29616;&#30340;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#32780;&#38590;&#20197;&#22788;&#29702;&#32467;&#26500;&#21270;&#23454;&#20307;&#12290;&#26631;&#31614;&#29983;&#25104;&#27169;&#22411;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#38480;&#21046;&#65292;&#27604;&#22914;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;Diffusion Models of Structured Knowledge&#65288;DiSK&#65289; - &#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;DiSK&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#22788;&#29702;&#25991;&#26412;&#12289;&#20998;&#31867;&#21644;&#36830;&#32493;&#25968;&#20540;&#25968;&#25454;&#65292;&#36825;&#26679;&#21487;&#20197;&#22312;&#22788;&#29702;&#25968;&#23383;&#26102;&#25552;&#39640;&#31934;&#30830;&#24230;&#12290;&#23427;&#37319;&#29992;&#25193;&#25955;&#35757;&#32451;&#26469;&#24314;&#27169;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DiSK&#22312;&#36229;&#36807;15&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#12289;&#21512;&#25104;&#21644;&#22635;&#20805;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;DiSK&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#25805;&#20316;&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Structured (dictionary-like) data presents challenges for left-to-right language models, as they can struggle with structured entities for a wide variety of reasons such as formatting and sensitivity to the order in which attributes are presented. Tabular generative models suffer from a different set of limitations such as their lack of flexibility. We introduce Diffusion Models of Structured Knowledge (DiSK) - a new architecture and training approach specialized for structured data. DiSK handles text, categorical, and continuous numerical data using a Gaussian mixture model approach, which allows for improved precision when dealing with numbers. It employs diffusion training to model relationships between properties. Experiments demonstrate DiSK's state-of-the-art performance on tabular data modeling, synthesis, and imputation on over 15 datasets across diverse domains. DiSK provides an effective inductive bias for generative modeling and manipulation of structured data. The technique
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2312.04828</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
Human-Readable Fingerprint for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04828
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#35782;&#21035;&#20986;&#22522;&#26412;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#39564;&#35777;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#22312;&#39044;&#35757;&#32451;&#21518;&#20445;&#25345;&#31283;&#23450;&#65292;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#37325;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#35757;&#32451;&#21644;&#37197;&#22871;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#35768;&#21487;&#35777;&#65292;&#20445;&#25252;LLM&#30340;&#29256;&#26435;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#30340;&#21442;&#25968;&#20462;&#25913;&#65292;&#30830;&#23450;LLM&#30340;&#21407;&#22987;&#22522;&#26412;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;LLM&#30340;&#20154;&#31867;&#21487;&#35835;&#25351;&#32441;&#65292;&#21487;&#20197;&#21807;&#19968;&#22320;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#27169;&#22411;&#21442;&#25968;&#25110;&#24178;&#25200;&#35757;&#32451;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#27169;&#22411;&#25910;&#25947;&#21518;&#65292;LLM&#21442;&#25968;&#30340;&#21521;&#37327;&#26041;&#21521;&#20445;&#25345;&#31283;&#23450;&#65292;&#36890;&#36807;&#21518;&#32493;&#30340;&#35757;&#32451;&#27493;&#39588;&#65292;&#21253;&#25324;&#25345;&#32493;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;RLHF&#65292;&#20960;&#20046;&#27809;&#26377;&#25200;&#21160;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35782;&#21035;&#22522;&#26412;&#27169;&#22411;&#30340;&#36275;&#22815;&#26465;&#20214;&#12290;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;LLM&#24182;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#39033;&#26469;&#25512;&#24320;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#21521;&#65292;&#39564;&#35777;&#20102;&#36825;&#31181;&#24517;&#35201;&#24615;&#65292;&#32467;&#26524;&#20351;&#24471;&#27169;&#22411;&#21463;&#25439;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#21521;&#23481;&#26131;&#21463;&#21040;&#31616;&#21333;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#22914;&#32500;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Protecting the copyright of large language models (LLMs) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. However, identifying the original base model of an LLM is challenging due to potential parameter alterations. In this study, we introduce a human-readable fingerprint for LLMs that uniquely identifies the base model without exposing model parameters or interfering with training. We first observe that the vector direction of LLM parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF, which makes it a sufficient condition to identify the base model. The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged. However, this direction is vulnerable to simple attacks like dimension 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>VALUED&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#26827;&#30424;&#28216;&#25103;-&#35937;&#26827;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;20&#19975;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#22270;&#20687;&#21644;&#19968;&#20010;&#35268;&#21017;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#25429;&#25417;&#35821;&#20041;&#21644;&#36923;&#36753;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12610</link><description>&lt;p&gt;
VALUED - &#35270;&#35273;&#21644;&#36923;&#36753;&#29702;&#35299;&#35780;&#20272;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VALUED -- Vision and Logical Understanding Evaluation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12610
&lt;/p&gt;
&lt;p&gt;
VALUED&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#26827;&#30424;&#28216;&#25103;-&#35937;&#26827;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;20&#19975;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#22270;&#20687;&#21644;&#19968;&#20010;&#35268;&#21017;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#25429;&#25417;&#35821;&#20041;&#21644;&#36923;&#36753;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26089;&#26399;&#25104;&#21151;&#24320;&#22987;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#24050;&#32463;&#36229;&#36807;&#20102;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#36825;&#20123;&#25216;&#26415;&#26080;&#27861;&#25429;&#25417;&#35821;&#20041;&#19978;&#19979;&#25991;&#21644;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26159;&#24120;&#24120;&#20381;&#36182;&#20110;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#26469;&#24471;&#20986;&#31572;&#26696;&#12290;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#20381;&#36182;&#20110;&#36981;&#23432;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22240;&#27492;&#24050;&#32463;&#36827;&#34892;&#20102;&#22810;&#27425;&#23581;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19968;&#20010;&#38480;&#21046;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#32570;&#20047;&#20855;&#26377;&#20016;&#23500;&#35268;&#21017;&#38598;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VALUE&#65288;&#35270;&#35273;&#21644;&#36923;&#36753;&#29702;&#35299;&#35780;&#20272;&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;20&#19975;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#22270;&#20687;&#21644;&#19968;&#20010;&#22522;&#20110;&#27969;&#34892;&#30340;&#26827;&#30424;&#28216;&#25103;&#8212;&#35937;&#26827;&#30340;&#35268;&#21017;&#38598;&#12290;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#35268;&#21017;&#38598;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#21487;&#25509;&#21463;&#30340;&#39044;&#27979;&#33539;&#22260;&#65292;&#24182;&#26088;&#22312;&#25506;&#32034;&#20851;&#38190;&#30340;&#35821;&#20041;&#21644;&#36923;&#36753;&#29702;&#35299;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting with early successes in computer vision tasks, deep learning based techniques have since overtaken state of the art approaches in a multitude of domains. However, it has been demonstrated time and again that these techniques fail to capture semantic context and logical constraints, instead often relying on spurious correlations to arrive at the answer. Since application of deep learning techniques to critical scenarios are dependent on adherence to domain specific constraints, several attempts have been made to address this issue. One limitation holding back a thorough exploration of this area, is a lack of suitable datasets which feature a rich set of rules. In order to address this, we present the VALUE (Vision And Logical Understanding Evaluation) Dataset, consisting of 200,000$+$ annotated images and an associated rule set, based on the popular board game - chess. The curated rule set considerably constrains the set of allowable predictions, and are designed to probe key s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#20010;&#20307;&#38388;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#19981;&#21516;&#20010;&#20307;&#30340;&#27963;&#21160;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26367;&#20195;&#20256;&#32479;&#25209;&#24402;&#19968;&#21270;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2310.18562</link><description>&lt;p&gt;
&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#36328;&#20154;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Optimization-Free Test-Time Adaptation for Cross-Person Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#20010;&#20307;&#38388;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#19981;&#21516;&#20010;&#20307;&#30340;&#27963;&#21160;&#27169;&#24335;&#65292;&#24182;&#37319;&#29992;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26367;&#20195;&#20256;&#32479;&#25209;&#24402;&#19968;&#21270;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27963;&#21160;&#27169;&#24335;&#22312;&#20010;&#20307;&#38388;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#65292;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#12290;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#21033;&#29992;&#27979;&#35797;&#25968;&#25454;&#27969;&#22312;&#23454;&#26102;&#25512;&#29702;&#20013;&#35843;&#25972;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#36825;&#22312;HAR&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TTA&#31639;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#20854;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26080;&#27861;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;OFTTA&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#12290;OFTTA&#20197;&#26080;&#38656;&#20248;&#21270;&#30340;&#26041;&#24335;&#21516;&#26102;&#35843;&#25972;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25351;&#25968;&#34928;&#20943;&#27979;&#35797;&#26102;&#24402;&#19968;&#21270;&#65288;EDTN&#65289;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#25209;&#24402;&#19968;&#21270;&#65288;CBN&#65289;&#23618;&#12290;EDTN&#36890;&#36807;&#32467;&#21512;CBN&#21644;&#27979;&#35797;&#26102;&#25209;&#24402;&#19968;&#21270;&#65288;TBN&#65289;&#26469;&#25552;&#21462;&#21487;&#38752;&#30340;&#29305;&#24449;&#20197;&#24212;&#23545;&#39046;&#22495;&#20559;&#31227;&#65292;&#20854;&#20013;TBN&#30340;&#24433;&#21709;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) models often suffer from performance degradation in real-world applications due to distribution shifts in activity patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning paradigm that aims to utilize the test stream to adjust predictions in real-time inference, which has not been explored in HAR before. However, the high computational cost of optimization-based TTA algorithms makes it intractable to run on resource-constrained edge devices. In this paper, we propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier simultaneously in an optimization-free manner. For the feature extractor, we propose Exponential DecayTest-time Normalization (EDTN) to replace the conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time batch Normalization (TBN) to extract reliable features against domain shifts with TBN's influence decrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22810;&#26679;&#24615;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26412;&#36136;&#12290;&#23427;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#22312;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#26679;&#24615;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#29702;&#35770;&#23545;&#20110;&#25552;&#39640;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2301.03962</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#30340;&#32479;&#19968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Unified Theory of Diversity in Ensemble Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22810;&#26679;&#24615;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22810;&#26679;&#24615;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#26412;&#36136;&#12290;&#23427;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#22312;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22810;&#26679;&#24615;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#29702;&#35770;&#23545;&#20110;&#25552;&#39640;&#38598;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#29702;&#35770;&#65292;&#35299;&#37322;&#20102;&#22312;&#21508;&#31181;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#22810;&#26679;&#24615;&#30340;&#26412;&#36136;&#12290;&#36825;&#20010;&#25361;&#25112;&#34987;&#31216;&#20026;&#38598;&#25104;&#23398;&#20064;&#30340;&#22307;&#26479;&#65292;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#24050;&#32463;&#26377;30&#22810;&#24180;&#20102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#20102;&#22810;&#26679;&#24615;&#23454;&#38469;&#19978;&#26159;&#38598;&#25104;&#25439;&#22833;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#20013;&#30340;&#19968;&#20010;&#38544;&#34255;&#32500;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#26063;&#31934;&#30830;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#22810;&#26679;&#24615;&#20998;&#35299;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#30340;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#65292;&#20363;&#22914;&#24179;&#26041;&#25439;&#22833;&#12289;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#27850;&#26494;&#25439;&#22833;&#12290;&#23545;&#20110;&#27809;&#26377;&#21487;&#21152;&#24615;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;0/1&#25439;&#22833;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#37327;&#21270;&#22810;&#26679;&#24615;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#20381;&#36182;&#20110;&#26631;&#31614;&#20998;&#24067;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#22810;&#26679;&#24615;&#26159;&#27169;&#22411;&#25311;&#21512;&#24230;&#30340;&#24230;&#37327;&#65292;&#19982;&#20559;&#24046;&#21644;&#26041;&#24046;&#20855;&#26377;&#30456;&#21516;&#30340;&#24847;&#20041;&#65292;&#20294;&#32771;&#34385;&#20102;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#24212;&#35813;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a theory of ensemble diversity, explaining the nature of diversity for a wide range of supervised learning scenarios. This challenge has been referred to as the holy grail of ensemble learning, an open research issue for over 30 years. Our framework reveals that diversity is in fact a hidden dimension in the bias-variance decomposition of the ensemble loss. We prove a family of exact bias-variance-diversity decompositions, for a wide range of losses in both regression and classification, e.g., squared, cross-entropy, and Poisson losses. For losses where an additive bias-variance decomposition is not available (e.g., 0/1 loss) we present an alternative approach: quantifying the effects of diversity, which turn out to be dependent on the label distribution. Overall, we argue that diversity is a measure of model fit, in precisely the same sense as bias and variance, but accounting for statistical dependencies between ensemble members. Thus, we should not be maximising diversity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#19988;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;(iPC)&#65292;&#36890;&#36807;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;iPC&#30456;&#27604;&#21407;&#22987;&#31639;&#27861;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2212.00720</link><description>&lt;p&gt;
&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#21644;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#12289;&#24555;&#36895;&#19988;&#23436;&#20840;&#33258;&#21160;&#30340;&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;(iPC)&#65292;&#36890;&#36807;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;iPC&#30456;&#27604;&#21407;&#22987;&#31639;&#27861;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#32593;&#32476;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#26681;&#26893;&#20110;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38750;&#24120;&#20302;&#25928;&#19988;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#25913;&#21464;&#31361;&#35302;&#26435;&#37325;&#30340;&#26356;&#26032;&#35268;&#21017;&#30340;&#26102;&#38388;&#35843;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#27604;&#21407;&#22987;&#31639;&#27861;&#26356;&#39640;&#25928;&#12289;&#26356;&#31283;&#23450;&#19988;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#34987;&#31216;&#20026;&#36882;&#22686;&#39044;&#27979;&#32534;&#30721;(iPC)&#65292;&#20063;&#27604;&#21407;&#22987;&#31639;&#27861;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#65292;&#22240;&#20026;&#23427;&#26159;&#23436;&#20840;&#33258;&#21160;&#30340;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;iPC&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#21644;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#65292;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#25910;&#25947;&#24615;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#20110;&#22823;&#37327;&#36229;&#21442;&#25968;&#20855;&#26377;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.
&lt;/p&gt;</description></item><item><title>TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14694</link><description>&lt;p&gt;
TA-RNN&#65306;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38754;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26102;&#38388;&#24863;&#30693;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records. (arXiv:2401.14694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14694
&lt;/p&gt;
&lt;p&gt;
TA-RNN&#21644;TA-RNN-AE&#26159;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20998;&#26512;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24182;&#39044;&#27979;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#36825;&#20123;&#26550;&#26500;&#32771;&#34385;&#20102;EHR&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#24615;&#21644;&#26102;&#38388;&#38388;&#38548;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#26426;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#21382;&#21490;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;EHR&#23545;&#20110;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#20808;&#36827;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#21307;&#30103;&#25552;&#20379;&#32773;&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20570;&#20986;&#31934;&#30830;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#20020;&#24202;&#20915;&#31574;&#12290;DL&#26041;&#27861;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;EHR&#20197;&#24314;&#27169;&#30142;&#30149;&#36827;&#23637;&#24182;&#39044;&#27979;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#35299;&#20915;EHR&#25968;&#25454;&#20013;&#19968;&#20123;&#22266;&#26377;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#22914;&#20020;&#24202;&#35775;&#38382;&#20043;&#38388;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;DL&#27169;&#22411;&#37117;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RNN&#30340;&#21487;&#35299;&#37322;DL&#26550;&#26500;&#65292;&#20998;&#21035;&#26159;&#26102;&#38388;&#24863;&#30693;RNN&#65288;TA-RNN&#65289;&#21644;TA-RNN-Autoencoder&#65288;TA-RNN-AE&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#27425;&#35775;&#38382;&#21644;&#22810;&#27425;&#26410;&#26469;&#35775;&#38382;&#20013;&#24739;&#32773;&#30340;&#20020;&#24202;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#35268;&#21017;&#26102;&#38388;&#38388;&#38548;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#23884;&#20837;&#30340;&#26041;&#27861;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elaps
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10034</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#36827;&#21270;&#35745;&#31639;&#65306;&#35843;&#26597;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#65292;&#22312;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#36824;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#36808;&#21521;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23613;&#31649;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#19982;LLMs&#22312;&#30446;&#26631;&#21644;&#26041;&#27861;&#35770;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20182;&#20204;&#20849;&#21516;&#30340;&#20248;&#21270;&#24615;&#36136;&#12289;&#40657;&#30418;&#29305;&#24615;&#21644;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#26041;&#38754;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#20026;LLM&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#20248;&#21270;&#26694;&#26550;&#65292;&#36824;&#21487;&#20197;&#22312;&#24212;&#29992;&#20013;&#20026;LLM&#36171;&#20104;&#28789;&#27963;&#30340;&#20840;&#23616;&#25628;&#32034;&#21644;&#36845;&#20195;&#26426;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLM&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#20351;&#24471;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#26234;&#33021;&#30340;&#25628;&#32034;&#65292;&#32780;&#20854;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21017;&#26377;&#21161;&#20110;&#23558;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2401.04319</link><description>&lt;p&gt;
&#26356;&#22909;&#22320;&#20102;&#35299;&#24744;&#30340;&#38656;&#27714;&#65306;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. (arXiv:2401.04319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#22686;&#24378;&#30340;LLMs&#26469;&#23454;&#29616;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#30340;&#26032;&#26041;&#24335;&#65292;&#20351;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#33021;&#22815;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23450;&#20301;&#26041;&#24335;&#65292;&#21363;&#38750;&#19987;&#19994;&#33829;&#38144;&#20154;&#21592;&#21487;&#20197;&#20165;&#20973;&#38656;&#27714;&#30340;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#36873;&#25321;&#30446;&#26631;&#29992;&#25143;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#23454;&#38469;&#30340;&#32467;&#26500;&#21270;&#36923;&#36753;&#35821;&#35328;&#65292;&#21363;&#23545;&#33829;&#38144;&#20154;&#21592;&#38656;&#27714;&#30340;&#32467;&#26500;&#21270;&#29702;&#35299;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;LLMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#26377;&#25928;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;1&#65289;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#31616;&#21333;&#30340;&#8220;&#35753;&#25105;&#20204;&#19968;&#27493;&#19968;&#27493;&#22320;&#24605;&#32771;&#8221;&#25552;&#31034;&#65292;&#35201;&#20040;&#22312;&#28436;&#31034;&#20013;&#25552;&#20379;&#22266;&#23450;&#30340;&#31034;&#20363;&#32780;&#19981;&#32771;&#34385;&#25552;&#31034;&#21644;&#38382;&#39064;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;&#19968;&#20123;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#32467;&#26500;&#21270;&#35821;&#35328;&#36716;&#25442;&#65289;&#20013;&#20351;LLMs&#26080;&#25928;&#12290;(2) &#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#38381;&#28304;&#27169;&#22411;&#25110;&#36807;&#24230;&#23454;&#29616;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple "Let's think step by step" spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or exces
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;</title><link>http://arxiv.org/abs/2401.04122</link><description>&lt;p&gt;
&#20174;&#25552;&#31034;&#24037;&#31243;&#21040;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#25552;&#31034;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
From Prompt Engineering to Prompt Science With Human in the Loop. (arXiv:2401.04122v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04122
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#20070;&#26500;&#24314;&#26041;&#27861;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26356;&#31995;&#32479;&#22320;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#22312;&#25105;&#20204;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#26159;&#19968;&#20010;&#38656;&#35201;&#22686;&#21152;&#23457;&#26597;&#30340;&#22320;&#26041;&#12290;LLMs&#34987;&#29992;&#20110;&#29983;&#25104;&#25110;&#20998;&#26512;&#30740;&#31350;&#25968;&#25454;&#30340;&#24212;&#29992;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20294;&#26159;&#65292;&#24403;&#36825;&#31181;&#24212;&#29992;&#34987;&#20020;&#26102;&#20915;&#31574;&#21644;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#25152;&#22256;&#25200;&#26102;&#65292;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#23427;&#22914;&#20309;&#24433;&#21709;&#30740;&#31350;&#12289;&#30740;&#31350;&#32467;&#26524;&#25110;&#32773;&#22522;&#20110;&#35813;&#30740;&#31350;&#30340;&#20219;&#20309;&#26410;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#38656;&#35201;&#26356;&#31185;&#23398;&#30340;&#26041;&#27861;&#26469;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;LLMs&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#19968;&#20123;&#31215;&#26497;&#30340;&#21162;&#21147;&#25903;&#25345;&#26356;&#31995;&#32479;&#30340;&#25552;&#31034;&#26500;&#24314;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#23454;&#29616;&#26399;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20135;&#29983;&#21487;&#22797;&#21046;&#21644;&#20855;&#26377;&#36275;&#22815;&#36879;&#26126;&#24230;&#12289;&#23458;&#35266;&#24615;&#25110;&#20005;&#35880;&#24615;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#36890;&#36807;&#23450;&#24615;&#26041;&#27861;&#26500;&#24314;&#20195;&#30721;&#20070;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20154;&#22312;&#24490;&#29615;&#21644;&#22810;&#38454;&#27573;&#39564;&#35777;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20026;&#26356;&#31995;&#32479;&#30340;&#30740;&#31350;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
As LLMs make their way into many aspects of our lives, one place that warrants increased scrutiny with LLM usage is scientific research. Using LLMs for generating or analyzing data for research purposes is gaining popularity. But when such application is marred with ad-hoc decisions and engineering solutions, we need to be concerned about how it may affect that research, its findings, or any future works based on that research. We need a more scientific approach to using LLMs in our research. While there are several active efforts to support more systematic construction of prompts, they are often focused more on achieving desirable outcomes rather than producing replicable and generalizable knowledge with sufficient transparency, objectivity, or rigor. This article presents a new methodology inspired by codebook construction through qualitative methods to address that. Using humans in the loop and a multi-phase verification processes, this methodology lays a foundation for more systema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.20007</link><description>&lt;p&gt;
&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20013;&#27748;&#26222;&#26862;&#37319;&#26679;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Bayesian Regret Bounds for Thompson Sampling in Reinforcement Learning. (arXiv:2310.20007v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#35777;&#26126;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#23545;&#20449;&#24687;&#27604;&#30340;&#31934;&#30830;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25214;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#20855;&#20307;&#30340;&#30028;&#38480;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#22810;&#31181;&#24773;&#22659;&#19979;&#65292;&#27748;&#26222;&#26862;&#37319;&#26679;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#25955;&#30340;&#20195;&#29702;&#29615;&#22659;&#31616;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#19968;&#33268;&#24615;&#23545;&#20449;&#24687;&#27604;&#36827;&#34892;&#20102;&#31934;&#30830;&#20998;&#26512;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#19981;&#22343;&#21248;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19978;&#30028;&#20272;&#35745;&#20026;$\widetilde{O}(H\sqrt{d_{l_1}T})$&#65292;&#20854;&#20013;$H$&#20026;&#22238;&#21512;&#38271;&#24230;&#65292;$d_{l_1}$&#20026;&#29615;&#22659;&#31354;&#38388;&#30340;Kolmogorov $l_1$&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#25214;&#21040;&#20102;$d_{l_1}$&#30340;&#20855;&#20307;&#30028;&#38480;&#65292;&#27604;&#22914;&#34920;&#26684;&#12289;&#32447;&#24615;&#21644;&#26377;&#38480;&#28151;&#21512;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#20854;&#31867;&#21035;&#25110;&#25913;&#36827;&#20102;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we prove the first Bayesian regret bounds for Thompson Sampling in reinforcement learning in a multitude of settings. We simplify the learning problem using a discrete set of surrogate environments, and present a refined analysis of the information ratio using posterior consistency. This leads to an upper bound of order $\widetilde{O}(H\sqrt{d_{l_1}T})$ in the time inhomogeneous reinforcement learning problem where $H$ is the episode length and $d_{l_1}$ is the Kolmogorov $l_1-$dimension of the space of environments. We then find concrete bounds of $d_{l_1}$ in a variety of settings, such as tabular, linear and finite mixtures, and discuss how how our results are either the first of their kind or improve the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;</title><link>http://arxiv.org/abs/2310.13007</link><description>&lt;p&gt;
XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#25209;&#21028;&#24615;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Critical Survey on Fairness Benefits of XAI. (arXiv:2310.13007v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13007
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25351;&#20986;XAI&#22312;&#23454;&#29616;&#20844;&#24179;&#29702;&#24819;&#26041;&#38754;&#23384;&#22312;&#28508;&#21147;&#21644;&#38480;&#21046;&#65292;&#21628;&#21505;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#25209;&#21028;&#24615;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#20856;&#22411;&#35770;&#36848;&#65292;&#20197;&#35299;&#24320;&#36825;&#20004;&#20010;&#27010;&#24565;&#20043;&#38388;&#30340;&#22810;&#32500;&#20851;&#31995;&#12290;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#38543;&#21518;&#30340;&#23450;&#24615;&#20869;&#23481;&#20998;&#26512;&#65292;&#25105;&#20204;&#20174;175&#31687;&#35770;&#25991;&#20013;&#35782;&#21035;&#20986;&#20851;&#20110;XAI&#30340;&#20844;&#24179;&#25928;&#30410;&#30340;&#19971;&#20010;&#20856;&#22411;&#35770;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20851;&#20110;&#36825;&#20123;&#35770;&#26029;&#30340;&#37325;&#35201;&#35686;&#21578;&#65292;&#24182;&#20026;&#26410;&#26469;&#22260;&#32469;XAI&#22312;&#29305;&#23450;&#20844;&#24179;&#29702;&#24819;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#20837;&#28857;&#12290;&#34429;&#28982;&#25991;&#29486;&#36890;&#24120;&#35748;&#20026;XAI&#26159;&#23454;&#29616;&#22810;&#20010;&#20844;&#24179;&#29702;&#24819;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#29702;&#24819;&#19982;XAI&#30340;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#40723;&#21169;&#23558;XAI&#35270;&#20026;&#24212;&#23545;&#31639;&#27861;&#20844;&#24179;&#36825;&#19968;&#22810;&#32500;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#20247;&#22810;&#24037;&#20855;&#20043;&#19968;&#65292;&#24182;&#26356;&#20855;&#20307;&#22320;&#35828;&#26126;&#21738;&#31181;XAI&#26041;&#27861;&#22914;&#20309;&#24110;&#21161;&#21738;&#20123;&#20154;&#35299;&#20915;&#21738;&#20123;&#20844;&#24179;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this critical survey, we analyze typical claims on the relationship between explainable AI (XAI) and fairness to disentangle the multidimensional relationship between these two concepts. Based on a systematic literature review and a subsequent qualitative content analysis, we identify seven archetypal claims from 175 papers on the alleged fairness benefits of XAI. We present crucial caveats with respect to these claims and provide an entry point for future discussions around the potentials and limitations of XAI for specific fairness desiderata. While the literature often suggests XAI to be an enabler for several fairness desiderata, we notice a misalignment between these desiderata and the capabilities of XAI. We encourage to conceive XAI as one of many tools to approach the multidimensional, sociotechnical challenge of algorithmic fairness and to be more specific about how exactly what kind of XAI method enables whom to address which fairness desideratum.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.02505</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#23558;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#30446;&#26631;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25918;&#22312;&#25193;&#25955;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#26031;&#22122;&#22768;&#21019;&#24314;&#38543;&#26426;&#36712;&#36857;&#65292;&#20351;&#20854;&#36828;&#31163;&#25968;&#25454;&#27969;&#24418;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#36828;&#31163;&#28508;&#22312;&#30446;&#26631;&#29366;&#24577;&#30340;&#36712;&#36857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#20284;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20010;&#31216;&#20026;Merlin&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#36873;&#25321;&#65292;&#29992;&#20110;&#21462;&#20195;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411; - &#32531;&#20914;&#21306;&#20013;&#30340;&#21453;&#21521;&#25773;&#25918;&#65292;&#21453;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01701</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#36328;&#36234;&#39046;&#22495;&#65306;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation. (arXiv:2310.01701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#29983;&#25104;&#28304;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20854;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#24212;&#29992;&#27169;&#22411;&#20174;&#30456;&#20851;&#28304;&#39046;&#22495;&#33719;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#19981;&#20805;&#36275;&#26631;&#27880;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#65288;&#22914;HIPAA&#12289;COPPA&#12289;FERPA&#31561;&#65289;&#30340;&#19981;&#26029;&#21152;&#24378;&#24341;&#21457;&#20102;&#23545;&#22312;&#32469;&#36807;&#23545;&#28304;&#25968;&#25454;&#30340;&#30452;&#25509;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#30340;&#20852;&#36259;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;SFDA&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#28304;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#26679;&#26412;&#19978;&#35757;&#32451;&#19968;&#20010;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#25509;&#36817;&#28304;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#20154;&#24037;&#29983;&#25104;&#30340;&#28304;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation (DA) is a method for enhancing a model's performance on a target domain with inadequate annotated data by applying the information the model has acquired from a related source domain with sufficient labeled data. The escalating enforcement of data-privacy regulations like HIPAA, COPPA, FERPA, etc. have sparked a heightened interest in adapting models to novel domains while circumventing the need for direct access to the source data, a problem known as Source-Free Domain Adaptation (SFDA). In this paper, we propose a novel framework for SFDA that generates source data using a text-to-image diffusion model trained on the target domain samples. Our method starts by training a text-to-image diffusion model on the labeled target domain samples, which is then fine-tuned using the pre-trained source model to generate samples close to the source data. Finally, we use Domain Adaptation techniques to align the artificially generated source data with the target domain data, resu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.09992</link><description>&lt;p&gt;
OpenAI&#25220;&#34989;&#20102;&#25105;&#20204;&#30340;&#31246;&#21153;&#26696;&#20363;&#65292;&#20294;GPT-4&#30495;&#30340;&#33021;&#22815;&#22788;&#29702;&#31246;&#21153;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
OpenAI Cribbed Our Tax Example, But Can GPT-4 Really Do Tax?. (arXiv:2309.09992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09992
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22788;&#29702;&#31246;&#21153;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#35299;&#37322;&#20102;OpenAI&#22312;GPT-4&#30340;&#30452;&#25773;&#28436;&#31034;&#20013;&#20351;&#29992;&#31246;&#27861;&#26696;&#20363;&#30340;&#26469;&#28304;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;GPT-4&#24471;&#21040;&#20102;&#38169;&#35823;&#30340;&#31572;&#26696;&#65292;&#20197;&#21450;&#23427;&#22914;&#20309;&#26080;&#27861;&#21487;&#38752;&#22320;&#35745;&#31639;&#31246;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors explain where OpenAI got the tax law example in its livestream demonstration of GPT-4, why GPT-4 got the wrong answer, and how it fails to reliably calculate taxes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08055</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#36827;&#34892;&#31616;&#21333;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#8212;&#8212;&#22312;&#20219;&#20309;&#26102;&#21051;&#65292;&#39044;&#35328;&#26426;&#37117;&#33021;&#32473;&#20986;&#19982;&#30446;&#21069;&#20026;&#27490;&#30475;&#21040;&#30340;&#25152;&#26377;&#31034;&#20363;&#19968;&#33268;&#30340;&#31867;&#20989;&#25968;&#12290;&#35813;&#27169;&#22411;&#26368;&#36817;&#30001;Assos&#31561;&#20154;&#65288;COLT'23&#65289;&#32771;&#34385;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#21160;&#26426;&#26159;&#26631;&#20934;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23376;&#31867;&#30340;Littlestone&#32500;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;Assos&#31561;&#20154;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32473;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#20110;Littlestone&#32500;&#24230;&#20026;d&#30340;&#31867;&#65292;&#26368;&#22810;&#20250;&#29359;C^d&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;C&#26159;&#19968;&#20010;&#26410;&#25351;&#23450;&#30340;&#32477;&#23545;&#24120;&#25968;&#19988;&#22823;&#20110;0&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26356;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#20102;Littlestone&#32500;&#24230;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#20197;&#21450;Assos&#31561;&#20154;&#30340;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C &gt; 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#24322;&#26500; SoC &#19978;&#24182;&#21457;&#25191;&#34892; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#32771;&#34385;&#20102;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#21644;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.05869</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500; SoC &#30340;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#24863;&#30693;&#30340;&#24182;&#21457; DNN &#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Shared Memory-contention-aware Concurrent DNN Execution for Diversely Heterogeneous System-on-Chips. (arXiv:2308.05869v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05869
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#24322;&#26500; SoC &#19978;&#24182;&#21457;&#25191;&#34892; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#32771;&#34385;&#20102;&#20849;&#20139;&#20869;&#23384;&#31454;&#20105;&#21644;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#31227;&#21160;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#20004;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65306;1&#65289;&#36890;&#24120;&#21516;&#26102;&#36830;&#32493;&#36816;&#34892;&#22810;&#20010;&#24037;&#20316;&#36127;&#36733;&#65292;&#20027;&#35201;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#25512;&#29702;&#65307;2&#65289;&#23427;&#20204;&#22312;&#23884;&#20837;&#20102;&#38024;&#23545;&#29305;&#23450;&#25805;&#20316;&#30340;&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#20849;&#20139;&#20869;&#23384;&#31995;&#32479;&#33455;&#29255;&#19978;&#36816;&#34892;&#12290;&#30446;&#21069;&#30340;&#25216;&#26415;&#32570;&#20047;&#39640;&#25928;&#30340;&#24615;&#33021;&#21644;&#36164;&#28304;&#31649;&#29702;&#25216;&#26415;&#65292;&#26080;&#27861;&#26368;&#22823;&#21270;&#24635;&#31995;&#32479;&#21534;&#21520;&#37327;&#25110;&#26368;&#23567;&#21270;&#31471;&#21040;&#31471;&#24037;&#20316;&#36127;&#36733;&#24310;&#36831;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HaX-CoNN &#30340;&#26032;&#26041;&#26696;&#65292;&#23558;&#24182;&#21457;&#25191;&#34892;&#30340; DNN &#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#30340;&#23618;&#36827;&#34892;&#29305;&#24449;&#21270;&#21644;&#26144;&#23556;&#21040; SoC &#20013;&#30340;&#22810;&#26679;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#29420;&#29305;&#22320;&#32771;&#34385;&#20102;&#27599;&#23618;&#30340;&#25191;&#34892;&#29305;&#24615;&#12289;&#20849;&#20139;&#20869;&#23384; (SM) &#30340;&#31454;&#20105;&#20197;&#21450;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#20197;&#25214;&#21040;&#26368;&#20248;&#30340;&#35843;&#24230;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312; NVIDIA Orin&#12289;NVIDIA Xavier &#21644; Qualcomm Snapdragon 865 SoC &#19978;&#35780;&#20272;&#20102; HaX-CoNN&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HaX-CoNN &#21487;&#20197;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two distinguishing features of state-of-the-art mobile and autonomous systems are 1) there are often multiple workloads, mainly deep neural network (DNN) inference, running concurrently and continuously; and 2) they operate on shared memory system-on-chips (SoC) that embed heterogeneous accelerators tailored for specific operations. State-of-the-art lacks efficient performance and resource management techniques necessary to either maximize total system throughput or minimize end-to-end workload latency. In this work, we propose HaX-CoNN, a novel scheme that characterizes and maps layers in concurrently executing DNN inference workloads to a diverse set of accelerators within a SoC. Our scheme uniquely takes per-layer execution characteristics, shared memory (SM) contention, and inter-accelerator transitions into account to find optimal schedules. We evaluate HaX-CoNN on NVIDIA Orin, NVIDIA Xavier, and Qualcomm Snapdragon 865 SoCs. Our experimental results indicate that HaX-CoNN minimiz
&lt;/p&gt;</description></item><item><title>Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.01285</link><description>&lt;p&gt;
Flows: &#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#30340;&#26500;&#24314;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Flows: Building Blocks of Reasoning and Collaborating AI. (arXiv:2308.01285v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01285
&lt;/p&gt;
&lt;p&gt;
Flows&#26159;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#35299;&#20026;&#33258;&#21253;&#21547;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#30340;&#25512;&#29702;&#21644;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;Flows&#21487;&#20197;&#26500;&#24314;&#20219;&#24847;&#22797;&#26434;&#24230;&#30340;&#20132;&#20114;&#65292;&#33021;&#22815;&#35206;&#30422;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#39640;&#24230;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#31995;&#32479;&#12290;&#36825;&#20026;&#32467;&#26500;&#21270;&#25512;&#29702;&#20197;&#21450;&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#21327;&#20316;&#21019;&#36896;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#36935;&#12290;&#20026;&#20102;&#20805;&#20998;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#65292;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#30740;&#31350;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27969;&#31243;&#30340;&#27010;&#24565;&#26694;&#26550;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#24314;&#27169;&#22797;&#26434;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#27969;&#31243;&#26159;&#35745;&#31639;&#30340;&#33258;&#21253;&#21547;&#26500;&#24314;&#27169;&#22359;&#65292;&#20855;&#26377;&#29420;&#31435;&#30340;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#21270;&#30340;&#22522;&#20110;&#28040;&#24687;&#30340;&#25509;&#21475;&#36827;&#34892;&#36890;&#20449;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#24471;&#27969;&#31243;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20219;&#24847;&#23884;&#22871;&#30340;&#20132;&#20114;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#20219;&#20309;&#20132;&#20114;&#37117;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#23454;&#29616;&#65292;&#21253;&#25324;&#20043;&#21069;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;-&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;-&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#12289;&#25552;&#31034;&#24037;&#31243;&#26041;&#26696;&#21644;&#24037;&#20855;&#22686;&#24378;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27969;&#31243;&#22312;&#20219;&#21153;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in artificial intelligence (AI) have produced highly capable and controllable systems. This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans. To fully realize this potential, it is essential to develop a principled way of designing and studying such structured interactions. For this purpose, we introduce the conceptual framework of Flows: a systematic approach to modeling complex interactions. Flows are self-contained building blocks of computation, with an isolated state, communicating through a standardized message-based interface. This modular design allows Flows to be recursively composed into arbitrarily nested interactions, with a substantial reduction of complexity. Crucially, any interaction can be implemented using this framework, including prior work on AI--AI and human--AI interactions, prompt engineering schemes, and tool augmentation. We demonstrate the potential of Flows on the task 
&lt;/p&gt;</description></item><item><title>Select2Col&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#21327;&#20316;&#24863;&#30693;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#24182;&#25490;&#38500;&#36127;&#38754;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16517</link><description>&lt;p&gt;
Select2Col: &#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#30340;&#21327;&#20316;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Select2Col: Leveraging Spatial-Temporal Importance of Semantic Information for Efficient Collaborative Perception. (arXiv:2307.16517v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16517
&lt;/p&gt;
&lt;p&gt;
Select2Col&#26159;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#36827;&#34892;&#39640;&#25928;&#21327;&#20316;&#24863;&#30693;&#30340;&#26032;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#24182;&#25490;&#38500;&#36127;&#38754;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20849;&#20139;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#21327;&#20316;&#22312;&#20811;&#26381;&#23396;&#31435;&#20195;&#29702;&#30340;&#24863;&#30693;&#33021;&#21147;&#38480;&#21046;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21327;&#20316;&#24863;&#30693;&#26041;&#27861;&#24448;&#24448;&#21482;&#20851;&#27880;&#35821;&#20041;&#20449;&#24687;&#30340;&#31354;&#38388;&#29305;&#24449;&#65292;&#32780;&#24573;&#35270;&#20102;&#26102;&#38388;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#21327;&#20316;&#30340;&#28508;&#22312;&#30410;&#22788;&#26410;&#24471;&#21040;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Select2Col&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#32771;&#34385;&#20102;&#35821;&#20041;&#20449;&#24687;&#30340;&#26102;&#31354;&#37325;&#35201;&#24615;&#12290;&#22312;Select2Col&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#36731;&#37327;&#32423;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20272;&#35745;&#35821;&#20041;&#20449;&#24687;&#37325;&#35201;&#24615;&#65288;IoSI&#65289;&#20197;&#25552;&#39640;&#24863;&#30693;&#24615;&#33021;&#30340;&#21512;&#20316;&#32773;&#36873;&#25321;&#26041;&#27861;&#65292;&#20174;&#32780;&#35782;&#21035;&#20986;&#26377;&#30410;&#30340;&#21512;&#20316;&#32773;&#65292;&#21516;&#26102;&#25490;&#38500;&#37027;&#20123;&#24102;&#26469;&#36127;&#38754;&#24433;&#21709;&#30340;&#21512;&#20316;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;HPHA&#30340;&#35821;&#20041;&#20449;&#24687;&#34701;&#21512;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#21382;&#21490;&#20808;&#39564;&#28151;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaboration by leveraging the shared semantic information plays a crucial role in overcoming the perception capability limitations of isolated agents. However, existing collaborative perception methods tend to focus solely on the spatial features of semantic information, while neglecting the importance of the temporal dimension. Consequently, the potential benefits of collaboration remain underutilized. In this article, we propose Select2Col, a novel collaborative perception framework that takes into account the {s}patial-t{e}mpora{l} importanc{e} of semanti{c} informa{t}ion. Within the Select2Col, we develop a collaborator selection method that utilizes a lightweight graph neural network (GNN) to estimate the importance of semantic information (IoSI) in enhancing perception performance, thereby identifying contributive collaborators while excluding those that bring negative impact. Moreover, we present a semantic information fusion algorithm called HPHA (historical prior hybrid atte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11305</link><description>&lt;p&gt;
&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#29992;&#20110;&#39034;&#24207;&#35270;&#39057;&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
Progressive Fourier Neural Representation for Sequential Video Compilation. (arXiv:2306.11305v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#25214;&#21040;&#33258;&#36866;&#24212;&#19988;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#26469;&#32534;&#30721;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;(NIR)&#22240;&#20854;&#23558;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#24182;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#26144;&#23556;&#20989;&#25968;&#36731;&#26494;&#37325;&#26500;&#25968;&#25454;&#30340;&#38750;&#20961;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;NIR&#26041;&#27861;&#20551;&#23450;&#30446;&#26631;&#25968;&#25454;&#21644;&#34920;&#31034;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19968;&#23545;&#19968;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#25110;&#30456;&#20284;&#24615;&#12290;&#36825;&#23548;&#33268;&#22312;&#22810;&#32452;&#22797;&#26434;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#38480;&#21046;&#20102;&#20854;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#21463;&#25345;&#32493;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#32534;&#30721;&#20250;&#35805;&#20013;&#32047;&#31215;&#21644;&#20256;&#36882;&#22810;&#20010;&#22797;&#26434;&#35270;&#39057;&#25968;&#25454;&#30340;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;NIR&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#28176;&#36827;&#20613;&#37324;&#21494;&#31070;&#32463;&#34920;&#31034;(PFNR)&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#33258;&#36866;&#24212;&#21644;&#32039;&#20945;&#30340;&#20613;&#37324;&#21494;&#31354;&#38388;&#23376;&#27169;&#22359;&#65292;&#20197;&#32534;&#30721;&#27599;&#20010;&#35757;&#32451;&#20250;&#35805;&#20013;&#30340;&#35270;&#39057;&#12290;&#36825;&#31181;&#31232;&#30095;&#30340;&#31070;&#32463;&#32534;&#30721;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25345;&#26377;&#33258;&#30001;&#26435;&#37325;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#36845;&#20195;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#22810;&#20010;&#39034;&#24207;&#35270;&#39057;&#25968;&#25454;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an imp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;</title><link>http://arxiv.org/abs/2305.14459</link><description>&lt;p&gt;
&#36890;&#36807;&#25688;&#35201;&#20108;&#20803;&#24615;&#21644;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Generation through Summarization Duality and Explicit Outline Control. (arXiv:2305.14459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29983;&#25104;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24320;&#25918;&#24335;&#38271;&#25991;&#26412;&#29983;&#25104;&#38754;&#20020;&#35821;&#20041;&#19981;&#36830;&#36143;&#21644;&#24773;&#33410;&#19981;&#21487;&#20449;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#35774;&#35745;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#30701;&#35821;&#25110;&#25277;&#35937;&#20449;&#21495;&#30340;&#22823;&#32434;&#26469;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#24448;&#24448;&#26159;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#12290;&#22312;&#20551;&#35774;&#25688;&#35201;&#20316;&#20026;&#24050;&#25104;&#29087;&#30340;&#22823;&#32434;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#12289;&#25688;&#35201;&#22686;&#24378;&#30340;&#22823;&#32434;&#30417;&#30563;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#25688;&#35201;&#20219;&#21153;&#30340;&#21452;&#37325;&#29305;&#24449;&#26469;&#25913;&#36827;&#22823;&#32434;&#39044;&#27979;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26126;&#30830;&#21644;&#21512;&#29702;&#30340;&#22823;&#32434;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22823;&#32434;&#30340;&#29983;&#25104;&#20855;&#26377;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#26080;&#35770;&#26159;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-2&#12289;BART&#65289;&#36824;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Vicuna&#12289;ChatGPT&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26174;&#24335;&#22823;&#32434;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#29983;&#25104;&#30340;&#22823;&#32434;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically open-ended long text generation poses significant challenges due to semantic incoherence and plot implausibility. Previous works usually alleviate this problem through outlines in the form of short phrases or abstractive signals by designing unsupervised tasks, which tend to be unstable and weakly interpretable.  Assuming that a summary serves as a mature outline, we introduce a two-stage, summary-enhanced outline supervised generation framework. This framework leverages the dual characteristics of the summarization task to improve outline prediction, resulting in more explicit and plausible outlines. Furthermore, we identify an underutilization issue in outline-based generation with both standard pretrained language models (e.g., GPT-2, BART) and large language models (e.g., Vicuna, ChatGPT). To address this, we propose a novel explicit outline control method for more effective utilization of generated outlines.
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.12138</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#33539;&#22260;&#65306;&#20840;&#38754;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Scope of ChatGPT in Software Engineering: A Thorough Investigation. (arXiv:2305.12138v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12138
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;ChatGPT&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#35821;&#20041;&#30340;&#29702;&#35299;&#21644;&#37096;&#20998;&#21151;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#36716;&#21270;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#34920;&#29616;&#20986;&#22312;&#20195;&#30721;&#21644;&#25991;&#26723;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#24037;&#31243;&#38656;&#35201;&#39640;&#21487;&#38752;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#65292;&#20351;ChatGPT&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#25104;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;ChatGPT&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#23558;AI&#27169;&#22411;&#24212;&#23545;SE&#20219;&#21153;&#25152;&#38656;&#30340;&#33021;&#21147;&#20998;&#20026;&#19977;&#31867;&#65306;1&#65289;&#35821;&#27861;&#29702;&#35299;&#65292;2&#65289;&#38745;&#24577;&#34892;&#20026;&#29702;&#35299;&#65292;&#21644;3&#65289;&#21160;&#24577;&#34892;&#20026;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;ChatGPT&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#12289;&#25511;&#21046;&#27969;&#31243;&#22270;&#65288;CFG&#65289;&#21644;&#35843;&#29992;&#22270;&#65288;CG&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#28041;&#21450;C&#12289;Java&#12289;Python&#21644;Solidity&#30340;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;ChatGPT&#34920;&#29616;&#20986;&#20102;&#23545;&#29702;&#35299;&#20195;&#30721;&#35821;&#27861;&#65288;AST&#65289;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#20195;&#30721;&#35821;&#20041;&#21644;&#37096;&#20998;&#21151;&#33021;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT demonstrates immense potential to transform software engineering (SE) by exhibiting outstanding performance in tasks such as code and document generation. However, the high reliability and risk control requirements of SE make the lack of interpretability for ChatGPT a concern. To address this issue, we carried out a study evaluating ChatGPT's capabilities and limitations in SE. We broke down the abilities needed for AI models to tackle SE tasks into three categories: 1) syntax understanding, 2) static behavior understanding, and 3) dynamic behavior understanding. Our investigation focused on ChatGPT's ability to comprehend code syntax and semantic structures, including abstract syntax trees (AST), control flow graphs (CFG), and call graphs (CG). We assessed ChatGPT's performance on cross-language tasks involving C, Java, Python, and Solidity. Our findings revealed that while ChatGPT excels at understanding code syntax (AST), it struggles with comprehending code semantics, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2210.01426</link><description>&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01426
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#36890;&#36807;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#22312;&#32447;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#22312;&#32447;&#35268;&#21010;&#23545;&#20110;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#22312;&#32447;&#35268;&#21010;&#65292;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#26426;&#21046;&#26469;&#26435;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;MCTS&#22312;&#35768;&#22810;&#31163;&#25955;&#20915;&#31574;&#39046;&#22495;&#65288;&#22914;&#22260;&#26827;&#12289;&#22269;&#38469;&#35937;&#26827;&#21644;&#23558;&#26827;&#65289;&#20013;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;&#32780;&#38024;&#23545;&#36830;&#32493;&#39046;&#22495;&#30340;MCTS&#25193;&#23637;&#20063;&#24050;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#39640;&#20998;&#25903;&#22240;&#23376;&#21644;&#23548;&#33268;&#25628;&#32034;&#26641;&#22823;&#23567;&#29190;&#28856;&#30340;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#32493;&#33945;&#29305;&#21345;&#27931;&#22270;&#25628;&#32034;&#65288;CMCGS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;MCTS&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22312;&#32447;&#35268;&#21010;&#29615;&#22659;&#12290;CMCGS&#21033;&#29992;&#20102;&#19968;&#20010;&#27934;&#23519;&#21147;&#65292;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#65292;&#23558;&#30456;&#20284;&#29366;&#24577;&#20043;&#38388;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#21487;&#20197;&#24471;&#21040;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;CMCGS&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#23558;&#30456;&#20284;&#29366;&#24577;&#32858;&#31867;&#25104;&#26377;&#38480;&#25968;&#37327;&#30340;&#38543;&#26426;&#21160;&#20316;&#36172;&#21338;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#20316;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which
&lt;/p&gt;</description></item><item><title>EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.08996</link><description>&lt;p&gt;
EDO-Net: &#20174;&#22270;&#21160;&#21147;&#23398;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics. (arXiv:2209.08996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08996
&lt;/p&gt;
&lt;p&gt;
EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#22270;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#29289;&#29702;&#23646;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20363;&#22914;&#20174;&#25289;&#20280;&#20132;&#20114;&#20013;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDO-Net&#65288;&#24377;&#24615;&#21487;&#21464;&#24418;&#23545;&#35937;-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20855;&#26377;&#19981;&#21516;&#24377;&#24615;&#23646;&#24615;&#30340;&#22823;&#37327;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#23646;&#24615;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;EDO-Net&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#21644;&#19968;&#20010;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22359;&#12290;&#21069;&#32773;&#36127;&#36131;&#25552;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#39044;&#27979;&#20197;&#22270;&#24418;&#34920;&#31034;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#20102;EDO-Net&#30340;&#33021;&#21147;&#65306;1&#65289;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;2&#65289;&#36716;&#31227;&#23398;&#20064;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned rep
&lt;/p&gt;</description></item></channel></rss>