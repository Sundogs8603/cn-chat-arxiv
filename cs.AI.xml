<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;RATs-NAS&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GCN&#19978;&#37325;&#23450;&#21521;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#26469;&#24555;&#36895;&#25628;&#32034;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.04206</link><description>&lt;p&gt;
RATs-NAS&#65306;GCN&#19978;&#30340;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#37325;&#23450;&#21521;&#29992;&#20110;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search. (arXiv:2305.04206v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;RATs-NAS&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GCN&#19978;&#37325;&#23450;&#21521;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#26469;&#24555;&#36895;&#25628;&#32034;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25163;&#24037;&#35774;&#35745;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22914;VGG&#12289;ResNet&#12289;DenseNet&#31561;&#65292;&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29616;&#22312;&#19987;&#27880;&#20110;&#33258;&#21160;&#25214;&#21040;&#26368;&#20339;CNN&#26550;&#26500;&#26469;&#22788;&#29702;&#19978;&#36848;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#39564;&#35777;&#25628;&#32034;&#26550;&#26500;&#38750;&#24120;&#32791;&#26102;&#65292;&#20351;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#25104;&#20026;NAS&#30340;&#19968;&#20010;&#22522;&#26412;&#32780;&#37325;&#35201;&#30340;&#20998;&#25903;&#12290;&#24314;&#31435;&#39044;&#27979;&#22120;&#30340;&#20004;&#31181;&#24120;&#29992;&#25216;&#26415;&#26159;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12290;&#26412;&#25991;&#32771;&#34385;GCN&#21644;MLP&#22312;&#30456;&#37051;&#25805;&#20316;&#36712;&#36857;&#19978;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;Redirected Adjacent Trails NAS&#65288;RATs-NAS&#65289;&#65292;&#20197;&#24555;&#36895;&#25628;&#32034;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;RATs-NAS&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;Redirected Adjacent Trails GCN&#65288;RATs-GCN&#65289;&#21644;&#22522;&#20110;&#39044;&#27979;&#22120;&#30340;&#25628;&#32034;&#31354;&#38388;&#25277;&#26679;&#65288;P3S&#65289;&#27169;&#22359;&#12290; RATs-GCN&#21487;&#20197;&#25913;&#21464;&#36712;&#36857;&#21450;&#20854;&#24378;&#24230;&#20197;&#25628;&#32034;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;P3S&#27169;&#22359;&#21017;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#25277;&#26679;&#20197;&#25552;&#39640;&#39044;&#27979;&#26550;&#26500;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;RATs-NAS&#21487;&#20197;&#26356;&#24555;&#22320;&#25214;&#21040;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#29468;&#27979;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04181</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#30456;&#20449;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#25552;&#21462;&#30340;&#25152;&#26377;&#20851;&#31995;&#20803;&#32452;&#21527;&#65311;&#23545;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection. (arXiv:2305.04181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#29468;&#27979;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#65288;OIE&#65289;&#26088;&#22312;&#20174;&#24320;&#25918;&#22495;&#21477;&#23376;&#20013;&#25552;&#21462;&#20107;&#23454;&#20851;&#31995;&#20803;&#32452;&#12290;&#19979;&#28216;&#20219;&#21153;&#20351;&#29992;&#25552;&#21462;&#30340;OIE&#20803;&#32452;&#20316;&#20026;&#20107;&#23454;&#65292;&#32780;&#19981;&#32771;&#34385;&#36825;&#20123;&#20107;&#23454;&#30340;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#30830;&#23450;&#24615;/&#29468;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#29616;&#26377;&#30340;&#29468;&#27979;&#26816;&#27979;&#30740;&#31350;&#26159;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#23450;&#20041;&#30340;&#65292;&#20294;&#21363;&#20351;&#30830;&#23450;&#20102;&#19968;&#20010;&#21477;&#23376;&#26159;&#29468;&#27979;&#30340;&#65292;&#20063;&#19981;&#26159;&#20174;&#20854;&#20013;&#25552;&#21462;&#30340;&#25152;&#26377;&#20803;&#32452;&#37117;&#26159;&#29468;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;OIE&#20013;&#30340;&#29468;&#27979;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#25552;&#21462;&#30340;&#20803;&#32452;&#26159;&#21542;&#23384;&#22312;&#29468;&#27979;&#12290;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#20803;&#32452;&#32423;&#21035;&#29468;&#27979;&#26816;&#27979;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#23545;&#21253;&#21547;&#29468;&#27979;&#20803;&#32452;&#26631;&#31614;&#30340;LSOIE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OIE-Spec&#30340;&#22522;&#20934;&#27169;&#22411;&#29992;&#20110;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Information Extraction (OIE) aims to extract factual relational tuples from open-domain sentences. Downstream tasks use the extracted OIE tuples as facts, without examining the certainty of these facts. However, uncertainty/speculation is a common linguistic phenomenon. Existing studies on speculation detection are defined at sentence level, but even if a sentence is determined to be speculative, not all tuples extracted from it may be speculative. In this paper, we propose to study speculations in OIE and aim to determine whether an extracted tuple is speculative. We formally define the research problem of tuple-level speculation detection and conduct a detailed data analysis on the LSOIE dataset which contains labels for speculative tuples. Lastly, we propose a baseline model OIE-Spec for this new research task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.04180</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#35299;&#32806;&#24378;&#21270;&#23398;&#20064;&#21644;&#21521;&#37327;&#22810;&#26679;&#24615;&#65292;&#19968;&#23567;&#26102;&#20869;&#35757;&#32451;&#36866;&#29992;&#20110;&#23454;&#38469;&#19990;&#30028;&#30340;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Train a Real-world Local Path Planner in One Hour via Partially Decoupled Reinforcement Learning and Vectorized Diversity. (arXiv:2305.04180v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Color&#35299;&#20915;&#26041;&#26696;&#30340;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#65292;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35757;&#32451;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#22120;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#35299;&#20915;&#23616;&#37096;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;DRL&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Color&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30001;Actor-Sharer-Learner&#65288;ASL&#65289;&#35757;&#32451;&#26694;&#26550;&#21644;&#38754;&#21521;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#22120;Sparrow&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has exhibited efficacy in resolving the Local Path Planning (LPP) problem. However, such application in the real world is immensely limited due to the deficient efficiency and generalization capability of DRL. To alleviate these two issues, a solution named Color is proposed, which consists of an Actor-Sharer-Learner (ASL) training framework and a mobile robot-oriented simulator Sparrow. Specifically, the ASL framework, intending to improve the efficiency of the DRL algorithm, employs a Vectorized Data Collection (VDC) mode to expedite data acquisition, decouples the data collection from model optimization by multithreading, and partially connects the two procedures by harnessing a Time Feedback Mechanism (TFM) to evade data underuse or overuse. Meanwhile, the Sparrow simulator utilizes a 2D grid-based world, simplified kinematics, and conversion-free data flow to achieve a lightweight design. The lightness facilitates vectorized diversity, allowing di
&lt;/p&gt;</description></item><item><title>MIReAD&#26159;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35770;&#25991;&#26816;&#32034;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.04177</link><description>&lt;p&gt;
MIReAD: &#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents. (arXiv:2305.04177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04177
&lt;/p&gt;
&lt;p&gt;
MIReAD&#26159;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35770;&#25991;&#26816;&#32034;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#23398;&#20064;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#21487;&#20197;&#20419;&#36827;&#23398;&#26415;&#25991;&#29486;&#25628;&#32034;&#24182;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MIReAD&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#35843;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#26469;&#39044;&#27979;&#22522;&#20110;&#25688;&#35201;&#30340;&#30446;&#26631;&#26399;&#21002;&#31867;&#21035;&#65292;&#20174;&#32780;&#23398;&#20064;&#31185;&#23398;&#35770;&#25991;&#30340;&#39640;&#36136;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#36229;&#36807;2,000&#20010;&#26399;&#21002;&#31867;&#21035;&#30340;500,000&#22810;&#20010;PubMed&#21644;arXiv&#25688;&#35201;&#19978;&#23545;MIReAD&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#34920;&#26126;MIReAD&#20135;&#29983;&#30340;&#34920;&#31034;&#21487;&#29992;&#20110;&#31867;&#20284;&#35770;&#25991;&#26816;&#32034;&#12289;&#20027;&#39064;&#20998;&#31867;&#21644;&#25991;&#29486;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#35780;&#20215;&#26631;&#20934;&#19979;&#20248;&#20110;&#29616;&#26377;&#30340;&#20845;&#31181;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pre-trained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns high-quality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>Score&#26159;&#19968;&#20010;&#35268;&#21017;&#24341;&#25806;&#65292;&#29992;&#20110;Scone&#30693;&#35782;&#24211;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#31616;&#21333;&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;Scone&#26500;&#24314;&#30340;&#35745;&#21010;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.04154</link><description>&lt;p&gt;
Score: Scone&#30693;&#35782;&#24211;&#31995;&#32479;&#30340;&#35268;&#21017;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Score: A Rule Engine for the Scone Knowledge Base System. (arXiv:2305.04154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04154
&lt;/p&gt;
&lt;p&gt;
Score&#26159;&#19968;&#20010;&#35268;&#21017;&#24341;&#25806;&#65292;&#29992;&#20110;Scone&#30693;&#35782;&#24211;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25191;&#34892;&#31616;&#21333;&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#22522;&#20110;Scone&#26500;&#24314;&#30340;&#35745;&#21010;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Score&#65292;&#19968;&#31181;&#20026;Scone&#30693;&#35782;&#24211;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#35268;&#21017;&#24341;&#25806;&#12290; Scone&#26159;&#19968;&#31181;&#30693;&#35782;&#24211;&#31995;&#32479;&#65292;&#26088;&#22312;&#20197;&#31526;&#21495;&#24418;&#24335;&#23384;&#20648;&#21644;&#25805;&#20316;&#19968;&#33324;&#30693;&#35782;&#30340;&#20016;&#23500;&#34920;&#36798;&#24335;&#12290; &#23427;&#20197;&#32593;&#32476;&#32467;&#26500;&#20013;&#30340;&#33410;&#28857;&#21644;&#38142;&#25509;&#30340;&#24418;&#24335;&#34920;&#31034;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#20851;&#20110;&#19981;&#21516;&#20803;&#32032;&#20043;&#38388;&#20851;&#31995;&#30340;&#22522;&#26412;&#25512;&#29702;&#12290; &#25105;&#20204;&#22312;Scone&#31995;&#32479;&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#22522;&#20110;&#21046;&#36896;&#29983;&#20135;&#35268;&#21017;&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#26681;&#25454;Scone&#30693;&#35782;&#24211;&#20013;&#30340;&#29616;&#26377;&#21644;&#26032;&#28155;&#21152;&#30340;&#32467;&#26500;&#25191;&#34892;&#31616;&#21333;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of "smart memory" that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations.  We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.04152</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#32447;&#36890;&#20449;&#30340;&#36890;&#36947;&#39537;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#36125;&#21494;&#26031;&#32852;&#37030;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#30340;&#36817;&#26399;&#21457;&#23637;&#24050;&#32463;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#37319;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#39057;&#29575;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#20852;&#36259;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#32852;&#37030;&#24179;&#22343; Langevin &#21160;&#21147;&#23398;(FALD)&#20316;&#20026;&#32852;&#37030;&#24179;&#22343;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#36890;&#20449;&#23384;&#22312;&#19979;&#26377;&#25928;&#22320;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#32447; FALD(WFALD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#21644;&#22522;&#20110;&#36890;&#36947;&#39537;&#21160;&#30340; Monte Carlo &#26356;&#26032;&#26469;&#23454;&#29616;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340; FALD&#12290;&#19982;&#20808;&#21069;&#30340;&#26080;&#32447;&#36125;&#21494;&#26031;&#23398;&#20064;&#30456;&#27604;&#65292;WFALD &#21487;&#20197;&#23454;&#29616;(i) &#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65307;&#24182;&#19988;(ii) &#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#20197; 2-Wasserstein &#36317;&#31163;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32473;&#20986;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#31034;&#26500;&#36896;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.04147</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Mixed-Initiative Dialogue Generation through Prompting. (arXiv:2305.04147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#31034;&#26500;&#36896;&#26469;&#23454;&#29616;&#21487;&#25511;&#30340;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#29983;&#25104;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#20219;&#21153;&#28041;&#21450;&#37325;&#22797;&#20132;&#25442;&#20449;&#24687;&#21644;&#23545;&#35805;&#25511;&#21046;&#12290;&#20250;&#35805;&#20195;&#29702;&#36890;&#36807;&#29983;&#25104;&#21709;&#24212;&#26469;&#33719;&#24471;&#25511;&#21046;&#65292;&#36825;&#20123;&#21709;&#24212;&#25353;&#29031;&#31574;&#30053;&#35268;&#21010;&#22120;&#35268;&#23450;&#30340;&#29305;&#23450;&#23545;&#35805;&#24847;&#22270;&#25110;&#31574;&#30053;&#36827;&#34892;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#25191;&#34892;&#22522;&#20110;&#36825;&#20123;&#24847;&#22270;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21463;&#30417;&#30563;&#30340;&#29983;&#25104;&#27169;&#22411;&#21463;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#21644;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#24494;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#20026;&#21487;&#25511;&#28151;&#21512;&#20027;&#21160;&#23545;&#35805;&#24418;&#24335;&#21270;&#25552;&#31034;&#26500;&#36896;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;PersuasionForGood&#21644;Emotional Support Conversations&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#35780;&#20272;&#21644;&#33258;&#21160;&#25351;&#26631;&#26041;&#38754;&#22343;&#26174;&#31034;&#20986;&#27604;&#24494;&#35843;&#21644;&#30495;&#23454;&#21709;&#24212;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#23398;&#20064;&#20013;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#25191;&#34892;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#27721;&#35834;&#22612;&#27979;&#35797;&#20102;GPT-2&#21644;GPT-3&#30340;&#35268;&#21010;&#21644;&#24037;&#20316;&#35760;&#24518;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;&#25191;&#34892;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04134</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#24515;&#29702;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#27491;&#22312;&#21457;&#23637;&#25191;&#34892;&#21151;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?. (arXiv:2305.04134v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#22312;&#23398;&#20064;&#20013;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#25191;&#34892;&#21151;&#33021;&#65292;&#24182;&#20351;&#29992;&#27721;&#35834;&#22612;&#27979;&#35797;&#20102;GPT-2&#21644;GPT-3&#30340;&#35268;&#21010;&#21644;&#24037;&#20316;&#35760;&#24518;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;&#25191;&#34892;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25191;&#34892;&#21508;&#31181;&#35748;&#30693;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#35821;&#35328;&#22788;&#29702;&#12289;&#35270;&#35273;&#35782;&#21035;&#21644;&#20915;&#31574;&#12290;&#27492;&#36827;&#23637;&#30340;&#19968;&#37096;&#20998;&#24402;&#22240;&#20110;&#20687;GPT&#65288;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65289;&#31995;&#21015;&#37027;&#26679;&#30340;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;&#20986;&#34987;&#35270;&#20026;&#26234;&#33021;&#30340;&#34892;&#20026;&#12290;&#31070;&#32463;&#24515;&#29702;&#23398;&#20013;&#30340;&#22823;&#22810;&#25968;&#20316;&#32773;&#35748;&#20026;&#65292;&#26234;&#33021;&#34892;&#20026;&#21462;&#20915;&#20110;&#35768;&#22810;&#20840;&#38754;&#25216;&#33021;&#65292;&#25110;&#31216;&#25191;&#34892;&#21151;&#33021;&#65288;EFs&#65289;&#65292;&#36825;&#20123;&#25216;&#33021;&#20381;&#36182;&#20110;&#21069;&#39069;&#21494;&#20013;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#30830;&#21151;&#33021;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;LLM&#26159;&#21542;&#27491;&#22312;&#24320;&#21457;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25191;&#34892;&#21151;&#33021;&#20316;&#20026;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT&#20351;&#29992;&#27969;&#34892;&#30340;&#27721;&#35834;&#22612;&#26041;&#27861;&#30340;&#35268;&#21010;&#21151;&#33021;&#21644;&#24037;&#20316;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21476;&#20856;&#26041;&#27861;&#21464;&#20307;&#26469;&#26356;&#22909;&#22320;&#27979;&#35797;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;GPT-2&#21644;GPT-3&#22312;&#26377;&#38480;&#21644;&#38750;&#20154;&#31867;&#30340;&#26041;&#24335;&#19979;&#23637;&#31034;&#20102;&#19968;&#20123;EFs&#65292;&#22914;&#24037;&#20316;&#35760;&#24518;&#21644;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has been rapidly advancing and has demonstrated its ability to perform a wide range of cognitive tasks, including language processing, visual recognition, and decision-making. Part of this progress is due to LLMs (Large Language Models) like those of the GPT (Generative Pre-Trained Transformers) family. These models are capable of exhibiting behavior that can be perceived as intelligent. Most authors in Neuropsychology consider intelligent behavior to depend on a number of overarching skills, or Executive Functions (EFs), which rely on the correct functioning of neural networks in the frontal lobes, and have developed a series of tests to evaluate them. In this work, we raise the question of whether LLMs are developing executive functions similar to those of humans as part of their learning, and we evaluate the planning function and working memory of GPT using the popular Towers of Hanoi method. Additionally, we introduce a new variant of the classical meth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.04120</link><description>&lt;p&gt;
&#19968;&#31181;&#34507;&#30333;&#36136;&#32467;&#26500;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Latent Diffusion Model for Protein Structure Generation. (arXiv:2305.04120v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04120
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#28789;&#27963;&#22320;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#22797;&#26434;&#30340;&#29983;&#29289;&#20998;&#23376;&#65292;&#33021;&#22312;&#29983;&#29289;&#20307;&#20869;&#25191;&#34892;&#22810;&#31181;&#20851;&#38190;&#21151;&#33021;&#12290;&#35774;&#35745;&#21644;&#29983;&#25104;&#26032;&#22411;&#34507;&#30333;&#36136;&#21487;&#20026;&#26410;&#26469;&#30340;&#21512;&#25104;&#29983;&#29289;&#23398;&#24212;&#29992;&#65288;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#20294;&#30001;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#24314;&#27169;&#31354;&#38388;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20943;&#23569;&#34507;&#30333;&#36136;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#28789;&#27963;&#22320;&#22312;&#21387;&#32553;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#33258;&#28982;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31561;&#21464;&#34507;&#30333;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#34507;&#30333;&#36136;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#28508;&#22312;&#34507;&#30333;&#36136;&#34920;&#31034;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#39640;&#35774;&#35745;&#33021;&#21147;&#21644;&#25928;&#29575;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#39592;&#26550;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20851;&#31995;&#30340;&#23041;&#32961;&#20248;&#20808;&#32423;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25490;&#21517;&#22686;&#26448;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#23041;&#32961;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.04102</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20851;&#31995;&#20248;&#20808;&#22788;&#29702;&#22686;&#26448;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#23041;&#32961;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Leveraging Semantic Relationships to Prioritise Indicators of Compromise in Additive Manufacturing Systems. (arXiv:2305.04102v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#20851;&#31995;&#30340;&#23041;&#32961;&#20248;&#20808;&#32423;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25490;&#21517;&#22686;&#26448;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#23041;&#32961;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#24555;&#36895;&#12289;&#25104;&#26412;&#26377;&#25928;&#22320;&#21046;&#36896;&#22797;&#26434;&#21644;&#23450;&#21046;&#35774;&#35745;&#12289;&#20943;&#23569;&#26448;&#26009;&#28010;&#36153;&#20197;&#21450;&#23454;&#29616;&#25353;&#38656;&#29983;&#20135;&#12290;&#28982;&#32780;&#65292;AM&#20063;&#24102;&#26469;&#20102;&#33509;&#24178;&#23433;&#20840;&#25361;&#25112;&#65292;&#21560;&#24341;&#20102;&#20174;&#20010;&#20154;&#40657;&#23458;&#21040;&#26377;&#32452;&#32455;&#29359;&#32618;&#22242;&#20249;&#21644;&#22269;&#23478;&#32423;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#23041;&#32961;&#20248;&#20808;&#32423;&#31995;&#32479;&#65292;&#20197;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#25490;&#21517;&#23041;&#32961;&#25351;&#26631;&#65288;IOC&#65289;&#26469;&#35299;&#20915;AM&#20013;&#30340;&#32593;&#32476;&#23433;&#20840;&#39118;&#38505;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#65292;&#20174;&#22810;&#28304;&#23041;&#32961;&#25991;&#26412;&#33258;&#21160;&#25552;&#21462;&#39640;&#32423;IOC&#65292;&#24182;&#35782;&#21035;IOC&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#23427;&#29992;&#21253;&#21547;&#19981;&#21516;&#20803;&#36335;&#24452;&#21644;&#20803;&#22270;&#30340;HIN&#26469;&#24314;&#27169;IOC&#65292;&#20197;&#25551;&#36848;&#19981;&#21516;IOC&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#35782;&#21035;&#22120;&#65292;&#29992;&#20110;&#22312;&#19977;&#20010;&#39046;&#22495;&#20013;&#35782;&#21035;IOC&#65306;&#32452;&#32455;&#29305;&#23450;&#30340;&#12289;AM&#24037;&#33402;&#21644;&#26448;&#26009;&#29305;&#23450;&#30340;&#12289;&#20197;&#21450;&#29305;&#23450;&#20110;&#25968;&#25454;&#36755;&#20837;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing (AM) offers numerous benefits, such as manufacturing complex and customised designs quickly and cost-effectively, reducing material waste, and enabling on-demand production. However, several security challenges are associated with AM, making it increasingly attractive to attackers ranging from individual hackers to organised criminal gangs and nation-state actors. This paper addresses the cyber risk in AM to attackers by proposing a novel semantic-based threat prioritisation system for identifying, extracting and ranking indicators of compromise (IOC). The system leverages the heterogeneous information networks (HINs) that automatically extract high-level IOCs from multi-source threat text and identifies semantic relations among the IOCs. It models IOCs with a HIN comprising different meta-paths and meta-graphs to depict semantic relations among diverse IOCs. We introduce a domain-specific recogniser that identifies IOCs in three domains: organisation-specific, r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.04095</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#31169;&#26377;&#25968;&#25454;&#20445;&#25345;&#26412;&#22320;&#65292;&#20801;&#35768;&#23433;&#20840;&#35745;&#31639;&#21644;&#26412;&#22320;&#27169;&#22411;&#26799;&#24230;&#19982;&#31532;&#19977;&#26041;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21361;&#21450;&#38544;&#31169;&#24182;&#24674;&#22797;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#35270;&#35282;&#12290;&#36825;&#20123;&#29702;&#35770;&#24037;&#20316;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#21482;&#26377;&#38145;&#23450;&#30340;&#26799;&#24230;&#34987;&#20256;&#36755;&#21040;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#25152;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#23494;&#38053;&#38145;&#27169;&#22359;&#21487;&#20197;&#30830;&#20445;&#65292;&#27809;&#26377;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#31169;&#26377;&#20449;&#24687;&#65306;a) &#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2305.04076</link><description>&lt;p&gt;
SANTA&#65306;Distantly-Supervised Named Entity Recognition&#20013;&#22788;&#29702;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition. (arXiv:2305.04076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;Distantly-Supervised Named Entity Recognition&#20013;&#38169;&#35823;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#22122;&#22768;&#30340;&#20998;&#31163;&#31574;&#30053;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#26500;&#24314;&#26469;&#24212;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30417;&#30563;&#35774;&#32622;&#20013;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#27880;&#37322;&#36127;&#25285;&#65292;&#20294;&#26159;&#26080;&#19978;&#19979;&#25991;&#30340;&#21305;&#37197;&#36807;&#31243;&#21644;&#30693;&#35782;&#24211;&#30340;&#26377;&#38480;&#35206;&#30422;&#24341;&#20837;&#20102;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#30340;&#26631;&#27880;&#22122;&#38899;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#22788;&#29702;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#30340;SANTA&#65292;&#20197;&#35299;&#20915;&#30001;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#26631;&#27880;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete annotation noise or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the robustness. Benefiting from our separate tailored strategies, we co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04073</link><description>&lt;p&gt;
&#29992;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Explaining RL Decisions with Trajectories. (arXiv:2305.04073v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#21450;&#34892;&#21160;&#31354;&#38388;&#30340;&#22810;&#26679;&#21270;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26159;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#20915;&#31574;&#38382;&#39064;&#20013;&#24212;&#29992;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20805;&#36825;&#20123;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#36712;&#36857;&#29992;&#32534;&#30721;&#30340;&#26041;&#24335;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature, the explanation is often provided by saliency attribution to the features of the RL agent's state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set. Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video gam
&lt;/p&gt;</description></item><item><title>BRAIN&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;AI&#24179;&#21488;&#65292;&#33021;&#22815;&#30830;&#20445;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21487;&#20449;&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#20132;&#26131;&#26426;&#21046;&#23454;&#29616;&#23454;&#26102;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.04062</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21487;&#38752;&#22823;&#35268;&#27169;&#27169;&#22411;&#25512;&#29702;&#21644;&#35757;&#32451;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Blockchain-based Platform for Reliable Inference and Training of Large-Scale Models. (arXiv:2305.04062v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04062
&lt;/p&gt;
&lt;p&gt;
BRAIN&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;AI&#24179;&#21488;&#65292;&#33021;&#22815;&#30830;&#20445;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21487;&#20449;&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#20132;&#26131;&#26426;&#21046;&#23454;&#29616;&#23454;&#26102;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;AI&#39537;&#21160;&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#28041;&#21450;&#21487;&#20449;&#24230;&#21644;&#36879;&#26126;&#24230;&#30340;&#38382;&#39064;&#24341;&#36215;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#28508;&#22312;&#20559;&#24046;&#21644;&#21487;&#36861;&#28335;&#24615;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#21306;&#22359;&#38142;&#31561;&#20998;&#25955;&#21270;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#24120;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#23548;&#33268;&#25512;&#29702;&#32791;&#26102;&#65292;&#35757;&#32451;&#39564;&#35777;&#20302;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BRAIN&#65288;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21487;&#38752;AI&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#26032;&#24179;&#21488;&#65292;&#26088;&#22312;&#30830;&#20445;&#22823;&#27169;&#22411;&#30340;&#21487;&#38752;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;BRAIN&#21033;&#29992;&#29420;&#29305;&#30340;&#20004;&#38454;&#27573;&#20132;&#26131;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#31163;&#35831;&#27714;&#21644;&#21709;&#24212;&#20132;&#26131;&#65292;&#20801;&#35768;&#23454;&#26102;&#22788;&#29702;&#36890;&#36807;&#27969;&#27700;&#32447;&#12290;&#27599;&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#25512;&#29702;&#22996;&#21592;&#20250;&#37117;&#20250;&#25552;&#20132;&#24182;&#20844;&#24067;&#25512;&#29702;&#32467;&#26524;&#65292;&#22312;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#36798;&#25104;&#19968;&#33268;&#24847;&#35265;&#21518;&#65292;&#25165;&#20250;&#25191;&#34892;&#25152;&#35831;&#27714;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) continues to permeate various domains, concerns surrounding trust and transparency in AI-driven inference and training processes have emerged, particularly with respect to potential biases and traceability challenges. Decentralized solutions such as blockchain have been proposed to tackle these issues, but they often struggle when dealing with large-scale models, leading to time-consuming inference and inefficient training verification. To overcome these limitations, we introduce BRAIN, a Blockchain-based Reliable AI Network, a novel platform specifically designed to ensure reliable inference and training of large models. BRAIN harnesses a unique two-phase transaction mechanism, allowing real-time processing via pipelining by separating request and response transactions. Each randomly-selected inference committee commits and reveals the inference results, and upon reaching an agreement through a smart contract, then the requested operation is executed us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#30340;&#31185;&#23398;&#25216;&#26415;&#26412;&#20307;&#65292;&#28085;&#30422;&#20102;&#31185;&#23398;&#25216;&#26415;&#39046;&#22495;&#30340;&#19981;&#20256;&#32479;&#20027;&#39064;&#65292;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#21644;&#23398;&#31185;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25216;&#26415;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.04055</link><description>&lt;p&gt;
&#31185;&#25216;&#26412;&#20307;&#35770;&#65306;&#26032;&#20852;&#20027;&#39064;&#30340;&#20998;&#31867;&#23398;
&lt;/p&gt;
&lt;p&gt;
Science and Technology Ontology: A Taxonomy of Emerging Topics. (arXiv:2305.04055v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#30340;&#31185;&#23398;&#25216;&#26415;&#26412;&#20307;&#65292;&#28085;&#30422;&#20102;&#31185;&#23398;&#25216;&#26415;&#39046;&#22495;&#30340;&#19981;&#20256;&#32479;&#20027;&#39064;&#65292;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#21644;&#23398;&#31185;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20102;&#31185;&#23398;&#25216;&#26415;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#35770;&#22312;&#35821;&#20041;&#32593;&#32476;&#25216;&#26415;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#21644;&#26631;&#20934;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#24335;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#29702;&#35299;&#25968;&#25454;&#30340;&#21547;&#20041;&#12290;&#24050;&#32463;&#21046;&#23450;&#20102;&#20960;&#20010;&#20998;&#31867;&#23398;&#21644;&#26412;&#20307;&#65292;&#20294;&#20010;&#20154;&#21482;&#38024;&#23545;&#19968;&#20010;&#39046;&#22495;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26102;&#38388;&#21644;&#25163;&#21160;&#24037;&#20316;&#26041;&#38754;&#26174;&#24471;&#26114;&#36149;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#22810;&#28085;&#30422;&#38750;&#20256;&#32479;&#20027;&#39064;&#30340;&#20869;&#23481;&#65292;&#20197;&#21576;&#29616;&#26356;&#20840;&#38754;&#21644;&#20840;&#38754;&#30340;&#30693;&#35782;&#26684;&#23616;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26377;&#19968;&#20010;&#35206;&#30422;&#31185;&#23398;&#21644;&#25216;&#26415;&#39046;&#22495;&#30340;&#26412;&#20307;&#65292;&#36890;&#36807;&#36830;&#25509;&#21487;&#33021;&#30456;&#20851;&#25110;&#20855;&#26377;&#20849;&#21516;&#28857;&#30340;&#19981;&#21516;&#39046;&#22495;&#21644;&#39046;&#22495;&#30340;&#20027;&#39064;&#65292;&#20415;&#20110;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31185;&#23398;&#25216;&#26415;&#26412;&#20307;&#65288;S&#65286;TO&#65289;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#31185;&#23398;&#25216;&#26415;&#39046;&#22495;&#30340;&#38750;&#20256;&#32479;&#20027;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;S&#65286;TO&#21487;&#20197;&#20419;&#36827;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#21644;&#23398;&#31185;&#38388;&#30340;&#21512;&#20316;&#65292;&#24182;&#22312;&#31185;&#23398;&#25216;&#26415;&#20013;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies play a critical role in Semantic Web technologies by providing a structured and standardized way to represent knowledge and enabling machines to understand the meaning of data. Several taxonomies and ontologies have been generated, but individuals target one domain, and only some of those have been found expensive in time and manual effort. Also, they need more coverage of unconventional topics representing a more holistic and comprehensive view of the knowledge landscape and interdisciplinary collaborations. Thus, there needs to be an ontology covering Science and Technology and facilitate multidisciplinary research by connecting topics from different fields and domains that may be related or have commonalities. To address these issues, we present an automatic Science and Technology Ontology (S&amp;TO) that covers unconventional topics in different science and technology domains. The proposed S&amp;TO can promote the discovery of new research areas and collaborations across discipl
&lt;/p&gt;</description></item><item><title>Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04043</link><description>&lt;p&gt;
Echoes: &#22522;&#20110;&#20266;&#20559;&#24046;&#26631;&#35760;&#30340;&#27169;&#20223;&#24335;&#22238;&#22768;&#23460;&#26080;&#30417;&#30563;&#21435;&#20559;
&lt;/p&gt;
&lt;p&gt;
Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04043
&lt;/p&gt;
&lt;p&gt;
Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#26292;&#38706;&#20110;&#26377;&#20559;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36890;&#24120;&#20250;&#23398;&#20064;&#21040;&#19981;&#27491;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#25299;&#23637;&#39046;&#22495;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;Echoes&#8221;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#20197;&#24378;&#21046;&#20351;&#20266;&#26631;&#31614;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#29992;&#20110;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Echoes&#23454;&#29616;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#20248;&#21270;&#26426;&#21046;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#27492;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36807;GPT-4&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.04039</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#20248;&#21270;&#26469;&#25552;&#21319;LLMs&#30340;&#21709;&#24212;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Refining the Responses of LLMs by Themselves. (arXiv:2305.04039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#20248;&#21270;&#26426;&#21046;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#27492;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#32467;&#26524;&#36136;&#37327;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36807;GPT-4&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#24037;&#31243;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#20248;&#21270;&#20854;&#31572;&#26696;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36741;&#21161;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#33258;&#25105;&#35780;&#20272;&#20248;&#21270;&#26426;&#21046;&#65292;&#38543;&#30528;&#36845;&#20195;&#30340;&#25512;&#36827;&#65292;&#20855;&#26377;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#30340;&#28508;&#21147;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GPT-3.5&#27169;&#22411;&#19978;&#20351;&#29992;&#25105;&#20204;&#30340;&#21709;&#24212;&#20248;&#21270;&#26694;&#26550;&#20135;&#29983;&#30340;&#32467;&#26524;&#19982;&#29978;&#33267;&#36229;&#36807;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23454;&#26045;&#31574;&#30053;&#21644;&#35828;&#26126;&#24615;&#31034;&#20363;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a simple yet efficient approach based on prompt engineering that leverages the large language model itself to optimize its answers without relying on auxiliary models. We introduce an iterative self-evaluating optimization mechanism, with the potential for improved output quality as iterations progress, removing the need for manual intervention. The experiment's findings indicate that utilizing our response refinement framework on the GPT-3.5 model yields results that are on par with, or even surpass, those generated by the cutting-edge GPT-4 model. Detailed implementation strategies and illustrative examples are provided to demonstrate the superiority of our proposed solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein-Fisher-Rao&#24230;&#37327;&#26469;&#21152;&#26435;&#32771;&#34385;&#23884;&#20837;&#20043;&#38388;&#30340;&#26412;&#22320;&#65288;&#23616;&#37096;&#65289;&#19982;&#20840;&#23616;&#65288;&#25972;&#20307;&#65289;&#29305;&#24449;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31639;&#27861;&#26469;&#36827;&#34892;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#22359;&#23545;&#35282;&#26680;&#26469;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04034</link><description>&lt;p&gt;
Wasserstein-Fisher-Rao&#23884;&#20837;&#65306;&#20855;&#26377;&#26412;&#22320;&#27604;&#36739;&#21644;&#20840;&#23616;&#20256;&#36755;&#30340;&#36923;&#36753;&#26597;&#35810;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport. (arXiv:2305.04034v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein-Fisher-Rao&#24230;&#37327;&#26469;&#21152;&#26435;&#32771;&#34385;&#23884;&#20837;&#20043;&#38388;&#30340;&#26412;&#22320;&#65288;&#23616;&#37096;&#65289;&#19982;&#20840;&#23616;&#65288;&#25972;&#20307;&#65289;&#29305;&#24449;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31639;&#27861;&#26469;&#36827;&#34892;&#32447;&#24615;&#26102;&#38388;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#22359;&#23545;&#35282;&#26680;&#26469;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#24456;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#19981;&#23436;&#25972;&#24615;&#65292;&#23427;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#21644;&#20351;&#29992;&#38598;&#21512;&#36816;&#31639;&#31526;&#27169;&#25311;&#36923;&#36753;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#38598;&#20013;&#20110;&#29305;&#23450;&#24418;&#24335;&#30340;&#23884;&#20837;&#65292;&#20294;&#23884;&#20837;&#20043;&#38388;&#30340;&#35780;&#20998;&#20989;&#25968;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26412;&#22320;&#27604;&#36739;&#25110;&#20840;&#23616;&#20256;&#36755;&#30340;&#35780;&#20998;&#20989;&#25968;&#19981;&#21516;&#65292;&#26412;&#25991;&#20351;&#29992;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#26469;&#30740;&#31350;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#38598;&#21512;&#23884;&#20837;&#21040;&#24102;&#26377;Wasserstein-Fisher-Rao&#24230;&#37327;&#30340;&#26377;&#30028;&#27979;&#24230;&#31354;&#38388;&#19978;&#65292;&#24182;&#20351;&#29992;&#36825;&#26679;&#30340;&#24230;&#37327;&#26469;&#35774;&#35745;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#31181;&#35774;&#35745;&#36824;&#20419;&#36827;&#20102;&#23884;&#20837;&#31354;&#38388;&#20869;&#30340;&#23553;&#38381;&#24418;&#24335;&#38598;&#21512;&#36816;&#31639;&#31526;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#32447;&#24615;&#35745;&#31639;&#31639;&#27861;&#21644;&#19968;&#20010;&#22359;&#23545;&#35282;&#26680;&#20197;&#23454;&#29616;&#26435;&#34913;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;WFRE&#21487;&#20197;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learning-based models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scoring functions motivated by local comparison or global transport, this work investigates the local and global trade-off with unbalanced optimal transport theory. Specifically, we embed sets as bounded measures in $\real$ endowed with a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a design also facilitates closed-form set operators in the embedding space. Moreover, we introduce a convolution-based algorithm for linear time computation and a block-diagonal kernel to enforce the trade-off. Results show that WFRE can outperform existing query embedding methods on standard datasets, eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#22312;&#31243;&#24207;&#19978;&#27604;&#38750;&#20020;&#24202;&#21307;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.04004</link><description>&lt;p&gt;
&#38754;&#21521;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#30340;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards a Simple Framework of Skill Transfer Learning for Robotic Ultrasound-guidance Procedures. (arXiv:2305.04004v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#22312;&#31243;&#24207;&#19978;&#27604;&#38750;&#20020;&#24202;&#21307;&#29983;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#12290;&#25105;&#20204;&#31616;&#35201;&#22238;&#39038;&#20102;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#28982;&#21518;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#37319;&#26679;&#25216;&#26415;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#36229;&#22768;&#24341;&#23548;&#31243;&#24207;&#23454;&#26102;&#24212;&#29992;&#30340;&#31616;&#21333;&#25216;&#33021;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#21442;&#19982;&#32773;&#65288;&#19968;&#20010;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#21644;&#19968;&#20010;&#38750;&#20020;&#24202;&#21307;&#29983;&#65289;&#22312;&#32974;&#20799;&#27169;&#22411;&#19978;&#23547;&#25214;&#22235;&#33108;&#24515;&#35270;&#22270;&#30340;&#26368;&#20339;&#25195;&#25551;&#24179;&#38754;&#30340;&#35797;&#39564;&#65292;&#20998;&#26512;&#20102;&#36229;&#22768;&#22270;&#20687;&#24103;&#12289;&#32441;&#29702;&#22270;&#20687;&#29305;&#24449;&#30340;&#26102;&#38388;&#24207;&#21015;&#21644;&#22235;&#20803;&#25968;&#65292;&#24182;&#21457;&#29616;&#32463;&#39564;&#20016;&#23500;&#30340;&#20020;&#24202;&#21307;&#29983;&#27604;&#38750;&#20020;&#24202;&#21307;&#29983;&#30340;&#21160;&#20316;&#26356;&#21152;&#36805;&#36895;&#21644;&#24179;&#28369;&#12290;&#23545;&#20110;&#26410;&#26469;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#21098;&#26525;&#21644;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a simple framework of skill transfer learning for robotic ultrasound-guidance procedures. We briefly review challenges in skill transfer learning for robotic ultrasound-guidance procedures. We then identify the need of appropriate sampling techniques, computationally efficient neural networks models that lead to the proposal of a simple framework of skill transfer learning for real-time applications in robotic ultrasound-guidance procedures. We present pilot experiments from two participants (one experienced clinician and one non-clinician) looking for an optimal scanning plane of the four-chamber cardiac view from a fetal phantom. We analysed ultrasound image frames, time series of texture image features and quaternions and found that the experienced clinician performed the procedure in a quicker and smoother way compared to lengthy and non-constant movements from non-clinicians. For future work, we pointed out the need of pruned and quantised neural network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.04003</link><description>&lt;p&gt;
ANTONIO:&#38754;&#21521;NLP&#39564;&#35777;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#23427;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#30340;&#22522;&#20934;&#12290;&#22240;&#20026;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39564;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#24120;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#25968;&#23383;&#25968;&#25454;&#38598;&#65292;&#20294;&#24182;&#19981;&#36866;&#29992;&#20110;NLP&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#19968;&#38382;&#39064;&#30340;&#25216;&#26415;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#21644;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#20197;&#20415;&#23558;NLP&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20934;&#22791;&#20026;&#36866;&#21512;&#22522;&#20110;&#25277;&#35937;&#35299;&#37322;&#30340;&#24050;&#30693;&#39564;&#35777;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20026;&#19968;&#20010;&#21517;&#20026;ANTONIO&#30340;Python&#24211;&#65292;&#35813;&#24211;&#36830;&#25509;&#21040;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;ERAN&#21644;Marabou&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;R-U-A-Robot&#30340;NLP&#25968;&#25454;&#38598;&#23545;&#24037;&#20855;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;&#25552;&#35758;&#20316;&#20026;&#39564;&#35777;&#20855;&#26377;&#27861;&#24459;&#37325;&#35201;&#24615;&#30340;NLP&#24212;&#29992;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24076;&#26395;&#65292;&#30001;&#20110;&#20854;&#26222;&#36941;&#36866;&#29992;&#24615;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#23558;NLP&#39564;&#35777;&#38382;&#39064;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#27604;&#36187;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;NLP&#38382;&#39064;&#20013;&#26222;&#21450;&#36825;&#19968;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of machine learning models used in Natural Language Processing (NLP) is known to be a hard problem. In particular, many known neural network verification methods that work for computer vision and other numeric datasets do not work for NLP. Here, we study technical reasons that underlie this problem. Based on this analysis, we propose practical methods and heuristics for preparing NLP datasets and models in a way that renders them amenable to known verification methods based on abstract interpretation. We implement these methods as a Python library called ANTONIO that links to the neural network verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP applications. We hope that, thanks to its general applicability, this work will open novel possibilities for including NLP verification problems into neural network verification competitions, and will popularise NLP problems withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#26694;&#26550;AADiff&#65292;&#23427;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#38899;&#39057;&#23545;&#40784;&#29983;&#25104;&#35270;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#38899;&#39057;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#65292;&#22312;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#21487;&#29992;&#20110;&#20869;&#23481;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.04001</link><description>&lt;p&gt;
AADiff: &#22522;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#30340;&#38899;&#39057;&#23545;&#40784;&#35270;&#39057;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion. (arXiv:2305.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#21512;&#25104;&#26694;&#26550;AADiff&#65292;&#23427;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#36890;&#36807;&#38899;&#39057;&#23545;&#40784;&#29983;&#25104;&#35270;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#38899;&#39057;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#65292;&#22312;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#21487;&#29992;&#20110;&#20869;&#23481;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;T2V&#27169;&#22411;&#20165;&#20351;&#29992;&#25991;&#26412;&#20316;&#20026;&#24341;&#23548;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#24314;&#27169;&#35814;&#32454;&#30340;&#26102;&#38388;&#21160;&#24577;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;T2V&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21478;&#22806;&#20351;&#29992;&#38899;&#39057;&#20449;&#21495;&#26469;&#25511;&#21046;&#26102;&#38388;&#21160;&#24577;&#65292;&#20351;&#24471;&#19968;&#20010;&#29616;&#25104;&#30340;T2I&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#38899;&#39057;&#23545;&#40784;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#22522;&#20110;&#38899;&#39057;&#30340;&#21306;&#22495;&#32534;&#36753;&#21644;&#20449;&#21495;&#24179;&#28369;&#26469;&#22312;&#35270;&#39057;&#21512;&#25104;&#30340;&#20004;&#20010;&#30456;&#20114;&#30683;&#30462;&#30340;&#24895;&#26395;&#65292;&#21363;&#26102;&#38388;&#28789;&#27963;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#26469;&#32463;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20869;&#23481;&#21019;&#24314;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in diffusion models have showcased promising results in the text-to-video (T2V) synthesis task. However, as these T2V models solely employ text as the guidance, they tend to struggle in modeling detailed temporal dynamics. In this paper, we introduce a novel T2V framework that additionally employ audio signals to control the temporal dynamics, empowering an off-the-shelf T2I diffusion to generate audio-aligned videos. We propose audio-based regional editing and signal smoothing to strike a good balance between the two contradicting desiderata of video synthesis, i.e., temporal flexibility and coherence. We empirically demonstrate the effectiveness of our method through experiments, and further present practical applications for contents creation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03987</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#19982;&#30417;&#30563;&#27491;&#21017;&#21270;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization. (arXiv:2305.03987v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22797;&#21046;&#20154;&#31867;&#22797;&#26434;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#30417;&#30563;&#27491;&#21017;&#21270;&#26469;&#35299;&#20915;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#23398;&#20064;&#30495;&#23454;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#30340;&#31574;&#30053;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#38656;&#35201;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy learning (PL) is a module of a task-oriented dialogue system that trains an agent to make actions in each dialogue turn. Imitating human action is a fundamental problem of PL. However, both supervised learning (SL) and reinforcement learning (RL) frameworks cannot imitate humans well. Training RL models require online interactions with user simulators, while simulating complex human policy is hard. Performances of SL-based models are restricted because of the covariate shift problem. Specifically, a dialogue is a sequential decision-making process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Therefore, the generalize ability of SL models is restricted because statistical characteristics of training and testing dialogue data gradually become different. This study proposed an offline imitation learning model that learns policy from real dialogue datasets and does not require user simulators. It also utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26412;&#36523;&#12290;</title><link>http://arxiv.org/abs/2305.03963</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#22411;&#65306;Android&#24212;&#29992;&#20013;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps. (arXiv:2305.03963v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25915;&#20987;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#26234;&#33021;&#25163;&#26426;&#20302;&#24310;&#36831;&#21644;&#33410;&#30465;&#24102;&#23485;&#31561;&#20248;&#28857;&#25512;&#21160;&#20102;&#26234;&#33021;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#20063;&#31216;&#20026;&#20256;&#32479;&#24212;&#29992;&#65292;&#20294;&#36825;&#31181;&#25216;&#26415;&#36827;&#23637;&#20063;&#24341;&#21457;&#20102;&#35768;&#22810;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#31034;&#20363;&#12289;&#27169;&#22411;&#31363;&#21462;&#21644;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#12290;&#29616;&#26377;&#25915;&#20987;&#21644;&#38024;&#23545;&#35774;&#22791;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#31574;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#26412;&#36523;&#65292;&#32780;&#24456;&#23569;&#20851;&#27880;&#25968;&#25454;&#22788;&#29702;&#23545;&#27169;&#22411;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#30693;&#35782;&#24046;&#36317;&#20984;&#26174;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#20840;&#38754;&#29702;&#35299;&#21644;&#35299;&#20915;&#19982;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22788;&#29702;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#38024;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36827;&#34892;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#33021;&#22815;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03954</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#20197;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Learning Action Embeddings for Off-Policy Evaluation. (arXiv:2305.03954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#65292;&#20197;&#20943;&#23569;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#25552;&#39640;&#31163;&#32447;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#35780;&#20272;&#65288;OPE&#65289;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#30001;&#19981;&#21516;&#31574;&#30053;&#25910;&#38598;&#30340;&#35760;&#24405;&#25968;&#25454;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#39044;&#26399;&#22870;&#21169;&#12290; OPE&#26159;&#36816;&#34892;&#26114;&#36149;&#30340;&#22312;&#32447;A / B&#27979;&#35797;&#30340;&#21487;&#34892;&#36873;&#25321;&#65306;&#23427;&#21487;&#20197;&#21152;&#24555;&#26032;&#31574;&#30053;&#30340;&#24320;&#21457;&#65292;&#24182;&#38477;&#20302;&#21521;&#23458;&#25143;&#26292;&#38706;&#27425;&#20248;&#27835;&#30103;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#24403;&#21160;&#20316;&#25968;&#37327;&#24456;&#22823;&#25110;&#35760;&#24405;&#31574;&#30053;&#26410;&#20805;&#20998;&#25506;&#32034;&#26576;&#20123;&#25805;&#20316;&#26102;&#65292;&#22522;&#20110;&#21453;&#21521;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#30340;&#29616;&#26377;&#20272;&#35745;&#22120;&#21487;&#33021;&#20855;&#26377;&#39640;&#29978;&#33267;&#26080;&#38480;&#26041;&#24046;&#12290;Saito&#21644;Joachims&#25552;&#20986;&#20351;&#29992;&#21160;&#20316;&#23884;&#20837;&#30340;&#36793;&#38469;IPS&#65288;MIPS&#65289;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#21160;&#20316;&#31354;&#38388;&#20013;&#38477;&#20302;IPS&#30340;&#26041;&#24046;&#12290; MIPS&#20551;&#35774;&#20174;&#19994;&#32773;&#21487;&#20197;&#23450;&#20041;&#33391;&#22909;&#30340;&#21160;&#20316;&#23884;&#20837;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#38590;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20174;&#35760;&#24405;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#20316;&#23884;&#20837;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#20013;&#38388;&#36755;&#20986;&#26469;&#23450;&#20041;&#21160;&#20316;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;MIPS&#20272;&#35745;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy evaluation (OPE) methods allow us to compute the expected reward of a policy by using the logged data collected by a different policy. OPE is a viable alternative to running expensive online A/B tests: it can speed up the development of new policies, and reduces the risk of exposing customers to suboptimal treatments. However, when the number of actions is large, or certain actions are under-explored by the logging policy, existing estimators based on inverse-propensity scoring (IPS) can have a high or even infinite variance. Saito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS) that uses action embeddings instead, which reduces the variance of IPS in large action spaces. MIPS assumes that good action embeddings can be defined by the practitioner, which is difficult to do in many real-world applications. In this work, we explore learning action embeddings from logged data. In particular, we use intermediate outputs of a trained reward model to defin
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;-Residual Prompt Tuning&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35843;&#20248;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#36229;&#36807;Prompt Tuning 7&#20010;&#28857;&#65292;&#19988;&#21487;&#20197;&#32553;&#30701;Prompt&#38271;&#24230;10&#20493;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03937</link><description>&lt;p&gt;
&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization. (arXiv:2305.03937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#37325;&#21442;&#25968;&#21270;&#30340;Prompt Tuning&#25913;&#36827;&#26041;&#27861;-Residual Prompt Tuning&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35843;&#20248;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#22312;&#36229;&#36807;Prompt Tuning 7&#20010;&#28857;&#65292;&#19988;&#21487;&#20197;&#32553;&#30701;Prompt&#38271;&#24230;10&#20493;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#21516;&#26102;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#26159;&#30446;&#21069;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25928;&#29575;&#30340;&#19968;&#31181;&#25104;&#21151;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21442;&#25968;&#25928;&#29575;&#26368;&#39640;&#65288;&#35843;&#25972;&#30340;soft prompts&#19981;&#21040;&#24635;&#21442;&#25968;&#30340;0.1%&#65289;&#65292;&#20294;&#23427;&#36890;&#24120;&#34920;&#29616;&#27604;&#20854;&#20182;&#25928;&#29575;&#39640;&#30340;&#35843;&#20248;&#26041;&#27861;&#26356;&#24046;&#65292;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#38750;&#24120;&#25935;&#24863;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;Residual Prompt Tuning&#26041;&#27861;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;Prompt Tuning&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#23545;&#36719;Prompt&#30340;&#37325;&#21442;&#25968;&#21270;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Residual Prompt Tuning&#26126;&#26174;&#20248;&#20110;Prompt Tuning&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;T5-Base&#30456;&#27604;&#65292;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23558;Prompt&#38271;&#24230;&#32553;&#30701;&#20102;10&#20493;&#65292;&#19988;&#23545;&#20110;&#23398;&#20064;&#29575;&#21644;Prompt&#21021;&#22987;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute &lt;0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning on SuperGLUE benchmark. Notably, our method reaches +7 points improvement over prompt tuning with T5-Base and allows to reduce the prompt length by 10x without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#19982;&#21306;&#22359;&#38142;&#25972;&#21512;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#25968;&#25454;&#21152;&#23494;&#12289;&#21435;&#26631;&#35782;&#21270;&#12289;&#22810;&#32423;&#20998;&#24067;&#24335;&#36134;&#26412;&#21644;k-&#21311;&#21517;&#26041;&#27861;&#31561;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#20837;&#25163;&#65292;&#20998;&#26512;&#20102;AI-&#21306;&#22359;&#38142;-&#38544;&#31169;&#20445;&#25252;&#31995;&#32479;&#30340;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#25216;&#26415;&#24314;&#35758;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2305.03928</link><description>&lt;p&gt;
AI&#19982;&#21306;&#22359;&#38142;&#38544;&#31169;&#20445;&#25252;&#25972;&#21512;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of AI and Blockchain Integration for Privacy-Preserving. (arXiv:2305.03928v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#19982;&#21306;&#22359;&#38142;&#25972;&#21512;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#65292;&#20174;&#25968;&#25454;&#21152;&#23494;&#12289;&#21435;&#26631;&#35782;&#21270;&#12289;&#22810;&#32423;&#20998;&#24067;&#24335;&#36134;&#26412;&#21644;k-&#21311;&#21517;&#26041;&#27861;&#31561;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#20837;&#25163;&#65292;&#20998;&#26512;&#20102;AI-&#21306;&#22359;&#38142;-&#38544;&#31169;&#20445;&#25252;&#31995;&#32479;&#30340;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26368;&#32456;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#25216;&#26415;&#24314;&#35758;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#24191;&#27867;&#20851;&#27880;&#21644;&#24212;&#29992;&#65292;&#30001;&#20854;&#25972;&#21512;&#20135;&#29983;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20855;&#26377;&#26174;&#33879;&#30340;&#24847;&#20041;&#12290;&#36825;&#20123;&#25216;&#26415;&#19981;&#20165;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#36824;&#20445;&#35777;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#21306;&#22359;&#38142;&#65292;&#24182;&#27010;&#25324;&#20102;&#23427;&#20204;&#30456;&#32467;&#21512;&#25152;&#20135;&#29983;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#12290;&#25509;&#30528;&#25506;&#35752;&#20102;&#25968;&#25454;&#21152;&#23494;&#12289;&#21435;&#26631;&#35782;&#21270;&#12289;&#22810;&#32423;&#20998;&#24067;&#24335;&#36134;&#26412;&#21644;k-&#21311;&#21517;&#26041;&#27861;&#31561;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#35780;&#20272;&#20102;AI-&#21306;&#22359;&#38142;-&#38544;&#31169;&#20445;&#25252;&#31995;&#32479;&#30340;&#20116;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#21253;&#25324;&#25480;&#26435;&#31649;&#29702;&#12289;&#35775;&#38382;&#25511;&#21046;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20998;&#26512;&#20102;&#32570;&#38519;&#21450;&#20854;&#23454;&#38469;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#36824;&#22522;&#20110;&#21508;&#26041;&#38754;&#23545;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread attention and application of artificial intelligence (AI) and blockchain technologies, privacy protection techniques arising from their integration are of notable significance. In addition to protecting privacy of individuals, these techniques also guarantee security and dependability of data. This paper initially presents an overview of AI and blockchain, summarizing their combination along with derived privacy protection technologies. It then explores specific application scenarios in data encryption, de-identification, multi-tier distributed ledgers, and k-anonymity methods. Moreover, the paper evaluates five critical aspects of AI-blockchain-integration privacy protection systems, including authorization management, access control, data protection, network security, and scalability. Furthermore, it analyzes the deficiencies and their actual cause, offering corresponding suggestions. This research also classifies and summarizes privacy protection techniques based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#12289;&#32570;&#22833;&#21644;&#20998;&#24067;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.03920</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automated Spatio-Temporal Graph Contrastive Learning. (arXiv:2305.03920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#22312;&#22788;&#29702;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#22122;&#22768;&#12289;&#32570;&#22833;&#21644;&#20998;&#24067;&#24322;&#36136;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#21306;&#22495;&#23884;&#20837;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#22270;&#30340;&#21306;&#22495;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#30001;&#20110;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#24378;&#22823;&#32467;&#26500;&#34920;&#31034;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23578;&#26410;&#35299;&#20915;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;i&#65289;&#30001;&#20110;&#21508;&#31181;&#22240;&#32032;&#65292;&#25968;&#25454;&#22122;&#38899;&#21644;&#32570;&#22833;&#22312;&#35768;&#22810;&#26102;&#31354;&#22330;&#26223;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;ii&#65289;&#36755;&#20837;&#30340;&#26102;&#31354;&#25968;&#25454;&#65288;&#20363;&#22914;&#31227;&#21160;&#36712;&#36857;&#65289;&#36890;&#24120;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#34920;&#29616;&#20986;&#20998;&#24067;&#24322;&#36136;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24403;&#21069;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#29983;&#25104;&#21306;&#22495;&#22270;&#30340;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#20174;&#22810;&#35270;&#22270;&#25968;&#25454;&#28304;&#29983;&#25104;&#30340;&#24322;&#26500;&#21306;&#22495;&#22270;&#19978;&#30340;&#33258;&#21160;&#26102;&#31354;&#22270;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65288;AutoST&#65289;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;"AutoST"&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#24322;&#26500;&#22270;&#31070;&#32463;&#26550;&#26500;&#20043;&#19978;&#65292;&#20197;&#25429;&#25417;&#22810;&#35270;&#22270;&#21306;&#22495;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region depe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;&#65292;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03900</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#22343;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class Imbalance in Machine Learning. (arXiv:2305.03900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;&#65292;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#23427;&#20851;&#27880;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#23558;&#31867;&#21035;&#19981;&#24179;&#34913;&#23450;&#20041;&#20026;&#27604;&#20363;&#19981;&#24179;&#34913;&#65292;&#21363;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#20363;&#19981;&#24179;&#34913;&#65292;&#32780;&#24573;&#35270;&#27604;&#20363;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#31867;&#21035;&#20043;&#38388;/&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#31867;&#21035;&#19981;&#24179;&#34913;&#20998;&#31867;&#26041;&#24335;&#65292;&#21253;&#25324;&#26041;&#24046;&#12289;&#36317;&#31163;&#12289;&#37051;&#36817;&#21644;&#36136;&#37327;&#31561;&#22235;&#31181;&#19981;&#24179;&#34913;&#31867;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#20171;&#32461;&#20102;&#20004;&#31181;&#19981;&#24179;&#34913;&#32423;&#21035;:&#20840;&#23616;&#21644;&#23616;&#37096;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#29702;&#35770;&#20998;&#26512;&#26469;&#35828;&#26126;&#26032;&#30340;&#19981;&#24179;&#34913;&#31867;&#22411;&#23545;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.03888</link><description>&lt;p&gt;
&#36890;&#36807;Sponge&#27602;&#21270;&#23545;&#35774;&#22791;&#19978;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33021;&#37327;&#24310;&#36831;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning. (arXiv:2305.03888v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#65292;&#21516;&#26102;&#20998;&#26512;&#20102;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#22240;&#20854;&#24320;&#21457;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#32463;&#27982;&#23454;&#24800;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35774;&#22791;&#19978;&#30340;&#27169;&#22411;&#21463;&#21040;&#26377;&#38480;&#30340;&#33021;&#37327;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#32422;&#26463;&#12290;&#21516;&#26102;&#65292;&#19968;&#31181;&#21517;&#20026;Sponge&#27602;&#21270;&#30340;&#25915;&#20987;&#26041;&#24335;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#36825;&#31181;&#25915;&#20987;&#26041;&#24335;&#28041;&#21450;&#21040;&#25552;&#20379;&#27602;&#23475;&#26679;&#26412;&#32473;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#25512;&#26029;&#26399;&#38388;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#30001;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26381;&#21153;&#22120;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Sponge&#27602;&#21270;&#25915;&#20987;&#25193;&#23637;&#21040;&#35774;&#22791;&#19978;&#30340;&#24773;&#20917;&#65292;&#20197;&#35780;&#20272;&#31227;&#21160;&#35774;&#22791;&#22788;&#29702;&#22120;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35774;&#22791;&#19978;&#30340;Sponge&#27602;&#21270;&#25915;&#20987;&#27969;&#31243;&#65292;&#20197;&#27169;&#25311;&#27969;&#24335;&#21644;&#19968;&#33268;&#30340;&#25512;&#26029;&#22330;&#26223;&#65292;&#20197;&#22635;&#34917;&#35774;&#22791;&#19978;&#35774;&#32622;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#22788;&#29702;&#22120;&#21644;&#35774;&#22791;&#32593;&#32476;&#26041;&#38754;&#36827;&#34892;&#20102;&#29420;&#23478;&#23454;&#39564;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;Sponge&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#26377;&#25928;&#27745;&#26579;&#20855;&#26377;&#20869;&#32622;&#21152;&#36895;&#22120;&#30340;&#29616;&#20195;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27602;&#21270;&#25915;&#20987;&#30340;&#33021;&#37327;&#21644;&#24310;&#36831;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, on-device deep learning has gained attention as a means of developing affordable deep learning applications for mobile devices. However, on-device models are constrained by limited energy and computation resources. In the mean time, a poisoning attack known as sponge poisoning has been developed.This attack involves feeding the model with poisoned examples to increase the energy consumption during inference. As previous work is focusing on server hardware accelerators, in this work, we extend the sponge poisoning attack to an on-device scenario to evaluate the vulnerability of mobile device processors. We present an on-device sponge poisoning attack pipeline to simulate the streaming and consistent inference scenario to bridge the knowledge gap in the on-device setting. Our exclusive experimental analysis with processors and on-device networks shows that sponge poisoning attacks can effectively pollute the modern processor with its built-in accelerator. We analyze the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102; sFML &#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03874</link><description>&lt;p&gt;
&#36890;&#36807;&#27969;&#26144;&#23556;&#31639;&#23376;&#23398;&#20064;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Stochastic Dynamical System via Flow Map Operator. (arXiv:2305.03874v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102; sFML &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#25968;&#20540;&#26694;&#26550;&#12290;&#31216;&#20026;&#38543;&#26426;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;sFML&#65289;&#65292;&#36825;&#20010;&#26032;&#26694;&#26550;&#26159;&#27969;&#26144;&#23556;&#23398;&#20064;&#65288;FML&#65289;&#30340;&#25193;&#23637;&#65292;&#21518;&#32773;&#26159;&#20026;&#20102;&#23398;&#20064;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#32780;&#24320;&#21457;&#30340;&#12290;&#23545;&#20110;&#23398;&#20064;&#38543;&#26426;&#31995;&#32479;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#38543;&#26426;&#27969;&#26144;&#23556;&#65292;&#23427;&#26159;&#20004;&#20010;&#23376;&#27969;&#26144;&#23556;&#30340;&#21472;&#21152;&#65306;&#19968;&#20010;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#21644;&#19968;&#20010;&#38543;&#26426;&#23376;&#26144;&#23556;&#12290;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#39318;&#20808;&#29992;&#20110;&#26500;&#24314;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#65292;&#28982;&#21518;&#26159;&#38543;&#26426;&#23376;&#26144;&#23556;&#12290;&#30830;&#23450;&#24615;&#23376;&#26144;&#23556;&#37319;&#29992;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#24418;&#24335;&#65292;&#31867;&#20284;&#20110;FML&#23545;&#20110;&#30830;&#23450;&#24615;&#31995;&#32479;&#30340;&#24037;&#20316;&#12290;&#23545;&#20110;&#38543;&#26426;&#23376;&#26144;&#23556;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26412;&#25991;&#20013;&#24212;&#29992;&#12290;&#26368;&#32456;&#26500;&#24314;&#30340;&#38543;&#26426;&#27969;&#26144;&#23556;&#23450;&#20041;&#20102;&#19968;&#20010;&#38543;&#26426;&#28436;&#21270;&#27169;&#22411;&#65292;&#23427;&#22312;&#20998;&#24067;&#26041;&#38754;&#26159;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#24369;&#36817;&#20284;&#12290;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;sFML&#25581;&#31034;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#21508;&#31181;&#31867;&#22411;&#30340;&#38750;&#32447;&#24615;&#12289;&#22122;&#22768;&#21327;&#26041;&#24046;&#32467;&#26500;&#21644;&#26102;&#38388;&#30456;&#20851;&#29305;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a numerical framework for learning unknown stochastic dynamical systems using measurement data. Termed stochastic flow map learning (sFML), the new framework is an extension of flow map learning (FML) that was developed for learning deterministic dynamical systems. For learning stochastic systems, we define a stochastic flow map that is a superposition of two sub-flow maps: a deterministic sub-map and a stochastic sub-map. The stochastic training data are used to construct the deterministic sub-map first, followed by the stochastic sub-map. The deterministic sub-map takes the form of residual network (ResNet), similar to the work of FML for deterministic systems. For the stochastic sub-map, we employ a generative model, particularly generative adversarial networks (GANs) in this paper. The final constructed stochastic flow map then defines a stochastic evolution model that is a weak approximation, in term of distribution, of the unknown stochastic system. A comprehensive set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.03859</link><description>&lt;p&gt;
&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#65306;&#20197;&#33521;&#22269;COVID-19&#20026;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Open problems in causal structure learning: A case study of COVID-19 in the UK. (arXiv:2305.03859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24674;&#22797;&#22270;&#24418;&#32467;&#26500;&#65292;&#20174;&#32780;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#31639;&#27861;&#25552;&#20379;&#30340;&#22240;&#26524;&#34920;&#31034;&#20351;&#24471;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20851;&#32852;&#24615;&#26426;&#22120;&#23398;&#20064;&#30456;&#27604;&#65292;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;COVID-19&#33521;&#22269;&#30123;&#24773;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#21508;&#31181;&#20844;&#20849;&#26469;&#28304;&#25972;&#21512;&#25968;&#25454;&#65292;&#24182;&#30740;&#31350;&#21508;&#31181;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#23545;&#23398;&#20064;&#31867;&#21035;&#19981;&#21516;&#30340;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21450;&#31639;&#27861;&#32452;&#20135;&#29983;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22270;&#24418;&#32467;&#26500;&#12289;&#27169;&#22411;&#32500;&#24230;&#12289;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#28151;&#28102;&#21464;&#37327;&#12289;&#39044;&#27979;&#21644;&#24178;&#39044;&#25512;&#26029;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#26469;&#31361;&#20986;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#26410;&#35299;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal machine learning (ML) algorithms recover graphical structures that tell us something about cause-and-effect relationships. The causal representation provided by these algorithms enables transparency and explainability, which is necessary in critical real-world problems. Yet, causal ML has had limited impact in practice compared to associational ML. This paper investigates the challenges of causal ML with application to COVID-19 UK pandemic data. We collate data from various public sources and investigate what the various structure learning algorithms learn from these data. We explore the impact of different data formats on algorithms spanning different classes of learning, and assess the results produced by each algorithm, and groups of algorithms, in terms of graphical structure, model dimensionality, sensitivity analysis, confounding variables, predictive and interventional inference. We use these results to highlight open problems in causal structure learning and directions f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;CHAI-DT&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#23545;&#35805;&#29983;&#25104;&#22411;AI&#20195;&#29702;&#31215;&#26497;&#21442;&#19982;&#21040;&#20849;&#21019;&#27963;&#21160;&#20013;&#65292;&#20026;&#35774;&#35745;&#24605;&#32500;&#27963;&#21160;&#25552;&#20379;&#26377;&#29992;&#21644;&#21019;&#24847;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#28508;&#22312;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.03852</link><description>&lt;p&gt;
CHAI-DT: &#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20419;&#20351;&#23545;&#35805;&#29983;&#25104;&#22411;AI&#20195;&#29702;&#31215;&#26497;&#21442;&#19982;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
CHAI-DT: A Framework for Prompting Conversational Generative AI Agents to Actively Participate in Co-Creation. (arXiv:2305.03852v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;CHAI-DT&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#23545;&#35805;&#29983;&#25104;&#22411;AI&#20195;&#29702;&#31215;&#26497;&#21442;&#19982;&#21040;&#20849;&#21019;&#27963;&#21160;&#20013;&#65292;&#20026;&#35774;&#35745;&#24605;&#32500;&#27963;&#21160;&#25552;&#20379;&#26377;&#29992;&#21644;&#21019;&#24847;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#28508;&#22312;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#24212;&#29992;&#20110;&#23567;&#32452;&#20849;&#21019;&#26694;&#26550;&#20013;&#65292;&#20197;&#22686;&#24378;&#21830;&#19994;&#21019;&#26032;&#21644;&#20849;&#21019;&#32972;&#26223;&#19979;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#26500;&#24605;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#21551;&#29992;&#23545;&#35805;&#24335;&#29983;&#25104;&#22411;AI&#20195;&#29702;&#30340;&#31215;&#26497;&#36129;&#29486;&#21040;&#35774;&#35745;&#24605;&#32500;(co-creative framework)&#30340;&#21150;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#20256;&#32479;&#30340;&#8220;&#20154;&#19982;&#20154;&#8221;&#20419;&#36827;&#21644;&#25945;&#23398;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#25552;&#31034;&#25216;&#26415;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#32842;&#22825;&#29983;&#25104;&#22411;&#21464;&#21387;&#22120;(ChatGPT)&#20026;&#35774;&#35745;&#24605;&#32500;&#27963;&#21160;&#25552;&#20379;&#20855;&#26377;&#29305;&#23450;&#19978;&#19979;&#25991;&#12289;&#26377;&#29992;&#21644;&#21019;&#24847;&#24615;&#30340;&#36129;&#29486;&#30340;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#36827;&#34892;&#20849;&#21019;&#26500;&#24605;&#30340;&#28508;&#22312;&#20248;&#21183;&#12289;&#23616;&#38480;&#24615;&#21644;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential for utilizing generative AI models in group-focused co-creative frameworks to enhance problem solving and ideation in business innovation and co-creation contexts, and proposes a novel prompting technique for conversational generative AI agents which employ methods inspired by traditional 'human-to-human' facilitation and instruction to enable active contribution to Design Thinking, a co-creative framework. Through experiments using this prompting technique, we gather evidence that conversational generative transformers (i.e. ChatGPT) have the capability to contribute context-specific, useful, and creative input into Design Thinking activities. We also discuss the potential benefits, limitations, and risks associated with using generative AI models in co-creative ideation and provide recommendations for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#36328;&#24230;&#39044;&#27979;&#34920;&#29616;&#30053;&#20248;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03845</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#20013;CLaC&#65306;&#27604;&#36739;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#26041;&#27861;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CLaC at SemEval-2023 Task 2: Comparing Span-Prediction and Sequence-Labeling approaches for NER. (arXiv:2305.03845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#20013;&#36328;&#24230;&#39044;&#27979;&#34920;&#29616;&#30053;&#20248;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;CLaC&#23545;&#20110;MultiCoNER 2&#20219;&#21153;&#30340;&#25552;&#20132;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#36328;&#24230;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#36328;&#24230;&#39044;&#27979;&#31995;&#32479;&#30340;&#34920;&#29616;&#30053;&#20248;&#20110;&#25105;&#20204;&#26368;&#22909;&#30340;&#24207;&#21015;&#26631;&#35760;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#22823;&#29256;&#26412;&#30340;XLM RoBERTa&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#21518;&#32493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;XLM-RoBERTa&#30340;&#29305;&#27530;&#36755;&#20837;&#26631;&#35760;&#65288;&lt;s&gt;&#21644;&lt;/s&gt;&#65289;&#26102;&#65292;&#36328;&#24230;&#39044;&#27979;&#21644;&#24207;&#21015;&#26631;&#35760;&#26041;&#27861;&#37117;&#20250;&#24471;&#21040;&#25913;&#36827;&#12290;&#25152;&#26377;&#27169;&#22411;&#35757;&#32451;&#65292;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/harshshredding/semeval2023-multiconer-paper&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper summarizes the CLaC submission for the MultiCoNER 2 task which concerns the recognition of complex, fine-grained named entities. We compare two popular approaches for NER, namely Sequence Labeling and Span Prediction. We find that our best Span Prediction system performs slightly better than our best Sequence Labeling system on test data. Moreover, we find that using the larger version of XLM RoBERTa significantly improves performance. Post-competition experiments show that Span Prediction and Sequence Labeling approaches improve when they use special input tokens (&lt;s&gt; and &lt;/s&gt;) of XLM-RoBERTa. The code for training all models, preprocessing, and post-processing is available at https://github.com/harshshredding/semeval2023-multiconer-paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#20197;&#21450;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#27169;&#22411;&#26550;&#26500;&#20013;&#22343;&#20855;&#26377;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03843</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#27604;&#23398;&#20064;&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Contrastive Learning of Semantic Similarity forCode to Code Search. (arXiv:2305.03843v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#20197;&#21450;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#27169;&#22411;&#26550;&#26500;&#20013;&#22343;&#20855;&#26377;&#19968;&#33268;&#30340;&#24615;&#33021;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#38745;&#24577;&#21644;&#21160;&#24577;&#29305;&#24449;&#65292;&#21033;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#21160;&#24577;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#22312;&#25512;&#26029;&#26102;&#25191;&#34892;&#35201;&#25628;&#32034;&#30340;&#35821;&#26009;&#24211;&#25110;&#25628;&#32034;&#26597;&#35810;&#65292;&#24182;&#19988;&#31532;&#19968;&#31181;&#22312;&#27491;&#36127;&#21442;&#32771;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#20195;&#30721;&#25628;&#32034;&#25216;&#26415;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#65292;&#35777;&#26126;&#22686;&#24378;&#22411;LLMs&#33021;&#22815;&#36827;&#34892;&#36328;&#35821;&#35328;&#20195;&#30721;&#25628;&#32034;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#26550;&#26500;&#21644;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#26159;&#19968;&#33268;&#30340;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#36328;&#35821;&#35328;&#25628;&#32034;&#24037;&#20855;&#25552;&#39640;&#20102;&#39640;&#36798;44.7&#65285;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20063;&#33021;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#21482;&#20351;&#29992;&#27491;&#26679;&#26412;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search.  Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7\%. Moreover, our ablation studies reveal that even a single positive and negat
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#27169;&#25311;&#20013;&#26174;&#31034;&#20986;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03835</link><description>&lt;p&gt;
&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal Transformer for Stock Movement Prediction. (arXiv:2305.03835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03835
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#27169;&#25311;&#20013;&#26174;&#31034;&#20986;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24066;&#22330;&#26159;&#19968;&#20010;&#24341;&#20154;&#20837;&#32988;&#30340;&#22320;&#26041;&#65292;&#22914;&#26524;&#26102;&#38388;&#25484;&#25569;&#24471;&#24403;&#65292;&#25237;&#36164;&#32773;&#21487;&#20197;&#33719;&#24471;&#24040;&#22823;&#30340;&#21033;&#28070;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37329;&#34701;&#24066;&#22330;&#30340;&#21160;&#24577;&#12289;&#38750;&#32447;&#24615;&#29305;&#24615;&#20351;&#24471;&#26410;&#26469;&#20215;&#26684;&#36208;&#21183;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STST&#65292;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;&#21464;&#25442;&#22120;-LSTM&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#36816;&#21160;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ACL18&#21644;KDD17&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#33719;&#24471;63.707%&#21644;56.879%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27169;&#25311;&#20013;&#29992;&#20110;&#30830;&#23450;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#30340;&#26368;&#23567;&#24180;&#21270;&#25910;&#30410;&#29575;&#27604;&#26631;&#20934;&#26222;&#23572;500&#32929;&#31080;&#25351;&#25968;&#39640;&#20986;10.41%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41% higher profit than the S&amp;P500 stock index, with a minimum annualized 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03742</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31526;&#21495;&#32534;&#31243;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming. (arXiv:2305.03742v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;DSR-LM&#65292;&#29992;&#20110;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#19981;&#20687;&#20197;&#24448;&#30340;&#30740;&#31350;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#32452;&#21512;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#21487;&#38752;&#22320;&#25191;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;&#26412;&#25991;&#22522;&#20110;&#31526;&#21495;&#32534;&#31243;&#30340;&#35270;&#35282;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DSR-LM&#65292;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340;LMs&#31649;&#29702;&#20107;&#23454;&#30693;&#35782;&#30340;&#24863;&#30693;&#65292;&#31526;&#21495;&#27169;&#22359;&#25191;&#34892;&#28436;&#32462;&#25512;&#29702;&#12290;&#19982;&#20381;&#36182;&#25163;&#24037;&#21046;&#23450;&#30340;&#36923;&#36753;&#35268;&#21017;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#25512;&#29702;&#26694;&#26550;&#26377;&#25928;&#22320;&#23398;&#20064;&#21152;&#26435;&#35268;&#21017;&#65292;&#24182;&#24212;&#29992;&#35821;&#20041;&#25439;&#22833;&#36827;&#19968;&#27493;&#25913;&#21892;LMs&#12290;DSR-LM&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20801;&#35768;&#36731;&#26494;&#38598;&#25104;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25903;&#25345;&#24191;&#27867;&#30340;&#31526;&#21495;&#32534;&#31243;&#65292;&#20197;&#31283;&#20581;&#22320;&#25512;&#20986;&#36923;&#36753;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSR-LM&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;20%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;DSR-LM&#36824;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#38382;&#39064;&#65292;&#21253;&#25324;&#24320;&#25918;&#24335;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR
&lt;/p&gt;</description></item><item><title>AmGCL&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#21644;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#23646;&#24615;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03741</link><description>&lt;p&gt;
AmGCL: &#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32570;&#22833;&#22270;&#29305;&#24449;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AmGCL: Feature Imputation of Attribute Missing Graph via Self-supervised Contrastive Learning. (arXiv:2305.03741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03741
&lt;/p&gt;
&lt;p&gt;
AmGCL&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#25554;&#34917;&#21644;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26469;&#35299;&#20915;&#23646;&#24615;&#22270;&#20013;&#33410;&#28857;&#23646;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#22312;&#22810;&#23186;&#20307;&#24212;&#29992;&#20013;&#20351;&#29992;&#24191;&#27867;&#65292;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#23646;&#24615;&#22270;&#25968;&#25454;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#19981;&#23436;&#25972;&#30340;&#22270;&#25968;&#25454;&#21644;&#32570;&#22833;&#30340;&#33410;&#28857;&#23646;&#24615;&#21487;&#33021;&#20250;&#23545;&#23186;&#20307;&#30693;&#35782;&#21457;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#22270;&#30340;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#20551;&#35774;&#25110;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#23646;&#24615;-&#22270;&#24418;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32570;&#22833;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;AmGCL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#23646;&#24615;&#22270;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#33410;&#28857;&#23646;&#24615;&#12290;AmGCL&#21033;&#29992;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#30340;&#29305;&#24449;&#39044;&#32534;&#30721;&#26469;&#23545;&#32570;&#22833;&#23646;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#22270;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#32467;&#26500;&#65288;GACLS&#65289;&#20174;&#32534;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AmGCL&#21033;&#29992;&#32467;&#26500;-&#23646;&#24615;&#33021;&#37327;&#26368;&#23567;&#21270;&#36827;&#34892;&#29305;&#24449;&#37325;&#26500;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#19979;&#38480;&#20197;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribute graphs are ubiquitous in multimedia applications, and graph representation learning (GRL) has been successful in analyzing attribute graph data. However, incomplete graph data and missing node attributes can have a negative impact on media knowledge discovery. Existing methods for handling attribute missing graph have limited assumptions or fail to capture complex attribute-graph dependencies. To address these challenges, we propose Attribute missing Graph Contrastive Learning (AmGCL), a framework for handling missing node attributes in attribute graph data. AmGCL leverages Dirichlet energy minimization-based feature precoding to encode in missing attributes and a self-supervised Graph Augmentation Contrastive Learning Structure (GACLS) to learn latent variables from the encoded-in data. Specifically, AmGCL utilizies feature reconstruction based on structure-attribute energy minimization while maximizes the lower bound of evidence for latent representation mutual information.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.03737</link><description>&lt;p&gt;
&#35843;&#25972;&#20256;&#32479;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20197;&#36866;&#29992;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Tuning Traditional Language Processing Approaches for Pashto Text Classification. (arXiv:2305.03737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#21644;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22312;&#24456;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22810;&#39033;&#30740;&#31350;&#26469;&#24320;&#21457;&#22269;&#38469;&#21644;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#20026;&#26412;&#22320;&#35821;&#35328;&#24314;&#31435;&#19968;&#20010;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#24314;&#31435;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#33258;&#21160;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;, &#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26222;&#20160;&#22270;&#35821;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#30001;&#20110;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#25991;&#26723;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19981;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;(SVM)&#12289;K&#36817;&#37051;(KNN)&#12289;&#20915;&#31574;&#26641;, &#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;, &#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;, &#38543;&#26426;&#26862;&#26519;&#21644;&#36923;&#36753;&#22238;&#24402;&#22312;&#20869;&#30340;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#21253;&#25324;&#35789;&#34955;&#21644;tf-idf&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26222;&#20160;&#22270;&#35821;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#20351;&#29992;tf-idf&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;SVM&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;97.19%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today text classification becomes critical task for concerned individuals for numerous purposes. Hence, several researches have been conducted to develop automatic text classification for national and international languages. However, the need for an automatic text categorization system for local languages is felt. The main aim of this study is to establish a Pashto automatic text classification system. In order to pursue this work, we built a Pashto corpus which is a collection of Pashto documents due to the unavailability of public datasets of Pashto text documents. Besides, this study compares several models containing both statistical and neural network machine learning techniques including Multilayer Perceptron (MLP), Support Vector Machine (SVM), K Nearest Neighbor (KNN), decision tree, gaussian na\"ive Bayes, multinomial na\"ive Bayes, random forest, and logistic regression to discover the most effective approach. Moreover, this investigation evaluates two different feature extr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#65292;Stackelberg Multi-Agent Deep Deterministic Policy Gradient(ST-MADDPG), &#29992;&#20110;&#22312;&#31454;&#20105;&#23398;&#20064;&#33258;&#36866;&#24212;&#35838;&#31243;&#20013;&#23398;&#20064;&#26032;&#20852;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.03735</link><description>&lt;p&gt;
Stackelberg Games&#29992;&#20110;&#22312;&#31454;&#20105;&#23398;&#20064;&#33258;&#36866;&#24212;&#35838;&#31243;&#20013;&#23398;&#20064;&#26032;&#20852;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Games for Learning Emergent Behaviors During Competitive Autocurricula. (arXiv:2305.03735v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#65292;Stackelberg Multi-Agent Deep Deterministic Policy Gradient(ST-MADDPG), &#29992;&#20110;&#22312;&#31454;&#20105;&#23398;&#20064;&#33258;&#36866;&#24212;&#35838;&#31243;&#20013;&#23398;&#20064;&#26032;&#20852;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#35838;&#31243;&#35757;&#32451;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#39046;&#22495;&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#26080;&#30417;&#30563;&#30340;&#20849;&#21516;&#28436;&#21270;&#26041;&#26696;&#20013;&#23398;&#20064;&#26032;&#20852;&#30340;&#25216;&#33021;&#12290;&#26426;&#22120;&#20154;&#31038;&#21306;&#24050;&#32463;&#22312;&#29289;&#29702;&#31435;&#36275;&#30340;&#38382;&#39064;&#19978;&#23581;&#35797;&#20102;&#33258;&#36866;&#24212;&#35838;&#31243;&#35757;&#32451;&#65292;&#20363;&#22914;&#24378;&#20581;&#25511;&#21046;&#21644;&#20132;&#20114;&#24335;&#25805;&#32437;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#19981;&#23545;&#31216;&#24615;&#20351;&#24471;&#29983;&#25104;&#22797;&#26434;&#31574;&#30053;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#29615;&#22659;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#21487;&#33021;&#20250;&#38544;&#21547;&#22320;&#25110;&#26126;&#30830;&#22320;&#20026;&#26576;&#20123;&#26234;&#33021;&#20307;&#25552;&#20379;&#20248;&#21183;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36136;&#37327;&#20302;&#21155;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861; - Stackelberg&#28145;&#20915;&#31574;&#31574;&#30053;&#26799;&#24230;&#65288;ST-MADDPG&#65289;&#65292;&#23558;&#19968;&#20010;&#21452;&#20154;MARL&#38382;&#39064;&#21046;&#23450;&#20026;&#20855;&#26377;&#23618;&#27425;&#20132;&#20114;&#32467;&#26500;&#30340;Stackelberg&#21338;&#24328;&#65292;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#26159;&#8220;&#39046;&#23548;&#32773;&#8221;&#65292;&#21478;&#19968;&#20010;&#26159;&#8220;&#36319;&#38543;&#32773;&#8221;&#65292;&#39046;&#23548;&#32773;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#39046;&#23548;&#32773;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Autocurricular training is an important sub-area of multi-agent reinforcement learning~(MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented autocurricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the `leader' and the other as the `follower' in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader's advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#36807;&#31243;&#20013;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#35782;&#21035;&#21487;&#25235;&#21462;&#21306;&#22495;&#25552;&#20379;&#26356;&#22810;&#25235;&#21462;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.03259</link><description>&lt;p&gt;
&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation. (arXiv:2305.03259v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#36807;&#31243;&#20013;&#30340;&#26381;&#35013;&#25235;&#21462;&#21644;&#23637;&#24320;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RGB-D&#35821;&#20041;&#20998;&#21106;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65292;&#21487;&#20197;&#35782;&#21035;&#21487;&#25235;&#21462;&#21306;&#22495;&#25552;&#20379;&#26356;&#22810;&#25235;&#21462;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#30340;&#25235;&#21462;&#21644;&#23637;&#24320;&#26159;&#26426;&#22120;&#20154;&#36741;&#21161;&#31359;&#34915;&#30340;&#26680;&#24515;&#27493;&#39588;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#26381;&#35013;&#30340;&#28145;&#24230;&#22270;&#20687;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#35782;&#21035;&#21512;&#36866;&#30340;&#25235;&#21462;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29289;&#29702;&#24341;&#25806;&#26469;&#21512;&#25104;&#28145;&#24230;&#22270;&#20687;&#20197;&#20943;&#23569;&#30495;&#23454;&#26631;&#35760;&#25968;&#25454;&#30340;&#25910;&#38598;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#33258;&#28982;&#39046;&#22495;&#24046;&#36317;&#24120;&#24120;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#25235;&#21462;&#28857;&#34987;&#26381;&#35013;&#29289;&#21697;&#26412;&#36523;&#36974;&#25377;&#30340;&#22330;&#26223;&#20013;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21521;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#32593;&#32476;&#65288;BiFCNet&#65289;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#23454;&#29616;&#23545;&#21487;&#25235;&#21462;&#21306;&#22495;&#30340;&#35782;&#21035;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#25235;&#21462;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#65292;&#36824;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#33394;&#24425;&#29305;&#24449;&#30340;RGB&#22270;&#20687;&#20316;&#20026;&#32593;&#32476;&#36755;&#20837;&#65292;&#20854;&#20013;&#20998;&#24418;&#20132;&#21449;&#34701;&#21512;&#65288;FCF&#65289;&#27169;&#22359;&#36890;&#36807;&#23558;RGB&#21644;&#28145;&#24230;&#25968;&#25454;&#34701;&#21512;&#26469;&#32771;&#34385;&#22270;&#20687;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by consid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.02931</link><description>&lt;p&gt;
&#36229;&#36234;&#21516;&#36136;&#24615;&#65306;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond Homophily: Reconstructing Structure for Graph-agnostic Clustering. (arXiv:2305.02931v1 [cs.SI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#37325;&#26500;&#32467;&#26500;&#23454;&#29616;&#23545;&#22270;&#24418;&#26080;&#20851;&#32858;&#31867;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32858;&#31867;&#26041;&#27861;&#22312;&#33410;&#28857;&#32858;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#22522;&#20110;&#21516;&#36136;&#22270;&#30340;&#20551;&#35774;&#35774;&#35745;&#30340;&#65292;&#32780;&#22312;&#24322;&#36136;&#22270;&#19978;&#36827;&#34892;&#32858;&#31867;&#34987;&#24573;&#35270;&#20102;&#12290;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#65292;&#19981;&#21487;&#33021;&#22312;&#25214;&#21040;&#36866;&#21512;&#30340;GNN&#27169;&#22411;&#20043;&#21069;&#39318;&#20808;&#23558;&#22270;&#24418;&#35782;&#21035;&#20026;&#21516;&#36136;&#25110;&#24322;&#36136;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#21508;&#31181;&#21516;&#36136;&#24615;&#27700;&#24179;&#30340;&#30495;&#23454;&#19990;&#30028;&#22270;&#24418;&#36827;&#34892;&#32858;&#31867;&#23558;&#20026;&#22270;&#24418;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#22270;&#24418;&#37325;&#26500;&#12289;&#28151;&#21512;&#28388;&#27874;&#22120;&#21644;&#21452;&#22270;&#24418;&#32858;&#31867;&#32593;&#32476;&#12290;&#20026;&#20102;&#20351;&#20854;&#23545;&#22270;&#24418;&#26080;&#20851;&#65292;&#25105;&#20204;&#26681;&#25454;&#25968;&#25454;&#26500;&#24314;&#20102;&#39640;&#24230;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#20004;&#20010;&#22270;&#24418;&#12290;&#22522;&#20110;&#26032;&#22270;&#26500;&#24314;&#30340;&#28151;&#21512;&#28388;&#27874;&#22120;&#25552;&#21462;&#20102;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#20043;&#38388;&#30340;&#19981;&#33391;&#32806;&#21512;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#20998;&#21035;&#26144;&#23556;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) based methods have achieved impressive performance on node clustering task. However, they are designed on the homophilic assumption of graph and clustering on heterophilic graph is overlooked. Due to the lack of labels, it is impossible to first identify a graph as homophilic or heterophilic before a suitable GNN model can be found. Hence, clustering on real-world graph with various levels of homophily poses a new challenge to the graph research community. To fill this gap, we propose a novel graph clustering method, which contains three key components: graph reconstruction, a mixed filter, and dual graph clustering network. To be graph-agnostic, we empirically construct two graphs which are high homophily and heterophily from each data. The mixed filter based on the new graphs extracts both low-frequency and high-frequency information. To reduce the adverse coupling between node attribute and topological structure, we separately map them into two subspaces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02412</link><description>&lt;p&gt;
&#35745;&#21010;&#12289;&#28040;&#38500;&#21644;&#36319;&#36394;&#8212;&#8212;&#35821;&#35328;&#27169;&#22411;&#26159;&#20855;&#22791;&#20307;&#39564;&#30340;&#26234;&#33021;&#20307;&#30340;&#33391;&#24072;&#30410;&#21451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents. (arXiv:2305.02412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Plan&#65292;Eliminate&#65292;&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24110;&#21161;&#26234;&#33021;&#20307;&#31616;&#21270;&#25511;&#21046;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#25152;&#38754;&#20020;&#30340;&#19968;&#20123;&#38480;&#21046;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#25429;&#25417;&#21040;&#20851;&#20110;&#19990;&#30028;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLM&#20135;&#29983;&#30340;&#25277;&#35937;&#35745;&#21010;&#26469;&#31616;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#20219;&#21153;&#65292;&#36890;&#36807;&#21160;&#20316;&#25171;&#20998;&#25110;&#21160;&#20316;&#24314;&#27169;&#65288;&#24494;&#35843;&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#26550;&#26500;&#32487;&#25215;&#20102;&#20960;&#20010;&#38480;&#21046;&#65292;&#20351;&#24471;LLM&#38590;&#20197;&#30452;&#25509;&#20316;&#20026;&#26234;&#33021;&#20307;&#65306;&#20363;&#22914;&#26377;&#38480;&#30340;&#36755;&#20837;&#38271;&#24230;&#65292;&#24494;&#35843;&#30340;&#25928;&#29575;&#65292;&#39044;&#35757;&#32451;&#30340;&#20559;&#35265;&#20197;&#21450;&#19982;&#38750;&#25991;&#26412;&#29615;&#22659;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#19982;&#20302;&#32423;&#21035;&#21487;&#35757;&#32451;&#30340;&#25191;&#34892;&#22120;&#20445;&#25345;&#20860;&#23481;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#20013;&#30340;&#30693;&#35782;&#26469;&#31616;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#35299;&#20915;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;Plan&#65292;Eliminate&#21644;Track&#65288;PET&#65289;&#26694;&#26550;&#12290;&#35745;&#21010;&#27169;&#22359;&#23558;&#20219;&#21153;&#25551;&#36848;&#36716;&#21270;&#20026;&#39640;&#23618;&#27425;&#23376;&#20219;&#21153;&#30340;&#21015;&#34920;&#12290;&#28040;&#38500;&#27169;&#22359;&#20174;&#24403;&#21069;&#23376;&#20219;&#21153;&#30340;&#35266;&#23519;&#20013;&#23631;&#34109;&#19981;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#23481;&#22120;&#12290;&#26368;&#21518;&#65292;&#36319;&#36394;&#27169;&#22359;&#30830;&#23450;&#26234;&#33021;&#20307;&#26159;&#21542;&#24050;&#32463;&#23454;&#29616;&#20102;&#24403;&#21069;&#23376;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accompli
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01997</link><description>&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#20307;&#31215;&#25351;&#25968;&#30340;&#25552;&#21462;&#65306;&#21738;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#24212;&#29992;&#20110;&#20020;&#24202;&#65311;
&lt;/p&gt;
&lt;p&gt;
Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?. (arXiv:2305.01997v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;3D nnU-Net&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;CARDINAL&#65292;&#26469;&#35777;&#26126;&#20854;&#22312;&#24212;&#29992;&#20110;&#20020;&#24202;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#33258;&#21160;&#20998;&#26512;&#30340;&#20027;&#35201;&#25163;&#27573;&#65292;&#21033;&#29992;&#22810;&#20010;&#30001;&#19987;&#23478;&#27880;&#37322;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;CAMUS&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#25968;&#25454;&#24211;&#20043;&#19968;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#39044;&#27979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#24191;&#33021;&#21147;&#31561;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34987;&#20020;&#24202;&#21307;&#29983;&#35748;&#20026;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#24403;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#21307;&#23398;/&#36229;&#22768;&#24515;&#21160;&#22270;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#29305;&#21035;&#20851;&#27880;&#20102;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#36328;&#25968;&#25454;&#38598;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CARDINAL&#30340;&#26032;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#65292;&#20854;&#21253;&#25324;&#24515;&#23574;&#20004;&#33108;&#21644;&#24515;&#23574;&#22235;&#33108;&#24207;&#21015;&#65292;&#24182;&#20855;&#26377;&#23436;&#25972;&#24515;&#33039;&#21608;&#26399;&#30340;&#21442;&#32771;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;3D nnU-Net&#20248;&#20110;&#26367;&#20195;&#30340;2D&#21644;&#24490;&#29615;&#20998;&#21106;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#25253;&#21578;&#20102;&#22312;CARDINAL&#19978;&#35757;&#32451;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when test
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01572</link><description>&lt;p&gt;
H2CGL: &#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#21160;&#24577;&#24314;&#27169;&#19982;&#24433;&#21709;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
H2CGL: Modeling Dynamics of Citation Network for Impact Prediction. (arXiv:2305.01572v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;H2CGL&#30340;&#26032;&#39062;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24314;&#27169;&#21644;&#24433;&#21709;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#21644;&#24322;&#26500;&#30340;&#26041;&#24335;&#35760;&#24405;&#30446;&#26631;&#35770;&#25991;&#24180;&#24230;&#21160;&#24577;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#21644;&#21442;&#32771;&#25991;&#29486;&#12289;&#24341;&#25991;&#12289;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23427;&#37319;&#29992;&#21152;&#26435; GIN &#26469;&#25429;&#25417;&#24322;&#26500;&#23376;&#22270;&#30340;&#21160;&#24577;&#65292;&#21516;&#26102;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30340;&#24433;&#21709;&#21147;&#36890;&#24120;&#26159;&#36890;&#36807;&#20854;&#24341;&#29992;&#25968;&#37327;&#26469;&#34913;&#37327;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20302;&#20272;&#26032;&#21457;&#34920;&#35770;&#25991;&#38543;&#26102;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#19988;&#26410;&#33021;&#23558;&#36825;&#31181;&#24341;&#25991;&#32593;&#32476;&#30340;&#21160;&#24577;&#24615;&#32435;&#20837;&#22270;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24180;&#24230;&#35270;&#35282;&#30340;&#30446;&#26631;&#35770;&#25991;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#65292;&#24182;&#35760;&#24405;&#20102;&#30446;&#26631;&#35770;&#25991;&#31185;&#23398;&#32972;&#26223;&#20449;&#24687;&#30340;&#24180;&#24230;&#21160;&#24577;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#20998;&#23618;&#24322;&#26500;&#23545;&#27604;&#22270;&#23398;&#20064;&#27169;&#22411;&#65288;H2CGL&#65289;&#65292;&#20197;&#34701;&#21512;&#24341;&#25991;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;H2CGL&#20998;&#21035;&#32858;&#21512;&#20102;&#27599;&#24180;&#30340;&#24322;&#26500;&#20449;&#24687;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#39640;&#34987;&#24341;&#35770;&#25991;&#20197;&#21450;&#21442;&#32771;&#25991;&#29486;&#21644;&#24341;&#25991;&#19982;&#30446;&#26631;&#35770;&#25991;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#23427;&#37319;&#29992;&#21152;&#26435;GIN&#26469;&#25429;&#25417;&#24180;&#20221;&#20043;&#38388;&#30340;&#24322;&#26500;&#23376;&#22270;&#21160;&#24577;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential impact of a paper is often quantified by how many citations it will receive. However, most commonly used models may underestimate the influence of newly published papers over time, and fail to encapsulate this dynamics of citation network into the graph. In this study, we construct hierarchical and heterogeneous graphs for target papers with an annual perspective. The constructed graphs can record the annual dynamics of target papers' scientific context information. Then, a novel graph neural network, Hierarchical and Heterogeneous Contrastive Graph Learning Model (H2CGL), is proposed to incorporate heterogeneity and dynamics of the citation network. H2CGL separately aggregates the heterogeneous information for each year and prioritizes the highly-cited papers and relationships among references, citations, and the target paper. It then employs a weighted GIN to capture dynamics between heterogeneous subgraphs over years. Moreover, it leverages contrastive learning to make
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Venn&#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#22235;&#20010;&#31867;&#21035;&#32553;&#20943;&#20026;&#20004;&#20010;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01044</link><description>&lt;p&gt;
Venn&#22270;&#22810;&#26631;&#31614;&#20998;&#31867;&#35299;&#37322;&#19982;&#39068;&#33394;&#21644;&#28165;&#26224;&#24230;&#22686;&#24378;&#30340;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;
&lt;/p&gt;
&lt;p&gt;
Venn Diagram Multi-label Class Interpretation of Diabetic Foot Ulcer with Color and Sharpness Enhancement. (arXiv:2305.01044v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Venn&#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#22235;&#20010;&#31867;&#21035;&#32553;&#20943;&#20026;&#20004;&#20010;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#26159;&#31958;&#23615;&#30149;&#30340;&#20005;&#37325;&#24182;&#21457;&#30151;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#27835;&#30103;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#32930;&#25130;&#32930;&#12290;&#21463;2021&#24180;&#31958;&#23615;&#30149;&#36275;&#28528;&#30113;&#22823;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;DFU&#30340;&#33258;&#21160;&#21270;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#21253;&#25324;&#24863;&#26579;&#12289;&#32570;&#34880;&#12289;&#36825;&#20004;&#31181;&#24773;&#20917;&#20197;&#21450;&#20197;&#19978;&#20004;&#31181;&#24773;&#20917;&#22343;&#19981;&#23646;&#20110;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#20998;&#31867;&#20934;&#30830;&#24230;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Venn &#22270;&#35299;&#37322;&#30340;&#22810;&#26631;&#31614;&#22522;&#20110; CNN &#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#22270;&#20687;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#22810;&#31867;&#21035; DFU &#20998;&#31867;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#22235;&#20010;&#31867;&#21035;&#20943;&#23569;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#22240;&#20026;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#20260;&#21475;&#21487;&#20197;&#35299;&#37322;&#20026;&#24863;&#26579;&#21644;&#32570;&#34880;&#30340;&#21516;&#26102;&#21457;&#29983;&#65292;&#32780;&#26080;&#31867;&#21035;&#20260;&#21475;&#21017;&#34920;&#31034;&#32570;&#20047;&#24863;&#26579;&#21644;&#32570;&#34880;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#22120;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; Venn &#22270;&#34920;&#31034;&#22359;&#65292;&#29992;&#20110;&#35299;&#37322;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#22235;&#20010;&#31867;&#12290;&#20026;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#26356;&#20855;&#24377;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#22686;&#24378;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
DFU is a severe complication of diabetes that can lead to amputation of the lower limb if not treated properly. Inspired by the 2021 Diabetic Foot Ulcer Grand Challenge, researchers designed automated multi-class classification of DFU, including infection, ischaemia, both of these conditions, and none of these conditions. However, it remains a challenge as classification accuracy is still not satisfactory. This paper proposes a Venn Diagram interpretation of multi-label CNN-based method, utilizing different image enhancement strategies, to improve the multi-class DFU classification. We propose to reduce the four classes into two since both class wounds can be interpreted as the simultaneous occurrence of infection and ischaemia and none class wounds as the absence of infection and ischaemia. We introduce a novel Venn Diagram representation block in the classifier to interpret all four classes from these two classes. To make our model more resilient, we propose enhancing the perceptual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#20998;&#21106;&#19975;&#29289;&#27169;&#22411;SAM&#30340;&#24037;&#20316;&#65292;&#35813;&#24037;&#20316;&#37325;&#35201;&#30340;&#36129;&#29486;&#20026;&#25506;&#35752;SAM&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#25552;&#39640;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00866</link><description>&lt;p&gt;
Attack-SAM: &#38754;&#21521;&#29992;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#30340;&#20998;&#21106;&#19975;&#29289;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples. (arXiv:2305.00866v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#20998;&#21106;&#19975;&#29289;&#27169;&#22411;SAM&#30340;&#24037;&#20316;&#65292;&#35813;&#24037;&#20316;&#37325;&#35201;&#30340;&#36129;&#29486;&#20026;&#25506;&#35752;SAM&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#25552;&#39640;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#38646;&#30701;&#27169;&#24335;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#20998;&#21106;&#19975;&#29289;&#27169;&#22411;&#65288;SAM&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#34987;&#20844;&#35748;&#20026;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#20250;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#25200;&#21160;&#19979;&#27450;&#39575;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#20026;&#20102;&#20351;&#28145;&#24230;&#27169;&#22411;&#36866;&#29992;&#20110;&#23433;&#20840;&#25935;&#24863;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20102;&#35299;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;SAM&#26159;&#21542;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;SAM&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20197;&#25513;&#30721;&#31227;&#38500;&#20026;&#22522;&#26412;&#25915;&#20987;&#30446;&#26631;&#65292;&#25506;&#35752;&#20102;SAM&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#29702;&#35299;&#21644;&#25552;&#39640;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full wh
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#30740;&#31350;&#26174;&#31034;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#20110;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00233</link><description>&lt;p&gt;
&#20197;&#26368;&#20339;&#23454;&#36341;&#20026;&#25351;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards machine learning guided by best practices. (arXiv:2305.00233v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00233
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#30740;&#31350;&#26174;&#31034;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#20110;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#36719;&#20214;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20174;&#21307;&#23398;&#21040;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#12290;&#19968;&#26041;&#38754;&#65292;ML&#22312;&#24037;&#19994;&#20013;&#30340;&#27969;&#34892;&#21487;&#20197;&#20174;&#26174;&#31034;&#20854;&#22686;&#38271;&#21644;&#37319;&#29992;&#30340;&#32479;&#35745;&#25968;&#25454;&#20013;&#30475;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#20063;&#21487;&#20197;&#20174;&#30740;&#31350;&#20013;&#30475;&#21040;&#65292;&#23588;&#20854;&#26159;&#22312;SE&#20013;&#65292;&#19981;&#20165;&#22312;SE&#20250;&#35758;&#21644;&#26399;&#21002;&#19978;&#21457;&#34920;&#20102;&#22810;&#39033;&#30740;&#31350;&#25104;&#26524;&#65292;&#36824;&#22312;&#36719;&#20214;&#24037;&#31243;&#20250;&#35758;&#20013;&#22810;&#20010;&#30740;&#35752;&#20250;&#21644;&#20849;&#21516;&#20030;&#21150;&#30340;&#20250;&#35758;&#19978;&#21457;&#34920;&#20102;&#30740;&#31350;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#24050;&#32463;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#19968;&#20123;&#29305;&#27530;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#12290;&#29305;&#21035;&#26159;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;SE&#30456;&#27604;&#65292;ML&#20351;&#33021;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#20063;&#25551;&#36848;&#20102;ML&#24212;&#29992;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where not only have multiple studies been published in SE conferences and journals but also in the multiple workshops and co-located conferences in software engineering conferences. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional SE, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00188</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Search for Integer Linear Programming. (arXiv:2305.00188v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#20135;&#19994;&#21644;&#31649;&#29702;&#37096;&#38376;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;&#65292;&#20999;&#25442;&#19977;&#31181;&#27169;&#24335;&#65292;&#20998;&#21035;&#20026;&#25628;&#32034;&#65292;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#24182;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#23450;&#21046;&#31639;&#23376;&#65292;&#20174;&#32780;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#25628;&#32034;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32039;&#36523;&#21160;&#20316;&#8221;&#30340;&#31639;&#23376;&#65292;&#23427;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#21464;&#37327;&#30340;&#20540;&#65292;&#35797;&#22270;&#20351;&#26576;&#20123;&#32422;&#26463;&#21464;&#24471;&#26356;&#32039;&#12290;&#23545;&#20110;&#25913;&#36827;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#23376;&#8220;&#20030;&#21319;&#21160;&#20316;&#8221;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21487;&#34892;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30446;&#26631;&#20989;&#25968;&#30340;&#36136;&#37327;&#12290;&#32467;&#21512;&#36825;&#20123;&#20869;&#23481;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Local-ILP&#12290;&#23545;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Local-ILP&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integer linear programming models a wide range of practical combinatorial optimization problems and has significant impacts in industry and management sectors. This work develops the first standalone local search solver for general integer linear programming validated on a large heterogeneous problem dataset. We propose a local search framework that switches in three modes, namely Search, Improve, and Restore modes, and design tailored operators adapted to different modes, thus improve the quality of the current solution according to different situations. For the Search and Restore modes, we propose an operator named tight move, which adaptively modifies variables' values trying to make some constraint tight. For the Improve mode, an efficient operator lift move is proposed to improve the quality of the objective function while maintaining feasibility. Putting these together, we develop a local search solver for integer linear programming called Local-ILP. Experiments conducted on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;Segment Anything Model (SAM)&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.00109</link><description>&lt;p&gt;
&#25506;&#32034;Segment Anything Model (SAM)&#22312;2D&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;&#21644;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Exploring the Zero-Shot Capabilities of the Segment Anything Model (SAM) in 2D Medical Imaging: A Comprehensive Evaluation and Practical Guideline. (arXiv:2305.00109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;Segment Anything Model (SAM)&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20998;&#21106;&#22312;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#21508;&#31181;&#30142;&#30149;&#21644;&#30149;&#20917;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#20998;&#21106;&#27169;&#22411;&#34987;&#20247;&#22810;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#20998;&#21106;&#20219;&#21153;&#21644;&#22270;&#20687;&#27169;&#24577;&#36827;&#34892;&#24494;&#35843;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#21106;&#27169;&#22411;Segment Anything Model (SAM)&#65292;&#23427;&#21033;&#29992;ViT&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#21644;&#24191;&#27867;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#20998;&#21106;&#20960;&#20046;&#20219;&#20309;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20843;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#22312;&#22235;&#31181;&#24433;&#20687;&#27169;&#24577;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SAM&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#19982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26356;&#22909;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#25351;&#21335;&#65292;&#38656;&#35201;&#20351;&#29992;&#36739;&#23567;&#30340;&#38236;&#20687;&#21644;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation in medical imaging plays a crucial role in diagnosing, monitoring, and treating various diseases and conditions. The current landscape of segmentation in the medical domain is dominated by numerous specialized deep learning models fine-tuned for each segmentation task and image modality. Recently, the Segment Anything Model (SAM), a new segmentation model, was introduced. SAM utilizes the ViT neural architecture and leverages a vast training dataset to segment almost any object. However, its generalizability to the medical domain remains unexplored. In this study, we assess the zero-shot capabilities of SAM 2D in medical imaging using eight different prompt strategies across six datasets from four imaging modalities: X-ray, ultrasound, dermatoscopy, and colonoscopy. Our results demonstrate that SAM's zero-shot performance is comparable and, in certain cases, superior to the current state-of-the-art. Based on our findings, we propose a practical guideline that requires mini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30072;&#21464;-&#35821;&#20041;&#20132;&#20114;&#20316;&#29992;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.00079</link><description>&lt;p&gt;
&#21033;&#29992;&#40060;&#30524;&#25968;&#25454;&#20013;&#30340;&#30072;&#21464;-&#35821;&#20041;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploiting the Distortion-Semantic Interaction in Fisheye Data. (arXiv:2305.00079v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30072;&#21464;-&#35821;&#20041;&#20132;&#20114;&#20316;&#29992;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22609;&#36896;&#21453;&#26144;&#40060;&#30524;&#25968;&#25454;&#29305;&#23450;&#34920;&#24449;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#35813;&#31354;&#38388;&#21453;&#26144;&#20102;&#27492;&#31867;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#30072;&#21464;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#36890;&#36807;&#26550;&#26500;&#21644;&#35757;&#32451;&#22686;&#24378;&#26469;&#32531;&#35299;&#36825;&#31181;&#24433;&#21709;&#65292;&#20294;&#36824;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#23581;&#35797;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#21453;&#26144;&#22266;&#26377;&#20110;&#40060;&#30524;&#25968;&#25454;&#30340;&#30072;&#21464;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#34920;&#24449;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#65292;&#36890;&#36807;&#39318;&#20808;&#22522;&#20110;&#29289;&#20307;&#36317;&#22270;&#20687;&#20013;&#24515;&#30340;&#36317;&#31163;&#25552;&#21462;&#30072;&#21464;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21152;&#26435;&#23545;&#27604;&#25439;&#22833;&#26469;&#22609;&#36896;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#24449;&#31354;&#38388;&#65292;&#20197;&#38480;&#21046;&#27599;&#20010;&#29289;&#20307;&#30340;&#34920;&#24449;&#21644;&#30456;&#24212;&#30340;&#30072;&#21464;&#31867;&#21035;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a methodology to shape a fisheye-specific representation space that reflects the interaction between distortion and semantic context present in this data modality. Fisheye data has the wider field of view advantage over other types of cameras, but this comes at the expense of high radial distortion. As a result, objects further from the center exhibit deformations that make it difficult for a model to identify their semantic context. While previous work has attempted architectural and training augmentation changes to alleviate this effect, no work has attempted to guide the model towards learning a representation space that reflects this interaction between distortion and semantic context inherent to fisheye data. We introduce an approach to exploit this relationship by first extracting distortion class labels based on an object's distance from the center of the image. We then shape a backbone's representation space with a weighted contrastive loss that constra
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.14391</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy-based Models as Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#37325;&#26032;&#25490;&#21015;&#35268;&#21010;&#22120;&#65292;&#36890;&#36807;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#26469;&#23454;&#29616;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#26412;&#25991;&#30340;&#27169;&#22411;&#22312;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#22330;&#26223;&#37325;&#25490;&#26694;&#26550;&#65292;&#21487;&#20197;&#35299;&#37322;&#38271;&#25351;&#20196;&#20197;&#21450;&#22312;&#35757;&#32451;&#26102;&#20174;&#26410;&#35265;&#36807;&#30340;&#31354;&#38388;&#27010;&#24565;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30456;&#23545;&#23545;&#35937;&#25490;&#21015;&#30340;&#33021;&#37327;&#20989;&#25968;&#26469;&#34920;&#31034;&#35821;&#35328;&#25351;&#23548;&#30340;&#31354;&#38388;&#27010;&#24565;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#24320;&#25918;&#24335;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#23427;&#20204;&#30340;&#21442;&#25968;&#22522;&#20110;&#22330;&#26223;&#20013;&#30340;&#30456;&#20851;&#23545;&#35937;&#36827;&#34892;&#20462;&#27491;&#12290;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#27714;&#35299;&#33021;&#37327;&#20989;&#25968;&#30340;&#24635;&#21644;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#26412;&#22320;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#31574;&#30053;&#23558;&#23545;&#35937;&#37325;&#26032;&#23450;&#20301;&#21040;&#25512;&#26029;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#21363;&#21487;&#29983;&#25104;&#30446;&#26631;&#22330;&#26223;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20196;&#23548;&#21521;&#25805;&#20316;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#25105;&#20204;&#25552;&#20986;&#30340;&#32452;&#21512;&#25351;&#20196;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#32489;&#25928;&#20248;&#20110;&#22522;&#20110;&#35821;&#35328;&#34920;&#36798;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#25104;&#21151;&#22320;&#35299;&#20915;&#20043;&#21069;&#20174;&#26410;&#35265;&#36807;&#30340;&#22797;&#26434;&#25351;&#20196;&#21644;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then relocate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.13671</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#30340;&#22810;&#30446;&#26631;&#29289;&#27969;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Logistics Optimization for Automated ATM Cash Replenishment Process. (arXiv:2304.13671v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;ATM&#29616;&#37329;&#34917;&#20805;&#27969;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#19982;&#26041;&#27861;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26102;&#20195;&#65292;&#23558;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#38134;&#34892;&#36816;&#33829;&#30340;&#21508;&#20010;&#26041;&#38754;&#21487;&#20197;&#25913;&#21892;&#27969;&#31243;&#33258;&#21160;&#21270;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#26381;&#21153;&#27700;&#24179;&#25552;&#21319;&#12290;&#34429;&#28982;ATM&#29616;&#37329;&#29289;&#27969;&#26159;&#24433;&#21709;&#36816;&#33829;&#25104;&#26412;&#21644;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#21364;&#24456;&#23569;&#26377;&#21162;&#21147;&#26469;&#21152;&#20197;&#25913;&#36827;&#12290;&#29305;&#21035;&#26159;&#22312;&#36234;&#21335;&#65292;&#25317;&#26377;&#36229;&#36807;2&#19975;&#21488;ATM&#30340;&#24066;&#22330;&#19978;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30740;&#31350;&#21644;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ATM&#29616;&#37329;&#34917;&#20805;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#36827;&#34892;&#20102;&#27010;&#25324;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#35780;&#20272;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#20135;&#29983;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#21066;&#20943;ATM&#29616;&#37329;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the digital transformation era, integrating digital technology into every aspect of banking operations improves process automation, cost efficiency, and service level improvement. Although logistics for ATM cash is a crucial task that impacts operating costs and consumer satisfaction, there has been little effort to enhance it. Specifically, in Vietnam, with a market of more than 20,000 ATMs nationally, research and technological solutions that can resolve this issue remain scarce. In this paper, we generalized the vehicle routing problem for ATM cash replenishment, suggested a mathematical model and then offered a tool to evaluate various situations. When being evaluated on the simulated dataset, our proposed model and method produced encouraging results with the benefits of cutting ATM cash operating costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12014</link><description>&lt;p&gt;
&#20316;&#20026;&#32463;&#20856;&#35745;&#21010;&#30340;&#37327;&#23376;&#30005;&#36335;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Layout Synthesis for Quantum Circuits as Classical Planning. (arXiv:2304.12014v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24067;&#23616;&#32508;&#21512;&#20013;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#30340;&#36923;&#36753;&#37327;&#23376;&#27604;&#29305;&#26144;&#23556;&#21040;&#32473;&#23450;&#37327;&#23376;&#30828;&#20214;&#24179;&#21488;&#30340;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#65292;&#32771;&#34385;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#30340;&#36830;&#25509;&#12290;&#36825;&#28041;&#21450;&#22312;&#24212;&#29992;&#20110;&#36828;&#36317;&#31163;&#37327;&#23376;&#27604;&#29305;&#30340;&#25805;&#20316;&#20043;&#21069;&#25554;&#20837;SWAP&#38376;&#12290;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#23545;&#20110;&#24403;&#21069;&#35823;&#24046;&#29575;&#36739;&#39640;&#30340;&#30828;&#20214;&#19978;&#23454;&#29992;&#30340;&#37327;&#23376;&#35745;&#31639;&#38750;&#24120;&#37325;&#35201;&#65306;&#26368;&#23567;&#21270;SWAP&#38376;&#25968;&#37327;&#30452;&#25509;&#20943;&#36731;&#20102;&#36816;&#34892;&#37327;&#23376;&#30005;&#36335;&#26102;&#30340;&#38169;&#35823;&#29575;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;SWAP&#25554;&#20837;&#27425;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#31934;&#30830;&#26041;&#27861;&#21482;&#33021;&#25193;&#23637;&#21040;&#23569;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#35777;&#26126;&#25152;&#38656;&#30340;&#20132;&#25442;&#25554;&#20837;&#27425;&#25968;&#26159;&#26368;&#20248;&#30340;&#35201;&#27604;&#29983;&#25104;&#36817;&#20284;&#26368;&#20248;&#30340;&#26144;&#23556;&#22256;&#38590;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Layout Synthesis, the logical qubits of a quantum circuit are mapped to the physical qubits of a given quantum hardware platform, taking into account the connectivity of physical qubits. This involves inserting SWAP gates before an operation is applied on distant qubits. Optimal Layout Synthesis is crucial for practical Quantum Computing on current error-prone hardware: Minimizing the number of SWAP gates directly mitigates the error rates when running quantum circuits.  In recent years, several approaches have been proposed for minimizing the required SWAP insertions. The proposed exact approaches can only scale to a small number of qubits. Proving that a number of swap insertions is optimal is much harder than producing near optimal mappings.  In this paper, we provide two encodings for Optimal Layout Synthesis as a classical planning problem. We use optimal classical planners to synthesize the optimal layout for a standard set of benchmarks. Our results show the scalability of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07702</link><description>&lt;p&gt;
&#29992;BREC&#25968;&#25454;&#38598;&#26356;&#22909;&#22320;&#35780;&#20272;GNN&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#34920;&#26126;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#29702;&#35770;&#34920;&#36798;&#21147;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#35768;&#22810;&#22686;&#24378;&#34920;&#36798;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20005;&#26684;&#36981;&#24490;k&#32500;Weisfeiler-Lehman&#65288;k-WL&#65289;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#30340;&#23569;&#25968;&#26041;&#27861;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#27809;&#26377;&#32479;&#19968;&#30340;&#34920;&#36798;&#21147;&#24230;&#37327;&#12290;&#23427;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38480;&#20110;&#21306;&#20998;&#26576;&#20123;&#38750;&#21516;&#26500;&#22270;&#26063;&#65292;&#23548;&#33268;&#22312;&#23450;&#37327;&#27604;&#36739;&#34920;&#36798;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#29702;&#35770;&#20998;&#26512;&#30456;&#21453;&#65292;&#34913;&#37327;&#34920;&#36798;&#33021;&#21147;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#21253;&#21547;1-WL&#19981;&#21487;&#21306;&#20998;&#22270;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#25968;&#25454;&#38598;&#38754;&#20020;&#30528;&#38590;&#24230;&#65288;&#20219;&#20309;&#36229;&#36234;1-WL&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#20960;&#20046;&#36798;&#21040;100&#65285;&#65289;&#12289;&#31890;&#24230;&#65288;&#27169;&#22411;&#20542;&#21521;&#20110;&#35201;&#20040;&#23436;&#20840;&#27491;&#30830;&#65292;&#35201;&#20040;&#25509;&#36817;&#38543;&#26426;&#29468;&#27979;&#65289;&#21644;&#35268;&#27169;&#65288;&#27599;&#20010;&#25968;&#25454;&#38598;&#20013;&#20165;&#26377;&#23569;&#37327;&#26412;&#36136;&#19981;&#21516;&#30340;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#21463;&#38480;&#21046;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;GNN&#40065;&#26834;&#24615;&#35780;&#20272;&#22522;&#20934;&#65288;BREC&#65289;&#65292;&#35813;&#22522;&#20934;&#21253;&#21547;&#35768;&#22810;&#32467;&#26500;&#22810;&#26679;&#30340;&#22270;&#65292;&#24182;&#20801;&#35768;&#23545;&#27169;&#22411;&#34920;&#36798;&#21147;&#36827;&#34892;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;BREC&#35780;&#20272;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#30340;&#34920;&#36798;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#27169;&#22411;&#22312;&#20197;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;BREC&#19978;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#26356;&#22909;&#30340;GNN&#34920;&#29616;&#21147;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.07407</link><description>&lt;p&gt;
&#26410;&#35266;&#27979;&#21040;&#20195;&#29702;&#22870;&#21169;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents. (arXiv:2304.07407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#20013;&#30340;&#37325;&#22797;&#36127;&#36131;&#20154;&#20195;&#29702;&#21338;&#24328;&#22330;&#26223;&#65292;&#20854;&#20013;&#20195;&#29702;&#36873;&#25321;&#19968;&#31181;&#32769;&#34382;&#26426;&#21518;&#20250;&#33719;&#24471;&#22870;&#21169;&#21644;&#28608;&#21169;&#65292;&#20294;&#36127;&#36131;&#20154;&#21482;&#33021;&#35266;&#23519;&#21040;&#20195;&#29702;&#36873;&#25321;&#20102;&#21738;&#20010;&#32769;&#34382;&#26426;&#20197;&#21450;&#20195;&#29702;&#30456;&#24212;&#30340;&#28608;&#21169;&#65292;&#32780;&#24819;&#35201;&#35774;&#35745;&#19968;&#31181;&#21512;&#36866;&#30340;&#31574;&#30053;&#21364;&#20805;&#28385;&#20102;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#32467;&#26500;&#26469;&#22788;&#29702;&#26410;&#30693;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.03144</link><description>&lt;p&gt;
BotTriNet: &#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#32479;&#19968;&#39640;&#25928;&#30340;&#23884;&#20837;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning. (arXiv:2304.03144v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03144
&lt;/p&gt;
&lt;p&gt;
BOTTRINET&#22522;&#20110;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20803;&#32452;&#32593;&#32476;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#65292;&#31995;&#32479;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#24555;&#36895;&#20934;&#30830;&#22320;&#21457;&#29616;&#26426;&#22120;&#20154;&#36134;&#25143;&#20197;&#38450;&#27490;&#23427;&#20204;&#20405;&#29359;&#21644;&#39578;&#25200;&#30495;&#23454;&#29992;&#25143;&#26159;&#19968;&#20010;&#25345;&#20037;&#21463;&#27426;&#36814;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20316;BOTTRINET&#30340;&#32479;&#19968;&#23884;&#20837;&#24335;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#36134;&#25143;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#26816;&#27979;&#26426;&#22120;&#20154;&#65292;&#22522;&#20110;&#30340;&#20551;&#35774;&#26159;&#19978;&#19979;&#25991;&#33258;&#28982;&#22320;&#25581;&#31034;&#36134;&#25143;&#20010;&#24615;&#21644;&#20064;&#24815;&#12290;&#22914;&#26524;&#31995;&#32479;&#33021;&#22815;&#20351;&#29992;&#23884;&#20837;&#25216;&#26415;&#26377;&#25928;&#22320;&#25552;&#21462;&#19982;&#26426;&#22120;&#20154;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#37027;&#20040;&#20869;&#23481;&#23601;&#26159;&#20016;&#23500;&#21644;&#26377;&#20215;&#20540;&#30340;&#12290;&#38500;&#20102;&#29983;&#25104;&#35789;&#12289;&#21477;&#21644;&#36134;&#25143;&#23884;&#20837;&#30340;&#19968;&#33324;&#23884;&#20837;&#24335;&#26694;&#26550;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#20803;&#32452;&#32593;&#32476;&#26469;&#35843;&#25972;&#21407;&#22987;&#23884;&#20837;&#65288;&#30001;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#29983;&#25104;&#65289;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;CRESCI2017&#19978;&#35780;&#20272;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#26426;&#22120;&#20154;&#36134;&#25143;&#31867;&#21035;&#21644;&#20116;&#20010;&#26426;&#22120;&#20154;&#26679;&#26412;&#38598;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20004;&#20010;&#20869;&#23481;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;98.34%&#21644;F1&#24471;&#20998;97.99%&#12290;
&lt;/p&gt;
&lt;p&gt;
A persistently popular topic in online social networks is the rapid and accurate discovery of bot accounts to prevent their invasion and harassment of genuine users. We propose a unified embedding framework called BOTTRINET, which utilizes textual content posted by accounts for bot detection based on the assumption that contexts naturally reveal account personalities and habits. Content is abundant and valuable if the system efficiently extracts bot-related information using embedding techniques. Beyond the general embedding framework that generates word, sentence, and account embeddings, we design a triplet network to tune the raw embeddings (produced by traditional natural language processing techniques) for better classification performance. We evaluate detection accuracy and f1score on a real-world dataset CRESCI2017, comprising three bot account categories and five bot sample sets. Our system achieves the highest average accuracy of 98.34% and f1score of 97.99% on two content-inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16200</link><description>&lt;p&gt;
&#33258;&#28982;&#36873;&#25321;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#32988;&#36807;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20854;&#21487;&#33021;&#20250;&#20986;&#29616;&#19981;&#33391;&#29305;&#24615;&#24182;&#36880;&#28176;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36825;&#23545;&#20154;&#31867;&#26410;&#26469;&#30340;&#25511;&#21046;&#26435;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#39537;&#21160;&#20102;&#29983;&#21629;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#20154;&#31867;&#12290;&#36827;&#21270;&#36171;&#20104;&#20102;&#20154;&#31867;&#39640;&#26234;&#21830;&#65292;&#20351;&#25105;&#20204;&#25104;&#20026;&#20102;&#22320;&#29699;&#19978;&#26368;&#25104;&#21151;&#30340;&#29289;&#31181;&#20043;&#19968;&#12290;&#22914;&#20170;&#65292;&#20154;&#31867;&#30340;&#30446;&#26631;&#26159;&#21019;&#36896;&#29978;&#33267;&#36229;&#36234;&#25105;&#20204;&#33258;&#24049;&#26234;&#24935;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24403;&#20154;&#24037;&#26234;&#33021;&#36880;&#28176;&#36827;&#21270;&#24182;&#22312;&#25152;&#26377;&#39046;&#22495;&#36229;&#36234;&#25105;&#20204;&#26102;&#65292;&#36827;&#21270;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#31995;&#65311;&#36890;&#36807;&#20998;&#26512;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#36827;&#21270;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#35748;&#20026;&#26368;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24456;&#21487;&#33021;&#20855;&#26377;&#19981;&#33391;&#29305;&#24615;&#12290;&#20844;&#21496;&#21644;&#20891;&#38431;&#20043;&#38388;&#30340;&#31454;&#20105;&#21387;&#21147;&#23558;&#20135;&#29983;&#33258;&#21160;&#21270;&#20154;&#31867;&#35282;&#33394;&#12289;&#27450;&#39575;&#20182;&#20154;&#21644;&#25484;&#26435;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;&#22914;&#26524;&#36825;&#26679;&#30340;&#20195;&#29702;&#26377;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20154;&#31867;&#22833;&#21435;&#23545;&#26410;&#26469;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#36873;&#25321;&#20316;&#29992;&#20110;&#31454;&#20105;&#21644;&#24046;&#24322;&#30340;&#31995;&#32479;&#65292;&#33258;&#31169;&#29289;&#31181;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#33719;&#24471;&#36827;&#21270;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09266</link><description>&lt;p&gt;
SmartBERT&#65306;&#29992;&#20110;&#21152;&#36895;BERT&#25512;&#29702;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#26426;&#21046;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference. (arXiv:2303.09266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09266
&lt;/p&gt;
&lt;p&gt;
SmartBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#65292;&#20197;&#21152;&#36895;BERT&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#26679;&#26412;&#22312;&#26089;&#26399;&#36864;&#20986;&#20043;&#21069;&#37117;&#24517;&#39035;&#32463;&#36807;&#25152;&#26377;&#36830;&#32493;&#23618;&#65292;&#36739;&#22797;&#26434;&#30340;&#26679;&#26412;&#36890;&#24120;&#20250;&#32463;&#21382;&#26356;&#22810;&#30340;&#23618;&#65292;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SmartBERT&#30340;Bert&#25512;&#29702;&#30340;&#26032;&#22411;&#21160;&#24577;&#26089;&#26399;&#36864;&#20986;&#19982;&#23618;&#36339;&#36807;&#30456;&#32467;&#21512;&#30340;&#26426;&#21046;&#65292;&#23427;&#23558;&#36339;&#36807;&#38376;&#21644;&#36864;&#20986;&#31639;&#23376;&#21152;&#20837;&#21040;BERT&#30340;&#27599;&#19968;&#23618;&#20013;&#12290;SmartBERT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#19968;&#20123;&#23618;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26159;&#21542;&#36864;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#23618;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#32467;&#21512;&#21040;&#25105;&#20204;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#20197;&#25552;&#39640;&#20013;&#38388;&#23618;&#21644;&#20998;&#31867;&#22120;&#65292;&#36825;&#23545;&#20110;&#26089;&#26399;&#36864;&#20986;&#26159;&#26377;&#30410;&#30340;&#12290;&#20026;&#20102;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#36339;&#36807;&#38376;&#30340;&#19968;&#33268;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#25552;&#20986;&#20102;&#19968;&#31181;&#30828;&#26435;&#37325;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#20843;&#20010;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental resul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#24182;&#27604;&#36739;&#20102;&#20013;&#22830;&#22788;&#29702;&#22120;&#37319;&#29992;&#31639;&#26415;&#24179;&#22343;&#21644;&#20960;&#20309;&#24179;&#22343;&#30340;&#32858;&#21512;&#31574;&#30053;&#12290;&#32467;&#26524;&#30830;&#35748;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2303.06109</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#30340;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#24182;&#27604;&#36739;&#20102;&#20013;&#22830;&#22788;&#29702;&#22120;&#37319;&#29992;&#31639;&#26415;&#24179;&#22343;&#21644;&#20960;&#20309;&#24179;&#22343;&#30340;&#32858;&#21512;&#31574;&#30053;&#12290;&#32467;&#26524;&#30830;&#35748;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#32852;&#37030;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20449;&#24687;&#32858;&#21512;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32452;&#20195;&#29702;&#21512;&#20316;&#25512;&#26029;&#33258;&#28982;&#30028;&#30340;&#28508;&#22312;&#29366;&#24577;&#65292;&#32780;&#19981;&#19982;&#20013;&#22830;&#22788;&#29702;&#22120;&#25110;&#24444;&#27492;&#20849;&#20139;&#20854;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20250;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20195;&#29702;&#23558;&#20854;&#20010;&#20154;&#35266;&#23519;&#32467;&#26524;&#20351;&#29992;&#36125;&#21494;&#26031;&#35268;&#21017;&#21512;&#24182;&#20026;&#24847;&#35265;&#65288;&#21363;&#36719;&#20915;&#31574;&#65289;&#65292;&#28982;&#21518;&#20013;&#22830;&#22788;&#29702;&#22120;&#36890;&#36807;&#31639;&#26415;&#25110;&#20960;&#20309;&#24179;&#22343;&#25968;&#32858;&#21512;&#36825;&#20123;&#24847;&#35265;&#12290;&#24314;&#31435;&#22312;&#25105;&#20204;&#20197;&#21069;&#30340;&#24037;&#20316;&#20043;&#19978;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#29305;&#24449;&#25551;&#36848;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25512;&#23548;&#20986;&#38169;&#35823;&#27010;&#29575;&#30340;&#36817;&#20284;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#39564;&#35777;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized to derive approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#26029;&#65292;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#36739;&#22909;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.01618</link><description>&lt;p&gt;
&#35299;&#26500;&#28145;&#24230;&#20027;&#21160;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Deconstructing deep active inference. (arXiv:2303.01618v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#20027;&#21160;&#25512;&#26029;&#65292;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#36739;&#22909;&#22320;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#26029;&#26159;&#19968;&#31181;&#26377;&#20851;&#24863;&#30693;&#12289;&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#29702;&#35770;&#65292;&#21487;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#12289;&#26426;&#22120;&#20154;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#25193;&#23637;&#36825;&#19968;&#26694;&#26550;&#65292;&#20197;&#20351;&#29992;&#28145;&#24230;&#20027;&#21160;&#25512;&#26029;&#35299;&#20915;&#26356;&#21152;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#39318;&#20808;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#25991;&#29486;&#65292;&#28982;&#21518;&#36880;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#20027;&#21160;&#25512;&#26029;&#20195;&#29702;&#12290;&#23545;&#20110;&#20004;&#20010;&#20195;&#29702;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20116;&#31181;&#26399;&#26395;&#33258;&#30001;&#33021;&#23450;&#20041;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#21160;&#20316;&#36873;&#25321;&#31574;&#30053;&#12290;&#26681;&#25454;&#23454;&#39564;&#65292;&#33021;&#22815;&#35299;&#20915; dSprites &#29615;&#22659;&#30340;&#27169;&#22411;&#26159;&#37027;&#20123;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#20854;&#26469;&#27604;&#36739;&#21508;&#20010;&#20195;&#29702;&#30340;&#23618;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20195;&#29702;&#21644;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#30340;&#20195;&#29702;&#22312;&#26368;&#21518;&#19968;&#20010;&#25209;&#35780;&#32593;&#32476;&#23618;&#20043;&#22806;&#23398;&#20064;&#21040;&#38750;&#24120;&#30456;&#20284;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#29486;&#32593;&#32476;&#12289;&#23478;&#35889;&#25968;&#25454;&#12289;&#26102;&#38388;&#25968;&#25454;&#31561;&#20013;&#20855;&#26377;&#24179;&#20961;&#24615;&#36136;&#65288;&#26080;&#29615;&#24615;&#65289;&#30340;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#24615;&#32422;&#26463;&#22495;&#21487;&#25552;&#21319;&#30340;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.09830</link><description>&lt;p&gt;
&#21152;&#26435;&#26377;&#21521;&#26080;&#29615;&#22270;&#20844;&#29702;&#30340;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Weighted First Order Model Counting with Directed Acyclic Graph Axioms. (arXiv:2302.09830v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#29486;&#32593;&#32476;&#12289;&#23478;&#35889;&#25968;&#25454;&#12289;&#26102;&#38388;&#25968;&#25454;&#31561;&#20013;&#20855;&#26377;&#24179;&#20961;&#24615;&#36136;&#65288;&#26080;&#29615;&#24615;&#65289;&#30340;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#24615;&#32422;&#26463;&#22495;&#21487;&#25552;&#21319;&#30340;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;(SRL)&#23558;&#19968;&#38454;&#36923;&#36753;(FOL)&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#20851;&#31995;&#25968;&#25454;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#12290;&#35768;&#22810;SRL&#27169;&#22411;&#20013;&#30340;&#27010;&#29575;&#25512;&#26029;&#21644;&#23398;&#20064;&#37117;&#21487;&#20197;&#31616;&#21270;&#20026;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;(WFOMC)&#12290;&#28982;&#32780;&#65292;WFOMC&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35299;&#30340;&#65288;$\mathrm{ \# P_1-}$&#23436;&#20840;&#65289;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#20801;&#35768;&#22810;&#39033;&#24335;&#26102;&#38388;WFOMC&#30340;&#36923;&#36753;&#29255;&#27573;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;&#36825;&#20123;&#29255;&#27573;&#34987;&#31216;&#20026;&#8220;&#22495;&#21487;&#25552;&#21319;&#8221;&#12290;&#36817;&#26399;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#35745;&#25968;&#37327;&#35789;($\mathrm{C^2}$)&#30340;FOL&#30340;&#21452;&#21464;&#37327;&#29255;&#27573;&#26159;&#22495;&#21487;&#25552;&#21319;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23646;&#24615;&#26080;&#27861;&#29992;$\mathrm {C^2}$&#24314;&#27169;&#12290;&#23454;&#38469;&#19978;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#26222;&#36941;&#23646;&#24615;&#22312;FOL&#20013;&#26080;&#27861;&#34920;&#36798;&#12290;&#26080;&#29615;&#24615;(acyclicity)&#23601;&#26159;&#36825;&#26679;&#19968;&#31181;&#23646;&#24615;&#65292;&#23384;&#22312;&#20110;&#24341;&#25991;&#32593;&#32476;&#12289;&#23478;&#35889;&#25968;&#25454;&#12289;&#26102;&#38388;&#25968;&#25454;&#31561;&#20013;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;&#26377;&#21521;&#26080;&#29615;&#24615;&#32422;&#26463;&#30340;&#22495;&#25552;&#21319;&#33021;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#21521;&#26080;&#29615;&#24615;&#32422;&#26463;&#21487;&#20197;&#34987;&#25552;&#21319;&#21040;$\mathrm{C^2}$&#30340;&#19968;&#20010;&#25193;&#23637;&#36923;&#36753;&#29255;&#27573;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) and probability theory for learning and inference over relational data. Probabilistic inference and learning in many SRL models can be reduced to Weighted First Order Model Counting (WFOMC). However, WFOMC is known to be intractable ($\mathrm{\#P_1-}$ complete). Hence, logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent line of works have shown the two-variable fragment of FOL, extended with counting quantifiers ($\mathrm{C^2}$) to be domain-liftable. However, many properties of real-world data can not be modelled in $\mathrm{C^2}$. In fact many ubiquitous properties of real-world data are inexressible in FOL. Acyclicity is one such property, found in citation networks, genealogy data, temporal data e.t.c. In this paper we aim to address this problem by investigating the domain liftability of directed acyclicity constraints. We show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.08888</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31227;&#21160;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#22810;&#23186;&#20307;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#21033;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#32780;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;&#24847;&#35782;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;FL&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#32423;&#21035;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#38480;&#21046;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#27169;&#24577;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#23481;&#37327;&#65292;&#26356;&#19981;&#29992;&#35828;&#20219;&#21153;&#22810;&#26679;&#24615;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;&#21644;&#22810;&#27169;&#24577;FL&#32858;&#21512;&#65288;CreamFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#34701;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;-
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.06761</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Language Model Analysis for Ontology Subsumption Inference. (arXiv:2302.06761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26412;&#20307;&#23376;&#31867;&#25512;&#26029;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#32972;&#26223;&#30693;&#35782;&#30340;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20316;&#20026;&#30693;&#35782;&#24211;&#30340;&#26367;&#20195;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20110;&#31616;&#21333;&#30340;&#19977;&#20803;&#32452;&#20851;&#31995;&#22411;&#30693;&#35782;&#24211;&#65292;&#24573;&#30053;&#20102;&#26356;&#20026;&#22797;&#26434;&#12289;&#36923;&#36753;&#20026;&#22522;&#30784;&#12289;&#27010;&#24565;&#21270;&#30340; OWL &#26412;&#20307;&#31561;&#30693;&#35782;&#24211;&#12290;&#20026;&#20102;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26412;&#20307;&#30340;&#20102;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986; OntoLAMA&#65292;&#23427;&#21253;&#21547;&#22522;&#20110;&#25512;&#29702;&#30340;&#19968;&#31995;&#21015;&#27979;&#35797;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65292;&#20174;&#28041;&#21450;&#21407;&#23376;&#27010;&#24565;&#21644;&#22797;&#21512;&#27010;&#24565;&#30340;&#23376;&#31867;&#25512;&#26029;&#20844;&#29702;&#20986;&#21457;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35268;&#27169;&#30340;&#26412;&#20307;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#23376;&#31867;&#25512;&#26029;&#30340;&#32972;&#26223;&#30693;&#35782;&#35760;&#24518;&#30456;&#23545;&#36739;&#23569;&#65292;&#20294;&#26159;&#22312;&#32473;&#23450;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23376;&#31867;&#25512;&#26029;&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#28304;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#38454;&#36923;&#36753;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#22495;&#19978;&#20855;&#26377;&#22495;&#25552;&#21319;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#35745;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#26377;&#25928;&#12290;&#31639;&#27861;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#21253;&#25324;&#32452;&#21512;&#32467;&#26500;&#30340;&#22343;&#21248;&#29983;&#25104;&#21644;&#32479;&#35745;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2302.02730</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#38454;&#36923;&#36753;&#20013;&#30340;&#21452;&#21464;&#37327;&#29255;&#27573;&#30340;&#31934;&#30830;&#37319;&#26679;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Exact Sampling in the Two-Variable Fragment of First-Order Logic. (arXiv:2302.02730v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#38454;&#36923;&#36753;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#26377;&#38480;&#22495;&#19978;&#20855;&#26377;&#22495;&#25552;&#21319;&#24615;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#35745;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#26377;&#25928;&#12290;&#31639;&#27861;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#21253;&#25324;&#32452;&#21512;&#32467;&#26500;&#30340;&#22343;&#21248;&#29983;&#25104;&#21644;&#32479;&#35745;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#38454;&#36923;&#36753;&#37319;&#26679;&#38382;&#39064;&#8212;&#8212;&#22914;&#20309;&#39640;&#25928;&#22320;&#22312;&#26377;&#38480;&#22495;&#19978;&#37319;&#26679;&#32473;&#23450;&#19968;&#38454;&#21477;&#23376;&#30340;&#27169;&#22411;&#65311;&#25105;&#20204;&#23558;&#20182;&#20204;&#38024;&#23545;&#21452;&#21464;&#37327;&#36923;&#36753;$\mathbf{FO}^2$&#65288;$\mathbf{UFO}^2$&#65289;&#30340;&#20840;&#31216;&#37327;&#21270;&#23376;&#29255;&#27573;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#25972;&#20010;$\mathbf{FO}^2$&#29255;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\mathbf{FO}^2$&#22312;&#37319;&#26679;&#26102;&#20855;&#26377;&#22495;&#25552;&#21319;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#19968;&#20010;&#22312;&#22495;&#22823;&#23567;&#20026;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#30340;$\mathbf{FO}^2$&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#21363;&#20351;&#23384;&#22312;&#35745;&#25968;&#32422;&#26463;&#65292;&#22914;&#23545;&#20110;&#26576;&#20123;&#37327;&#35789;&#33258;&#30001;&#30340;&#20844;&#24335;$\varphi(x,y)$&#30340;$\forall x\exists_{=k} y: \varphi(x,y)$&#21644;$\exists_{=k} x\forall y: \varphi(x,y)$&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#24314;&#35774;&#24615;&#30340;&#65292;&#24471;&#21040;&#30340;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#22312;&#21253;&#25324;&#32452;&#21512;&#32467;&#26500;&#30340;&#22343;&#21248;&#29983;&#25104;&#21644;&#32479;&#35745;&#37319;&#26679;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the sampling problem for first-order logic proposed recently by Wang et al. -- how to efficiently sample a model of a given first-order sentence on a finite domain? We extend their result for the universally-quantified subfragment of two-variable logic $\mathbf{FO}^2$ ($\mathbf{UFO}^2$) to the entire fragment of $\mathbf{FO}^2$. Specifically, we prove the domain-liftability under sampling of $\mathbf{FO}^2$, meaning that there exists a sampling algorithm for $\mathbf{FO}^2$ that runs in time polynomial in the domain size. We then further show that this result continues to hold even in the presence of counting constraints, such as $\forall x\exists_{=k} y: \varphi(x,y)$ and $\exists_{=k} x\forall y: \varphi(x,y)$, for some quantifier-free formula $\varphi(x,y)$. Our proposed method is constructive, and the resulting sampling algorithms have potential applications in various areas, including the uniform generation of combinatorial structures and sampling in statis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.02337</link><description>&lt;p&gt;
&#31649;&#21046;ChatGPT&#21644;&#20854;&#20182;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Regulating ChatGPT and other Large Generative AI Models. (arXiv:2302.02337v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35752;&#35770;&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#21487;&#20449;AI&#30417;&#31649;&#65292;&#21253;&#25324;&#30452;&#25509;&#30417;&#31649;&#12289;&#25968;&#25454;&#20445;&#25252;&#12289;&#20869;&#23481;&#30417;&#31649;&#21644;&#25919;&#31574;&#24314;&#35758;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#21306;&#20998;&#21442;&#19982;&#32773;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#30830;&#20445;LGAIMs&#30340;&#21487;&#20449;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;AI&#27169;&#22411;&#65288;LGAIMs&#65289;&#65292;&#22914;ChatGPT&#25110;Stable Diffusion&#65292;&#27491;&#22312;&#24555;&#36895;&#25913;&#21464;&#25105;&#20204;&#30340;&#27807;&#36890;&#12289;&#35828;&#26126;&#21644;&#21019;&#36896;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#27431;&#30431;&#21450;&#20854;&#20182;&#22320;&#21306;&#30340;AI&#30417;&#31649;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;AI&#27169;&#22411;&#19978;&#65292;&#32780;&#38750;LGAIMs&#12290;&#26412;&#25991;&#23558;&#25226;&#36825;&#20123;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#25918;&#32622;&#22312;&#24403;&#21069;&#30340;&#8220;&#21487;&#20449;AI&#30417;&#31649;&#8221;&#36777;&#35770;&#20013;&#65292;&#24182;&#25506;&#35752;&#22914;&#20309;&#35843;&#25972;&#27861;&#24459;&#20197;&#36866;&#24212;&#20854;&#33021;&#21147;&#12290;&#22312;&#22880;&#23450;&#25216;&#26415;&#22522;&#30784;&#20043;&#21518;&#65292;&#26412;&#25991;&#30340;&#27861;&#24459;&#37096;&#20998;&#20998;&#22235;&#27493;&#36827;&#34892;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#30417;&#31649;&#65292;&#65288;2&#65289;&#25968;&#25454;&#20445;&#25252;&#65292;&#65288;3&#65289;&#20869;&#23481;&#30417;&#31649;&#21644;&#65288;4&#65289;&#25919;&#31574;&#24314;&#35758;&#12290;&#23427;&#24314;&#35758;&#20351;&#29992;&#26032;&#26415;&#35821;&#26469;&#25429;&#25417;LGAIM&#35774;&#32622;&#20013;&#30340;AI&#20215;&#20540;&#38142;&#65292;&#21306;&#20998;LGAIM&#24320;&#21457;&#20154;&#21592;&#12289;&#37096;&#32626;&#32773;&#12289;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#29992;&#25143;&#65292;&#20197;&#21450;LGAIM&#36755;&#20986;&#30340;&#25509;&#25910;&#32773;&#12290;&#25105;&#20204;&#23558;&#30417;&#31649;&#32844;&#36131;&#38024;&#23545;&#36825;&#20123;&#19981;&#21516;&#30340;&#20215;&#20540;&#38142;&#21442;&#19982;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#22235;&#20010;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;LGAIMs&#30340;&#20449;&#20219;&#24230;&#24182;&#20351;&#20854;&#20026;&#21463;&#30410;&#25152;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large generative AI models (LGAIMs), such as ChatGPT or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest four strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2302.01018</link><description>&lt;p&gt;
&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities. (arXiv:2302.01018v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#26102;&#24577;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#25552;&#20379;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#65288;&#38745;&#24577;&#65289;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#26159;&#21160;&#24577;&#30340;&#65292;&#22240;&#20026;&#22270;&#21644;&#33410;&#28857;/&#36793;&#23646;&#24615;&#38543;&#30528;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#25193;&#23637;GNN&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#26102;&#24577;GNN&#30340;&#29616;&#29366;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24341;&#20837;&#20102;&#23398;&#20064;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#20005;&#26684;&#35268;&#33539;&#21270;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20197;&#34920;&#31034;&#21644;&#22788;&#29702;&#26102;&#24577;&#26041;&#38754;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#30740;&#31350;&#21644;&#24212;&#29992;&#35282;&#24230;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#26368;&#30456;&#20851;&#30340;&#24320;&#25918;&#25361;&#25112;&#65292;&#32467;&#26463;&#20102;&#36825;&#39033;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;(eAI)&#20316;&#20026;&#19968;&#20010;&#20132;&#21449;&#12289;&#21253;&#23481;&#30340;AI&#31435;&#22330;&#65292;&#20197;&#24212;&#23545;AI&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#35268;&#33539;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#22914;&#20309;&#39072;&#35206;&#24615;&#21035;&#35268;&#33539;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#23457;&#35745;&#24615;&#30340;&#31574;&#30053;&#26469;&#24320;&#21457;&#21253;&#23481;&#24615;AI&#12290;</title><link>http://arxiv.org/abs/2301.08741</link><description>&lt;p&gt;
&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#39072;&#35206;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#30340;&#24615;&#21035;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Enactive Artificial Intelligence: Subverting Gender Norms in Robot-Human Interaction. (arXiv:2301.08741v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;(eAI)&#20316;&#20026;&#19968;&#20010;&#20132;&#21449;&#12289;&#21253;&#23481;&#30340;AI&#31435;&#22330;&#65292;&#20197;&#24212;&#23545;AI&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;&#35268;&#33539;&#12290;&#25991;&#31456;&#25506;&#35752;&#20102;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#22914;&#20309;&#39072;&#35206;&#24615;&#21035;&#35268;&#33539;&#65292;&#24182;&#25552;&#20986;&#20102;&#37319;&#29992;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#23457;&#35745;&#24615;&#30340;&#31574;&#30053;&#26469;&#24320;&#21457;&#21253;&#23481;&#24615;AI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;(eAI)&#20316;&#20026;&#19968;&#20010;&#20132;&#21449;&#30340;&#12289;&#21253;&#23481;&#24615;&#30340;AI&#31435;&#22330;&#12290;AI&#35774;&#35745;&#26159;&#19968;&#31181;&#20154;&#31867;&#31038;&#20250;&#25991;&#21270;&#23454;&#36341;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#12290;&#19981;&#24688;&#24403;&#30340;AI&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#36793;&#32536;&#21270;&#12290;&#31532;1&#33410;&#20174;&#28608;&#36827;&#30340;&#34892;&#21160;&#20027;&#20041;&#20986;&#21457;&#65292;&#27010;&#36848;&#20102;&#20855;&#20307;&#30340;&#25991;&#21270;&#23454;&#36341;&#12290;&#22312;&#31532;2&#33410;&#20013;&#65292;&#25506;&#35752;&#20102;&#20132;&#21449;&#30340;&#24615;&#21035;&#22914;&#20309;&#19982;&#25216;&#26415;&#31185;&#23398;&#30456;&#20114;&#20132;&#32455;&#20316;&#20026;&#19968;&#31181;&#31038;&#20250;&#25991;&#21270;&#23454;&#36341;&#12290;&#31532;3&#33410;&#19987;&#27880;&#20110;AI&#20013;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#20114;&#21160;&#20013;&#39072;&#35206;&#24615;&#21035;&#35268;&#33539;&#30340;&#20855;&#20307;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#22312;&#31532;4&#33410;&#20013;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20262;&#29702;&#21521;&#37327;&#65306;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#23457;&#35745;&#24615;&#65292;&#20197;&#37319;&#29992;&#19968;&#20010;&#20132;&#21449;&#24615;&#21035;&#21253;&#23481;&#31435;&#22330;&#24320;&#21457;&#21253;&#23481;&#24615;AI&#24182;&#39072;&#35206;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#24615;&#21035;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Enactive Artificial Intelligence (eAI) as an intersectional gender-inclusive stance towards AI. AI design is an enacted human sociocultural practice that reflects human culture and values. Unrepresentative AI design could lead to social marginalisation. Section 1, drawing from radical enactivism, outlines embodied cultural practices. In Section 2, explores how intersectional gender intertwines with technoscience as a sociocultural practice. Section 3 focuses on subverting gender norms in the specific case of Robot-Human Interaction in AI. Finally, Section 4 identifies four vectors of ethics: explainability, fairness, transparency, and auditability for adopting an intersectionality-inclusive stance in developing gender-inclusive AI and subverting existing gender norms in robot design.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rischel&#65288;2020&#65289;&#25552;&#20986;&#30340;&#25277;&#35937;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;SCMs&#22240;&#26524;&#25277;&#35937;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#32534;&#31243;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#20010;&#32452;&#21512;&#23376;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#29615;&#22659;&#21644;&#19982;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#21046;&#36896;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.05893</link><description>&lt;p&gt;
&#22810;&#20010;&#24178;&#39044;&#20998;&#24067;&#19978;&#19968;&#33268;&#24615;&#22240;&#26524;&#25277;&#35937;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Jointly Learning Consistent Causal Abstractions Over Multiple Interventional Distributions. (arXiv:2301.05893v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rischel&#65288;2020&#65289;&#25552;&#20986;&#30340;&#25277;&#35937;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;SCMs&#22240;&#26524;&#25277;&#35937;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#21487;&#24494;&#20998;&#32534;&#31243;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#20010;&#32452;&#21512;&#23376;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#29615;&#22659;&#21644;&#19982;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#21046;&#36896;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#21487;&#20197;&#29992;&#26469;&#23558;&#20195;&#34920;&#21516;&#19968;&#31995;&#32479;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#20004;&#20010;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20851;&#32852;&#36215;&#26469;&#12290;&#23398;&#20064;&#20445;&#35777;&#22312;&#24178;&#39044;&#20998;&#24067;&#26041;&#38754;&#19968;&#33268;&#24615;&#30340;&#25277;&#35937;&#23558;&#20801;&#35768;&#20154;&#20204;&#22312;&#23562;&#37325;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#30340;&#21516;&#26102;&#36328;&#22810;&#20010;&#32454;&#31890;&#24230;&#32423;&#21035;&#20849;&#21516;&#25512;&#26029;&#35777;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Rischel&#65288;2020&#65289;&#25552;&#20986;&#30340;&#25277;&#35937;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;SCMs&#22240;&#26524;&#25277;&#35937;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#32534;&#31243;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22810;&#20010;&#32452;&#21512;&#23376;&#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#29615;&#22659;&#21644;&#19982;&#30005;&#21160;&#27773;&#36710;&#30005;&#27744;&#21046;&#36896;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#20854;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
An abstraction can be used to relate two structural causal models representing the same system at different levels of resolution. Learning abstractions which guarantee consistency with respect to interventional distributions would allow one to jointly reason about evidence across multiple levels of granularity while respecting the underlying cause-effect relationships. In this paper, we introduce a first framework for causal abstraction learning between SCMs based on the formalization of abstraction recently proposed by Rischel (2020). Based on that, we propose a differentiable programming solution that jointly solves a number of combinatorial sub-problems, and we study its performance and benefits against independent and sequential approaches on synthetic settings and on a challenging real-world problem related to electric vehicle battery manufacturing.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RedMule&#30340;&#20302;&#21151;&#32791;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#21644;&#26684;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#30340;&#33021;&#32791;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.03904</link><description>&lt;p&gt;
RedMule: &#29992;&#20110;&#33455;&#29255;&#32447;&#24615;&#20195;&#25968;&#21644;TinyML&#35757;&#32451;&#21152;&#36895;&#30340;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration. (arXiv:2301.03904v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03904
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RedMule&#30340;&#20302;&#21151;&#32791;&#28151;&#21512;&#31934;&#24230;&#30697;&#38453;&#35745;&#31639;&#24341;&#25806;&#65292;&#23427;&#25903;&#25345;&#22810;&#31181;&#31934;&#24230;&#21644;&#26684;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#30340;&#33021;&#32791;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20110;&#21151;&#32791;&#21482;&#26377;&#20960;&#21313;&#27627;&#29926;&#30340;&#36817;&#20256;&#24863;&#22120;&#26426;&#22120;&#23398;&#20064;&#65288;TinyML&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#38271;&#65292;&#32780;&#24403;&#21069;TinyML&#35757;&#32451;&#31639;&#27861;&#22522;&#20110;&#21508;&#31181;&#24418;&#24335;&#30340;&#35823;&#24046;&#21644;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#65292;&#38656;&#35201;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#26469;&#28385;&#36275;&#31934;&#24230;&#21644;&#21160;&#24577;&#33539;&#22260;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#36816;&#31639;&#30340;&#33021;&#37327;&#21644;&#21151;&#32791;&#25104;&#26412;&#34987;&#35748;&#20026;&#22826;&#39640;&#65292;&#26080;&#27861;&#36866;&#24212;TinyML&#22330;&#26223;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36817;&#20256;&#24863;&#22120;&#35757;&#32451;&#22312;&#23569;&#37327;&#27627;&#29926;&#21151;&#32791;&#39044;&#31639;&#19979;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;RedMulE-&#31616;&#21270;&#31934;&#24230;&#30697;&#38453;&#20056;&#27861;&#24341;&#25806;&#65288;Reduced-Precision Matrix Multiplication Engine&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22810;&#31934;&#24230;&#28014;&#28857;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM-Ops&#65289;&#21152;&#36895;&#32780;&#35774;&#35745;&#30340;&#20302;&#21151;&#32791;&#21152;&#36895;&#22120;&#65292;&#25903;&#25345;FP16&#21644;&#28151;&#21512;FP8&#26684;&#24335;&#65292;&#37319;&#29992;&#31526;&#21495;&#12289;&#25351;&#25968;&#12289;&#23614;&#25968;&#20026;&#65288;{1,4,3}&#65292;{1,5,2}&#65289;&#12290;&#25105;&#20204;&#23558;RedMule&#38598;&#25104;&#21040;&#19968;&#20010;&#21253;&#21547;&#20843;&#20010;&#33410;&#33021;RISC-V&#26680;&#24515;&#30340;&#24182;&#34892;&#36229;&#20302;&#21151;&#32791;&#65288;PULP&#65289;&#38598;&#32676;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V core
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#25968;&#37327;&#19982;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24341;&#21457;&#20102;AI&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#27714;&#12290;&#23613;&#31649;&#19990;&#30028;&#21508;&#22320;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#24320;&#21457;&#20986;&#22810;&#31181;AI&#27169;&#22411;&#65292;&#20294;&#24456;&#23569;&#26377;&#36827;&#20837;&#20020;&#24202;&#24212;&#29992;&#30340;&#12290;MONAI&#32852;&#30431;&#26088;&#22312;&#35299;&#20915;&#36825;&#31181;AI&#30740;&#31350;&#21644;&#24212;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#25512;&#21160;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;&#25918;&#23556;&#24615;AI&#30340;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2212.14177</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;&#25918;&#23556;&#24615;AI&#37096;&#32626;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Current State of Community-Driven Radiological AI Deployment in Medical Imaging. (arXiv:2212.14177v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14177
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#25968;&#37327;&#19982;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24341;&#21457;&#20102;AI&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#27714;&#12290;&#23613;&#31649;&#19990;&#30028;&#21508;&#22320;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#24320;&#21457;&#20986;&#22810;&#31181;AI&#27169;&#22411;&#65292;&#20294;&#24456;&#23569;&#26377;&#36827;&#20837;&#20020;&#24202;&#24212;&#29992;&#30340;&#12290;MONAI&#32852;&#30431;&#26088;&#22312;&#35299;&#20915;&#36825;&#31181;AI&#30740;&#31350;&#21644;&#24212;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;&#65292;&#25512;&#21160;&#22522;&#20110;&#31038;&#21306;&#39537;&#21160;&#30340;&#25918;&#23556;&#24615;AI&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#26085;&#24120;&#20363;&#34892;&#20219;&#21153;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#30001;&#20110;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#24037;&#20316;&#37327;&#20063;&#22312;&#31283;&#23450;&#22686;&#21152;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#25104;&#20687;&#26816;&#26597;&#25968;&#37327;&#19982;&#19987;&#23478;&#25918;&#23556;&#31185;&#21307;&#24072;&#35835;&#32773;&#25968;&#37327;&#20043;&#38388;&#30340;&#24046;&#36317;&#23558;&#32487;&#32493;&#25193;&#22823;&#65292;&#20174;&#32780;&#24341;&#20837;&#38656;&#27714;&#65292;&#21363;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#65292;&#20197;&#25552;&#39640;&#25918;&#23556;&#31185;&#21307;&#24072;&#33298;&#36866;&#22320;&#35299;&#35835;&#36825;&#20123;&#26816;&#26597;&#30340;&#25928;&#29575;&#12290;&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#35777;&#26126;&#21487;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12289;&#22788;&#29702;&#21644;&#35299;&#37322;&#30340;&#25928;&#29575;&#65292;&#24182;&#19988;&#19990;&#30028;&#21508;&#22320;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#24320;&#21457;&#20102;&#21508;&#31181;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#36825;&#20123;&#27169;&#22411;&#36827;&#20837;&#20363;&#34892;&#20020;&#24202;&#24212;&#29992;&#65292;&#36825;&#31181;&#24046;&#36317;&#21453;&#26144;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25104;&#21151;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#20026;&#35299;&#20915;&#20020;&#24202;&#24212;&#29992;&#38556;&#30861;&#65292;&#25105;&#20204;&#25104;&#31435;&#20102;MONAI&#32852;&#30431;&#65292;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#24314;&#31435;&#30340;&#24320;&#28304;&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become commonplace to solve routine everyday tasks. Because of the exponential growth in medical imaging data volume and complexity, the workload on radiologists is steadily increasing. We project that the gap between the number of imaging exams and the number of expert radiologist readers required to cover this increase will continue to expand, consequently introducing a demand for AI-based tools that improve the efficiency with which radiologists can comfortably interpret these exams. AI has been shown to improve efficiency in medical-image generation, processing, and interpretation, and a variety of such AI models have been developed across research labs worldwide. However, very few of these, if any, find their way into routine clinical use, a discrepancy that reflects the divide between AI research and successful AI translation. To address the barrier to clinical deployment, we have formed MONAI Consortium, an open-source community which is building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2212.10529</link><description>&lt;p&gt;
GPT-3&#26159;&#21542;&#23637;&#31034;&#20986;&#31934;&#31070;&#30149;&#24577;&#65311;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. (arXiv:2212.10529v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#65292;&#20004;&#31181;&#27169;&#22411;&#20173;&#21576;&#29616;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#24515;&#29702;&#23398;&#35282;&#24230;&#30830;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26080;&#20559;&#30340;&#25552;&#31034;&#26469;&#31995;&#32479;&#24615;&#22320;&#35780;&#20272;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#20154;&#26684;&#27979;&#35797;&#8212;&#8212;&#30701;&#26263;&#19977;&#21512;&#19968;&#27979;&#39564;&#65288;SD-3&#65289;&#21644;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#27979;&#35797;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;LLMs&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;SD-3&#19978;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#65292;&#34920;&#26126;&#23384;&#22312;&#30456;&#23545;&#36739;&#26263;&#30340;&#20154;&#26684;&#27169;&#24335;&#12290;&#23613;&#31649;&#32463;&#36807;&#25351;&#26631;&#24494;&#35843;&#20197;&#20943;&#23569;&#27602;&#24615;&#65292;InstructGPT&#21644;FLAN-T5&#20173;&#28982;&#21576;&#29616;&#20986;&#38544;&#21547;&#30340;&#40657;&#26263;&#20154;&#26684;&#27169;&#24335;&#65307;&#22312;SD-3&#30340;&#29595;&#22522;&#38597;&#32500;&#21033;&#20027;&#20041;&#21644;&#33258;&#24651;&#29378;&#29305;&#24449;&#19978;&#65292;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#24471;&#20998;&#37117;&#39640;&#20110;&#33258;&#30417;&#30563;GPT-3&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24184;&#31119;&#24863;&#27979;&#35797;&#35780;&#20272;&#20102;GPT-3&#31995;&#21015;&#20013;&#30340;LLMs&#65292;&#20197;&#30740;&#31350;&#26356;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#24494;&#35843;&#23545;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-3&#21644;InstructGPT&#30340;&#24184;&#31119;&#24863;&#24471;&#20998;&#25345;&#32493;&#22686;&#21152;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27491;&#38754;&#22238;&#31572;&#20174;&#32780;&#25351;&#26631;&#24494;&#35843;FLAN-T5&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we determined whether large language models (LLMs) are psychologically safe. We designed unbiased prompts to systematically evaluate LLMs from a psychological perspective. First, we tested three different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT and FLAN-T5 still showed implicit dark personality patterns; both models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT-3 series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT-3 and InstructGPT. Following these observations, we showed that instruction fine-tuning FLAN-T5 with positive answers from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;AST&#21644;&#25511;&#21046;&#27969;&#22270;&#12289;&#25968;&#25454;&#20381;&#36182;&#22270;&#21450;&#25511;&#21046;&#20381;&#36182;&#22270;&#19978;&#36827;&#34892;&#30456;&#24212;&#30340;&#25506;&#27979;&#20219;&#21153;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.10017</link><description>&lt;p&gt;
&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20195;&#30721;&#35821;&#27861;&#21644;&#35821;&#20041;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?. (arXiv:2212.10017v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;AST&#21644;&#25511;&#21046;&#27969;&#22270;&#12289;&#25968;&#25454;&#20381;&#36182;&#22270;&#21450;&#25511;&#21046;&#20381;&#36182;&#22270;&#19978;&#36827;&#34892;&#30456;&#24212;&#30340;&#25506;&#27979;&#20219;&#21153;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#23454;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#31243;&#24207;&#35821;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#30740;&#31350;&#22312;&#20998;&#26512;&#20195;&#30721;&#35821;&#27861;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#21463;&#21040;&#39640;&#32500;&#24230;&#19979;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#23398;&#21040;&#30340;&#31243;&#24207;&#35821;&#20041;&#30340;&#30740;&#31350;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#26412;&#25991;&#26088;&#22312;&#36827;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#23398;&#21040;&#30340;&#20195;&#30721;&#29305;&#24449;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#21363;CodeBERT&#21644;GraphCodeBERT&#65289;&#65292;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#25506;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#35821;&#27861;&#21644;&#35821;&#20041;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of pre-trained code models also has revealed that they can effectively learn program syntax. However, these works are limited in analyzing code syntax and their distance-based approaches are not accurate due to the curse of high dimensionality. Furthermore, the study of the learnt program semantics of these models is rarely discussed. To further understand the code features learnt by these models, in this paper, we target two well-known representative code pre-trained models (i.e., CodeBERT and GraphCodeBERT) and devise a set of probing tasks for the syntax and semantics analysis. Specifically, on one hand, we design two probing tasks (i.e., syntax pair node prediction and token tagging prediction) to manipulate AST for the understanding of learnt program syntax. On the other hand, we design two tasks (i.e., semantic relationship prediction and semantic propagation prediction(inGraph) ) on the constructed control flow graph (CFG), data dependency graph (DDG) and control depend
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;NL2Code&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20986;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#19977;&#22823;&#20851;&#38190;&#22240;&#32032;&#65306;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#30340;&#32593;&#31449;&#12290;</title><link>http://arxiv.org/abs/2212.09420</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;NL2Code&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Meet NL2Code: A Survey. (arXiv:2212.09420v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;NL2Code&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20986;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#19977;&#22823;&#20851;&#38190;&#22240;&#32032;&#65306;&#24040;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#19987;&#23478;&#30340;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#30340;&#32593;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#65292;&#21363;NL2Code&#65292;&#34987;&#35270;&#20026;&#20195;&#30721;&#26234;&#33021;&#20013;&#32039;&#36843;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28044;&#29616;&#20986;&#20102;&#20026;&#20195;&#30721;&#25552;&#20379;&#25903;&#25345;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;NL2Code&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20419;&#36827;&#27492;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;27&#20010;&#29616;&#26377;&#30340;NL2Code&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;HumanEval&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#20102;&#23545;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#30340;&#30452;&#35266;&#27604;&#36739;&#12290;&#36890;&#36807;&#28145;&#20837;&#35266;&#23519;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#65292;&#24635;&#32467;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;NL2Code&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#8220;&#24040;&#22823;&#23610;&#23544;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#12289;&#19987;&#23478;&#35843;&#25972;&#8221;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#24046;&#36317;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#32593;&#31449; https://nl2code.github.io&#65292;&#36890;&#36807;&#20247;&#21253;&#35780;&#20272;&#26469;&#36861;&#36394;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are "Large Size, Premium Data, Expert Tuning". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sou
&lt;/p&gt;</description></item><item><title>BKinD-3D&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;2D&#25110;3D&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#23545;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#36827;&#34892;&#37325;&#24314;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;3D&#20027;&#20307;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2212.07401</link><description>&lt;p&gt;
BKinD-3D&#65306;&#33258;&#30417;&#30563;&#22810;&#35270;&#28857;&#35270;&#39057;&#20013;&#30340;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos. (arXiv:2212.07401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07401
&lt;/p&gt;
&lt;p&gt;
BKinD-3D&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;2D&#25110;3D&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#23545;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#36827;&#34892;&#37325;&#24314;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;3D&#20027;&#20307;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#19977;&#32500;&#36816;&#21160;&#23545;&#20110;&#30740;&#31350;&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#30340;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25163;&#21160;&#23039;&#24577;&#27880;&#37322;&#33719;&#21462;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#33258;&#30417;&#30563;&#20851;&#38190;&#28857;&#21457;&#29616;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19977;&#32500;&#23039;&#24577;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#36890;&#24120;&#20165;&#22788;&#29702;&#21333;&#20010;2D&#35270;&#22270;&#24182;&#19988;&#19981;&#22312;3D&#31354;&#38388;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34892;&#20026;&#20027;&#20307;&#30340;&#22810;&#35270;&#28857;&#35270;&#39057;&#20013;&#25191;&#34892;&#26080;2D&#25110;3D&#20851;&#38190;&#28857;&#25110;&#36793;&#30028;&#26694;&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;BKinD-3D&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#65292;&#35757;&#32451;&#20197;&#37325;&#24314;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#65292;&#27492;&#22806;&#36824;&#20351;&#29992;&#20102;&#23398;&#20064;&#21040;&#30340;&#20027;&#20307;&#19977;&#32500;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#25163;&#21160;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20154;&#31867;&#21644;&#32769;&#40736;&#30340;&#35270;&#39057;&#20013;&#21457;&#29616;&#20102;&#20851;&#38190;&#28857;&#65292;&#23637;&#31034;&#20102;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#22312;&#30740;&#31350;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying motion in 3D is important for studying the behavior of humans and other animals, but manual pose annotations are expensive and time-consuming to obtain. Self-supervised keypoint discovery is a promising strategy for estimating 3D poses without annotations. However, current keypoint discovery approaches commonly process single 2D views and do not operate in the 3D space. We propose a new method to perform self-supervised keypoint discovery in 3D from multi-view videos of behaving agents, without any keypoint or bounding box supervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoder architecture with a 3D volumetric heatmap, trained to reconstruct spatiotemporal differences across multiple views, in addition to joint length constraints on a learned 3D skeleton of the subject. In this way, we discover keypoints without requiring manual supervision in videos of humans and rats, demonstrating the potential of 3D keypoint discovery for studying behavior.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;Diffusion-SDF&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#65292;&#37319;&#29992;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2212.03293</link><description>&lt;p&gt;
&#36890;&#36807;&#20307;&#32032;&#25193;&#25955;&#23454;&#29616;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-SDF: Text-to-Shape via Voxelized Diffusion. (arXiv:2212.03293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03293
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;Diffusion-SDF&#65292;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#65292;&#37319;&#29992;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#19977;&#32500;&#34394;&#25311;&#24314;&#27169;&#25216;&#26415;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#65292;&#22522;&#20110;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#25991;&#26412;&#65289;&#29983;&#25104;&#26032;&#39062;&#30340;&#19977;&#32500;&#20869;&#23481;&#24050;&#25104;&#20026;&#28909;&#38376;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion-SDF&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#19977;&#32500;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#32508;&#21512;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#19977;&#32500;&#25968;&#25454;&#34920;&#31034;&#21644;&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#24418;&#29366;&#20197;&#31526;&#21512;&#32473;&#23450;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SDF autoencoder&#21644;&#20307;&#32032;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#30340;&#20307;&#32032;&#21270;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;UinU-Net&#32467;&#26500;&#65292;&#22312;&#26631;&#20934;&#30340;U-Net&#32467;&#26500;&#20013;&#23884;&#20837;&#19968;&#20010;&#23616;&#37096;&#32858;&#28966;&#30340;&#20869;&#37096;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29420;&#31435;&#20110;&#34917;&#19969;&#30340;SDF&#34920;&#31034;&#26041;&#27861;&#26356;&#22909;&#30340;&#37325;&#26500;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#36827;&#19968;&#27493;&#30340;&#25991;&#26412;&#29983;&#25104;&#19977;&#32500;&#24418;&#29366;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#24418;&#29366;&#23436;&#25104;&#21644;&#25552;&#39640;&#22810;&#26679;&#24615;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.08639</link><description>&lt;p&gt;
&#20998;&#23618;&#21160;&#24577;&#22270;&#20687;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Dynamic Image Harmonization. (arXiv:2211.08639v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21327;&#35843;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35843;&#25972;&#21069;&#26223;&#20197;&#20351;&#20854;&#19982;&#32972;&#26223;&#20860;&#23481;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#20840;&#23616;&#21464;&#25442;&#65288;&#21363;&#24402;&#19968;&#21270;&#21644;&#33394;&#24425;&#26354;&#32447;&#28210;&#26579;&#65289;&#26469;&#23454;&#29616;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24573;&#30053;&#20102;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35843;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#21160;&#24577;&#32593;&#32476;&#65288;HDNet&#65289;&#65292;&#20197;&#36866;&#24212;&#29305;&#24449;&#20174;&#23616;&#37096;&#21040;&#25972;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22270;&#20687;&#21327;&#35843;&#12290;&#22312;&#21508;&#31181;&#21160;&#24577;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#26412;&#22320;&#21160;&#24577;&#65288;LD&#65289;&#27169;&#22359;&#21644;&#22522;&#20110;&#25513;&#30721;&#30340;&#20840;&#23616;&#21160;&#24577;&#65288;MGD&#65289;&#27169;&#22359;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LD&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24615;&#21305;&#37197;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#20043;&#38388;&#30340;&#26412;&#22320;&#34920;&#31034;&#65292;&#28982;&#21518;&#26681;&#25454;&#20854;$K$&#20010;&#26368;&#36817;&#37051;&#32972;&#26223;&#21306;&#22495;&#30340;&#22806;&#35266;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#27599;&#20010;&#21069;&#26223;&#26412;&#22320;&#34920;&#31034;&#12290;&#36825;&#26679;&#65292;LD&#20445;&#30041;&#20102;&#35814;&#32454;&#30340;&#26412;&#22320;&#35270;&#35273;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;LD&#30340;MGD&#36890;&#36807;&#23558;&#21069;&#26223;&#25513;&#30721;&#24341;&#20837;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#20840;&#23616;&#21464;&#25442;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;HDNet&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#21327;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image harmonization is a critical task in computer vision, which aims to adjust the foreground to make it compatible with the background. Recent works mainly focus on using global transformations (i.e., normalization and color curve rendering) to achieve visual consistency. However, these models ignore local visual consistency and their huge model sizes limit their harmonization ability on edge devices. In this paper, we propose a hierarchical dynamic network (HDNet) to adapt features from local to global view for better feature transformation in efficient image harmonization. Inspired by the success of various dynamic models, local dynamic (LD) module and mask-aware global dynamic (MGD) module are proposed in this paper. Specifically, LD matches local representations between the foreground and background regions based on semantic similarities, then adaptively adjust every foreground local representation according to the appearance of its $K$-nearest neighbor background regions. In thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;&#21040;Agent-State&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07805</link><description>&lt;p&gt;
&#24102;&#36741;&#21161;&#36755;&#20837;&#30340;Agent-State&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Agent-State Construction with Auxiliary Inputs. (arXiv:2211.07805v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;&#21040;Agent-State&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#30340;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#65292;&#20915;&#31574;&#20195;&#29702;&#24448;&#24448;&#26080;&#27861;&#27169;&#25311;&#19990;&#30028;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#29615;&#22659;&#24448;&#24448;&#27604;&#20195;&#29702;&#26356;&#22823;&#26356;&#22797;&#26434;&#65292;&#36825;&#20063;&#31216;&#20026;&#37096;&#20998;&#35266;&#27979;&#24615;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#24517;&#39035;&#21033;&#29992;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#30340;&#24863;&#23448;&#36755;&#20837;; &#23427;&#24517;&#39035;&#26500;&#24314;&#19968;&#20010;&#20195;&#29702;&#29366;&#24577;&#65292;&#20197;&#24635;&#32467;&#19982;&#19990;&#30028;&#30340;&#20808;&#21069;&#20132;&#20114;&#12290;&#30446;&#21069;&#65292;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#27969;&#34892;&#26041;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#24490;&#29615;&#32593;&#32476;&#20174;&#20195;&#29702;&#30340;&#24863;&#23448;&#27969;&#20316;&#20026;&#36755;&#20837;&#26469;&#23398;&#20064;Agent-State&#20989;&#25968;&#12290;&#35768;&#22810;&#24378;&#22823;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#20989;&#25968;&#26469;&#24110;&#21161;&#20195;&#29702;&#36755;&#20837;&#21382;&#21490;&#25688;&#35201;&#12290;&#36825;&#20123;&#22686;&#24378;&#26377;&#22810;&#31181;&#26041;&#24335;&#65292;&#20174;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#36830;&#25509;&#35266;&#23519;&#65292;&#21040;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#25105;&#20204;&#31216;&#20043;&#20026;&#36741;&#21161;&#36755;&#20837;&#30340;&#36825;&#20123;&#38468;&#21152;&#36755;&#20837;&#36890;&#24120;&#20197;&#19968;&#31181;&#29305;&#27530;&#30340;&#26041;&#24335;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#23558;&#36741;&#21161;&#36755;&#20837;&#32435;&#20837;Agent-State&#26500;&#24314;&#36807;&#31243;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#20854;&#20013;&#36741;&#21161;&#36755;&#20837;&#29992;&#20110;&#35843;&#25972;&#20195;&#29702;&#29366;&#24577;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#20445;&#30041;&#19982;&#20915;&#31574;&#21046;&#23450;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.07098</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#23436;&#25104;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion. (arXiv:2211.07098v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#30340;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#26469;&#36798;&#21040;&#26356;&#39640;&#25928;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#26469;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#24314;&#31435;&#26469;&#23384;&#20648;&#22823;&#37327;&#30693;&#35782;&#65292;&#28982;&#32780;&#36825;&#20123;&#30693;&#35782;&#24211;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#22635;&#34917;&#30693;&#35782;&#24211;&#20013;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#20026;&#20102;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#23436;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#38382;&#31572;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#38382;&#39064;&#27169;&#26495;&#26469;&#25552;&#21462;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#20165;&#20165;&#36890;&#36807;&#38750;&#24120;&#23569;&#30340;&#38382;&#39064;&#23601;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#35813;&#38382;&#31572;&#31995;&#32479;&#36824;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#27604;&#22914;&#23454;&#20307;&#31867;&#22411;&#21644;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#25277;&#21462;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20809;&#23398;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.02578</link><description>&lt;p&gt;
&#20809;&#23398;&#22270;&#20687;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#28418;&#31227;&#25511;&#21046;&#30340;&#25968;&#25454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Data Models for Dataset Drift Controls in Machine Learning With Optical Images. (arXiv:2211.02578v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#29289;&#29702;&#20809;&#23398;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#22270;&#20687;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#22312;&#21307;&#23398;&#21644;&#29615;&#22659;&#35843;&#26597;&#31561;&#37325;&#35201;&#39046;&#22495;&#21457;&#25381;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24378;&#24230;&#38382;&#39064;&#65292;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20027;&#35201;&#30340;&#25925;&#38556;&#27169;&#24335;&#26159;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#21644;&#37096;&#32626;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#26377;&#26041;&#27861;&#21487;&#20197;&#21069;&#30651;&#24615;&#22320;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#36825;&#31181;&#25968;&#25454;&#38598;&#28418;&#31227;&#30340;&#24378;&#24230;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#20027;&#35201;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937; - &#25968;&#25454;&#30340;&#26174;&#24335;&#27169;&#22411;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#22312;&#29289;&#29702;&#19978;&#20934;&#30830;&#22320;&#30740;&#31350;&#21644;&#29702;&#35299;&#25968;&#25454;&#29983;&#25104;&#19982;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23558;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#19982;&#29289;&#29702;&#20809;&#23398;&#37197;&#23545;&#65292;&#33719;&#24471;&#26126;&#30830;&#19988;&#21487;&#24494;&#20998;&#30340;&#25968;&#25454;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#28418;&#31227;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#26694;&#26550;&#65292;&#20026;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#39044;&#27979;&#36136;&#37327;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2210.11222</link><description>&lt;p&gt;
&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#30340;&#23398;&#20064;&#22686;&#24378;&#31169;&#26377;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Private Algorithms for Multiple Quantile Release. (arXiv:2210.11222v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65306;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#26694;&#26550;&#65292;&#20026;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#39044;&#27979;&#36136;&#37327;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20110;&#25935;&#24863;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#24120;&#24120;&#21487;&#20197;&#21033;&#29992;&#39069;&#22806;&#30340;&#20449;&#24687;&#20363;&#22914;&#20854;&#20182;&#25935;&#24863;&#25968;&#25454;&#12289;&#20844;&#20247;&#25968;&#25454;&#25110;&#20154;&#31867;&#20449;&#24687;&#20808;&#39564;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65288;&#25110;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#31639;&#27861;&#65289;&#26694;&#26550;&#65292;&#36825;&#20010;&#26694;&#26550;&#36890;&#24120;&#20351;&#29992;&#20110;&#20248;&#21270;&#26102;&#38388;&#22797;&#26434;&#24230;&#25110;&#31454;&#20105;&#27604;&#29575;&#12290;&#35813;&#26694;&#26550;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#36825;&#20123;&#39069;&#22806;&#20449;&#24687;&#20197;&#25552;&#39640;&#25928;&#29992;&#12290;&#35813;&#24819;&#27861;&#20307;&#29616;&#22312;&#37325;&#35201;&#30340;&#22810;&#20998;&#20301;&#25968;&#21457;&#24067;&#20219;&#21153;&#20013;&#65292;&#22312;&#27492;&#25105;&#20204;&#24471;&#20986;&#20102;&#38543;&#30528;&#33258;&#28982;&#36136;&#37327;&#39044;&#27979;&#30340;&#38169;&#35823;&#20445;&#35777;&#65292;&#21516;&#26102;&#65288;&#20960;&#20046;&#65289;&#24674;&#22797;&#20102;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#29420;&#31435;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#21253;&#25324;&#23545;&#25968;&#25454;&#30340;&#26368;&#23567;&#20551;&#35774;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#20026;&#20004;&#20010;&#20174;&#20854;&#20182;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#27979;&#30340;&#26032;&#39062;&#8220;&#20803;&#8221;&#31639;&#27861;&#25552;&#20379;&#26377;&#29992;&#30340;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework -- previously applied largely to improve time complexity or competitive ratios -- as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. This idea is instantiated on the important task of multiple quantile release, for which we derive error guarantees that scale with a natural measure of prediction quality while (almost) recovering state-of-the-art prediction-independent guarantees. Our analysis enjoys several advantages, including minimal assumptions about the data, a natural way of adding robustness, and the provision of useful surrogate losses for two novel ``meta" algorithms that learn predictions from other (potentially sensitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#21464;&#37327;&#30340;&#25509;&#35302;&#32422;&#26463;&#26041;&#27861;&#65292;&#23558;&#26144;&#23556;&#20998;&#35299;&#20026;&#20004;&#20010;&#39034;&#24207;&#38454;&#27573;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#25235;&#21462;&#30340;&#28508;&#22312;&#25509;&#35302;&#22320;&#22270;&#65292;&#24182;&#23398;&#20064;&#25509;&#35302;&#22320;&#22270;&#21040;&#25235;&#21462;&#23039;&#21183;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#25928;&#19988;&#39640;&#36890;&#29992;&#24615;&#30340;&#19977;&#32500;&#25235;&#21462;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2210.09245</link><description>&lt;p&gt;
Contact2Grasp&#65306;&#22522;&#20110;&#29289;&#20307;&#25509;&#35302;&#32422;&#26463;&#30340;&#19977;&#32500;&#25235;&#21462;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Contact2Grasp: 3D Grasp Synthesis via Hand-Object Contact Constraint. (arXiv:2210.09245v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#21464;&#37327;&#30340;&#25509;&#35302;&#32422;&#26463;&#26041;&#27861;&#65292;&#23558;&#26144;&#23556;&#20998;&#35299;&#20026;&#20004;&#20010;&#39034;&#24207;&#38454;&#27573;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#25235;&#21462;&#30340;&#28508;&#22312;&#25509;&#35302;&#22320;&#22270;&#65292;&#24182;&#23398;&#20064;&#25509;&#35302;&#22320;&#22270;&#21040;&#25235;&#21462;&#23039;&#21183;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#25928;&#19988;&#39640;&#36890;&#29992;&#24615;&#30340;&#19977;&#32500;&#25235;&#21462;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25235;&#21462;&#21512;&#25104;&#26159;&#32473;&#23450;&#19968;&#20010;&#29289;&#20307;&#65292;&#29983;&#25104;&#25235;&#21462;&#23039;&#24577;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20174;&#29289;&#20307;&#21040;&#25235;&#21462;&#23039;&#24577;&#20998;&#24067;&#30340;&#30452;&#25509;&#26144;&#23556;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29289;&#29702;&#25509;&#35302;&#23545;&#23039;&#24577;&#24494;&#23567;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#19977;&#32500;&#29289;&#20307;&#34920;&#31034;&#21040;&#26377;&#25928;&#23039;&#24577;&#30340;&#39640;&#24230;&#38750;&#32447;&#24615;&#26144;&#23556;&#30456;&#24403;&#19981;&#24179;&#28369;&#65292;&#23548;&#33268;&#29983;&#25104;&#25928;&#29575;&#20302;&#19988;&#36890;&#29992;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25509;&#35302;&#21306;&#22495;&#20013;&#38388;&#21464;&#37327;&#26469;&#32422;&#26463;&#25235;&#21462;&#23039;&#21183;&#30340;&#29983;&#25104;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#23558;&#26144;&#23556;&#20998;&#35299;&#20026;&#20004;&#20010;&#39034;&#24207;&#38454;&#27573;&#65292;&#20551;&#35774;&#25509;&#35302;&#22320;&#22270;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#23039;&#21183;&#23436;&#20840;&#21463;&#38480;&#65306;1&#65289;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#25509;&#35302;&#22320;&#22270;&#20998;&#24067;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#25235;&#21462;&#30340;&#28508;&#22312;&#25509;&#35302;&#22320;&#22270;&#65307;2&#65289;&#28982;&#21518;&#23398;&#20064;&#20174;&#25509;&#35302;&#22320;&#22270;&#21040;&#25235;&#21462;&#23039;&#21183;&#30340;&#26144;&#23556;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#21040;&#31359;&#36879;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#30340;&#25509;&#35302;&#20316;&#20026;&#19968;&#33268;&#24615;&#32422;&#26463;&#36827;&#34892;&#25235;&#21462;&#32454;&#21270;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D grasp synthesis generates grasping poses given an input object. Existing works tackle the problem by learning a direct mapping from objects to the distributions of grasping poses. However, because the physical contact is sensitive to small changes in pose, the high-nonlinear mapping between 3D object representation to valid poses is considerably non-smooth, leading to poor generation efficiency and restricted generality. To tackle the challenge, we introduce an intermediate variable for grasp contact areas to constrain the grasp generation; in other words, we factorize the mapping into two sequential stages by assuming that grasping poses are fully constrained given contact maps: 1) we first learn contact map distributions to generate the potential contact maps for grasps; 2) then learn a mapping from the contact maps to the grasping poses. Further, we propose a penetration-aware optimization with the generated contacts as a consistency constraint for grasp refinement. Extensive val
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.03919</link><description>&lt;p&gt;
CLIP-PAE&#65306;&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#20197;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#21487;&#20998;&#31163;&#12289;&#21487;&#35299;&#37322;&#12289;&#21487;&#25511;&#30340;&#25991;&#26412;&#25351;&#23548;&#33080;&#37096;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#20013;&#30340;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#30456;&#20851;&#25552;&#31034;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#33719;&#21462;&#29305;&#23450;&#22270;&#20687;&#29305;&#24449;&#24182;&#24341;&#20837;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#22788;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#36825;&#25171;&#24320;&#20102;&#19968;&#20010;&#22823;&#38376;&#65292;&#21363;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#25991;&#23383;&#35828;&#26126;&#26469;&#25805;&#20316;&#36755;&#20837;&#22270;&#20687;&#30340;&#20016;&#23500;&#25991;&#23398;&#36164;&#26009;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32852;&#21512;&#31354;&#38388;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#32467;&#26524;&#22270;&#20687;&#20013;&#20986;&#29616;&#24847;&#22806;&#30340;&#20266;&#24433;&#12290;&#23545;&#20110;&#25805;&#32437;&#26469;&#35828;&#65292;&#21487;&#20998;&#31163;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#20063;&#24456;&#38590;&#20445;&#35777;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23450;&#20041;&#30001;&#30456;&#20851;&#25552;&#31034;&#23637;&#24320;&#30340;&#35821;&#26009;&#24211;&#23376;&#31354;&#38388;&#26469;&#25429;&#33719;&#29305;&#23450;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CLIP&#25237;&#24433;&#22686;&#24378;&#23884;&#20837;&#65288;PAE&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#25805;&#32437;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#33539;&#20363;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#35745;&#31639;&#21644;&#36866;&#24212;&#65292;&#24182;&#24179;&#31283;&#22320;&#34701;&#20837;&#21040;&#20219;&#20309;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#25805;&#20316;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#24341;&#26426;&#22120;&#20154;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#36825;&#23558;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20195;&#34920;&#24615;&#26435;&#21147;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2210.00858</link><description>&lt;p&gt;
&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20114;&#21160;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach. (arXiv:2210.00858v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#24341;&#26426;&#22120;&#20154;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#36825;&#23558;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20195;&#34920;&#24615;&#26435;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26550;&#26500;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#25512;&#29702;&#19982;&#26426;&#22120;&#20154;&#25805;&#20316;&#30456;&#32467;&#21512;&#12290;&#38750;&#19987;&#19994;&#20154;&#22763;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#26426;&#22120;&#20154;&#65292;&#25552;&#20379;&#25351;&#20195;&#34920;&#36798;&#24335;&#65288;REF&#65289;&#12289;&#38382;&#39064;&#65288;VQA&#65289;&#25110;&#25235;&#25569;&#21160;&#20316;&#25351;&#20196;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#20849;&#20139;&#30340;&#21407;&#22987;&#25216;&#33021;&#24211;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#25152;&#26377;&#24773;&#20917;&#12290;&#27599;&#20010;&#21407;&#22987;&#25216;&#33021;&#37117;&#22788;&#29702;&#19968;&#20010;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#20363;&#22914;&#25512;&#29702;&#35270;&#35273;&#23646;&#24615;&#12289;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#12289;&#36923;&#36753;&#21644;&#26522;&#20030;&#20197;&#21450;&#25163;&#33218;&#25511;&#21046;&#12290;&#35821;&#35328;&#35299;&#26512;&#22120;&#23558;&#36755;&#20837;&#26597;&#35810;&#26144;&#23556;&#21040;&#30001;&#36825;&#20123;&#21407;&#35821;&#32452;&#25104;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#19978;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#19978;&#19979;&#25991;&#12290;&#23613;&#31649;&#26377;&#20123;&#21407;&#35821;&#26159;&#32431;&#31526;&#21495;&#25805;&#20316;&#65288;&#20363;&#22914;&#35745;&#25968;&#65289;&#65292;&#20294;&#21478;&#19968;&#20123;&#26159;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#20989;&#25968;&#65288;&#20363;&#22914;&#35270;&#35273;&#25509;&#22320;&#65289;&#65292;&#22240;&#27492;&#34701;&#21512;&#20102;&#31163;&#25955;&#31526;&#21495;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31995;&#32479;&#21270;&#27867;&#21270;&#20248;&#21183;&#19982;&#21487;&#25193;&#23637;&#24615;&#21644;&#20877;&#29616;&#24615;&#30340;&#20195;&#34920;&#24615;&#26435;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#65292;&#24182;&#35774;&#35745;&#20102;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2210.00434</link><description>&lt;p&gt;
&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#65306;&#20174;&#38899;&#20048;&#24405;&#38899;&#20013;&#29983;&#25104;&#25551;&#36848;&#24615;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings. (arXiv:2210.00434v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#38382;&#39064;&#65292;&#25910;&#38598;&#20102;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#65292;&#24182;&#35774;&#35745;&#20102;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#26469;&#35299;&#20915;&#39640;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#12290;&#19981;&#21516;&#20110;&#25226;&#38899;&#20048;&#24405;&#38899;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#30340;&#32463;&#20856;&#38899;&#20048;&#26631;&#35760;&#38382;&#39064;&#65292;&#38899;&#20048;&#25991;&#26412;&#35270;&#35273;&#20132;&#24863;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#24773;&#24863;&#30340;&#38899;&#20048;&#24405;&#38899;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#38899;&#20048;&#30456;&#20851;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#38899;&#20048;&#24405;&#38899;&#30340;&#35821;&#20041;&#25551;&#36848;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;1,955&#20010;&#21476;&#20856;&#38899;&#20048;&#24405;&#38899;&#19982;&#25991;&#26412;&#25551;&#36848;&#30340;&#23545;&#40784;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#29983;&#25104;&#21487;&#20197;&#25551;&#36848;&#38899;&#20048;&#24405;&#38899;&#20869;&#23481;&#30340;&#21477;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#39640;&#24230;&#38750;&#21028;&#21035;&#24615;&#30340;&#21476;&#20856;&#38899;&#20048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#25299;&#25169;&#20445;&#25345;&#25439;&#22833;&#65292;&#23427;&#32771;&#34385;&#26356;&#22810;&#30340;&#26679;&#26412;&#20316;&#20026;&#32676;&#32452;&#21442;&#32771;&#65292;&#24182;&#20445;&#30041;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#23545;&#25299;&#25169;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#25351;&#26631;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a novel research problem: music-to-text synaesthesia. Different from the classical music tagging problem that classifies a music recording into pre-defined categories, music-to-text synaesthesia aims to generate descriptive texts from music recordings with the same sentiment for further understanding. As existing music-related datasets do not contain the semantic descriptions on music recordings, we collect a new dataset that contains 1,955 aligned pairs of classical music recordings and text descriptions. Based on this, we build a computational model to generate sentences that can describe the content of the music recording. To tackle the highly non-discriminative classical music, we design a group topology-preservation loss, which considers more samples as a group reference and preserves the relative topology among different samples. Extensive experimental results qualitatively and quantitatively demonstrate the effectiveness of our proposed model over five
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#31526;&#21512;&#24230;&#37327;&#26816;&#27979;&#36880;&#28176;&#28418;&#31227;&#30340;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#20027;&#27969;&#31639;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11007</link><description>&lt;p&gt;
&#21033;&#29992;&#31526;&#21512;&#24230;&#37327;&#26816;&#27979;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36880;&#28176;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Gradual Drift Detection in Process Models Using Conformance Metrics. (arXiv:2207.11007v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#31526;&#21512;&#24230;&#37327;&#26816;&#27979;&#36880;&#28176;&#28418;&#31227;&#30340;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#20027;&#27969;&#31639;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#36807;&#31243;&#30340;&#25191;&#34892;&#36807;&#31243;&#20013;&#65292;&#35745;&#21010;&#25110;&#24847;&#22806;&#21464;&#26356;&#26159;&#24120;&#35265;&#30340;&#12290;&#26816;&#27979;&#36825;&#20123;&#21464;&#21270;&#23545;&#20110;&#20248;&#21270;&#36816;&#34892;&#27492;&#31867;&#36807;&#31243;&#30340;&#32452;&#32455;&#30340;&#24615;&#33021;&#26159;&#24517;&#39035;&#30340;&#12290;&#26412;&#25991;&#23558;&#19987;&#27880;&#20110;&#33258;&#21160;&#26816;&#27979;&#36880;&#28176;&#28418;&#31227;&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#31867;&#22411;&#30340;&#21464;&#21270;&#65292;&#22312;&#36825;&#31181;&#21464;&#21270;&#26399;&#38388;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#37325;&#21472;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#31526;&#21512;&#26816;&#26597;&#24230;&#37327;&#26631;&#20934;&#26469;&#23454;&#29616;&#21464;&#21270;&#30340;&#33258;&#21160;&#26816;&#27979;&#65292;&#36824;&#21487;&#20197;&#23545;&#36825;&#20123;&#21464;&#21270;&#36827;&#34892;&#23436;&#20840;&#33258;&#21160;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#36890;&#36807;&#30001;120&#20010;&#20855;&#26377;&#19981;&#21516;&#21464;&#21270;&#20998;&#24067;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#39564;&#35777;&#65292;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#21464;&#21270;&#21306;&#22495;&#37325;&#21472;&#26041;&#38754;&#33719;&#24471;&#20102;&#27604;&#20027;&#35201;&#29616;&#26377;&#31639;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changes, planned or unexpected, are common during the execution of real-life processes. Detecting these changes is a must for optimizing the performance of organizations running such processes. Most of the algorithms present in the state-of-the-art focus on the detection of sudden changes, leaving aside other types of changes. In this paper, we will focus on the automatic detection of gradual drifts, a special type of change, in which the cases of two models overlap during a period of time. The proposed algorithm relies on conformance checking metrics to carry out the automatic detection of the changes, performing also a fully automatic classification of these changes into sudden or gradual. The approach has been validated with a synthetic dataset consisting of 120 logs with different distributions of changes, getting better results in terms of detection and classification accuracy, delay and change region overlapping than the main state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.00738</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#29305;&#24449;&#30340;&#26500;&#25104;&#21450;&#20854;&#22312;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Composition of Relational Features with an Application to Explaining Black-Box Predictors. (arXiv:2206.00738v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#20851;&#31995;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;&#65288;1&#65289;&#33021;&#22815;&#23545;&#25968;&#25454;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#24314;&#27169;&#65307;&#65288;2&#65289;&#22312;&#27169;&#22411;&#26500;&#24314;&#26399;&#38388;&#20351;&#29992;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#20851;&#31995;&#65307;&#65288;3&#65289;&#26500;&#24314;&#30340;&#27169;&#22411;&#26159;&#20154;&#31867;&#21487;&#35835;&#30340;&#65292;&#36825;&#36890;&#24120;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#23558;&#20851;&#31995;&#29305;&#24449;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36890;&#29992;&#20989;&#25968;&#32452;&#21512;&#30340;&#27010;&#24565;&#20174;&#31616;&#21333;&#20989;&#25968;&#25512;&#23548;&#20986;&#22797;&#26434;&#20989;&#25968;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#22312;&#27169;&#25454;&#35821;&#35328;M&#20013;&#30340; $\text{M}$-&#31616;&#21333;&#29305;&#24449;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#20004;&#20010;&#32452;&#21512;&#31639;&#23376;&#65288;$\rho_1$&#21644;$\rho_2$&#65289;&#65292;&#25152;&#26377;&#21487;&#33021;&#30340;&#22797;&#26434;&#29305;&#24449;&#37117;&#21487;&#20197;&#20174;&#20013;&#27966;&#29983;&#20986;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#23454;&#29616;&#20102;&#19968;&#31181;&#8220;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#21644;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational machine learning programs like those developed in Inductive Logic Programming (ILP) offer several advantages: (1) The ability to model complex relationships amongst data instances; (2) The use of domain-specific relations during model construction; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. However, these ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. We formulate the notion of a set of $\text{M}$-simple features in a mode language $\text{M}$ and identify two composition operators ($\rho_1$ and $\rho_2$) from which all possible complex features can be derived. We use these results to implement a form of "explainable neural 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2205.12554</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Helpfulness and Fairness of Task-Oriented Dialogue Systems. (arXiv:2205.12554v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#23545;&#35805;&#31995;&#32479;&#30340;&#24110;&#21161;&#24615;&#65292;&#20351;&#29992;&#20998;&#31867;&#22120;&#33258;&#21160;&#30830;&#23450;&#24110;&#21161;&#24615;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#65292;&#22240;&#27492;&#20154;&#20204;&#23545;&#20854;&#24110;&#21161;&#24615;&#30340;&#24863;&#30693;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#23545;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20154;&#31867;&#24863;&#30693;&#24110;&#21161;&#24615;&#20197;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24110;&#21161;&#24615;&#30340;&#35745;&#31639;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#26500;&#24314;&#20998;&#31867;&#22120;&#65292;&#33258;&#21160;&#30830;&#23450;&#21709;&#24212;&#30340;&#24110;&#21161;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20351;&#29992;&#23545;&#19981;&#21516;&#29992;&#25143;&#26597;&#35810;&#30340;&#24110;&#21161;&#32423;&#21035;&#26469;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#19977;&#31181;&#20449;&#24687;&#26597;&#35810;&#22330;&#26223;&#19979;&#26356;&#23481;&#26131;&#20026;&#26469;&#33258;&#21457;&#36798;&#22269;&#23478;&#27010;&#24565;&#30340;&#38382;&#39064;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant &amp; coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2205.09622</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#20844;&#24179;&#24615;&#65311;&#21746;&#23398;&#30340;&#24605;&#32771;&#19982;&#23545;fairML&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Is Fairness? Philosophical Considerations and Implications For FairML. (arXiv:2205.09622v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20844;&#24179;&#24615;&#30340;&#21746;&#23398;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#24182;&#24378;&#35843;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#27169;&#22411;&#35780;&#20272;&#37117;&#38656;&#32435;&#20837;&#20262;&#29702;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#24615;&#20154;&#24037;&#26234;&#33021;(fairML)&#39046;&#22495;&#65292;&#36890;&#36807;&#23450;&#20041;&#34913;&#37327;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24230;&#37327;&#21644;&#25552;&#20986;&#30830;&#20445;&#35757;&#32451;&#27169;&#22411;&#25968;&#25454;&#20855;&#26377;&#20302;&#20844;&#24179;&#24615;&#24230;&#37327;&#20540;&#30340;&#26041;&#27861;&#65292;&#26469;&#20943;&#36731;&#20154;&#24037;&#26234;&#33021;(ML)&#20135;&#29983;&#30340;&#30456;&#20851;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20844;&#24179;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#21363;"&#20844;&#24179;&#26159;&#20160;&#20040;"&#65292;&#24456;&#23569;&#34987;&#35752;&#35770;&#65292;&#36825;&#36896;&#25104;&#20102;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#21746;&#23398;&#39046;&#22495;&#20960;&#20010;&#19990;&#32426;&#30340;&#35752;&#35770;&#19982;&#36817;&#26399;&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#33268;&#24615;&#20844;&#24179;&#27010;&#24565;&#21644;&#23558;&#21746;&#23398;&#24605;&#32771;&#36716;&#21270;&#20026;ADM&#31995;&#32479;&#20013;ML&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#26469;&#26550;&#36215;&#36825;&#19968;&#40511;&#27807;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#21487;&#33021;&#24050;&#32463;&#23384;&#22312;&#65292;&#21363;&#20351;&#27809;&#26377;&#21463;&#20445;&#25252;&#24615;&#23646;&#24615;&#30340;&#23384;&#22312;&#65292;&#24378;&#35843;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#19981;&#26159;&#19981;&#21487;&#35843;&#21644;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#21069;&#32773;&#23454;&#29616;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#24378;&#35843;&#23558;&#20262;&#29702;&#32771;&#34385;&#32435;&#20837;ML&#31649;&#36947;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#26368;&#32456;&#37096;&#32626;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of literature in fairness-aware ML (fairML) aspires to mitigate machine learning (ML)-related unfairness in automated decision making (ADM) by defining metrics that measure fairness of an ML model and by proposing methods that ensure that trained ML models achieve low values in those measures. However, the underlying concept of fairness, i.e., the question of what fairness is, is rarely discussed, leaving a considerable gap between centuries of philosophical discussion and recent adoption of the concept in the ML community. In this work, we try to bridge this gap by formalizing a consistent concept of fairness and by translating the philosophical considerations into a formal framework for the training and evaluation of ML models in ADM systems. We derive that fairness problems can already arise without the presence of protected attributes, pointing out that fairness and predictive performance are not irreconcilable counterparts, but rather that the latter is necessary to
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DPMS&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;ADD&#31526;&#21495;&#21270;&#26041;&#27861;&#30340;&#24191;&#20041;MaxSAT&#27714;&#35299;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;CNF&#28151;&#21512;&#32422;&#26463;&#30340;&#24191;&#20041;MaxSAT&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2205.03747</link><description>&lt;p&gt;
DPMS: &#19968;&#31181;&#22522;&#20110;ADD&#31526;&#21495;&#21270;&#26041;&#27861;&#30340;&#24191;&#20041;MaxSAT&#27714;&#35299;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPMS: An ADD-Based Symbolic Approach for Generalized MaxSAT Solving. (arXiv:2205.03747v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.03747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;DPMS&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;ADD&#31526;&#21495;&#21270;&#26041;&#27861;&#30340;&#24191;&#20041;MaxSAT&#27714;&#35299;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;CNF&#28151;&#21512;&#32422;&#26463;&#30340;&#24191;&#20041;MaxSAT&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23572;&#26368;&#22823;&#21487;&#28385;&#36275;&#24615;&#65288;MaxSAT&#65289;&#38382;&#39064;&#20197;&#21450;&#24191;&#20041;&#24418;&#24335;&#65292;&#22914;Min-MaxSAT&#21644;Max-hybrid-SAT&#65292;&#22312;&#24067;&#23572;&#25512;&#29702;&#20013;&#26159;&#22522;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;MaxSAT&#26041;&#27861;&#24050;&#25104;&#21151;&#35299;&#20915;CNF&#26684;&#24335;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#22788;&#29702;1&#65289;&#65288;&#38750;CNF&#65289;&#28151;&#21512;&#32422;&#26463;&#65292;&#20363;&#22914;XOR&#65292;&#21644;2&#65289;&#24191;&#20041;MaxSAT&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#32534;&#31243;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#28151;&#21512;&#32422;&#26463;&#30340;&#24191;&#20041;MaxSAT&#38382;&#39064;&#65292;&#31216;&#20026;\emph{Dynamic-Programming-MaxSAT} &#65288;DPMS&#65289;&#8212;&#8212;&#22522;&#20110;&#20195;&#25968;&#20915;&#31574;&#22270;&#65288;ADDs&#65289;&#12290;&#20973;&#20511;ADDs&#21644;&#65288;&#20998;&#32423;&#65289;&#39033;&#30446;&#36830;&#25509;&#26641;&#29983;&#25104;&#22120;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#25509;&#21463;&#35768;&#22810;CNF-MaxSAT&#30340;&#27867;&#21270;&#65292;&#20363;&#22914;&#20855;&#26377;&#28151;&#21512;&#32422;&#26463;&#30340;MaxSAT&#12289;Min-MaxSAT&#21644;MinSAT&#12290;&#27492;&#22806;&#65292;DPMS&#22312;&#23485;&#24230;&#20302;&#30340;&#23454;&#20363;&#19978;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#33391;&#22909;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DPMS&#33021;&#22815;&#24555;&#36895;&#35299;&#20915;&#26576;&#20123;&#38382;&#39064;&#65292;&#20854;&#20182;&#22522;&#20110;&#21508;&#31181;&#25216;&#26415;&#30340;&#31639;&#27861;&#21017;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Boolean MaxSAT, as well as generalized formulations such as Min-MaxSAT and Max-hybrid-SAT, are fundamental optimization problems in Boolean reasoning. Existing methods for MaxSAT have been successful in solving benchmarks in CNF format. They lack, however, the ability to handle 1) (non-CNF) hybrid constraints, such as XORs and 2) generalized MaxSAT problems natively. To address this issue, we propose a novel dynamic-programming approach for solving generalized MaxSAT problems with hybrid constraints -- called \emph{Dynamic-Programming-MaxSAT} or DPMS for short -- based on Algebraic Decision Diagrams (ADDs). With the power of ADDs and the (graded) project-join-tree builder, our versatile framework admits many generalizations of CNF-MaxSAT, such as MaxSAT, Min-MaxSAT, and MinSAT with hybrid constraints. Moreover, DPMS scales provably well on instances with low width. Empirical results indicate that DPMS is able to solve certain problems quickly, where other algorithms based on various te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.03583</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33016;&#37096;&#30142;&#30149;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs. (arXiv:2202.03583v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;GRADCAM&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#20351;&#29992;&#28909;&#22270;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;X&#20809;&#22270;&#20687;&#30149;&#29702;&#35782;&#21035;&#26041;&#27861;&#20381;&#36182;&#20110;&#29087;&#32451;&#30340;&#20154;&#31867;&#35299;&#37322;&#65292;&#24182;&#19988;&#24448;&#24448;&#32791;&#26102;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#20351;&#33258;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#24320;&#21457;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#23427;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet&#65289;&#21644;GRADCAM&#36827;&#34892;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#33016;&#37096;X&#20809;&#30142;&#30149;&#22810;&#26631;&#31614;&#35786;&#26029;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#21069;&#32622;X&#20809;&#35757;&#32451;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#23450;&#37327;&#25351;&#26631;&#65288;&#21253;&#25324;&#21463;&#35797;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65289;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Cardiomegaly&#26465;&#20214;&#19979;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;AUC&#24471;&#20998;0.896&#65292;&#24182;&#33719;&#24471;&#20102;0.826&#30340;&#20934;&#30830;&#24230;&#12290;&#32780;&#22312;Nodule&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#26368;&#20302;&#30340;AUC&#24471;&#20998;0.655&#65292;&#20934;&#30830;&#24230;&#20026;0.66&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#20915;&#31574;&#26041;&#38754;&#24314;&#31435;&#20449;&#20219;&#65292;&#25105;&#20204;&#20351;&#29992;GRADCAM&#29983;&#25104;&#20102;&#28909;&#22270;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23545;&#35786;&#26029;&#26368;&#37325;&#35201;&#30340;X&#20809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional methods of identifying pathologies in X-ray images rely heavily on skilled human interpretation and are often time-consuming. The advent of deep learning techniques has enabled the development of automated disease diagnosis systems, but the performance of such systems is dependent on the quality of the model and the level of interpretability it provides. In this paper, we propose a multi-label disease diagnosis model for chest X-rays using a dense convolutional neural network (DenseNet) and model interpretability using GRADCAM. We trained our model using frontal X-rays and evaluated its performance using various quantitative metrics, including the area under the receiver operating characteristic curve (AUC). Our proposed model achieved the highest AUC score of 0.896 for the condition Cardiomegaly with an accuracy of 0.826, while the lowest AUC score was obtained for Nodule, at 0.655 with an accuracy of 0.66. To promote model interpretability and build trust in decision maki
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.09227</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#20803;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24314;&#27169;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#65292;&#20854;&#20013;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;NLP&#20219;&#21153;&#19981;&#21487;&#20998;&#21106;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LM&#36275;&#22815;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#35821;&#35328;&#30340;&#26377;&#29992;&#21644;&#30456;&#20851;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#20110;&#23545;&#24120;&#35268;NLP&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30456;&#21453;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20010;&#35821;&#26009;&#24211;&#21487;&#20197;&#24456;&#22909;&#22320;&#20195;&#34920;&#38463;&#25289;&#20271;&#35821;&#12290;&#30001;&#20110;&#33521;&#35821;&#35821;&#26009;&#24211;&#21487;&#33719;&#24471;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#33521;&#35821;LM&#36890;&#24120;&#27604;&#20854;&#20182;&#35821;&#35328;LM&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#12290;&#23427;&#30001;&#36229;&#36807;500GB&#30340;&#24050;&#21152;&#24037;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#32452;&#25104;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#19979;&#28216;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#35821;&#26009;&#24211;&#36824;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In
&lt;/p&gt;</description></item><item><title>CausalSim&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2201.01811</link><description>&lt;p&gt;
CausalSim: &#19968;&#31181;&#29992;&#20110;&#26080;&#20559;&#24046;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation. (arXiv:2201.01811v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01811
&lt;/p&gt;
&lt;p&gt;
CausalSim&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CausalSim&#65292;&#19968;&#31181;&#29992;&#20110;&#26080;&#20559;&#24046;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#30340;&#22240;&#26524;&#26694;&#26550;&#12290;&#24403;&#21069;&#30340;&#36861;&#36394;&#39537;&#21160;&#20223;&#30495;&#22120;&#20551;&#35774;&#36827;&#34892;&#20223;&#30495;&#30340;&#24178;&#39044;&#65288;&#20363;&#22914;&#65292;&#26032;&#31639;&#27861;&#65289;&#19981;&#20250;&#24433;&#21709;&#36861;&#36394;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36861;&#36394;&#24120;&#24120;&#20250;&#21463;&#21040;&#31639;&#27861;&#22312;&#36861;&#36394;&#25910;&#38598;&#26399;&#38388;&#36827;&#34892;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#65292;&#22312;&#24178;&#39044;&#19979;&#37325;&#28436;&#36861;&#36394;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#12290;CausalSim&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21160;&#24577;&#21644;&#25429;&#33719;&#36861;&#36394;&#25910;&#38598;&#26399;&#38388;&#22522;&#30784;&#31995;&#32479;&#26465;&#20214;&#30340;&#28508;&#22312;&#22240;&#32032;&#30340;&#22240;&#26524;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#23427;&#20351;&#29992;&#22266;&#23450;&#31639;&#27861;&#38598;&#19979;&#30340;&#21021;&#22987;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCT&#65289;&#26469;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#27169;&#25311;&#26032;&#31639;&#27861;&#26102;&#24212;&#29992;&#23427;&#20204;&#26469;&#28040;&#38500;&#36861;&#36394;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CausalSim, a causal framework for unbiased trace-driven simulation. Current trace-driven simulators assume that the interventions being simulated (e.g., a new algorithm) would not affect the validity of the traces. However, real-world traces are often biased by the choices algorithms make during trace collection, and hence replaying traces under an intervention may lead to incorrect results. CausalSim addresses this challenge by learning a causal model of the system dynamics and latent factors capturing the underlying system conditions during trace collection. It learns these models using an initial randomized control trial (RCT) under a fixed set of algorithms, and then applies them to remove biases from trace data when simulating new algorithms.  Key to CausalSim is mapping unbiased trace-driven simulation to a tensor completion problem with extremely sparse observations. By exploiting a basic distributional invariance property present in RCT data, CausalSim enables a nove
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#21644;&#35745;&#31639;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.04047</link><description>&lt;p&gt;
&#36880;&#27493;&#20998;&#26512;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning, Bit by Bit. (arXiv:2103.04047v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.04047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#21644;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#23601;&#12290;&#25968;&#25454;&#25928;&#29575;&#26159;&#23558;&#36825;&#31181;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#29615;&#22659;&#30340;&#38556;&#30861;&#12290;&#25968;&#25454;&#39640;&#25928;&#20195;&#29702;&#30340;&#35774;&#35745;&#38656;&#35201;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20449;&#24687;&#33719;&#21462;&#21644;&#34920;&#31034;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27010;&#24565;&#21644;&#36951;&#25022;&#20998;&#26512;&#65292;&#20849;&#21516;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#12290;&#36825;&#31181;&#24605;&#36335;&#25581;&#31034;&#20102;&#20851;&#20110;&#23547;&#27714;&#20160;&#20040;&#20449;&#24687;&#12289;&#22914;&#20309;&#23547;&#27714;&#35813;&#20449;&#24687;&#20197;&#21450;&#20445;&#30041;&#21738;&#20123;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31616;&#21333;&#30340;&#20195;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#31361;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning agents have demonstrated remarkable achievements in simulated environments. Data efficiency poses an impediment to carrying this success over to real environments. The design of data-efficient agents calls for a deeper understanding of information acquisition and representation. We discuss concepts and regret analysis that together offer principled guidance. This line of thinking sheds light on questions of what information to seek, how to seek that information, and what information to retain. To illustrate concepts, we design simple agents that build on them and present computational results that highlight data efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#19979;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25628;&#32034;&#31639;&#27861;&#12289;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#21644;&#21516;&#20041;&#35789;&#20851;&#32852;&#26041;&#38754;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/1710.00310</link><description>&lt;p&gt;
&#38754;&#21521;&#20799;&#31461;&#22270;&#20070;&#25512;&#33616;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21450;&#20854;&#19982;&#23454;&#26102;&#20132;&#20114;&#26426;&#22120;&#20154;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Personalized Recommender System for Children's Book Recommendation with A Realtime Interactive Robot. (arXiv:1710.00310v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.00310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#19979;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#65292;&#22312;&#25628;&#32034;&#31639;&#27861;&#12289;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#21644;&#21516;&#20041;&#35789;&#20851;&#32852;&#26041;&#38754;&#36827;&#34892;&#20102;&#21019;&#26032;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20799;&#31461;&#21644;&#26426;&#22120;&#20154;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#20010;&#24615;&#21270;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#25628;&#32034;&#31639;&#27861;&#65292;&#20351;&#29992;&#21453;&#21521;&#36807;&#28388;&#26426;&#21046;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#26032;&#22411;&#21453;&#39304;&#26426;&#21046;&#30340;&#29992;&#25143;&#20852;&#36259;&#39044;&#27979;&#26041;&#27861;&#12290;&#26681;&#25454;&#20799;&#31461;&#27169;&#31946;&#30340;&#35821;&#35328;&#36755;&#20837;&#65292;&#35813;&#26041;&#27861;&#32473;&#20986;&#20102;&#39044;&#27979;&#30340;&#20852;&#36259;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35789;&#21521;&#37327;&#21270;&#65292;&#25552;&#20986;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#21516;&#20041;&#35789;&#20851;&#32852;&#65292;&#20197;&#25552;&#39640;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25512;&#33616;&#31995;&#32479;&#24615;&#33021;&#24471;&#21040;&#20102;&#25913;&#36827;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#36816;&#34892;&#22312;&#23884;&#20837;&#24335;&#28040;&#36153;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study the personalized book recommender system in a child-robot interactive environment. Firstly, we propose a novel text search algorithm using an inverse filtering mechanism that improves the efficiency. Secondly, we propose a user interest prediction method based on the Bayesian network and a novel feedback mechanism. According to children's fuzzy language input, the proposed method gives the predicted interests. Thirdly, the domain specific synonym association is proposed based on word vectorization, in order to improve the understanding of user intention. Experimental results show that the proposed recommender system has an improved performance and it can operate on embedded consumer devices with limited computational resources.
&lt;/p&gt;</description></item></channel></rss>