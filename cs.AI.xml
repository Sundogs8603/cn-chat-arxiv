<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02817</link><description>&lt;p&gt;
&#20248;&#21270;&#22411;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#32508;&#36848;&#65306;&#20174;&#32463;&#20856;&#21040;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23558;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#23618;&#36816;&#21160;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#35299;&#20915;&#38271;&#26102;&#22495;&#12289;&#21160;&#24577;&#20219;&#21153;&#12290;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#19987;&#27880;&#20110;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#23450;&#20041;&#30446;&#26631;&#26465;&#20214;&#30340;&#28151;&#21512;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#24320;&#25918;&#24335;&#30446;&#26631;&#12289;&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#29305;&#21035;&#36866;&#21512;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#36816;&#21160;&#21644;&#25805;&#20316;&#38382;&#39064;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#23457;&#35270;&#20102;&#22522;&#20110;&#20248;&#21270;&#30340;TAMP&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#35268;&#21010;&#39046;&#22495;&#34920;&#31034;&#65292;&#21253;&#25324;&#21160;&#20316;&#25551;&#36848;&#35821;&#35328;&#21644;&#26102;&#24577;&#36923;&#36753;&#65292;&#65288;ii&#65289;TAMP&#21508;&#32452;&#20214;&#30340;&#20010;&#21035;&#35299;&#20915;&#31574;&#30053;&#65292;&#21253;&#25324;&#20154;&#24037;&#26234;&#33021;&#35268;&#21010;&#21644;&#36712;&#36857;&#20248;&#21270;&#65288;TO&#65289;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22522;&#20110;&#36923;&#36753;&#30340;&#20219;&#21153;&#35268;&#21010;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;TO&#20043;&#38388;&#30340;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02817v1 Announce Type: cross  Abstract: Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36924;&#30495;&#12289;&#20960;&#20309;&#36924;&#30495;&#21644;&#29289;&#20307;&#36924;&#30495;&#12290;</title><link>https://arxiv.org/abs/2404.00815</link><description>&lt;p&gt;
&#22522;&#20110;LiDAR&#25193;&#25955;&#27169;&#22411;&#30340;&#36924;&#30495;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Scene Generation with LiDAR Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#23454;&#29616;&#20102;&#27169;&#24335;&#36924;&#30495;&#12289;&#20960;&#20309;&#36924;&#30495;&#21644;&#29289;&#20307;&#36924;&#30495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#36924;&#30495;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#36866;&#24212;LiDAR&#22330;&#26223;&#29983;&#25104;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LiDAR&#25193;&#25955;&#27169;&#22411;&#65288;LiDMs&#65289;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#27969;&#31243;&#20013;&#24341;&#20837;&#20960;&#20309;&#20808;&#39564;&#65292;&#20174;&#32780;&#29983;&#25104;&#20855;&#26377;LiDAR&#36924;&#30495;&#25928;&#26524;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00815v1 Announce Type: cross  Abstract: Diffusion models (DMs) excel in photo-realistic image synthesis, but their adaptation to LiDAR scene generation poses a substantial hurdle. This is primarily because DMs operating in the point space struggle to preserve the curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of their representation power. In this paper, we propose LiDAR Diffusion Models (LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to capture the realism of LiDAR scenes by incorporating geometric priors into the learning pipeline. Our method targets three major desiderata: pattern realism, geometry realism, and object realism. Specifically, we introduce curve-wise compression to simulate real-world LiDAR patterns, point-wise coordinate supervision to learn scene geometry, and patch-wise encoding for a full 3D object context. With these three core designs, our method achieves competitive performance on unconditional LiDAR g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00044</link><description>&lt;p&gt;
UAlign: &#26080;&#27169;&#26495;&#21270;&#30340;&#38750;&#30417;&#30563;&#24335;SMILES&#23545;&#40784;&#25512;&#21160;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#35268;&#21010;&#22312;&#26377;&#26426;&#21270;&#24037;&#34892;&#19994;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#21333;&#27493;&#36870;&#21512;&#25104;&#39044;&#27979;&#26159;&#35268;&#21010;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#36817;&#24180;&#26469;&#30001;&#20110;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#27493;&#39588;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#31243;&#24230;&#30340;&#39069;&#22806;&#21270;&#23398;&#30693;&#35782;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UAlign&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21040;&#24207;&#21015;&#30340;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#31649;&#32447;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#12290;&#22522;&#20110;&#20998;&#23376;&#32467;&#26500;&#22312;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#20197;&#29983;&#25104;&#21453;&#24212;&#29289;&#12290;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.17101</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65306;&#19968;&#20010;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
AI Consciousness is Inevitable: A Theoretical Computer Science Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26426;&#22120;&#27169;&#22411;&#65292;&#25903;&#25345;&#20102;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#36825;&#19968;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;&#24847;&#35782;&#65292;&#36825;&#26159;&#25968;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#35745;&#31639;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#20026;&#24847;&#35782;&#24320;&#21457;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26426;&#22120;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#21463;&#21040;&#20102;&#33406;&#20262;&#183;&#22270;&#28789;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#20271;&#32435;&#24503;&#183;&#24052;&#23572;&#26031;&#24847;&#35782;&#21095;&#22330;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#38750;&#24120;&#31616;&#21333;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#39640;&#23618;&#27425;&#19978;&#19982;&#35768;&#22810;&#20851;&#20110;&#20154;&#31867;&#21644;&#21160;&#29289;&#24847;&#35782;&#30340;&#20027;&#35201;&#31185;&#23398;&#29702;&#35770;&#30456;&#19968;&#33268;&#65292;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#26029;&#65306;&#26426;&#22120;&#24847;&#35782;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17101v1 Announce Type: new  Abstract: We look at consciousness through the lens of Theoretical Computer Science, a branch of mathematics that studies computation under resource limitations. From this perspective, we develop a formal machine model for consciousness. The model is inspired by Alan Turing's simple yet powerful model of computation and Bernard Baars' theater model of consciousness. Though extremely simple, the model aligns at a high level with many of the major scientific theories of human and animal consciousness, supporting our claim that machine consciousness is inevitable.
&lt;/p&gt;</description></item><item><title>Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.16427</link><description>&lt;p&gt;
Re2LLM: &#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16427
&lt;/p&gt;
&lt;p&gt;
Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#26085;&#30410;&#34987;&#30475;&#20316;&#26159;&#22686;&#24378;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;(SBR)&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#19982;SBR&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#22240;&#32570;&#20047;&#20219;&#21153;&#29305;&#23450;&#21453;&#39304;&#32780;&#38590;&#20197;&#25214;&#21040;&#24341;&#23548;LLMs&#27491;&#30830;&#25512;&#29702;&#30340;&#26368;&#20339;&#25552;&#31034;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#20339;&#12290;&#23613;&#31649;&#21518;&#32773;&#35797;&#22270;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24494;&#35843;LLMs&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20381;&#36182;&#24320;&#28304;&#39592;&#24178;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SBR&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Re2LLM)&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21453;&#23556;&#24335;&#25506;&#32034;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#35266;&#23519;&#21040;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#65292;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#21644;&#20419;&#36827;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.00567</link><description>&lt;p&gt;
&#25674;&#24179;&#38271;&#31243;&#20002;&#22833;&#26223;&#35266;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00567
&lt;/p&gt;
&lt;p&gt;
&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#35266;&#23519;&#21040;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#65292;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#21644;&#20419;&#36827;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;CDFSL&#65289;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#30340;&#20016;&#23500;&#35757;&#32451;&#26679;&#26412;&#36716;&#31227;&#20808;&#21069;&#30693;&#35782;&#65292;&#20197;&#20174;&#30446;&#26631;&#22495;&#30340;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#12290;&#26412;&#25991;&#38024;&#23545;CDFSL&#22312;&#36328;&#19981;&#21516;&#39046;&#22495;&#20256;&#36755;&#30693;&#35782;&#21644;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#24494;&#35843;&#27169;&#22411;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23558;&#20998;&#26512;&#25439;&#22833;&#26223;&#35266;&#20174;&#21442;&#25968;&#31354;&#38388;&#25193;&#23637;&#21040;&#34920;&#31034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#21516;&#26102;&#35299;&#37322;CDFSL&#27169;&#22411;&#30340;&#20256;&#36755;&#21644;&#24494;&#35843;&#22256;&#38590;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#34920;&#31034;&#31354;&#38388;&#25439;&#22833;&#26223;&#35266;&#20013;&#30340;&#23574;&#38160;&#26497;&#23567;&#20540;&#23548;&#33268;&#38590;&#20197;&#36716;&#31227;&#21644;&#24494;&#35843;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#24179;&#22374;&#24615;&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#30701;&#31243;&#24179;&#22374;&#24615;&#32780;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#22686;&#24378;&#21487;&#36716;&#31227;&#24615;&#24182;&#20419;&#36827;&#24494;&#35843;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00567v1 Announce Type: cross  Abstract: Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited training data in the target domain by leveraging prior knowledge transferred from source domains with abundant training samples. CDFSL faces challenges in transferring knowledge across dissimilar domains and fine-tuning models with limited training data. To address these challenges, we initially extend the analysis of loss landscapes from the parameter space to the representation space, which allows us to simultaneously interpret the transferring and fine-tuning difficulties of CDFSL models. We observe that sharp minima in the loss landscapes of the representation space result in representations that are hard to transfer and fine-tune. Moreover, existing flatness-based methods have limited generalization ability due to their short-range flatness. To enhance the transferability and facilitate fine-tuning, we introduce a simple yet effective approach to achieve
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.14891</link><description>&lt;p&gt;
LLMBind: &#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMBind: A Unified Modality-Task Integration Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#27169;&#24577;&#20219;&#21153;&#30340;&#28789;&#27963;&#36755;&#20837;&#21644;&#36755;&#20986;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#38598;&#25104;&#33021;&#21147;&#26377;&#38480;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24102;&#22836;&#25506;&#32034;&#24182;&#25552;&#20986;&#20102;LLMBind&#65292;&#19968;&#31181;&#29992;&#20110;&#27169;&#24577;&#20219;&#21153;&#38598;&#25104;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#27169;&#22411;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;LLMBind&#21487;&#20197;&#20197;&#22810;&#31181;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#35299;&#37322;&#36755;&#20837;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#21516;&#19987;&#23478;&#20043;&#38388;&#30340;&#21327;&#20316;&#23454;&#29616;&#19981;&#21516;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26377;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;40&#19975;&#26465;&#25351;&#20196;&#25968;&#25454;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35299;&#38145;&#20102;&#20132;&#20114;&#24335;&#35270;&#35273;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14891v1 Announce Type: cross  Abstract: While recent progress in multimodal large language models tackles various modality tasks, they posses limited integration capabilities for complex multi-modality tasks, consequently constraining the development of the field. In this work, we take the initiative to explore and propose the LLMBind, a unified framework for modality task integration, which binds Large Language Models and corresponding pre-trained task models with task-specific tokens. Consequently, LLMBind can interpret inputs and produce outputs in versatile combinations of image, text, video, and audio. Specifically, we introduce a Mixture-of-Experts technique to enable effective learning for different multimodal tasks through collaboration among diverse experts. Furthermore, we create a multi-task dataset comprising 400k instruction data, which unlocks the ability for interactive visual generation and editing tasks. Extensive experiments show the effectiveness of our fr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08174</link><description>&lt;p&gt;
&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#22270;&#24418;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#23545;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#34920;&#24615;&#33410;&#28857;&#65288;&#31216;&#20026;&#22320;&#26631;&#65289;&#26469;&#34920;&#31034;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#23569;&#37327;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#65292;&#23427;&#20204;&#20316;&#20026;&#33410;&#28857;&#20301;&#32622;&#30340;&#21442;&#32771;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#31574;&#30053;&#23545;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;&#28041;&#21450;&#22320;&#26631;&#30340;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#19978;&#30028;&#12290;&#22312;&#24130;&#24459;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22320;&#26631;&#20026;&#33410;&#28857;&#20043;&#38388;&#36317;&#31163;&#25552;&#20379;&#20102;&#28176;&#36817;&#23436;&#20840;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#65288;HPLC&#65289;&#26041;&#27861;&#12290;HPLC&#23558;&#22320;&#26631;&#36873;&#25321;&#21644;&#22270;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#22270;&#34987;&#20998;&#21106;&#20026;&#36830;&#36890;&#23494;&#38598;&#30340;&#32858;&#31867;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#12290;HPLC&#21033;&#29992;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#33410;&#28857;&#20301;&#32622;&#20449;&#24687;&#30340;&#23618;&#32423;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#36890;&#29992;&#33021;&#21147;&#30340;&#21457;&#23637;&#12289;&#24615;&#33021;&#20445;&#38556;&#12289;&#30446;&#26631;&#23545;&#40784;&#12289;&#24191;&#27867;&#24212;&#29992;&#12289;&#32463;&#27982;&#21464;&#38761;&#12289;&#20840;&#27665;&#21442;&#19982;&#12289;&#31038;&#20250;&#36127;&#36131;&#20219;&#37096;&#32626;&#12289;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#12289;&#25216;&#26415;&#27835;&#29702;&#21644;&#21746;&#23398;&#21464;&#38761;&#31649;&#29702;&#12290;&#35201;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04464</link><description>&lt;p&gt;
&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ten Hard Problems in Artificial Intelligence We Must Get Right
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#21313;&#20010;&#38590;&#39064;&#65292;&#21253;&#25324;&#36890;&#29992;&#33021;&#21147;&#30340;&#21457;&#23637;&#12289;&#24615;&#33021;&#20445;&#38556;&#12289;&#30446;&#26631;&#23545;&#40784;&#12289;&#24191;&#27867;&#24212;&#29992;&#12289;&#32463;&#27982;&#21464;&#38761;&#12289;&#20840;&#27665;&#21442;&#19982;&#12289;&#31038;&#20250;&#36127;&#36131;&#20219;&#37096;&#32626;&#12289;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#12289;&#25216;&#26415;&#27835;&#29702;&#21644;&#21746;&#23398;&#21464;&#38761;&#31649;&#29702;&#12290;&#35201;&#35299;&#20915;&#36825;&#20123;&#38590;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;AI2050&#20013;&#38459;&#30861;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#24341;&#21457;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#30340;"&#38590;&#39064;"&#65306;&#65288;1&#65289;&#21457;&#23637;&#31995;&#32479;&#30340;&#36890;&#29992;&#33021;&#21147;&#65307;&#65288;2&#65289;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20854;&#35757;&#32451;&#36807;&#31243;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#23558;&#31995;&#32479;&#30446;&#26631;&#19982;&#20154;&#31867;&#30446;&#26631;&#23545;&#40784;&#65307;&#65288;4&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65307;&#65288;5&#65289;&#24212;&#23545;&#32463;&#27982;&#21464;&#38761;&#65307;&#65288;6&#65289;&#30830;&#20445;&#20840;&#27665;&#21442;&#19982;&#65307;&#65288;7&#65289;&#21516;&#26102;&#30830;&#20445;&#31038;&#20250;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65307;&#65288;8&#65289;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#24341;&#21457;&#30340;&#22320;&#32536;&#25919;&#27835;&#21464;&#38761;&#65307;&#65288;9&#65289;&#25512;&#21160;&#23545;&#25216;&#26415;&#30340;&#20581;&#20840;&#27835;&#29702;&#65307;&#20197;&#21450;&#65288;10&#65289;&#31649;&#29702;&#29983;&#27963;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#20013;&#30340;&#21746;&#23398;&#21464;&#38761;&#12290;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#30456;&#20851;&#39046;&#22495;&#65292;&#25351;&#20986;&#20102;&#37325;&#35201;&#30340;&#26368;&#36817;&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#25512;&#36827;&#30340;&#26041;&#24335;&#12290;[&#27880;&#65306;&#26412;&#35770;&#25991;&#22238;&#39038;&#30340;&#25991;&#29486;&#26102;&#38388;&#25130;&#33267;2023&#24180;1&#26376;&#12290;]
&lt;/p&gt;
&lt;p&gt;
We explore the AI2050 "hard problems" that block the promise of AI and cause AI risks: (1) developing general capabilities of the systems; (2) assuring the performance of AI systems and their training processes; (3) aligning system goals with human goals; (4) enabling great applications of AI in real life; (5) addressing economic disruptions; (6) ensuring the participation of all; (7) at the same time ensuring socially responsible deployment; (8) addressing any geopolitical disruptions that AI causes; (9) promoting sound governance of the technology; and (10) managing the philosophical disruptions for humans living in the age of AI. For each problem, we outline the area, identify significant recent work, and suggest ways forward. [Note: this paper reviews literature through January 2023.]
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65306;&#36827;&#23637;&#19982;&#25361;&#25112;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Model based Multi-Agents: A Survey of Progress and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01680
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31687;&#32508;&#36848;&#32473;&#20986;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#37325;&#35201;&#26041;&#38754;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;LLMs&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23427;&#20204;&#34987;&#29992;&#20316;&#33258;&#20027;&#26234;&#33021;&#20307;&#26469;&#33258;&#21160;&#23436;&#25104;&#35768;&#22810;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#23558;&#19968;&#20010;LLM&#29992;&#20316;&#21333;&#20010;&#35268;&#21010;&#25110;&#20915;&#31574;&#26234;&#33021;&#20307;&#30340;&#21457;&#23637;&#65292;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#22797;&#26434;&#38382;&#39064;&#27714;&#35299;&#21644;&#19990;&#30028;&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#20026;&#31038;&#21306;&#25552;&#20379;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#39046;&#22495;&#30340;&#32508;&#36848;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31687;&#32508;&#36848;&#25991;&#31456;&#65292;&#28145;&#20837;&#35752;&#35770;&#20102;&#22522;&#20110;LLMs&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#22522;&#26412;&#26041;&#38754;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35753;&#35835;&#32773;&#23545;&#20197;&#19979;&#38382;&#39064;&#33719;&#24471;&#23454;&#36136;&#24615;&#35265;&#35299;&#65306;LLM-based&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#21738;&#20123;&#39046;&#22495;&#21644;&#29615;&#22659;&#65311;&#36825;&#20123;&#26234;&#33021;&#20307;&#26159;&#22914;&#20309;&#24314;&#27169;&#21644;&#36890;&#20449;&#30340;&#65311;&#20160;&#20040;&#26426;&#21046;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22686;&#38271;&#65311;&#23545;&#20110;&#37027;&#20123;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#20154;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#19968;&#20123;&#35201;&#28857;&#21644;&#25361;&#25112;.
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize t
&lt;/p&gt;</description></item><item><title>Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.03781</link><description>&lt;p&gt;
Lite-Mind: &#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Lite-Mind: Towards Efficient and Robust Brain Representation Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03781
&lt;/p&gt;
&lt;p&gt;
Lite-Mind&#26088;&#22312;&#35299;&#20915;fMRI&#35299;&#30721;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#31283;&#20581;&#30340;&#33041;&#34920;&#31034;&#32593;&#32476;&#65292;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#35774;&#22791;&#19978;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#37096;&#32626;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;fMRI&#26041;&#27861;&#35299;&#30721;&#22823;&#33041;&#20013;&#30340;&#35270;&#35273;&#20449;&#24687;&#30340;&#30740;&#31350;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#12290;&#25361;&#25112;&#22312;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;fMRI&#20449;&#21495;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#23548;&#33268;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#30340;&#20302;&#31934;&#24230;&#12290;MindEye&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#39640;&#21442;&#25968;&#35745;&#25968;&#30340;&#28145;&#24230;MLP&#65288;&#27599;&#20010;&#21463;&#35797;&#32773;&#30340;996M MLP&#20027;&#24178;&#65289;&#23558;fMRI&#23884;&#20837;&#23545;&#40784;&#21040;CLIP&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26368;&#32456;&#38544;&#34255;&#23618;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;fMRI&#21040;&#22270;&#20687;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20869;&#65292;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#26174;&#30528;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#38656;&#35201;&#35757;&#32451;&#29305;&#23450;&#20110;&#21463;&#35797;&#32773;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#22823;&#37327;&#30340;&#21442;&#25968;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#37096;&#32626;fMRI&#35299;&#30721;&#26102;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38656;&#35201;&#20026;&#27599;&#20010;&#21463;&#35797;&#32773;&#25552;&#20379;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03781v2 Announce Type: replace-cross  Abstract: Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;</title><link>https://arxiv.org/abs/2312.03631</link><description>&lt;p&gt;
&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Open-Vocabulary Caption Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#35299;&#20915;&#22270;&#20687;&#23383;&#24149;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MOCHa&#26469;&#32531;&#35299;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#26465;&#20214;&#30340;&#25991;&#26412;&#29983;&#25104;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#22270;&#20687;&#23383;&#24149;&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#19982;&#32473;&#23450;&#22270;&#20687;&#26080;&#27861;&#25512;&#26029;&#30340;&#34394;&#20551;&#32454;&#33410;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22270;&#20687;&#23383;&#24149;&#20013;&#22823;&#22810;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#23545;&#35937;&#21015;&#34920;&#26469;&#32531;&#35299;&#25110;&#35780;&#20272;&#24187;&#35273;&#65292;&#24573;&#30053;&#20102;&#23454;&#36341;&#20013;&#21457;&#29983;&#30340;&#22823;&#22810;&#25968;&#24187;&#35273;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#24320;&#25918;&#35789;&#27719;&#35774;&#32622;&#20013;&#22270;&#20687;&#23383;&#24149;&#20013;&#30340;&#24187;&#35273;&#65292;&#21253;&#25324;&#37327;&#21270;&#23427;&#20204;&#30340;&#23384;&#22312;&#24182;&#20248;&#21270;&#20197;&#20943;&#36731;&#36825;&#31181;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;OpenCHAIR&#22522;&#20934;&#21033;&#29992;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#26469;&#35780;&#20272;&#24320;&#25918;&#35789;&#27719;&#25551;&#36848;&#24187;&#35273;&#65292;&#22312;&#22810;&#26679;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#27969;&#34892;&#30340;CHAIR&#22522;&#20934;&#12290;&#20026;&#20102;&#22312;&#24207;&#21015;&#32423;&#21035;&#19978;&#32531;&#35299;&#24320;&#25918;&#35789;&#27719;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MOCHa&#65292;&#19968;&#31181;&#21033;&#29992;&#36827;&#23637;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03631v2 Announce Type: replace-cross  Abstract: While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, namely, the generation of spurious details that cannot be inferred from the given image. Existing methods largely use closed-vocabulary object lists to mitigate or evaluate hallucinations in image captioning, ignoring most types of hallucinations that occur in practice. To this end, we propose a framework for addressing hallucinations in image captioning in the open-vocabulary setting, including quantifying their presence and optimizing to mitigate such hallucinations. Our OpenCHAIR benchmark leverages generative foundation models to evaluate open-vocabulary caption hallucinations, surpassing the popular CHAIR benchmark in both diversity and accuracy. To mitigate open-vocabulary hallucinations at the sequence level, we propose MOCHa, an approach harnessing advancements in
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.03056</link><description>&lt;p&gt;
LitSumm&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
LitSumm: Large language models for literature summarisation of non-coding RNAs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03056
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Motivation: &#22312;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#25972;&#29702;&#24037;&#20316;&#20013;&#65292;&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;&#21457;&#24067;&#36895;&#24230;&#30340;&#25345;&#32493;&#22686;&#21152;&#65292;&#20877;&#21152;&#19978;&#20840;&#29699;&#22266;&#23450;&#25968;&#37327;&#30340;&#31574;&#23637;&#20154;&#21592;&#65292;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#24211;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#24456;&#23569;&#26377;&#30693;&#35782;&#24211;&#26377;&#36164;&#28304;&#21487;&#20197;&#25193;&#23637;&#21040;&#25152;&#26377;&#30456;&#20851;&#25991;&#29486;&#65292;&#32780;&#25152;&#26377;&#30693;&#35782;&#24211;&#37117;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#33258;&#24049;&#30340;&#21162;&#21147;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#38750;&#32534;&#30721;RNA&#29983;&#25104;&#25991;&#29486;&#25688;&#35201;&#65292;&#39318;&#27425;&#20943;&#36731;&#20102;RNA&#31185;&#23398;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21830;&#19994;LLM&#21644;&#19968;&#31995;&#21015;&#25552;&#31034;&#21644;&#26816;&#26597;&#20174;&#25991;&#29486;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#20107;&#23454;&#20934;&#30830;&#30340;&#25688;&#35201;&#21450;&#20934;&#30830;&#30340;&#24341;&#29992;&#12290;&#20154;&#24037;&#35780;&#20272;&#38024;&#23545;&#25688;&#35201;&#23376;&#38598;&#36827;&#34892;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#34987;&#35780;&#20026;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#26368;&#24120;&#29992;&#30340;&#33258;&#21160;&#21270;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03056v2 Announce Type: replace-cross  Abstract: Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated e
&lt;/p&gt;</description></item><item><title>&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2209.01621</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interactive Question Answering Systems: Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.01621
&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#26159;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#32467;&#21512;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#38382;&#24182;&#19982;&#31995;&#32479;&#21160;&#24577;&#20132;&#20114;&#65292;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#36328;  &#25688;&#35201;: &#38382;&#31572;&#31995;&#32479;&#34987;&#20844;&#35748;&#20026;&#22312;&#32593;&#32476;&#19978;&#23547;&#27714;&#20449;&#24687;&#30340;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25163;&#27573;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#20449;&#24687;&#23547;&#25214;&#32773;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#20986;&#38382;&#39064;&#26469;&#33719;&#24471;&#31616;&#27905;&#30340;&#22238;&#31572;&#12290;&#20132;&#20114;&#24335;&#38382;&#31572;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#24182;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20301;&#20110;&#38382;&#31572;&#21644;&#23545;&#35805;&#31995;&#32479;&#30340;&#20132;&#38598;&#22788;&#12290;&#19968;&#26041;&#38754;&#65292;&#29992;&#25143;&#21487;&#20197;&#29992;&#26222;&#36890;&#35821;&#35328;&#25552;&#38382;&#24182;&#25214;&#21040;&#22905;&#38382;&#39064;&#30340;&#23454;&#38469;&#22238;&#31572;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21021;&#22987;&#35831;&#27714;&#20013;&#23384;&#22312;&#22810;&#20010;&#21487;&#33021;&#30340;&#22238;&#22797;&#12289;&#24456;&#23569;&#25110;&#27169;&#26865;&#20004;&#21487;&#65292;&#31995;&#32479;&#21487;&#20197;&#23558;&#38382;&#31572;&#20250;&#35805;&#24310;&#38271;&#20026;&#23545;&#35805;&#12290;&#36890;&#36807;&#20801;&#35768;&#29992;&#25143;&#25552;&#20986;&#26356;&#22810;&#38382;&#39064;&#65292;&#20132;&#20114;&#24335;&#38382;&#31572;&#20351;&#29992;&#25143;&#33021;&#22815;&#21160;&#24577;&#22320;&#19982;&#31995;&#32479;&#20132;&#20114;&#24182;&#33719;&#24471;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20132;&#20114;&#24335;&#38382;&#31572;&#31995;&#32479;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.01621v2 Announce Type: replace-cross  Abstract: Question answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their query by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to dynamically interact with the system and receive more precise results. This survey offers a detailed overview of the interactive question-ans
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.16669</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#20026;&#22825;&#27668;&#39044;&#25253;&#24102;&#26469;&#20102;&#31532;&#20108;&#27425;&#38761;&#21629;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16669
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#23548;&#33268;&#20102;&#20960;&#31181;&#22823;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#20102;&#22825;&#27668;&#39044;&#25253;&#21487;&#33021;&#36814;&#26469;&#31532;&#20108;&#27425;&#38761;&#21629;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#39044;&#25253;&#27169;&#22411;&#30340;&#28436;&#21464;&#65292;&#24182;&#22312;&#30830;&#23450;&#30340;&#20849;&#21516;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#21457;&#23637;&#30340;&#8220;&#19977;&#22823;&#35268;&#21017;&#8221;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38761;&#21629;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#31616;&#35201;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#26410;&#26469;&#21457;&#23637;&#21069;&#26223;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#23558;&#25972;&#20010;&#25968;&#20540;&#39044;&#25253;&#36807;&#31243;&#36827;&#34892;&#25972;&#21512;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20854;&#20182;&#20449;&#24687;&#32508;&#21512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14724</link><description>&lt;p&gt;
&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#30340;&#35843;&#26597;&#65306;&#24517;&#35201;&#24615;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22797;&#26434;&#35821;&#35328;&#30340;&#24378;&#22823;&#33021;&#21147;&#20351;&#24471;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#28044;&#20837;&#21040;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#24182;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#25509;&#21463;&#12290;&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#25193;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;LLMs&#28508;&#22312;&#30340;&#35823;&#29992;&#65292;&#24182;&#20445;&#25252;&#33402;&#26415;&#34920;&#36798;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#39046;&#22495;&#20813;&#21463;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#23475;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#65292;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#26816;&#27979;&#22120;&#25216;&#26415;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25512;&#21160;&#22240;&#32032;&#21253;&#25324;&#27700;&#21360;&#25216;&#26415;&#12289;&#38646;&#26679;&#26412;&#26041;&#27861;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12289;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12289;&#23558;LLMs&#20316;&#20026;&#26816;&#27979;&#22120;&#20197;&#21450;&#20154;&#31867;&#36741;&#21161;&#26041;&#27861;&#30340;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#30740;&#31350;&#31361;&#30772;&#65292;&#24182;&#24378;&#35843;&#20102;&#36843;&#20999;&#30340;&#38656;&#27714;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressin
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00074</link><description>&lt;p&gt;
SocREval&#65306;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation. (arXiv:2310.00074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SocREval&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;GPT-4&#21644;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#36827;&#34892;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22797;&#26434;&#25512;&#29702;&#27169;&#22411;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#24403;&#21069;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20197;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#35780;&#20272;&#23427;&#20204;&#30340;&#36880;&#27493;&#25512;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#35780;&#20272;&#25351;&#26631;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#35780;&#20272;&#27169;&#22411;&#23548;&#20986;&#30340;&#25512;&#29702;&#38142;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#20154;&#24037;&#32534;&#20889;&#30340;&#25512;&#29702;&#38142;&#21487;&#33021;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#24182;&#19988;&#20854;&#33719;&#21462;&#36890;&#24120;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#29616;&#26377;&#30340;&#26080;&#21442;&#32771;&#25512;&#29702;&#25351;&#26631;&#28040;&#38500;&#20102;&#20154;&#24037;&#21046;&#20316;&#25512;&#29702;&#38142;&#30340;&#38656;&#27714;&#20316;&#20026;&#21442;&#32771;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22312;&#20855;&#26377;&#20154;&#24037;&#25512;&#29702;&#38142;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#22797;&#26434;&#21270;&#20102;&#27969;&#31243;&#24182;&#24341;&#21457;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#33258;&#21160;&#35780;&#20272;&#25512;&#29702;&#38142;&#36136;&#37327;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#24037;&#21046;&#20316;&#21442;&#32771;&#30340;&#38656;&#27714;&#12290;&#21033;&#29992;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23450;&#21046;&#21270;&#25552;&#31034;&#26469;&#22686;&#24378;&#26080;&#21442;&#32771;&#25512;&#29702;&#35780;&#20272;&#65292;&#36825;&#23601;&#26159;&#25105;&#20204;&#31216;&#20043;&#20026;SocREval&#65288;&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comprehensively assess the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains to assess the model-derived chains. However, such ``gold-standard'' human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning metrics eliminate the need for human-crafted reasoning chains as references, but they typically require fine-tuning on datasets with human-derived reasoning chains, which complicates the process and raises concerns regarding generalizability across diverse datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, obviating the need for human-crafted references. Leveraging the Socratic method, we devise tailored prompts to enhance reference-free reasoning evaluation, which we term SocREval (Socratic metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.05927</link><description>&lt;p&gt;
&#38024;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26469;&#33258;&#29983;&#29289;&#20449;&#21495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#20154;&#20204;&#30340;&#36523;&#24515;&#29366;&#24577;&#36827;&#34892;&#32508;&#21512;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#28304;&#20110;&#20219;&#21153;&#35268;&#33539;&#30340;&#21464;&#21270;&#25110;&#32773;&#27169;&#24577;&#32452;&#21512;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#24863;&#30693;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;$\texttt{bio}$FAME&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;$\texttt{bio}$FAME&#21253;&#21547;&#19968;&#20010;&#39057;&#29575;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#19982;&#36755;&#20837;&#30340;&#38271;&#24230;&#21644;&#37319;&#26679;&#29575;&#26080;&#20851;&#12290;&#20026;&#20102;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;&#33258;&#32534;&#30721;&#12290;&#26368;&#32456;&#30340;&#26550;&#26500;&#26377;&#25928;&#22320;&#25429;&#33719;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#39057;&#29575;&#29305;&#24449;&#21644;&#27169;&#24577;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item></channel></rss>