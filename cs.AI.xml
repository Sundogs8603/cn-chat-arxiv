<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#21253;&#25324;513&#20159;&#20010;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.06619</link><description>&lt;p&gt;
Aya&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#25918;&#35775;&#38382;&#25910;&#34255;&#21697;
&lt;/p&gt;
&lt;p&gt;
Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#21253;&#25324;513&#20159;&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#35768;&#22810;&#31361;&#30772;&#30340;&#22522;&#30784;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#36817;&#30340;&#25104;&#23601;&#37117;&#24402;&#21151;&#20110;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21709;&#24212;&#25351;&#20196;&#12290;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#38656;&#35201;&#29305;&#21035;&#26500;&#24314;&#21644;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20960;&#20046;&#37117;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#36328;&#36234;65&#31181;&#35821;&#35328;&#30340;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#12290;&#25105;&#20204;&#19982;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;&#27969;&#21033;&#35828;&#32773;&#21512;&#20316;&#65292;&#25910;&#38598;&#25351;&#20196;&#21644;&#23436;&#25104;&#30340;&#33258;&#28982;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;114&#31181;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#27169;&#26495;&#21270;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#20849;&#26377;5.13&#20159;&#20010;&#23454;&#20363;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#36164;&#28304;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;Aya&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27169;&#26495;&#21270;&#21644;&#32763;&#35793;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#23558;&#20854;&#36328;&#36234;&#20102;114&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;"&#25991;&#26412;&#21040;&#35745;&#21010;"&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;LLMs&#29992;&#20110;&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#20197;&#21450;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20351;&#29992;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#21010;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.06608</link><description>&lt;p&gt;
TIC&#65306;&#21033;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#31934;&#30830;&#36827;&#34892;&#8220;&#25991;&#26412;&#21040;&#35745;&#21010;&#8221;&#30340;&#32763;&#35793;-&#25512;&#26029;-&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;"&#25991;&#26412;&#21040;&#35745;&#21010;"&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;LLMs&#29992;&#20110;&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#20197;&#21450;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20351;&#29992;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#21010;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#29983;&#25104;&#35745;&#21010;&#30340;&#38382;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35745;&#21010;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#35745;&#21010;&#24037;&#20855;&#22312;&#35745;&#21010;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#20351;&#29992;&#32467;&#26500;&#21270;&#35821;&#35328;&#65288;&#22914;Planning Domain Definition Language&#65288;PDDL&#65289;&#65289;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#65288;&#20219;&#21153;PDDL&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#35745;&#31639;&#35745;&#21010;&#12290;&#19982;&#30452;&#25509;&#20351;&#29992;LLMs&#29983;&#25104;&#20219;&#21153;PDDL&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65288;a&#65289;&#32763;&#35793;&#65306;&#20165;&#20351;&#29992;LLMs&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#36923;&#36753;&#21487;&#35299;&#37322;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#65288;b&#65289;&#25512;&#26029;&#65306;&#20351;&#29992;&#36923;&#36753;&#25512;&#29702;&#22120;&#65288;&#30446;&#21069;&#26159;Answer Set Programming solver&#65289;&#20174;&#20013;&#38388;&#34920;&#31034;&#20013;&#25512;&#23548;&#20986;&#39069;&#22806;&#30340;&#36923;&#36753;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#65288;c&#65289;&#32534;&#35793;&#65306;&#29983;&#25104;&#30446;&#26631;&#35745;&#21010;&#30340;PDDL&#25551;&#36848;&#30340;&#32534;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
&lt;/p&gt;</description></item><item><title>RQP-SGD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#37096;&#32626;&#30340;&#20302;&#20869;&#23384;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06606</link><description>&lt;p&gt;
RQP-SGD&#65306;&#36890;&#36807;&#22024;&#26434;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06606
&lt;/p&gt;
&lt;p&gt;
RQP-SGD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#37096;&#32626;&#30340;&#20302;&#20869;&#23384;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20013;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20852;&#36215;&#20419;&#20351;&#20102;&#23545;&#22312;&#36793;&#32536;&#37096;&#32626;&#23454;&#26102;&#12289;&#39640;&#25928;&#12289;&#23433;&#20840;&#25968;&#25454;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#23454;&#20540;&#26435;&#37325;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#22312;&#22823;&#22411;&#27169;&#22411;&#19978;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#20351;&#29992;&#37327;&#21270;&#31163;&#25955;&#26435;&#37325;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#20302;&#32500;&#27169;&#22411;&#20063;&#38656;&#35201;&#20445;&#25252;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RQP-SGD&#65292;&#19968;&#31181;&#29992;&#20110;&#20302;&#20869;&#23384;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#38544;&#31169;&#20445;&#25252;&#37327;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#19982;&#38543;&#26426;&#37327;&#21270;&#30456;&#32467;&#21512;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#21487;&#34913;&#37327;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#20984;&#30446;&#26631;&#21644;&#37327;&#21270;&#32422;&#26463;&#30340;ML&#20219;&#21153;&#19978;&#23454;&#26045;RQP-SGD&#30340;&#25928;&#29992;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of IoT devices has prompted the demand for deploying machine learning at-the-edge with real-time, efficient, and secure data processing. In this context, implementing machine learning (ML) models with real-valued weight parameters can prove to be impractical particularly for large models, and there is a need to train models with quantized discrete weights. At the same time, these low-dimensional models also need to preserve privacy of the underlying dataset. In this work, we present RQP-SGD, a new approach for privacy-preserving quantization to train machine learning models for low-memory ML-at-the-edge. This approach combines differentially private stochastic gradient descent (DP-SGD) with randomized quantization, providing a measurable privacy guarantee in machine learning. In particular, we study the utility convergence of implementing RQP-SGD on ML tasks with convex objectives and quantization constraints and demonstrate its efficacy over deterministic quantization. Throug
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#22312;&#36229;&#20986;&#35757;&#32451;&#39046;&#22495;&#30340;&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#26144;&#23556;&#19981;&#36275;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#65292;&#20811;&#26381;&#27867;&#21270;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2402.06599</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Out-Of-Distribution Generalization of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06599
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#22312;&#36229;&#20986;&#35757;&#32451;&#39046;&#22495;&#30340;&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#26144;&#23556;&#19981;&#36275;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#65292;&#20811;&#26381;&#27867;&#21270;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#22312;&#22495;&#22806;&#22330;&#26223;&#21644;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#19979;&#65292;&#24403;&#21069;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#21512;&#25104;&#22270;&#20687;&#12289;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20559;&#31227;&#21644;&#21307;&#23398;&#20197;&#21450;&#20998;&#23376;&#22270;&#20687;&#31561;&#19987;&#19994;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;MLLMs&#22312;&#36229;&#20986;&#24120;&#35268;&#35757;&#32451;&#39046;&#22495;&#30340;&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#30452;&#25509;&#24212;&#29992;&#32780;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#12290;&#20026;&#20102;&#20102;&#35299;&#24615;&#33021;&#19981;&#21487;&#38752;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#20551;&#35774;&#36827;&#34892;&#20102;&#20998;&#26512;&#65306;&#35821;&#20041;&#38169;&#35823;&#35299;&#37322;&#12289;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#19981;&#36275;&#21644;&#26144;&#23556;&#19981;&#36275;&#12290;&#32467;&#26524;&#34920;&#26126;&#26144;&#23556;&#19981;&#36275;&#26159;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319; MLLMs &#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#20811;&#26381;&#27867;&#21270;&#38556;&#30861;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102; ICL &#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerabil
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;Android&#29615;&#22659;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;AndroidArena&#29615;&#22659;&#21644;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#20197;&#21450;&#38477;&#20302;&#20154;&#21147;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06596</link><description>&lt;p&gt;
&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;Android&#29615;&#22659;&#20013;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Understanding the Weakness of Large Language Model Agents within a Complex Android Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#22797;&#26434;&#30340;Android&#29615;&#22659;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;AndroidArena&#29615;&#22659;&#21644;&#22522;&#20934;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#20197;&#21450;&#38477;&#20302;&#20154;&#21147;&#25104;&#26412;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#26234;&#33021;&#20195;&#29702;&#36816;&#29992;&#21040;&#35832;&#22914;&#27983;&#35272;&#22120;&#21644;&#28216;&#25103;&#31561;&#39046;&#22495;&#29305;&#23450;&#36719;&#20214;&#19978;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#25805;&#20316;&#31995;&#32479;&#31561;&#36890;&#29992;&#36719;&#20214;&#31995;&#32479;&#26102;&#65292;LLM&#20195;&#29702;&#38754;&#20020;&#19977;&#22823;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#21160;&#20316;&#31354;&#38388;&#24191;&#38420;&#19988;&#21160;&#24577;&#65292;&#20351;LLM&#20195;&#29702;&#38590;&#20197;&#20445;&#25345;&#26356;&#26032;&#30340;&#29702;&#35299;&#21644;&#25552;&#20379;&#20934;&#30830;&#30340;&#22238;&#22797;&#12290;&#20854;&#27425;&#65292;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#32463;&#24120;&#38656;&#35201;&#24212;&#29992;&#38388;&#30340;&#21327;&#20316;&#65292;&#35201;&#27714;LLM&#20195;&#29702;&#20855;&#22791;&#36828;&#35265;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;&#31532;&#19977;&#65292;&#20195;&#29702;&#38656;&#35201;&#35782;&#21035;&#19982;&#29992;&#25143;&#32422;&#26463;&#26465;&#20214;&#65288;&#22914;&#23433;&#20840;&#38382;&#39064;&#21644;&#20559;&#22909;&#65289;&#30456;&#31526;&#30340;&#26368;&#20248;&#35299;&#12290;&#36825;&#20123;&#25361;&#25112;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#20102;AndroidArena&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#29616;&#20195;&#25805;&#20316;&#31995;&#32479;&#19978;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#29615;&#22659;&#21644;&#22522;&#20934;&#12290;&#20026;&#35299;&#20915;&#39640;&#20154;&#21147;&#25104;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#12289;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#22522;&#20934;&#12290;&#22312;&#20219;&#21153;&#35780;&#20272;&#20013;&#65292;AndroidArena&#37319;&#29992;&#20934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;&#25351;&#26631;&#26469;&#34917;&#20805;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have empowered intelligent agents to execute intricate tasks within domain-specific software such as browsers and games. However, when applied to general-purpose software systems like operating systems, LLM agents face three primary challenges. Firstly, the action space is vast and dynamic, posing difficulties for LLM agents to maintain an up-to-date understanding and deliver accurate responses. Secondly, real-world tasks often require inter-application cooperation}, demanding farsighted planning from LLM agents. Thirdly, agents need to identify optimal solutions aligning with user constraints, such as security concerns and preferences. These challenges motivate AndroidArena, an environment and benchmark designed to evaluate LLM agents on a modern operating system. To address high-cost of manpower, we design a scalable and semi-automated method to construct the benchmark. In the task evaluation, AndroidArena incorporates accurate and adaptive metrics to add
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;</title><link>https://arxiv.org/abs/2402.06590</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#65306;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Predictive representations: building blocks of intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06590
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#35268;&#23450;&#20102;&#20160;&#20040;&#26679;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#26159;&#26377;&#29992;&#30340;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#23427;&#20204;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#29702;&#35770;&#35266;&#28857;&#19982;&#35748;&#30693;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#32487;&#20219;&#32773;&#34920;&#24449;&#65288;SR&#65289;&#21450;&#20854;&#24191;&#20041;&#24418;&#24335;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#22823;&#33041;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#34701;&#21512;&#34920;&#26126;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
&lt;/p&gt;</description></item><item><title>G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.06584</link><description>&lt;p&gt;
G-SciEdBERT: &#29992;&#20110;&#24503;&#35821;&#31185;&#23398;&#35780;&#20272;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06584
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#20026;&#21508;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#24503;&#35821;&#20013;&#30340;&#24503;&#35821;BERT [G-BERT]&#65289;&#30340;&#33258;&#21160;&#35780;&#20998;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33258;&#21160;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#38382;&#39064;&#30340;&#20070;&#38754;&#22238;&#31572;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26631;&#20934;&#30340;G-BERT&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#31185;&#23398;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#23398;&#29983;&#30340;&#20889;&#20316;&#39118;&#26684;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65288;G-SciEdBERT&#65289;&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;G-BERT&#65292;&#22312;5M&#20010;&#26631;&#35760;&#30340;PISA 2015&#22269;&#38469;&#23398;&#29983;&#35780;&#20272;&#30340;50K&#20010;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;59&#20010;&#35780;&#20272;&#39033;&#30446;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#26816;&#26597;&#20102;&#35780;&#20998;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;G-BERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;G-SciEdBERT&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#20854;&#35780;&#20998;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.06563</link><description>&lt;p&gt;
&#21307;&#23398;&#30340;&#26263;&#29289;&#36136;&#20013;&#38544;&#34255;&#30528;&#20160;&#20040;&#65311;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#22788;&#29702;&#20002;&#22833;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#20154;&#35760;&#24405;&#65288;EPR&#65289;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#21253;&#21547;&#37325;&#35201;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#29702;&#35299;&#21644;&#22788;&#29702;&#36825;&#20123;&#32570;&#22833;&#25968;&#25454;&#26159;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#35299;&#20915;&#65292;&#21487;&#33021;&#23548;&#33268;&#20998;&#26512;&#20013;&#30340;&#20559;&#35265;&#21644;&#20851;&#38190;&#32467;&#35770;&#30340;&#25197;&#26354;&#12290;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#26377;&#20851;&#65292;&#23545;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#21487;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#32479;&#35745;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#35299;&#37322;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20013;&#24515;&#30340;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#20197;&#21450;&#33521;&#22269;&#26368;&#22823;&#30340;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65288;TARN&#65289;&#20013;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#12290;&#22312;&#23545;56,961&#20010;&#19982;&#20799;&#31461;&#24613;&#35786;&#37096;&#23601;&#35786;&#30456;&#20851;&#30340;&#21021;&#27493;&#29983;&#21629;&#20307;&#24449;&#21644;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20002;&#22833;&#25968;&#25454;&#24456;&#21487;&#33021;&#26159;&#38750;&#38543;&#26426;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic patient records (EPRs) produce a wealth of data but contain significant missing information. Understanding and handling this missing data is an important part of clinical data analysis and if left unaddressed could result in bias in analysis and distortion in critical conclusions. Missing data may be linked to health care professional practice patterns and imputation of missing data can increase the validity of clinical decisions. This study focuses on statistical approaches for understanding and interpreting the missing data and machine learning based clinical data imputation using a single centre's paediatric emergency data and the data from UK's largest clinical audit for traumatic injury database (TARN). In the study of 56,961 data points related to initial vital signs and observations taken on children presenting to an Emergency Department, we have shown that missing data are likely to be non-random and how these are linked to health care professional practice patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22916;&#24819;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#38454;&#28436;&#31639;&#27861;&#30340;&#38190;&#20540;&#29256;&#26412;&#65292;QBBN&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65292;&#20294;&#21487;&#20197;&#20351;&#29992;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.06557</link><description>&lt;p&gt;
&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65306;&#36923;&#36753;&#22270;&#27169;&#22411;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
The Quantified Boolean Bayesian Network: Theory and Experiments with a Logical Graphical Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22916;&#24819;&#38382;&#39064;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#38454;&#28436;&#31639;&#27861;&#30340;&#38190;&#20540;&#29256;&#26412;&#65292;QBBN&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65292;&#20294;&#21487;&#20197;&#20351;&#29992;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#21270;&#24067;&#23572;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;QBBN&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#36923;&#36753;&#21644;&#27010;&#29575;&#25512;&#29702;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;QBBN&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#21363;LLM&#20250;&#20986;&#29616;&#22916;&#24819;&#29616;&#35937;&#12290;&#30001;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26500;&#24314;&#26041;&#24335;&#65292;&#23427;&#26080;&#27861;&#20135;&#29983;&#22916;&#24819;&#65292;&#22240;&#20026;&#23427;&#21482;&#33021;&#36820;&#22238;&#21487;&#20197;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#37197;&#32622;&#19968;&#20010;&#21547;&#26377;&#26080;&#38480;&#25968;&#37327;&#24067;&#23572;&#21464;&#37327;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#31867;&#35821;&#35328;&#32972;&#21518;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#31181;&#38190;-&#20540;&#29256;&#26412;&#30340;&#19968;&#38454;&#28436;&#31639;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#20854;&#19968;&#33268;&#24615;&#21644;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#23436;&#20840;&#35266;&#27979;&#25968;&#25454;&#19978;&#26159;&#26131;&#20110;&#35757;&#32451;&#30340;&#65292;&#20294;&#25512;&#29702;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#31934;&#30830;&#25512;&#29702;&#26159;&#19981;&#21487;&#35299;&#30340;&#65288;&#21363;$N$&#20010;&#21464;&#37327;&#30340;&#25512;&#29702;&#22797;&#26434;&#24230;&#20026;$\Omega(2^N)$&#65289;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24490;&#29615;&#20449;&#24565;&#20256;&#25773;&#65288;LBP&#65289;&#30340;&#20351;&#29992;&#65292;&#23427;&#24182;&#19981;...
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Quantified Boolean Bayesian Network (QBBN), which provides a unified view of logical and probabilistic reasoning. The QBBN is meant to address a central problem with the Large Language Model (LLM), which has become extremely popular in Information Retrieval, which is that the LLM hallucinates. A Bayesian Network, by construction, cannot hallucinate, because it can only return answers that it can explain. We show how a Bayesian Network over an unbounded number of boolean variables can be configured to represent the logical reasoning underlying human language. We do this by creating a key-value version of the First-Order Calculus, for which we can prove consistency and completeness. We show that the model is trivially trained over fully observed data, but that inference is non-trivial. Exact inference in a Bayesian Network is intractable (i.e. $\Omega(2^N)$ for $N$ variables). For inference, we investigate the use of Loopy Belief Propagation (LBP), which is not 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;LLaMA&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#24335;&#65292;&#22312;CASE 2024&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#21644;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.06549</link><description>&lt;p&gt;
Bryndza&#22312;ClimateActivism 2024&#19978;: &#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-4&#21644;LLaMA&#36827;&#34892;&#31435;&#22330;&#12289;&#30446;&#26631;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;LLaMA&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#24335;&#65292;&#22312;CASE 2024&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#21644;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;CASE 2024&#27668;&#20505;&#34892;&#21160;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#20316;&#20026;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#38646;&#27425;&#25110;&#23569;&#27425;&#35757;&#32451;&#24773;&#20917;&#19979;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#26469;&#36827;&#34892;&#25512;&#29305;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;LLM&#33021;&#21542;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;LLaMA&#30340;&#28040;&#34701;&#30740;&#31350;&#20197;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/NaiveNeuron/bryndza-case-2024&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study details our approach for the CASE 2024 Shared Task on Climate Activism Stance and Hate Event Detection, focusing on Hate Speech Detection, Hate Speech Target Identification, and Stance Detection as classification challenges. We explored the capability of Large Language Models (LLMs), particularly GPT-4, in zero- or few-shot settings enhanced by retrieval augmentation and re-ranking for Tweet classification. Our goal was to determine if LLMs could match or surpass traditional methods in this context.   We conducted an ablation study with LLaMA for comparison, and our results indicate that our models significantly outperformed the baselines, securing second place in the Target Detection task. The code for our submission is available at https://github.com/NaiveNeuron/bryndza-case-2024
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06532</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#29992;&#20110;&#20195;&#29702;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Bayesian Optimization for Surrogate Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#65292;&#23558;&#20248;&#21270;&#36712;&#36857;&#38480;&#21046;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#27169;&#22411;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#20013;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#19981;&#26597;&#35810;&#30495;&#23454;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#20248;&#21270;&#23398;&#20064;&#21040;&#30340;&#20195;&#29702;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#32463;&#24120;&#36935;&#21040;&#20195;&#29702;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#29983;&#25104;&#23545;&#25239;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;GABO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;Lipschitz&#26377;&#30028;&#28304;&#25209;&#35780;&#23478;&#27169;&#22411;&#26469;&#32422;&#26463;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#22312;&#20195;&#29702;&#20989;&#25968;&#21487;&#38752;&#30340;&#21306;&#22495;&#20869;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36830;&#32493;&#36755;&#20837;&#31354;&#38388;&#20808;&#39564;&#30340;&#19968;&#23450;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21160;&#24577;&#35843;&#25972;&#28304;&#25209;&#35780;&#23478;&#27491;&#21017;&#21270;&#30340;&#24378;&#24230;&#12290;&#22312;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#30340;&#22810;&#20010;&#31163;&#32447;&#20248;&#21270;&#20219;&#21153;&#20013;&#65292;GABO&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/michael-s-yao/gabo &#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline model-based policy optimization seeks to optimize a learned surrogate objective function without querying the true oracle objective during optimization. However, inaccurate surrogate model predictions are frequently encountered along the optimization trajectory. To address this limitation, we propose generative adversarial Bayesian optimization (GABO) using adaptive source critic regularization, a task-agnostic framework for Bayesian optimization that employs a Lipschitz-bounded source critic model to constrain the optimization trajectory to regions where the surrogate function is reliable. We show that under certain assumptions for the continuous input space prior, our algorithm dynamically adjusts the strength of the source critic regularization. GABO outperforms existing baselines on a number of different offline optimization tasks across a variety of scientific domains. Our code is available at https://github.com/michael-s-yao/gabo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20026;&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#24182;&#19988;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.06509</link><description>&lt;p&gt;
&#22312;&#21512;&#36866;&#30340;&#26102;&#38388;&#25552;&#20986;&#27491;&#30830;&#30340;&#38382;&#39064;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#30340;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20026;&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#24182;&#19988;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28548;&#28165;&#38382;&#39064;&#26159;&#19968;&#31181;&#22312;&#35821;&#35328;&#20351;&#29992;&#20013;&#34920;&#36798;&#35823;&#35299;&#12289;&#27495;&#20041;&#21644;&#26410;&#26126;&#31034;&#30340;&#37325;&#35201;&#23545;&#35805;&#24037;&#20855;&#12290;&#34429;&#28982;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#25552;&#38382;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#24456;&#38590;&#29983;&#25104;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#26412;&#25991;&#20197;&#21327;&#20316;&#23545;&#35805;&#20219;&#21153;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#21453;&#26144;&#20154;&#31867;&#23547;&#27714;&#28548;&#28165;&#30340;&#34892;&#20026;&#65292;&#36825;&#34920;&#26126;&#20351;&#29992;&#20154;&#31867;&#28548;&#28165;&#38382;&#39064;&#26469;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#21487;&#33021;&#19981;&#26159;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clarification questions are an essential dialogue tool to signal misunderstanding, ambiguities, and under-specification in language use. While humans are able to resolve uncertainty by asking questions since childhood, modern dialogue systems struggle to generate effective questions. To make progress in this direction, in this work we take a collaborative dialogue task as a testbed and study how model uncertainty relates to human uncertainty -- an as yet under-explored problem. We show that model uncertainty does not mirror human clarification-seeking behavior, which suggests that using human clarification questions as supervision for deciding when to ask may not be the most effective way to resolve model uncertainty. To address this issue, we propose an approach to generating clarification questions based on model uncertainty estimation, compare it to several alternatives, and show that it leads to significant improvements in terms of task success. Our findings highlight the importanc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2402.06506</link><description>&lt;p&gt;
&#20351;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying point clouds at the facade-level using geometric features and deep learning networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31435;&#38754;&#32454;&#33410;&#30340;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#31435;&#38754;&#32423;&#21035;&#19978;&#23545;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#26159;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#23383;&#21103;&#26412;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36825;&#31181;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20960;&#20309;&#29305;&#24449;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#23545;&#31435;&#38754;&#32423;&#21035;&#30340;&#28857;&#20113;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26089;&#26399;&#34701;&#21512;&#30340;&#29305;&#24449;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#34917;&#20607;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#22312;&#25429;&#25417;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20419;&#36827;&#35821;&#20041;&#20998;&#21106;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D building models with facade details are playing an important role in many applications now. Classifying point clouds at facade-level is key to create such digital replicas of the real world. However, few studies have focused on such detailed classification with deep neural networks. We propose a method fusing geometric features with deep learning networks for point cloud classification at facade-level. Our experiments conclude that such early-fused features improve deep learning methods' performance. This method can be applied for compensating deep learning networks' ability in capturing local geometric information and promoting the advancement of semantic segmentation.
&lt;/p&gt;</description></item><item><title>ACTER&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;RL&#31574;&#30053;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.06503</link><description>&lt;p&gt;
ACTER: &#29992;&#20110;&#35299;&#37322;&#21644;&#35786;&#26029;RL&#31574;&#30053;&#30340;&#22810;&#26679;&#19988;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06503
&lt;/p&gt;
&lt;p&gt;
ACTER&#26159;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;RL&#31574;&#30053;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#22833;&#36133;&#22914;&#20309;&#21457;&#29983;&#20197;&#21450;&#22914;&#20309;&#38450;&#27490;&#26159;&#20026;&#20102;&#23454;&#29616;&#35843;&#35797;&#12289;&#32500;&#25252;&#29992;&#25143;&#20449;&#20219;&#21644;&#24320;&#21457;&#20010;&#24615;&#21270;&#31574;&#30053;&#32780;&#24517;&#35201;&#30340;&#12290;&#21453;&#20107;&#23454;&#25512;&#29702;&#32463;&#24120;&#34987;&#29992;&#26469;&#24402;&#21646;&#21644;&#29702;&#35299;&#22833;&#36133;&#65292;&#36890;&#36807;&#23547;&#25214;&#26368;&#25509;&#36817;&#30340;&#21487;&#33021;&#19990;&#30028;&#20197;&#36991;&#20813;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;RL&#20013;&#30340;&#21453;&#20107;&#23454;&#29366;&#24577;&#35299;&#37322;&#21482;&#33021;&#20351;&#29992;&#24403;&#21069;&#29366;&#24577;&#29305;&#24449;&#26469;&#35299;&#37322;&#32467;&#26524;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#39044;&#38450;&#36127;&#32467;&#26524;&#30340;&#21487;&#34892;&#24615;&#25514;&#26045;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTER&#65288;&#29992;&#20110;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#32467;&#26524;&#30340;&#21487;&#34892;&#21453;&#20107;&#23454;&#24207;&#21015;&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29983;&#25104;&#21487;&#34892;&#30340;&#21453;&#20107;&#23454;&#24207;&#21015;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36991;&#20813;&#22833;&#36133;&#30340;&#21487;&#34892;&#24314;&#35758;&#12290;ACTER&#30740;&#31350;&#23548;&#33268;&#22833;&#36133;&#30340;&#21160;&#20316;&#65292;&#24182;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;NSGA-II&#29983;&#25104;&#21487;&#20197;&#26368;&#23567;&#21270;&#25913;&#21464;&#19988;&#20855;&#26377;&#39640;&#30830;&#23450;&#24615;&#30340;&#21453;&#20107;&#23454;&#21160;&#20316;&#24207;&#21015;&#65292;&#20197;&#38450;&#27490;&#22833;&#36133;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how failure occurs and how it can be prevented in reinforcement learning (RL) is necessary to enable debugging, maintain user trust, and develop personalized policies. Counterfactual reasoning has often been used to assign blame and understand failure by searching for the closest possible world in which the failure is avoided. However, current counterfactual state explanations in RL can only explain an outcome using just the current state features and offer no actionable recourse on how a negative outcome could have been prevented. In this work, we propose ACTER (Actionable Counterfactual Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for generating counterfactual sequences that provides actionable advice on how failure can be avoided. ACTER investigates actions leading to a failure and uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of actions that prevent it with minimal changes and high certainty even in stochastic 
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31181;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#22312;&#32447;&#25968;&#25454;&#20013;&#36935;&#21040;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06500</link><description>&lt;p&gt;
&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#23454;&#26102;&#26816;&#27979;&#26681;&#26412;&#21407;&#22240;&#65292;&#24212;&#29992;&#20110;IT&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
On the Fly Detection of Root Causes from Observed Data with Application to IT Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#19968;&#31181;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#26681;&#26412;&#21407;&#22240;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#22312;&#32447;&#25968;&#25454;&#20013;&#36935;&#21040;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#38408;&#20540;&#30340;IT&#31995;&#32479;&#30340;&#26032;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#27492;&#31867;&#31995;&#32479;&#20013;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#24403;&#26681;&#26412;&#21407;&#22240;&#27809;&#26377;&#22240;&#26524;&#20851;&#32852;&#26102;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#27491;&#30830;&#30340;&#65307;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#24178;&#39044;&#26469;&#25918;&#26494;&#36825;&#31181;&#20551;&#35774;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21450;&#20854;&#22522;&#20110;&#20195;&#29702;&#30340;&#25193;&#23637;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#22312;&#36935;&#21040;&#22312;&#32447;&#25968;&#25454;&#20013;&#30340;&#26032;&#24322;&#24120;&#26102;&#36827;&#34892;&#23376;&#22270;&#36941;&#21382;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#26469;&#33258;&#26367;&#20195;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25110;&#30495;&#23454;IT&#30417;&#25511;&#25968;&#25454;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new structural causal model tailored for representing threshold-based IT systems and presents a new algorithm designed to rapidly detect root causes of anomalies in such systems. When root causes are not causally related, the method is proven to be correct; while an extension is proposed based on the intervention of an agent to relax this assumption. Our algorithm and its agent-based extension leverage causal discovery from offline data and engage in subgraph traversal when encountering new anomalies in online data. Our extensive experiments demonstrate the superior performance of our methods, even when applied to data generated from alternative structural causal models or real IT monitoring data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06492</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#23884;&#20837;&#22312;Transformer&#20013;&#24341;&#23548;&#31995;&#32479;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35757;&#32451;&#36807;&#22797;&#26434;&#25968;&#25454;&#38598;&#21518;&#33021;&#22815;&#25512;&#24191;&#21040;&#32467;&#26500;&#21644;&#23454;&#20307;&#30340;&#26032;&#32452;&#21512;&#65292;&#20294;&#22312;&#22797;&#26434;&#24230;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#19978;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#35757;&#32451;&#38598;&#36275;&#22815;&#22797;&#26434;&#26102;&#65292;&#27169;&#22411;&#20351;&#29992;&#31995;&#32479;&#24615;&#30340;&#27880;&#24847;&#27169;&#24335;&#23545;&#20855;&#26377;&#20849;&#21516;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#36827;&#34892;&#32534;&#30721;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SQ-Transformer&#65288;&#32467;&#26500;&#21270;&#37327;&#21270;&#65289;&#65292;&#21363;&#20351;&#20351;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;&#35757;&#32451;&#38598;&#65292;&#20063;&#33021;&#26126;&#30830;&#22320;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#40723;&#21169;&#31995;&#32479;&#24615;&#12290;&#22312;&#23884;&#20837;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#23548;&#21521;&#30340;&#21521;&#37327;&#37327;&#21270;&#65288;SoVQ&#65289;&#65292;&#23558;&#21333;&#35789;&#23884;&#20837;&#32858;&#31867;&#25104;&#33509;&#24178;&#31867;&#20855;&#26377;&#32467;&#26500;&#31561;&#20215;&#30340;&#23454;&#20307;&#12290;&#22312;&#27880;&#24847;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31995;&#32479;&#24615;&#27880;&#24847;&#23618;&#65288;SAL&#65289;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#31995;&#32479;&#24615;&#27491;&#21017;&#21270;&#23618;&#65288;SRL&#65289;&#65292;&#23427;&#20204;&#37117;&#22312;&#37327;&#21270;&#30340;&#35789;&#23884;&#20837;&#19978;&#25805;&#20316;&#65292;&#20197;&#20415;&#20197;&#19981;&#21464;&#25110;&#31867;&#20284;&#30340;&#27880;&#24847;&#27169;&#24335;&#32534;&#30721;&#20855;&#26377;&#30456;&#21516;&#32467;&#26500;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#21521;&#27861;&#24459;&#30028;&#20154;&#22763;&#20171;&#32461;&#25968;&#23398;&#36923;&#36753;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#21407;&#29702;&#21450;&#20854;&#22312;&#20132;&#36890;&#27861;&#35268;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25506;&#35752;&#25968;&#23398;&#36923;&#36753;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#38480;&#21046;&#21644;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.06487</link><description>&lt;p&gt;
&#12298;&#23130;&#23035;&#19982;&#27491;&#20041;&#12299;&#65306;&#20154;&#24037;&#26234;&#33021;&#12289;&#27861;&#24459;&#12289;&#36923;&#36753;&#12289;&#35821;&#35328;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#22312;&#20132;&#36890;&#27861;&#35268;&#21644;&#21307;&#30103;&#20445;&#20581;&#26041;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Le Nozze di Giustizia. Interactions between Artificial Intelligence, Law, Logic, Language and Computation with some case studies in Traffic Regulations and Health Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21521;&#27861;&#24459;&#30028;&#20154;&#22763;&#20171;&#32461;&#25968;&#23398;&#36923;&#36753;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#21407;&#29702;&#21450;&#20854;&#22312;&#20132;&#36890;&#27861;&#35268;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25506;&#35752;&#25968;&#23398;&#36923;&#36753;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#38480;&#21046;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#23558;&#19968;&#20123;&#22522;&#26412;&#30340;&#25968;&#23398;&#36923;&#36753;&#30693;&#35782;&#20256;&#36798;&#32473;&#19982;&#20154;&#24037;&#26234;&#33021;&#21512;&#20316;&#30340;&#27861;&#24459;&#30028;&#20154;&#22763;&#12290;&#22312;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#26412;&#27010;&#24565;&#21518;&#65292;&#25105;&#20204;&#20915;&#23450;&#20165;&#38480;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#19981;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#22522;&#20110;&#35268;&#21017;&#30340;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20197;&#31616;&#21333;&#30340;&#24418;&#24335;&#25551;&#36848;&#36825;&#20123;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#30475;&#21040;&#25968;&#23398;&#36923;&#36753;&#22914;&#20309;&#19982;&#27861;&#24459;&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#25968;&#23398;&#36923;&#36753;&#23545;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#38480;&#21046;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23558;&#25226;&#25968;&#23398;&#36923;&#36753;&#19982;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38480;&#21046;&#21644;&#30456;&#20114;&#20316;&#29992;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#36923;&#36753;&#12289;&#35745;&#31639;&#21644;&#25968;&#23398;&#12290;&#23637;&#31034;&#30456;&#20114;&#20316;&#29992;&#30340;&#31034;&#20363;&#20027;&#35201;&#26469;&#33258;&#27431;&#27954;&#20132;&#36890;&#27861;&#35268;&#12290;&#26412;&#25991;&#26368;&#21518;&#36824;&#20250;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#22330;&#26223;&#21644;&#23545;&#22609;&#36896;&#31038;&#20250;&#30340;&#22522;&#26412;&#26426;&#21046;&#36827;&#34892;&#19968;&#20123;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important aim of this paper is to convey some basics of mathematical logic to the legal community working with Artificial Intelligence. After analysing what AI is, we decide to delimit ourselves to rule-based AI leaving Neural Networks and Machine Learning aside. Rule based AI allows for Formal methods which are described in a rudimentary form. We will then see how mathematical logic interacts with legal rule-based AI practice. We shall see how mathematical logic imposes limitations and complications to AI applications. We classify the limitations and interactions between mathematical logic and legal AI in three categories: logical, computational and mathematical. The examples to showcase the interactions will largely come from European traffic regulations. The paper closes off with some reflections on how and where AI could be used and on basic mechanisms that shape society.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#34920;&#36798;&#26448;&#26009;&#30340;&#27835;&#30103;&#24615;&#25925;&#20107;&#21019;&#20316;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#23478;&#24237;&#21019;&#36896;&#24615;&#22320;&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20256;&#32479;&#34920;&#36798;&#26448;&#26009;&#26469;&#22806;&#21270;&#20182;&#20204;&#30340;&#24605;&#24819;&#21644;&#24863;&#21463;&#12290;</title><link>https://arxiv.org/abs/2402.06472</link><description>&lt;p&gt;
"&#24403;&#20182;&#24863;&#21040;&#23506;&#20919;&#26102;&#65292;&#20182;&#21435;&#20102;&#28023;&#39532;&#40060;" - &#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#34701;&#20837;&#22810;&#29289;&#36136;&#25925;&#20107;&#21019;&#20316;&#20013;&#30340;&#23478;&#24237;&#34920;&#36798;&#33402;&#26415;&#30103;&#27861;
&lt;/p&gt;
&lt;p&gt;
"When He Feels Cold, He Goes to the Seahorse"-Blending Generative AI into Multimaterial Storymaking for Family Expressive Arts Therapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#34920;&#36798;&#26448;&#26009;&#30340;&#27835;&#30103;&#24615;&#25925;&#20107;&#21019;&#20316;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21457;&#29616;&#23478;&#24237;&#21019;&#36896;&#24615;&#22320;&#34701;&#21512;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20256;&#32479;&#34920;&#36798;&#26448;&#26009;&#26469;&#22806;&#21270;&#20182;&#20204;&#30340;&#24605;&#24819;&#21644;&#24863;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#21019;&#20316;&#20316;&#20026;&#19968;&#31181;&#32508;&#21512;&#24615;&#30340;&#34920;&#36798;&#33402;&#26415;&#30103;&#27861;&#24418;&#24335;&#65292;&#26159;&#20419;&#36827;&#23478;&#24237;&#27807;&#36890;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#22312;&#27835;&#30103;&#24615;&#25925;&#20107;&#21019;&#20316;&#20013;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#34920;&#36798;&#26448;&#26009;&#30340;&#25972;&#21512;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#23545;&#20110;&#22914;&#20309;&#22312;&#23478;&#24237;&#21644;&#27835;&#30103;&#24072;&#20013;&#25552;&#20379;&#25903;&#25345;&#30340;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#24433;&#21709;&#20063;&#23384;&#22312;&#32570;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32463;&#36807;&#20102;&#20026;&#26399;&#20116;&#21608;&#30340;&#27835;&#30103;&#25925;&#20107;&#21019;&#20316;&#20250;&#35805;&#65292;&#28041;&#21450;&#19971;&#20010;&#23478;&#24237;&#22312;&#19987;&#19994;&#27835;&#30103;&#24072;&#30340;&#25351;&#23548;&#19979;&#36827;&#34892;&#12290;&#22312;&#36825;&#20123;&#20250;&#35805;&#20013;&#65292;&#23478;&#24237;&#20351;&#29992;&#20256;&#32479;&#30340;&#33402;&#26415;&#26448;&#26009;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26469;&#21019;&#20316;&#21644;&#21457;&#23637;&#20182;&#20204;&#30340;&#23478;&#24237;&#25925;&#20107;&#12290;&#36890;&#36807;&#20016;&#23500;&#30340;&#32463;&#39564;&#25968;&#25454;&#21644;&#22235;&#21517;&#19987;&#19994;&#27835;&#30103;&#24072;&#30340;&#35780;&#35770;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#23478;&#24237;&#22914;&#20309;&#21019;&#36896;&#24615;&#22320;&#34701;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20256;&#32479;&#34920;&#36798;&#26448;&#26009;&#26469;&#22806;&#21270;&#20182;&#20204;&#30340;&#24605;&#24819;&#21644;&#24863;&#21463;&#12290;&#36890;&#36807;&#34920;&#36798;&#30103;&#27861;&#36830;&#32493;&#20307;&#65288;ETC&#65289;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#34920;&#36798;&#26448;&#26009;&#30340;&#27835;&#30103;&#24847;&#20041;&#12290;&#25903;&#25345;&#20799;&#31461;&#12289;&#29238;&#27597;&#30340;&#21487;&#21462;&#20132;&#20114;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Storymaking, as an integrative form of expressive arts therapy, is an effective means to foster family communication. Yet, the integration of generative AI as expressive materials in therapeutic storymaking remains underexplored. And there is a lack of HCI implications on how to support families and therapists in this context. Addressing this, our study involved five weeks of storymaking sessions with seven families guided by a professional therapist. In these sessions, the families used both traditional art-making materials and image-based generative AI to create and evolve their family stories. Via the rich empirical data and commentaries from four expert therapists, we contextualize how families creatively melded AI and traditional expressive materials to externalize their ideas and feelings. Through the lens of Expressive Therapies Continuum (ETC), we characterize the therapeutic implications of AI as expressive materials. Desirable interaction qualities to support children, parent
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#21270;Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25552;&#21462;&#36807;&#21435;&#32463;&#39564;&#30340;&#20449;&#24687;&#20016;&#23500;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20803;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06402</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;Transformer&#26159;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Transformers are Efficient Meta-Reinforcement Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06402
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;Transformer&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25552;&#21462;&#36807;&#21435;&#32463;&#39564;&#30340;&#20449;&#24687;&#20016;&#23500;&#36164;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20803;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22312;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#23618;&#27425;&#21270;Transformer&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;HTrMRL&#65289;&#12290;HTrMRL&#26088;&#22312;&#35299;&#20915;&#20351;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#22312;&#20197;&#21069;&#26410;&#35265;&#20219;&#21153;&#20013;&#26377;&#25928;&#25191;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#21435;&#30340;&#32463;&#39564;&#20316;&#20026;&#20449;&#24687;&#20016;&#23500;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25552;&#28860;&#21644;&#24212;&#29992;&#21040;&#26032;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#33021;&#22815;&#36229;&#36234;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#20803;&#35757;&#32451;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;Meta-World&#22522;&#20934;&#30340;&#21508;&#31181;&#27169;&#25311;&#20219;&#21153;&#19978;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22686;&#24378;&#20102;&#20195;&#29702;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36824;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;AI&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Hierarchical Transformers for Meta-Reinforcement Learning (HTrMRL), a powerful online meta-reinforcement learning approach. HTrMRL aims to address the challenge of enabling reinforcement learning agents to perform effectively in previously unseen tasks. We demonstrate how past episodes serve as a rich source of information, which our model effectively distills and applies to new contexts. Our learned algorithm is capable of outperforming the previous state-of-the-art and provides more efficient meta-training while significantly improving generalization capabilities. Experimental results, obtained across various simulated tasks of the Meta-World Benchmark, indicate a significant improvement in learning efficiency and adaptability compared to the state-of-the-art on a variety of tasks. Our approach not only enhances the agent's ability to generalize from limited data but also paves the way for more robust and versatile AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21576;&#29616;&#20102;&#19968;&#31181;&#20351;&#29992;SAT&#27714;&#35299;&#22120;&#33258;&#21160;&#21457;&#29616;&#22256;&#38590;&#24402;&#32422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31526;&#21495;&#26144;&#23556;&#30340;&#23436;&#25104;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#20998;&#31867;&#20102;&#25968;&#21315;&#20010;NP&#23436;&#20840;&#30340;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20869;&#37096;&#19977;&#20803;&#32452;&#31995;&#32479;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26080;&#38480;&#23478;&#26063;&#30340;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.06397</link><description>&lt;p&gt;
&#20351;&#29992;SAT&#27714;&#35299;&#22120;&#33258;&#21160;&#21457;&#29616;&#22256;&#38590;&#24402;&#32422;
&lt;/p&gt;
&lt;p&gt;
Finding hardness reductions automatically using SAT solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21576;&#29616;&#20102;&#19968;&#31181;&#20351;&#29992;SAT&#27714;&#35299;&#22120;&#33258;&#21160;&#21457;&#29616;&#22256;&#38590;&#24402;&#32422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31526;&#21495;&#26144;&#23556;&#30340;&#23436;&#25104;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#20998;&#31867;&#20102;&#25968;&#21315;&#20010;NP&#23436;&#20840;&#30340;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20869;&#37096;&#19977;&#20803;&#32452;&#31995;&#32479;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26080;&#38480;&#23478;&#26063;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23436;&#25104;&#38382;&#39064;(&#21363;&#20915;&#31574;&#38382;&#39064;&#65292;&#21363;&#37096;&#20998;&#32467;&#26500;&#33021;&#21542;&#23436;&#25104;&#20026;&#23436;&#25972;&#32467;&#26500;)&#22312;&#35768;&#22810;&#32452;&#21512;&#32467;&#26500;&#20013;&#26159;NP&#23436;&#20840;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25991;&#29486;&#20013;&#30340;&#24402;&#32422;&#37117;&#26159;&#25163;&#24037;&#23436;&#25104;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#26500;&#24314;&#24402;&#32422;&#30340;&#31639;&#27861;&#12290;&#20351;&#29992;&#22522;&#20110;SAT&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#31105;&#27490;&#23376;&#32467;&#26500;&#30340;&#31526;&#21495;&#26144;&#23556;&#30340;&#23436;&#25104;&#38382;&#39064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#65292;&#24182;&#20998;&#31867;&#20102;&#25968;&#21315;&#20010;NP&#23436;&#20840;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#21015;&#34920;&#21253;&#25324;&#20102;&#30001;Knuth&#24341;&#20837;&#30340;&#29992;&#20110;&#24179;&#38754;&#28857;&#37197;&#32622;&#20844;&#29702;&#21270;&#30340;&#20869;&#37096;&#19977;&#20803;&#32452;&#31995;&#32479;&#12290;&#26368;&#21518;&#20294;&#24182;&#38750;&#26368;&#19981;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#20869;&#37096;&#19977;&#20803;&#32452;&#31995;&#32479;&#25512;&#24191;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26080;&#38480;&#23478;&#26063;&#30340;&#32467;&#26500;&#65292;&#23545;&#20110;&#36825;&#20123;&#32467;&#26500;&#65292;&#23436;&#25104;&#38382;&#39064;&#26159;NP&#23436;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we show that the completion problem, i.e. the decision problem whether a partial structure can be completed to a full structure, is NP-complete for many combinatorial structures. While the gadgets for most reductions in literature are found by hand, we present an algorithm to construct gadgets in a fully automated way. Using our framework which is based on SAT, we present the first thorough study of the completion problem on sign mappings with forbidden substructures by classifying thousands of structures for which the completion problem is NP-complete. Our list in particular includes interior triple systems, which were introduced by Knuth towards an axiomatization of planar point configurations. Last but not least, we give an infinite family of structures generalizing interior triple system to higher dimensions for which the completion problem is NP-complete.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25552;&#31034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#39118;&#26684;&#30340;&#32472;&#30011;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#34701;&#20837;&#20102;&#20182;&#20204;&#30340;&#23457;&#32654;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.06389</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#23457;&#32654;&#20559;&#22909;&#30340;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20010;&#24615;&#21270;: &#20197;&#24247;&#23450;&#26031;&#22522;&#21019;&#20316;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Human Aesthetic Preference-Based Large Text-to-Image Model Personalization: Kandinsky Generation as an Example
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#25552;&#31034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#39118;&#26684;&#30340;&#32472;&#30011;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#34701;&#20837;&#20102;&#20182;&#20204;&#30340;&#23457;&#32654;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#29983;&#25104;&#33021;&#21147;&#30340;&#36827;&#27493;&#65292;&#33402;&#26415;&#30028;&#31215;&#26497;&#37319;&#29992;GenAI&#65288;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#65289;&#26469;&#21019;&#20316;&#32472;&#30011;&#20869;&#23481;&#12290;&#22823;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#32654;&#35266;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26159;&#38750;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#24120;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#35797;&#35823;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#26500;&#24605;&#26377;&#25928;&#30340;&#25552;&#31034;&#26469;&#36798;&#21040;&#20182;&#20204;&#24819;&#35201;&#30340;&#32467;&#26524;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25552;&#31034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33402;&#26415;&#39118;&#26684;&#30340;&#32472;&#30011;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#34701;&#20837;&#20102;&#20182;&#20204;&#30340;&#23457;&#32654;&#20559;&#22909;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;&#8220;&#35821;&#20041;&#27880;&#20837;&#8221;&#26469;&#23450;&#21046;&#29305;&#23450;&#33402;&#26415;&#39118;&#26684;&#30340;&#33402;&#26415;&#23478;&#27169;&#22411;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#36890;&#36807;&#23454;&#26102;&#36845;&#20195;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#20248;&#21270;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#20165;&#20381;&#38752;&#29992;&#25143;&#23545;&#33402;&#26415;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#23457;&#32654;&#35780;&#20215;&#21644;&#20559;&#22909;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advancement of neural generative capabilities, the art community has actively embraced GenAI (generative artificial intelligence) for creating painterly content. Large text-to-image models can quickly generate aesthetically pleasing outcomes. However, the process can be non-deterministic and often involves tedious trial-and-error, as users struggle with formulating effective prompts to achieve their desired results. This paper introduces a prompting-free generative approach that empowers users to automatically generate personalized painterly content that incorporates their aesthetic preferences in a customized artistic style. This approach involves utilizing ``semantic injection'' to customize an artist model in a specific artistic style, and further leveraging a genetic algorithm to optimize the prompt generation process through real-time iterative human feedback. By solely relying on the user's aesthetic evaluation and preference for the artist model-generated images, this a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22320;&#36136;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#23454;&#29616;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;</title><link>https://arxiv.org/abs/2402.06377</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;
&lt;/p&gt;
&lt;p&gt;
High-Precision Geosteering via Reinforcement Learning and Particle Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06377
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#31890;&#23376;&#28388;&#27874;&#30340;&#22320;&#36136;&#23450;&#21521;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#25968;&#25454;&#22788;&#29702;&#23454;&#29616;&#39640;&#31934;&#24230;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#36136;&#23450;&#21521;&#26159;&#38075;&#20117;&#20316;&#19994;&#20013;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#20256;&#32479;&#19978;&#28041;&#21450;&#23545;&#21508;&#31181;&#25968;&#25454;&#28304;&#65288;&#22914;&#20117;&#27979;&#25968;&#25454;&#65289;&#30340;&#25163;&#21160;&#35299;&#35835;&#12290;&#36825;&#24341;&#20837;&#20102;&#20027;&#35266;&#20559;&#35265;&#21644;&#19981;&#19968;&#33268;&#30340;&#31243;&#24207;&#12290;&#23398;&#26415;&#30028;&#23581;&#35797;&#36890;&#36807;&#36138;&#23146;&#20248;&#21270;&#21644;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#65288;ADP&#65289;&#26469;&#35299;&#20915;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;&#20248;&#21270;&#38382;&#39064;&#65292;&#26174;&#31034;&#20986;&#20102;&#19968;&#23450;&#30340;&#28508;&#21147;&#65292;&#20294;&#32570;&#20047;&#36866;&#24212;&#29616;&#23454;&#22810;&#26679;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#22870;&#21169;&#30340;&#36845;&#20195;&#23398;&#20064;&#26469;&#20419;&#36827;&#26368;&#20248;&#20915;&#31574;&#12290;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#20363;&#22914;&#31890;&#23376;&#28388;&#27874;&#65288;PF&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#20449;&#24687;&#30340;&#34917;&#20805;&#31574;&#30053;&#65292;&#29992;&#20110;&#22320;&#36136;&#23450;&#21521;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;RL&#30340;&#22320;&#36136;&#23450;&#21521;&#19982;PF&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#29616;&#23454;&#30340;&#22320;&#36136;&#23450;&#21521;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;PF&#22788;&#29702;&#23454;&#26102;&#20117;&#27979;&#25968;&#25454;&#65292;&#20272;&#35745;&#20117;&#30340;&#20301;&#32622;&#30456;&#23545;&#20110;&#22320;&#23618;&#65292;&#28982;&#21518;&#23558;&#20854;&#20449;&#24687;&#29992;&#20110;&#22522;&#20110;RL&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geosteering, a key component of drilling operations, traditionally involves manual interpretation of various data sources such as well-log data. This introduces subjective biases and inconsistent procedures. Academic attempts to solve geosteering decision optimization with greedy optimization and Approximate Dynamic Programming (ADP) showed promise but lacked adaptivity to realistic diverse scenarios. Reinforcement learning (RL) offers a solution to these challenges, facilitating optimal decision-making through reward-based iterative learning. State estimation methods, e.g., particle filter (PF), provide a complementary strategy for geosteering decision-making based on online information. We integrate an RL-based geosteering with PF to address realistic geosteering scenarios. Our framework deploys PF to process real-time well-log data to estimate the location of the well relative to the stratigraphic layers, which then informs the RL-based decision-making process. We compare our method
&lt;/p&gt;</description></item><item><title>CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.06360</link><description>&lt;p&gt;
CoSearchAgent:&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06360
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#25628;&#32034;&#25903;&#25345;&#22810;&#20010;&#29992;&#25143;&#20849;&#21516;&#23436;&#25104;&#29305;&#23450;&#30340;&#25628;&#32034;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#25554;&#20214;&#35774;&#35745;&#22312;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#20869;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#21327;&#20316;&#20064;&#24815;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#29992;&#25143;&#20132;&#20114;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#19981;&#24471;&#19981;&#20381;&#36182;&#20110;"&#21561;&#29275;&#22823;&#29579;"&#33539;&#20363;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#19982;&#29992;&#25143;&#33258;&#28982;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23454;&#29616;&#22797;&#26434;&#30340;&#20449;&#24687;&#25628;&#32034;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CoSearchAgent&#65292;&#19968;&#31181;&#30001;LLM&#39537;&#21160;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#12290;CoSearchAgent&#34987;&#35774;&#35745;&#20026;Slack&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;&#35813;&#24179;&#21488;&#19978;&#30340;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;AI&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#31038;&#20250;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#24605;&#24819;&#65292;&#20026;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#35745;&#31639;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.06359</link><description>&lt;p&gt;
&#20026;AI&#25512;&#29702;&#24314;&#27169;&#20154;&#31867;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Modelling Human Values for AI Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;AI&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#31038;&#20250;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#24605;&#24819;&#65292;&#20026;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#35745;&#31639;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#26368;&#37325;&#35201;&#30340;&#31038;&#20250;&#25361;&#25112;&#20043;&#19968;&#26159;&#26500;&#24314;&#20854;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;AI&#31995;&#32479;&#65292;&#25110;&#26159;&#20854;&#20351;&#20154;&#24037;&#21644;&#20154;&#24037;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#31038;&#21306;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#20851;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#20854;&#26126;&#30830;&#30340;&#35745;&#31639;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26410;&#26377;&#20154;&#23581;&#35797;&#36807;&#36825;&#31181;&#27169;&#22411;&#65292;&#36825;&#22312;&#32771;&#34385;&#21040;&#23558;&#20215;&#20540;&#35266;&#19982;AI&#25972;&#21512;&#30340;&#30740;&#31350;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#12290;&#25105;&#20204;&#20197;&#31038;&#20250;&#24515;&#29702;&#23398;&#39046;&#22495;&#36817;&#20960;&#21313;&#24180;&#26469;&#30740;&#31350;&#20154;&#31867;&#20215;&#20540;&#35266;&#24615;&#36136;&#30340;&#22823;&#37327;&#30740;&#31350;&#20026;&#36215;&#28857;&#65292;&#33268;&#21147;&#20110;&#25552;&#20379;&#36825;&#26679;&#19968;&#20010;&#24418;&#24335;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22914;&#20309;&#20026;&#22522;&#20110;AI&#30340;&#20215;&#20540;&#25512;&#29702;&#25552;&#20379;&#22522;&#30784;&#35013;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#21040;&#31038;&#20250;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#20851;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#22312;AI&#20013;&#38598;&#25104;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of today's most significant societal challenges is building AI systems whose behaviour, or the behaviour it enables within communities of interacting agents (human and artificial), aligns with human values. To address this challenge, we detail a formal model of human values for their explicit computational representation. To our knowledge, this has not been attempted as yet, which is surprising given the growing volume of research integrating values within AI. Taking as our starting point the wealth of research investigating the nature of human values from social psychology over the last few decades, we set out to provide such a formal model. We show how this model can provide the foundational apparatus for AI-based reasoning over values, and demonstrate its applicability in real-world use cases. We illustrate how our model captures the key ideas from social psychology research and propose a roadmap for future integrated, and interdisciplinary, research into human values in AI. The
&lt;/p&gt;</description></item><item><title>ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.06334</link><description>&lt;p&gt;
ExaRanker-Open: &#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#21512;&#25104;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06334
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ExaRanker&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;(IR)&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#38468;&#21152;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;IR&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21021;&#22987;&#32467;&#26524;&#26159;&#22522;&#20110;&#19987;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#65292;&#36825;&#23548;&#33268;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#20854;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ExaRanker-Open&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#19981;&#21516;&#30340;LLMs&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#65292;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20063;&#26159;&#26377;&#20248;&#21183;&#30340;&#65292;ExaRanker&#30340;&#24615;&#33021;&#36229;&#36807;&#30446;&#26631;&#22522;&#32447;0
&lt;/p&gt;
&lt;p&gt;
ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06326</link><description>&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning on Temporal Interaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#20132;&#20114;&#22270;&#19978;&#36827;&#34892;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#39044;&#27979;&#38454;&#27573;&#25152;&#38754;&#20020;&#30340;&#26102;&#38388;&#24046;&#24322;&#21644;&#35821;&#20041;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;(TIGs)&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;TIGs&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;TIG&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#8220;&#39044;&#35757;&#32451;&#65292;&#39044;&#27979;&#8221;&#35757;&#32451;&#33539;&#24335;&#20013;&#20381;&#28982;&#38754;&#20020;&#30528;&#20004;&#20010;&#38590;&#39064;&#12290;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#20005;&#37325;&#21066;&#24369;&#20102;&#27169;&#22411;&#22312;&#21160;&#24577;&#28436;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#36965;&#36828;&#26410;&#26469;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;&#20854;&#27425;&#65292;&#39044;&#25991;&#26412;&#20219;&#21153;&#21644;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24212;&#29992;&#22330;&#26223;&#20013;&#24456;&#38590;&#23545;&#40784;&#20854;&#23398;&#20064;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Interaction Graphs (TIGs) are widely utilized to represent real-world systems. To facilitate representation learning on TIGs, researchers have proposed a series of TIG models. However, these models are still facing two tough gaps between the pre-training and downstream predictions in their ``pre-train, predict'' training paradigm. First, the temporal discrepancy between the pre-training and inference data severely undermines the models' applicability in distant future predictions on the dynamically evolving data. Second, the semantic divergence between pretext and downstream tasks hinders their practical applications, as they struggle to align with their learning and prediction capabilities across application scenarios.   Recently, the ``pre-train, prompt'' paradigm has emerged as a lightweight mechanism for model generalization. Applying this paradigm is a potential solution to solve the aforementioned challenges. However, the adaptation of this paradigm to TIGs is not straig
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#65292;&#25361;&#25112;&#20256;&#32479;&#30340;&#23558;&#38899;&#39057;&#23450;&#20041;&#20026;&#8220;&#20551;&#8221;&#25110;&#8220;&#30495;&#8221;&#30340;&#33539;&#24335;&#12290;&#30740;&#31350;&#32773;&#20204;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#20102;&#30830;&#23450;&#8220;&#35821;&#38899;&#32534;&#36753;&#8221;&#19978;&#65292;&#36825;&#21253;&#25324;&#20256;&#32479;&#30340;&#20462;&#25913;&#20197;&#21450;TTS&#21512;&#25104;&#21644;VC&#31995;&#32479;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.06304</link><description>&lt;p&gt;
&#23545;&#35821;&#38899;&#30495;&#23454;&#24615;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Approach to Voice Authenticity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06304
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#65292;&#25361;&#25112;&#20256;&#32479;&#30340;&#23558;&#38899;&#39057;&#23450;&#20041;&#20026;&#8220;&#20551;&#8221;&#25110;&#8220;&#30495;&#8221;&#30340;&#33539;&#24335;&#12290;&#30740;&#31350;&#32773;&#20204;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#20102;&#30830;&#23450;&#8220;&#35821;&#38899;&#32534;&#36753;&#8221;&#19978;&#65292;&#36825;&#21253;&#25324;&#20256;&#32479;&#30340;&#20462;&#25913;&#20197;&#21450;TTS&#21512;&#25104;&#21644;VC&#31995;&#32479;&#12290;&#20182;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20266;&#36896;&#20027;&#35201;&#30001;&#20110;&#26368;&#36817;&#25991;&#23383;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#25216;&#26415;&#30340;&#36827;&#23637;&#32780;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#31038;&#20250;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#20027;&#27969;&#30340;&#20551;&#35774;&#26159;&#26410;&#32463;&#25913;&#21160;&#30340;&#20154;&#31867;&#35821;&#38899;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#30340;&#65292;&#32780;&#20266;&#36896;&#30340;&#35821;&#38899;&#26469;&#33258;&#20110;TTS&#21512;&#25104;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#20108;&#20803;&#21306;&#20998;&#26159;&#36807;&#20110;&#31616;&#21270;&#30340;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#25913;&#21464;&#25773;&#25918;&#36895;&#24230;&#21487;&#20197;&#29992;&#20110;&#24694;&#24847;&#29992;&#36884;&#65292;&#27604;&#22914;&#8220;&#37257;&#37202;&#30340;&#21335;&#24076;&#183;&#20329;&#27931;&#35199;&#8221;&#20107;&#20214;&#12290;&#21516;&#26679;&#22320;&#65292;&#38899;&#39057;&#21098;&#36753;&#30340;&#32534;&#36753;&#21487;&#20197;&#22312;&#26032;&#38395;&#25253;&#36947;&#25110;&#25773;&#23458;&#20013;&#36827;&#34892;&#36947;&#24503;&#30340;&#32553;&#20943;&#25110;&#27010;&#25324;&#65292;&#20294;&#20063;&#21487;&#20197;&#21046;&#36896;&#35823;&#23548;&#24615;&#30340;&#21465;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#19978;&#30340;&#36716;&#21464;&#65292;&#25670;&#33073;&#20102;&#23558;&#38899;&#39057;&#23450;&#20041;&#20026;&#8220;&#20551;&#8221;&#36824;&#26159;&#8220;&#30495;&#8221;&#30340;&#20108;&#20803;&#33539;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#30830;&#23450;&#8220;&#35821;&#38899;&#32534;&#36753;&#8221;&#65292;&#23427;&#21253;&#25324;&#20256;&#32479;&#20462;&#25913;&#65292;&#22914;&#28388;&#27874;&#22120;&#21644;&#21098;&#20999;&#65292;&#20197;&#21450;TTS&#21512;&#25104;&#21644;VC&#31995;&#32479;&#12290;&#25105;&#20204;&#21010;&#20998;&#20102;6&#20010;&#31867;&#21035;&#65292;&#24182;&#22312;M-AILABS&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#31574;&#21010;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice faking, driven primarily by recent advances in text-to-speech (TTS) synthesis technology, poses significant societal challenges. Currently, the prevailing assumption is that unaltered human speech can be considered genuine, while fake speech comes from TTS synthesis. We argue that this binary distinction is oversimplified. For instance, altered playback speeds can be used for malicious purposes, like in the 'Drunken Nancy Pelosi' incident. Similarly, editing of audio clips can be done ethically, e.g., for brevity or summarization in news reporting or podcasts, but editing can also create misleading narratives. In this paper, we propose a conceptual shift away from the binary paradigm of audio being either 'fake' or 'real'. Instead, our focus is on pinpointing 'voice edits', which encompass traditional modifications like filters and cuts, as well as TTS synthesis and VC systems. We delineate 6 categories and curate a new challenge dataset rooted in the M-AILABS corpus, for which w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#30340;&#26032;&#22411;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#31216;&#20026;Fourier Tree Growing (FTG)&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31354;&#38388;&#30452;&#25509;&#36827;&#34892;&#20248;&#21270;&#26469;&#36991;&#20813;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#19968;&#32500;&#22522;&#20934;&#38382;&#39064;&#19978;&#19982;&#20256;&#32479;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06299</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Functional Analysis Approach to Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#20998;&#26512;&#30340;&#26032;&#22411;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#31216;&#20026;Fourier Tree Growing (FTG)&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#31354;&#38388;&#30452;&#25509;&#36827;&#34892;&#20248;&#21270;&#26469;&#36991;&#20813;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#19968;&#32500;&#22522;&#20934;&#38382;&#39064;&#19978;&#19982;&#20256;&#32479;&#30340;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;(SR)&#30001;&#20110;&#20381;&#36182;&#20110;&#21512;&#25104;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#30340;&#34920;&#36798;&#24335;&#32780;&#23545;&#38543;&#26426;&#25628;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#20986;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#36951;&#20256;&#32534;&#31243;(GP)&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#34920;&#31034;&#36827;&#34892;SR&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SR&#26041;&#27861;&#65292;&#31216;&#20026;Fourier Tree Growing (FTG)&#65292;&#23427;&#20511;&#37492;&#20102;&#20989;&#25968;&#20998;&#26512;&#30340;&#35265;&#35299;&#12290;&#36825;&#31181;&#26032;&#30340;&#35270;&#35282;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#22312;&#19968;&#20010;&#19981;&#21516;&#30340;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#32463;&#20856;&#30340;&#19968;&#32500;&#22522;&#20934;&#38382;&#39064;&#19978;&#30456;&#27604;&#20256;&#32479;&#30340;GP&#26041;&#27861;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20026;&#20102;&#35782;&#21035;&#21644;&#35299;&#37322;GP&#21644;FTG&#30340;&#38480;&#21046;&#22240;&#32032;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#20855;&#26377;&#39640;&#27425;&#25968;&#22810;&#39033;&#24335;&#65288;&#26368;&#39640;100&#27425;&#65289;&#30340;&#22823;&#35268;&#27169;&#22810;&#39033;&#24335;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#23558;&#20989;&#25968;&#20998;&#26512;&#26041;&#27861;&#24212;&#29992;&#20110;SR&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) poses a significant challenge for randomized search heuristics due to its reliance on the synthesis of expressions for input-output mappings. Although traditional genetic programming (GP) algorithms have achieved success in various domains, they exhibit limited performance when tree-based representations are used for SR. To address these limitations, we introduce a novel SR approach called Fourier Tree Growing (FTG) that draws insights from functional analysis. This new perspective enables us to perform optimization directly in a different space, thus avoiding intricate symbolic expressions. Our proposed algorithm exhibits significant performance improvements over traditional GP methods on a range of classical one-dimensional benchmarking problems. To identify and explain limiting factors of GP and FTG, we perform experiments on a large-scale polynomials benchmark with high-order polynomials up to degree 100. To the best of the authors' knowledge, this work rep
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06287</link><description>&lt;p&gt;
AI&#65292;&#19982;&#20154;&#30456;&#36935;&#65306;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#39640;&#39118;&#38505;&#20219;&#21153;&#21644;&#20915;&#31574;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#20154;&#31867;&#29616;&#22312;&#19981;&#26029;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#27599;&#22825;&#36827;&#34892;&#27169;&#22411;&#30340;&#22521;&#35757;&#21644;&#20351;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#32771;&#34385;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#20294;&#20854;&#20998;&#31867;&#31232;&#30095;&#19988;&#30446;&#26631;&#21508;&#24322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.06264</link><description>&lt;p&gt;
LLaVA-Docent&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#30340;&#25945;&#23398;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#65292;&#20197;&#25903;&#25345;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#12290;&#36890;&#36807;&#32508;&#36848;&#25991;&#29486;&#21644;&#19987;&#23478;&#21672;&#35810;&#65292;&#26500;&#24314;&#20102;&#25968;&#25454;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#35813;&#26694;&#26550;&#29983;&#25104;&#20102;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;MLLM&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#35299;&#20915;&#20256;&#32479;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#36164;&#28304;&#38480;&#21046;&#21644;&#20027;&#27969;&#25945;&#32946;&#20013;&#30340;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#20559;&#37325;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#37492;&#36175;&#23545;&#20110;&#22521;&#20859;&#23398;&#20064;&#32773;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#24773;&#24863;&#26234;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#24120;&#38754;&#20020;&#33402;&#26415;&#36164;&#28304;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#24369;&#21183;&#23398;&#29983;&#65292;&#24182;&#19988;&#22312;&#20027;&#27969;&#25945;&#32946;&#20013;&#36807;&#24230;&#24378;&#35843;&#31185;&#23398;&#25216;&#26415;&#24037;&#31243;&#21644;&#25968;&#23398;&#31185;&#30446;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#33402;&#26415;&#37492;&#36175;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#24320;&#21457;&#20102;LLaVA-Docent&#27169;&#22411;&#26469;&#21033;&#29992;&#36825;&#20123;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#21672;&#35810;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#25454;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#34394;&#25311;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#34987;GPT-4&#21033;&#29992;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;MLLM&#65288;&#21363;LLaVA-Docent&#65289;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#20845;&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Art appreciation is vital in nurturing critical thinking and emotional intelligence among learners. However, traditional art appreciation education has often been hindered by limited access to art resources, especially for disadvantaged students, and an imbalanced emphasis on STEM subjects in mainstream education. In response to these challenges, recent technological advancements have paved the way for innovative solutions. This study explores the application of multi-modal large language models (MLLMs) in art appreciation education, focusing on developing LLaVA-Docent, a model that leverages these advancements. Our approach involved a comprehensive literature review and consultations with experts in the field, leading to developing a robust data framework. Utilizing this framework, we generated a virtual dialogue dataset that was leveraged by GPT-4. This dataset was instrumental in training the MLLM, named LLaVA-Docent. Six researchers conducted quantitative and qualitative evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.06262</link><description>&lt;p&gt;
&#20851;&#20110;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#36807;&#24230;&#38656;&#27714;&#65292;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#20173;&#28982;&#26114;&#36149;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#38190;&#20540;&#32531;&#23384;&#20063;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#38024;&#23545;&#32473;&#23450;&#39044;&#31639;&#19979;&#32500;&#25252;&#38190;&#20540;&#32531;&#23384;&#24320;&#38144;&#30340;&#39537;&#36880;&#31574;&#30053;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#29616;&#26377;&#39537;&#36880;&#31574;&#30053;&#22312;&#37325;&#35201;&#24615;&#35780;&#20998;&#35745;&#31639;&#21644;&#39537;&#36880;&#33539;&#22260;&#26500;&#24314;&#20004;&#20010;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20808;&#21069;&#31574;&#30053;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#32531;&#23384;&#39537;&#36880;&#31574;&#30053;&#12290;&#28085;&#30422;&#20102;&#39044;&#22635;&#20805;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;RoCo&#30340;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;RoCo&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;AI&#21161;&#25163;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Robin&#30340;&#22686;&#24378;&#22411;&#23545;&#35805;&#22411;AI&#21161;&#25163;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#20114;&#27169;&#24335;&#21644;&#23545;&#35805;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06229</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#35843;&#35797;&#30340;&#20132;&#20114;&#27169;&#24335;&#65306;&#22686;&#24378;AI&#21161;&#25163;&#30340;&#23545;&#35805;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;AI&#21161;&#25163;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;Robin&#30340;&#22686;&#24378;&#22411;&#23545;&#35805;&#22411;AI&#21161;&#25163;&#65292;&#36890;&#36807;&#20351;&#29992;&#20132;&#20114;&#27169;&#24335;&#21644;&#23545;&#35805;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#36805;&#36895;&#26222;&#21450;&#12290;&#19982;LLMs&#30340;&#23545;&#35805;&#20132;&#20114;&#20351;&#31243;&#24207;&#21592;&#33021;&#22815;&#33719;&#21462;&#21508;&#31181;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#22312;&#27809;&#26377;&#36275;&#22815;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#31435;&#21363;&#37319;&#21462;&#34892;&#21160;&#65292;&#23548;&#33268;&#38544;&#21547;&#30340;&#20551;&#35774;&#21644;&#19981;&#20934;&#30830;&#30340;&#22238;&#24212;&#12290;&#24320;&#21457;&#32773;&#21644;LLMs&#20043;&#38388;&#30340;&#23545;&#35805;&#20027;&#35201;&#20197;&#38382;&#31572;&#23545;&#30340;&#24418;&#24335;&#36827;&#34892;&#65292;&#24320;&#21457;&#32773;&#36127;&#36131;&#25552;&#20986;&#27491;&#30830;&#30340;&#38382;&#39064;&#24182;&#22312;&#22810;&#20010;&#22238;&#21512;&#20013;&#32500;&#25345;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20132;&#20114;&#27169;&#24335;&#21644;&#23545;&#35805;&#20998;&#26512;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#29992;&#20110;&#35843;&#35797;&#30340;&#23545;&#35805;&#22411;AI&#21161;&#25163;Robin&#12290;&#36890;&#36807;&#23545;12&#20301;&#34892;&#19994;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#30340;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;LLM&#37197;&#22791;&#20197;&#19979;&#21151;&#33021;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#65306;(1)&#21033;&#29992;&#25554;&#20837;&#25193;&#23637;&#20132;&#20114;&#27169;&#24335;&#65292;(2)&#20419;&#36827;&#36718;&#27969;&#21457;&#35328;&#65292;(3)&#21033;&#29992;&#35843;&#35797;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread availability of Large Language Models (LLMs) within Integrated Development Environments (IDEs) has led to their speedy adoption. Conversational interactions with LLMs enable programmers to obtain natural language explanations for various software development tasks. However, LLMs often leap to action without sufficient context, giving rise to implicit assumptions and inaccurate responses. Conversations between developers and LLMs are primarily structured as question-answer pairs, where the developer is responsible for asking the the right questions and sustaining conversations across multiple turns. In this paper, we draw inspiration from interaction patterns and conversation analysis -- to design Robin, an enhanced conversational AI-assistant for debugging. Through a within-subjects user study with 12 industry professionals, we find that equipping the LLM to -- (1) leverage the insert expansion interaction pattern, (2) facilitate turn-taking, and (3) utilize debugging wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65292;&#24182;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06204</link><description>&lt;p&gt;
&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65306;&#23427;&#33021;&#35299;&#20915;&#30340;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65292;&#24182;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#25797;&#38271;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21516;&#26679;&#25797;&#38271;&#20316;&#20026;&#35780;&#20272;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;TriviaQA&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#20010;LLM&#21644;&#19968;&#20010;&#24320;&#28304;LM&#22312;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#35780;&#20272;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;LLM&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36739;&#29983;&#25104;&#20219;&#21153;&#20302;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19981;&#24544;&#23454;&#30340;&#35780;&#20272;&#24773;&#20917;&#65292;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#39046;&#22495;&#20013;&#20934;&#30830;&#35780;&#20272;&#31572;&#26696;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;LLM&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#29702;&#35299;&#8220;&#29983;&#25104;AI&#24726;&#35770;&#8221;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#20197;&#21450;&#23457;&#26597;&#27169;&#22411;&#35780;&#20272;&#20013;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of "the Generative AI Paradox" (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.06196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06196
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#33258;2022&#24180;11&#26376;ChatGPT&#21457;&#24067;&#20197;&#26469;&#12290;LLMs&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#21313;&#20159;&#21442;&#25968;&#26469;&#33719;&#24471;&#24191;&#27867;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#31526;&#21512;&#32553;&#25918;&#23450;&#24459;&#30340;&#39044;&#27979;&#12290;LLMs&#30340;&#30740;&#31350;&#39046;&#22495;&#23613;&#31649;&#38750;&#24120;&#26032;&#65292;&#20294;&#22312;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#26368;&#33879;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;&#19977;&#20010;&#27969;&#34892;&#30340;LLM&#31995;&#21015;&#65288;GPT&#12289;LLaMA&#12289;PaLM&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#36129;&#29486;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;LLM&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#20934;&#22791;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#65292;&#23457;&#26597;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#27969;&#34892;LLM&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06188</link><description>&lt;p&gt;
&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A self-supervised framework for learning whole slide representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06188
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;S3L&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#20999;&#29255;&#30340;&#34920;&#31034;&#12290;&#23427;&#32467;&#21512;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#37197;&#23545;&#35270;&#22270;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#20010;&#20999;&#29255;&#25104;&#20687;&#23545;&#20110;&#29983;&#29289;&#21307;&#23398;&#26174;&#24494;&#38236;&#21644;&#35745;&#31639;&#30149;&#29702;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21315;&#20806;&#20687;&#32032;&#30340;&#22823;&#23567;&#12289;&#22810;&#26679;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#29305;&#24449;&#12289;&#31354;&#38388;&#24322;&#36136;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;/&#19981;&#23384;&#22312;&#30340;&#25968;&#25454;&#27880;&#37322;&#65292;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687; (WSIs) &#26500;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#31361;&#26174;&#20102;&#20165;&#20381;&#38752;&#30417;&#30563;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25972;&#20010;&#20999;&#29255;&#34920;&#31034;&#12290;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#20197;&#20026;&#19979;&#28216;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;&#30284;&#30151;&#35786;&#26029;&#25110;&#20998;&#23376;&#36951;&#20256;&#39044;&#27979;&#65289;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;WSI&#35270;&#35273;&#29305;&#24449;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#33258;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#23398;&#20064;&#65288;S3L&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21315;&#20806;&#20687;&#32032;&#35268;&#27169;&#30340;WSI&#33258;&#30417;&#30563;&#12290;S3L&#23558;&#26469;&#33258;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#25968;&#25454;&#36716;&#25442;&#31574;&#30053;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#33258;&#30417;&#30563;&#30340;&#37197;&#23545;&#35270;&#22270;&#12290;S3L&#21033;&#29992;&#20869;&#22312;&#30340;&#21306;&#22495;&#24322;&#36136;&#24615;&#12289;&#32452;&#32455;&#23398;&#29305;&#24449;&#30340;&#21487;&#21464;&#24615;&#21644;&#20449;&#24687;&#20887;&#20313;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy wi
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SpinePose&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#65292;&#26080;&#38656;&#25163;&#21160;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.06185</link><description>&lt;p&gt;
&#24320;&#21457;&#21644;&#39564;&#35777;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Development and validation of an artificial intelligence model to accurately predict spinopelvic parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SpinePose&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#65292;&#26080;&#38656;&#25163;&#21160;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#33034;&#30424;&#30406;&#23545;&#40784;&#19982;&#20020;&#24202;&#30151;&#29366;&#30340;&#25913;&#21892;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#33034;&#30424;&#30406;&#25918;&#23556;&#23398;&#21442;&#25968;&#30340;&#27979;&#37327;&#36153;&#26102;&#19988;&#35266;&#23519;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20540;&#24471;&#20851;&#27880;&#12290;&#33258;&#21160;&#27979;&#37327;&#24037;&#20855;&#33021;&#22815;&#20197;&#36805;&#36895;&#32780;&#19968;&#33268;&#30340;&#26041;&#24335;&#36827;&#34892;&#27979;&#37327;&#65292;&#20294;&#29616;&#26377;&#24037;&#20855;&#20173;&#28982;&#21463;&#21040;&#26576;&#31181;&#31243;&#24230;&#30340;&#25163;&#21160;&#36755;&#20837;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;SpinePose&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#33034;&#30424;&#30406;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective. Achieving appropriate spinopelvic alignment has been shown to be associated with improved clinical symptoms. However, measurement of spinopelvic radiographic parameters is time-intensive and interobserver reliability is a concern. Automated measurement tools have the promise of rapid and consistent measurements, but existing tools are still limited by some degree of manual user-entry requirements. This study presents a novel artificial intelligence (AI) tool called SpinePose that automatically predicts spinopelvic parameters with high accuracy without the need for manual entry.   Methods. SpinePose was trained and validated on 761 sagittal whole-spine X-rays to predict sagittal vertical axis (SVA), pelvic tilt (PT), pelvic incidence (PI), sacral slope (SS), lumbar lordosis (LL), T1-pelvic angle (T1PA), and L1-pelvic angle (L1PA). A separate test set of 40 X-rays was labeled by 4 reviewers, including fellowship-trained spine surgeons and a fellowship-trained radiologist with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#38899;&#20048;&#30340;&#29305;&#23450;&#23646;&#24615;&#32780;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#20197;&#21450;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06178</link><description>&lt;p&gt;
MusicMagus: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#32534;&#36753;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20462;&#25913;&#29983;&#25104;&#38899;&#20048;&#30340;&#29305;&#23450;&#23646;&#24615;&#32780;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#33021;&#20197;&#21450;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#19981;&#26029;&#36827;&#27493;&#20026;&#38899;&#20048;&#21019;&#36896;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#38899;&#20048;&#29983;&#25104;&#36890;&#24120;&#28041;&#21450;&#36845;&#20195;&#30340;&#25913;&#36827;&#65292;&#22914;&#20309;&#32534;&#36753;&#29983;&#25104;&#30340;&#38899;&#20048;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32534;&#36753;&#36825;&#31181;&#27169;&#22411;&#29983;&#25104;&#30340;&#38899;&#20048;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#23646;&#24615;&#65288;&#22914;&#39118;&#26684;&#12289;&#24773;&#24863;&#21644;&#20048;&#22120;&#65289;&#30340;&#20462;&#25913;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#26041;&#38754;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#32534;&#36753;&#36716;&#21270;&#20026;&#28508;&#22312;&#31354;&#38388;&#25805;&#32437;&#65292;&#24182;&#28155;&#21152;&#39069;&#22806;&#30340;&#32422;&#26463;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#12290;&#23427;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#38899;&#20048;&#25193;&#25955;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#39118;&#26684;&#21644;&#38899;&#33394;&#36716;&#25442;&#35780;&#20272;&#20013;&#26174;&#31034;&#20986;&#20248;&#20110;&#38646;&#26679;&#26412;&#21644;&#26576;&#20123;&#30417;&#30563;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#23454;&#38469;&#38899;&#20048;&#32534;&#36753;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-music generation models have opened new avenues in musical creativity. However, music generation usually involves iterative refinements, and how to edit the generated music remains a significant challenge. This paper introduces a novel approach to the editing of music generated by such models, enabling the modification of specific attributes, such as genre, mood and instrument, while maintaining other aspects unchanged. Our method transforms text editing to \textit{latent space manipulation} while adding an extra constraint to enforce consistency. It seamlessly integrates with existing pretrained text-to-music diffusion models without requiring additional training. Experimental results demonstrate superior performance over both zero-shot and certain supervised baselines in style and timbre transfer evaluations. Additionally, we showcase the practical applicability of our approach in real-world music editing scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#38646;&#21806;&#20013;&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#22312;&#32771;&#34385;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#30340;&#30446;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.06158</link><description>&lt;p&gt;
&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Assortment Planning with Sponsored Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#38646;&#21806;&#20013;&#24102;&#26377;&#36190;&#21161;&#20135;&#21697;&#30340;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#20197;&#23454;&#29616;&#22312;&#32771;&#34385;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#21697;&#31867;&#35268;&#21010;&#23545;&#20110;&#20225;&#19994;&#30340;&#25104;&#21151;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#36190;&#21161;&#20135;&#21697;&#22312;&#22312;&#32447;&#24066;&#22330;&#30340;&#26085;&#30410;&#31361;&#20986;&#22320;&#20301;&#65292;&#38646;&#21806;&#21830;&#22312;&#26377;&#25928;&#31649;&#29702;&#20135;&#21697;&#21697;&#31867;&#26041;&#38754;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;&#21697;&#31867;&#35268;&#21010;&#30740;&#31350;&#22823;&#22810;&#24573;&#35270;&#20102;&#36190;&#21161;&#20135;&#21697;&#30340;&#23384;&#22312;&#21450;&#20854;&#23545;&#25972;&#20307;&#25512;&#33616;&#25928;&#26524;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20182;&#20204;&#36890;&#24120;&#31616;&#21270;&#22320;&#20551;&#35774;&#25152;&#26377;&#20135;&#21697;&#37117;&#26159;&#26377;&#26426;&#20135;&#21697;&#25110;&#38750;&#36190;&#21161;&#20135;&#21697;&#12290;&#36825;&#20010;&#30740;&#31350;&#31354;&#30333;&#31361;&#26174;&#20102;&#22312;&#36190;&#21161;&#20135;&#21697;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26356;&#28145;&#20837;&#25506;&#35752;&#21697;&#31867;&#35268;&#21010;&#25361;&#25112;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#22312;&#23384;&#22312;&#36190;&#21161;&#20135;&#21697;&#30340;&#24773;&#20917;&#19979;&#23558;&#21697;&#31867;&#35268;&#21010;&#38382;&#39064;&#24314;&#27169;&#20026;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#35745;&#31639;&#20986;&#19968;&#31181;&#26368;&#20248;&#30340;&#21697;&#31867;&#35268;&#21010;&#26041;&#26696;&#65292;&#26082;&#33021;&#20248;&#21270;&#39044;&#26399;&#25910;&#20837;&#65292;&#21448;&#33021;&#32771;&#34385;&#21040;&#36190;&#21161;&#20135;&#21697;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving landscape of retail, assortment planning plays a crucial role in determining the success of a business. With the rise of sponsored products and their increasing prominence in online marketplaces, retailers face new challenges in effectively managing their product assortment in the presence of sponsored products. Remarkably, previous research in assortment planning largely overlooks the existence of sponsored products and their potential impact on overall recommendation effectiveness. Instead, they commonly make the simplifying assumption that all products are either organic or non-sponsored. This research gap underscores the necessity for a more thorough investigation of the assortment planning challenge when sponsored products are in play. We formulate the assortment planning problem in the presence of sponsored products as a combinatorial optimization task. The ultimate objective is to compute an assortment plan that optimizes expected revenue while considerin
&lt;/p&gt;</description></item><item><title>DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;p&gt;
DeAL&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26102;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DeAL: Decoding-time Alignment for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06147
&lt;/p&gt;
&lt;p&gt;
DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#26399;&#26395;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20869;&#23481;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#23545;&#40784;&#19978;&#65292;&#36890;&#36807;&#35832;&#22914;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#31561;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#26377;&#25928;&#22320;&#25945;&#23548;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#26080;&#27861;&#25972;&#21512;&#22810;&#20010;&#33258;&#23450;&#20041;&#22870;&#21169;&#21644;&#20381;&#36182;&#27169;&#22411;&#24320;&#21457;&#32773;&#23545;&#36890;&#29992;&#21644;&#38745;&#24577;&#21407;&#21017;&#30340;&#29702;&#35299;&#26159;&#20027;&#35201;&#23616;&#38480;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27531;&#30041;&#24046;&#36317;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20063;&#20540;&#24471;&#36136;&#30097;&#65288;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#35757;&#32451;&#21518;&#20173;&#28982;&#23481;&#26131;&#34987;&#36234;&#29425;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeAL&#65292;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#65288;DeAL&#65289;&#30340;&#26694;&#26550;&#12290;&#26680;&#24515;&#24605;&#24819;&#22312;&#20110;&#23558;&#35299;&#30721;&#35270;&#20026;&#19968;&#20010;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#20419;&#20351;&#20351;&#29992;&#21508;&#31181;&#23545;&#40784;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20197;&#32534;&#31243;&#32422;&#26463;&#20026;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#20013;&#33410;&#28857;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#33021;&#23545;&#19981;&#21516;&#33410;&#28857;&#30340;&#25299;&#25169;&#35282;&#33394;&#36827;&#34892;&#20010;&#24615;&#21270;&#20256;&#25773;&#65292;&#24182;&#20943;&#23569;&#20256;&#25773;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.06128</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#20256;&#25773;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Node-wise Propagation for Large-scale Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#20013;&#33410;&#28857;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#27492;&#26041;&#27861;&#33021;&#23545;&#19981;&#21516;&#33410;&#28857;&#30340;&#25299;&#25169;&#35282;&#33394;&#36827;&#34892;&#20010;&#24615;&#21270;&#20256;&#25773;&#65292;&#24182;&#20943;&#23569;&#20256;&#25773;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#22522;&#20110;&#22270;&#30340;Web&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#39640;&#25928;&#36816;&#34892;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21487;&#25193;&#23637;&#30340;GNN&#20542;&#21521;&#20110;&#20197;&#30456;&#21516;&#30340;&#20256;&#25773;&#35268;&#21017;&#22788;&#29702;&#22270;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#65292;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#25299;&#25169;&#29420;&#29305;&#24615;&#65307;&#29616;&#26377;&#30340;&#33410;&#28857;&#32423;&#20256;&#25773;&#20248;&#21270;&#31574;&#30053;&#22312;&#22797;&#26434;&#30340;Web&#35268;&#27169;&#22270;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#38656;&#35201;&#23545;&#33410;&#28857;&#30340;&#23616;&#37096;&#23646;&#24615;&#36827;&#34892;&#20840;&#38754;&#25551;&#32472;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;Web&#35268;&#27169;&#22270;&#20013;&#30340;&#19981;&#21516;&#33410;&#28857;&#20855;&#26377;&#19981;&#21516;&#30340;&#25299;&#25169;&#35282;&#33394;&#65292;&#22240;&#27492;&#26080;&#24046;&#21035;&#22320;&#20256;&#25773;&#25110;&#24573;&#35270;&#23616;&#37096;&#19978;&#19979;&#25991;&#21487;&#33021;&#20250;&#24433;&#21709;&#33410;&#28857;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#23567;&#35268;&#27169;&#24773;&#26223;&#26080;&#27861;&#21305;&#37197;Web&#35268;&#27169;&#22270;&#20013;&#30340;&#36825;&#31181;&#22797;&#26434;&#25299;&#25169;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#24212;&#24615;&#25299;&#25169;&#24863;&#30693;&#20256;&#25773;&#65288;ATP&#65289;&#26041;&#27861;&#65292;&#20943;&#23569;&#28508;&#22312;&#30340;&#39640;&#20559;&#24046;&#20256;&#25773;&#21644;&#39069;&#22806;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scalable graph neural networks (GNNs) have emerged as a promising technique, which exhibits superior predictive performance and high running efficiency across numerous large-scale graph-based web applications. However, (i) Most scalable GNNs tend to treat all nodes in graphs with the same propagation rules, neglecting their topological uniqueness; (ii) Existing node-wise propagation optimization strategies are insufficient on web-scale graphs with intricate topology, where a full portrayal of nodes' local properties is required. Intuitively, different nodes in web-scale graphs possess distinct topological roles, and therefore propagating them indiscriminately or neglect local contexts may compromise the quality of node representations. This intricate topology in web-scale graphs cannot be matched by small-scale scenarios. To address the above issues, we propose \textbf{A}daptive \textbf{T}opology-aware \textbf{P}ropagation (ATP), which reduces potential high-bias propagation and extrac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06118</link><description>&lt;p&gt;
ViGoR&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25913;&#36827;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06118
&lt;/p&gt;
&lt;p&gt;
ViGoR&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#25552;&#39640;&#20102;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#23545;&#25509;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#35270;&#35273;&#23545;&#25509;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#24191;&#27867;&#30693;&#35782;&#19982;&#22270;&#20687;&#24863;&#30693;&#30456;&#32467;&#21512;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#23545;&#25509;&#65292;&#23548;&#33268;&#38169;&#35823;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#30340;&#19981;&#23384;&#22312;&#22330;&#26223;&#20803;&#32032;&#12289;&#36951;&#28431;&#37325;&#35201;&#30340;&#22330;&#26223;&#37096;&#20998;&#65292;&#20197;&#21450;&#25512;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#26102;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;ViGoR&#65288;&#36890;&#36807;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#36827;&#34892;&#35270;&#35273;&#23545;&#25509;&#65289;&#65292;&#23427;&#21033;&#29992;&#32454;&#31890;&#24230;&#22870;&#21169;&#24314;&#27169;&#26469;&#26174;&#33879;&#25552;&#21319;&#22522;&#20110;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;LVLMs&#30340;&#35270;&#35273;&#23545;&#25509;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#36890;&#36807;&#20351;&#29992;&#27604;&#23436;&#20840;&#30417;&#30563;&#26356;&#20415;&#23452;&#30340;&#20154;&#24037;&#35780;&#20272;&#21644;&#33258;&#21160;&#21270;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#22810;&#20010;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
By combining natural language understanding and the generation capabilities and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented reasoning capabilities in the real world. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucinating nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes and relationships between objects. To address these issues, we introduce a novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through numerous metrics on several benchmarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#32534;&#31243;&#25945;&#32946;&#30340;&#37325;&#35201;&#36235;&#21183;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27979;&#35797;&#20102;&#26426;&#22120;&#20154;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4V&#22312;&#25152;&#26377;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#29983;&#25104;&#22359;&#29366;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.06116</link><description>&lt;p&gt;
&#29992;&#20110;&#32534;&#31243;&#21644;&#26426;&#22120;&#20154;&#25945;&#32946;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)
&lt;/p&gt;
&lt;p&gt;
LLMs for Coding and Robotics Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#32534;&#31243;&#25945;&#32946;&#30340;&#37325;&#35201;&#36235;&#21183;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27979;&#35797;&#20102;&#26426;&#22120;&#20154;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4V&#22312;&#25152;&#26377;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#29983;&#25104;&#22359;&#29366;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#22320;&#21306;&#27491;&#22312;&#37319;&#29992;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26426;&#22120;&#20154;&#32534;&#31243;&#25945;&#32946;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#20026;&#20102;&#25945;&#23567;&#23401;&#23376;&#20204;&#22914;&#20309;&#32534;&#31243;&#21644;&#21442;&#21152;&#26426;&#22120;&#20154;&#25361;&#25112;&#65292;&#20154;&#20204;&#24320;&#22987;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#12289;&#29983;&#25104;&#21644;&#20462;&#25913;&#26426;&#22120;&#20154;&#20195;&#30721;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#32534;&#31243;&#25945;&#32946;&#30340;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#20960;&#31181;&#20027;&#27969;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#32479;&#32534;&#31243;&#20219;&#21153;&#21644;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#22359;&#29366;&#22270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4V&#22312;&#25152;&#26377;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#29983;&#25104;&#22359;&#29366;&#22270;&#20687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models and multimodal large language models have revolutionized artificial intelligence recently. An increasing number of regions are now embracing these advanced technologies. Within this context, robot coding education is garnering increasing attention. To teach young children how to code and compete in robot challenges, large language models are being utilized for robot code explanation, generation, and modification. In this paper, we highlight an important trend in robot coding education. We test several mainstream large language models on both traditional coding tasks and the more challenging task of robot code generation, which includes block diagrams. Our results show that GPT-4V outperforms other models in all of our tests but struggles with generating block diagram images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;CHEESE&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#20102;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#29305;&#24449;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20316;&#24330;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.06107</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#32771;&#35797;&#20013;&#30340;&#20316;&#24330;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning for Cheating Detection and Localization in Online Examinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;CHEESE&#65292;&#35813;&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#20102;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#29305;&#24449;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#20316;&#24330;&#34892;&#20026;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#24180;&#20896;&#29366;&#30149;&#27602;&#30149;&#27969;&#34892;&#30123;&#24773;&#30340;&#34067;&#24310;&#23548;&#33268;&#35768;&#22810;&#35838;&#31243;&#21644;&#32771;&#35797;&#21464;&#25104;&#22312;&#32447;&#24418;&#24335;&#12290;&#32771;&#35797;&#30417;&#32771;&#31995;&#32479;&#20013;&#30340;&#20316;&#24330;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#22312;&#20445;&#35777;&#36828;&#31243;&#32771;&#35797;&#30340;&#20844;&#24179;&#24615;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20316;&#24330;&#34892;&#20026;&#24456;&#23569;&#35265;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#22312;&#20316;&#24330;&#34892;&#20026;&#26816;&#27979;&#20219;&#21153;&#20013;&#27809;&#26377;&#20840;&#38754;&#32771;&#34385;&#22836;&#37096;&#23039;&#21183;&#12289;&#20957;&#35270;&#35282;&#24230;&#12289;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#20449;&#24687;&#31561;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#25552;&#20986;&#20102;CHEESE&#65292;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20316;&#24330;&#26816;&#27979;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23454;&#29616;&#24369;&#30417;&#30563;&#30340;&#26631;&#31614;&#29983;&#25104;&#22120;&#21644;&#23398;&#20064;&#21028;&#21035;&#24615;&#29305;&#24449;&#30340;&#29305;&#24449;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#23558;3D&#21367;&#31215;&#25552;&#21462;&#30340;&#36523;&#20307;&#23039;&#21183;&#21644;&#32972;&#26223;&#29305;&#24449;&#19982;OpenFace 2.0&#25429;&#33719;&#30340;&#30524;&#30555;&#20957;&#35270;&#12289;&#22836;&#37096;&#23039;&#21183;&#21644;&#38754;&#37096;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#25340;&#25509;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#36865;&#20837;&#26102;&#31354;&#22270;&#27169;&#22359;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of the Coronavirus disease-2019 epidemic has caused many courses and exams to be conducted online. The cheating behavior detection model in examination invigilation systems plays a pivotal role in guaranteeing the equality of long-distance examinations. However, cheating behavior is rare, and most researchers do not comprehensively take into account features such as head posture, gaze angle, body posture, and background information in the task of cheating behavior detection. In this paper, we develop and present CHEESE, a CHEating detection framework via multiplE inStancE learning. The framework consists of a label generator that implements weak supervision and a feature encoder to learn discriminative features. In addition, the framework combines body posture and background features extracted by 3D convolution with eye gaze, head posture and facial features captured by OpenFace 2.0. These features are fed into the spatio-temporal graph module by stitching to analyze the spa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#38754;&#20020;&#30528;&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#25972;&#21512;&#12289;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#19981;&#31283;&#23450;&#12289;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#20197;&#21450;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#23581;&#35797;&#22823;&#22810;&#26159;&#23396;&#31435;&#30340;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06098</link><description>&lt;p&gt;
&#26469;&#65292;&#35265;&#65292;&#32988;&#65306;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#21069;&#30340;&#20247;&#22810;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Veni, Vidi, Vici: Solving the Myriad of Challenges before Knowledge Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06098
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23398;&#20064;&#38754;&#20020;&#30528;&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#25972;&#21512;&#12289;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#19981;&#31283;&#23450;&#12289;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#20197;&#21450;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#23581;&#35797;&#22823;&#22810;&#26159;&#23396;&#31435;&#30340;&#65292;&#38656;&#35201;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;(KG)&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#22823;&#35268;&#27169;&#38142;&#25509;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;KG&#30340;&#24040;&#22823;&#35268;&#27169;&#65292;&#22270;&#35889;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#27169;&#24335;&#26816;&#27979;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;KG&#23398;&#20064;&#31995;&#32479;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20102;&#36171;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22270;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#22235;&#20010;&#20851;&#38190;&#19981;&#36275;&#65292;&#36825;&#20123;&#19981;&#36275;&#21516;&#26102;&#38480;&#21046;&#20102;KG&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#20154;&#31867;&#19982;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#30340;&#26368;&#20339;&#25509;&#21475;&#33021;&#21147;&#12290;&#36825;&#20123;&#19981;&#36275;&#21253;&#25324;&#65306;1)&#32570;&#20047;&#19987;&#23478;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;2)&#23545;KG&#20013;&#33410;&#28857;&#24230;&#25968;&#26497;&#31471;&#24615;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;3)&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32570;&#20047;&#23545;&#19981;&#30830;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#32771;&#34385;&#65292;4)&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35299;&#20915;&#27599;&#20010;&#38382;&#39064;&#30340;&#29616;&#26377;&#23581;&#35797;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25351;&#20986;&#27599;&#20010;&#23581;&#35797;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19982;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#30340;&#23581;&#35797;&#30456;&#38548;&#31163;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) have become increasingly common for representing large-scale linked data. However, their immense size has required graph learning systems to assist humans in analysis, interpretation, and pattern detection. While there have been promising results for researcher- and clinician- empowerment through a variety of KG learning systems, we identify four key deficiencies in state-of-the-art graph learning that simultaneously limit KG learning performance and diminish the ability of humans to interface optimally with these learning systems. These deficiencies are: 1) lack of expert knowledge integration, 2) instability to node degree extremity in the KG, 3) lack of consideration for uncertainty and relevance while learning, and 4) lack of explainability. Furthermore, we characterise state-of-the-art attempts to solve each of these problems and note that each attempt has largely been isolated from attempts to solve the other problems. Through a formalisation of these probl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25299;&#25169;&#29305;&#24449;&#23398;&#20064;&#26435;&#37325;&#26469;&#27169;&#25311;KGE&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#20855;&#26377;&#39044;&#20808;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#36328;&#22270;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06097</link><description>&lt;p&gt;
TWIG&#65306;&#36890;&#36807;&#27169;&#25311;KGE&#27169;&#22411;&#23454;&#29616;&#39044;&#20808;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#36328;&#22270;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25299;&#25169;&#29305;&#24449;&#23398;&#20064;&#26435;&#37325;&#26469;&#27169;&#25311;KGE&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#24182;&#20855;&#26377;&#39044;&#20808;&#20248;&#21270;&#36229;&#21442;&#25968;&#21644;&#36328;&#22270;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TWIG&#65288;Topologically-Weighted Intelligence Generation&#65289;&#30340;&#26032;&#39062;&#30340;&#12289;&#26080;&#38656;&#23884;&#20837;&#30340;&#27169;&#25311;KGE&#36755;&#20986;&#30340;&#33539;&#24335;&#65292;&#23427;&#21482;&#20351;&#29992;&#20102;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#12290;TWIG&#20174;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#23398;&#20064;&#26435;&#37325;&#65292;&#27809;&#26377;&#23545;&#23454;&#20307;&#25110;&#36793;&#30340;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;UMLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21333;&#20010;TWIG&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#25152;&#26377;&#36229;&#21442;&#25968;&#37197;&#32622;&#19979;&#26368;&#20808;&#36827;&#30340;ComplEx-N3 KGE&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#23427;&#21482;&#20351;&#29992;&#20102;2590&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#20294;&#20934;&#30830;&#39044;&#27979;&#20102;1215&#20010;&#19981;&#21516;&#36229;&#21442;&#25968;&#32452;&#21512;&#30340;&#32467;&#26524;&#65292;&#30456;&#24403;&#20110;29322000&#20010;&#21442;&#25968;&#30340;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges. Our experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26681;&#33550;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#23548;&#33268;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#33455;&#29255;&#19978;&#23545;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#21644;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06086</link><description>&lt;p&gt;
&#29992;&#26681;&#33550;&#26469;&#24179;&#34913;&#20559;&#26012;&#30340;&#20837;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Rhizomes to Load Balance Skewed In-Degree Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26681;&#33550;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#23548;&#33268;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#33455;&#29255;&#19978;&#23545;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#26681;&#33550;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#22522;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#28040;&#24687;&#39537;&#21160;&#22270;&#22788;&#29702;&#65292;&#35299;&#20915;&#30001;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#24341;&#36215;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22270;&#30340;&#26681;&#33550;&#26500;&#36896;&#20026;&#20219;&#20309;&#25968;&#37327;&#30340;&#21333;&#20010;&#22823;&#20837;&#24230;&#39030;&#28857;&#21019;&#24314;&#20102;&#22810;&#20010;&#21629;&#21517;&#39030;&#28857;&#22320;&#22336;&#12290;&#28982;&#21518;&#65292;&#20854;&#20182;&#39030;&#28857;&#21487;&#20197;&#25351;&#21521;&#20219;&#20309;&#19968;&#20010;&#21629;&#21517;&#22320;&#22336;&#65292;&#20174;&#32780;&#20849;&#20139;&#20837;&#24230;&#36127;&#36733;&#12290;&#26681;&#33550;&#20869;&#37096;&#36827;&#34892;&#36890;&#20449;&#24182;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#25552;&#20379;&#39030;&#28857;&#30340;&#32479;&#19968;&#21644;&#27491;&#30830;&#30340;&#35270;&#22270;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21253;&#21547;&#39640;&#24230;&#20559;&#26012;&#30340;&#20837;&#24230;&#20998;&#24067;&#30340;&#36755;&#20837;&#22270;&#25968;&#25454;&#38598;&#19978;&#65292;&#23545;&#22823;&#33455;&#29255;&#22823;&#23567;&#19978;&#30340;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#12290;&#25913;&#36827;&#26469;&#33258;&#20110;&#22312;&#20869;&#23384;&#22788;&#29702;&#20803;&#32032;&#20043;&#38388;&#20849;&#20139;&#20837;&#24230;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#38477;&#20302;&#32593;&#32476;&#33455;&#29255;&#30340;&#20105;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper aims to address load imbalance caused by high in-degree distribution in graphs by applying the idea of rhizome to vertex-centric message-driven graph processing. Rhizome construction of the graph creates multiple named vertex address for any number of single large in-degree vertices. It then allows other vertices to point to any of the named addresses thus sharing the in-degree load. The rhizomes internally communicate and remain consistent to provide a unified and correct view of the vertex. Simulated experimental results show performance speed ups for BFS graph traversal on large chip sizes for the tested input graph datasets containing highly skewed in-degree distribution. The improvements come from sharing the in-degree compute workload among memory-processing elements and also lowering contention on the network-on-chip.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SubGen&#30340;&#39640;&#25928;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Attention&#27169;&#22359;&#20013;&#36827;&#34892;&#22312;&#32447;&#32858;&#31867;&#21644;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23376;&#32447;&#24615;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.06082</link><description>&lt;p&gt;
SubGen&#65306;&#23376;&#32447;&#24615;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#20196;&#29260;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SubGen: Token Generation in Sublinear Time and Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06082
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SubGen&#30340;&#39640;&#25928;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;Attention&#27169;&#22359;&#20013;&#36827;&#34892;&#22312;&#32447;&#32858;&#31867;&#21644;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23376;&#32447;&#24615;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24191;&#27867;&#30340;&#20869;&#23384;&#38656;&#27714;&#20351;&#24471;&#22312;&#38271;&#19978;&#19979;&#25991;&#20196;&#29260;&#29983;&#25104;&#29615;&#22659;&#20013;&#37096;&#32626;&#23427;&#20204;&#23384;&#22312;&#25361;&#25112;&#12290;LLM&#35299;&#30721;&#22120;&#30340;&#24040;&#22823;&#20869;&#23384;&#21344;&#29992;&#37327;&#26469;&#33258;&#20110;&#22312;&#27880;&#24847;&#27169;&#22359;&#20013;&#23384;&#20648;&#25152;&#26377;&#20808;&#21069;&#20196;&#29260;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#26159;&#30001;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#25152;&#24378;&#21046;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#24320;&#21457;&#19968;&#31181;&#39640;&#25928;&#30340;&#38190;&#20540;&#32531;&#23384;&#21387;&#32553;&#25216;&#26415;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#27880;&#24847;&#27169;&#22359;&#20013;&#30340;&#38190;&#23884;&#20837;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#32858;&#31867;&#20542;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#20851;&#38190;&#27934;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#23376;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#32531;&#23384;&#26041;&#27861;&#65292;&#37319;&#29992;&#38190;&#20196;&#29260;&#30340;&#22312;&#32447;&#32858;&#31867;&#21644;&#20540;&#30340;&#22312;&#32447;l2&#37319;&#26679;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#21487;&#20197;&#35777;&#26126;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27880;&#24847;&#35299;&#30721;&#31639;&#27861;&#65292;&#31216;&#20026;SubGen&#12290;&#36825;&#20010;&#31639;&#27861;&#19981;&#20165;&#30830;&#20445;&#20102;&#23376;&#32447;&#24615;&#20869;&#23384;&#21344;&#29992;&#21644;&#23376;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#36824;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#35823;&#24046;&#30028;&#12290;&#32463;&#39564;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online $\ell_2$ sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed SubGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.06079</link><description>&lt;p&gt;
DiscDiff: DNA&#24207;&#21015;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiscDiff: Latent Diffusion Model for DNA Sequence Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#29992;&#20110;&#29983;&#25104;DNA&#24207;&#21015;&#65292;&#21253;&#25324;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#29289;&#31181;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#24182;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#20135;&#29983;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DNA&#24207;&#21015;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;DiscDiff&#65292;&#19968;&#31181;&#20026;&#29983;&#25104;&#31163;&#25955;DNA&#24207;&#21015;&#32780;&#23450;&#21046;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#65292;&#20197;&#21450;Absorb-Escape&#65292;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#36825;&#20123;&#24207;&#21015;&#30340;&#21518;&#35757;&#32451;&#31639;&#27861;&#12290;Absorb-Escape&#36890;&#36807;&#32416;&#27491;&#22312;&#28508;&#22312;&#31354;&#38388;&#21644;&#36755;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#8220;&#33293;&#20837;&#35823;&#24046;&#8221;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#24207;&#21015;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;DNA&#24207;&#21015;&#29983;&#25104;&#26041;&#38754;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#32780;&#19988;&#22312;&#29983;&#25104;&#30701;&#24207;&#21015;&#21644;&#38271;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;EPD-GenDNA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28085;&#30422;15&#20010;&#29289;&#31181;&#30340;&#12289;&#32508;&#21512;&#24615;&#30340;DNA&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;160,000&#20010;&#29420;&#29305;&#24207;&#21015;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#33021;&#25512;&#21160;DNA&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#23545;&#22522;&#22240;&#27835;&#30103;&#21644;&#34507;&#30333;&#36136;&#29983;&#20135;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel framework for DNA sequence generation, comprising two key components: DiscDiff, a Latent Diffusion Model (LDM) tailored for generating discrete DNA sequences, and Absorb-Escape, a post-training algorithm designed to refine these sequences. Absorb-Escape enhances the realism of the generated sequences by correcting `round errors' inherent in the conversion process between latent and input spaces. Our approach not only sets new standards in DNA sequence generation but also demonstrates superior performance over existing diffusion models, in generating both short and long DNA sequences. Additionally, we introduce EPD-GenDNA, the first comprehensive, multi-species dataset for DNA generation, encompassing 160,000 unique sequences from 15 species. We hope this study will advance the generative modelling of DNA, with potential implications for gene therapy and protein production.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21487;&#25215;&#21463;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06078</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#21487;&#25215;&#21463;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Models for Affordance Learning using Bayesian Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#21487;&#25215;&#21463;&#24615;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25215;&#21463;&#24615;&#26159;&#25551;&#36848;&#21160;&#20316;&#12289;&#29289;&#20307;&#21644;&#25928;&#26524;&#20043;&#38388;&#20851;&#31995;&#30340;&#22522;&#26412;&#25551;&#36848;&#31526;&#12290;&#23427;&#20204;&#20026;&#26426;&#22120;&#20154;&#39044;&#27979;&#25928;&#26524;&#12289;&#35782;&#21035;&#21160;&#20316;&#12289;&#36873;&#25321;&#29289;&#20307;&#21644;&#26681;&#25454;&#26399;&#26395;&#30446;&#26631;&#35268;&#21010;&#34892;&#20026;&#25552;&#20379;&#20102;&#25163;&#27573;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#20027;&#23398;&#20064;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#25506;&#32034;&#19990;&#30028;&#24182;&#20174;&#24863;&#23448;&#20307;&#39564;&#20013;&#23398;&#20064;&#36825;&#20123;&#21487;&#25215;&#21463;&#24615;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#26377;&#20102;&#23398;&#20064;&#32534;&#30721;&#36825;&#20123;&#30693;&#35782;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#36125;&#21494;&#26031;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#21644;&#20887;&#20313;&#65292;&#20294;&#20197;&#24448;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#31163;&#25955;&#24863;&#23448;&#25968;&#25454;&#30340;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23548;&#33268;&#20005;&#37325;&#38169;&#35823;&#12290;&#26412;&#25991;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMMs&#65289;&#23545;&#20256;&#24863;&#22120;&#36827;&#34892;&#27010;&#29575;&#34920;&#31034;&#65292;&#24182;&#26126;&#30830;&#32771;&#34385;&#27599;&#20010;&#31163;&#25955;&#21487;&#25215;&#21463;&#24615;&#27010;&#24565;&#20013;&#21253;&#21547;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#27491;&#30830;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordances are fundamental descriptors of relationships between actions, objects and effects. They provide the means whereby a robot can predict effects, recognize actions, select objects and plan its behavior according to desired goals. This paper approaches the problem of an embodied agent exploring the world and learning these affordances autonomously from its sensory experiences. Models exist for learning the structure and the parameters of a Bayesian Network encoding this knowledge. Although Bayesian Networks are capable of dealing with uncertainty and redundancy, previous work considered complete observability of the discrete sensory data, which may lead to hard errors in the presence of noise. In this paper we consider a probabilistic representation of the sensors by Gaussian Mixture Models (GMMs) and explicitly taking into account the probability distribution contained in each discrete affordance concept, which can lead to a more correct learning.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#25112;&#20105;&#28216;&#25103;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#12289;&#25913;&#21892;&#20915;&#31574;&#36895;&#24230;&#21644;&#36136;&#37327;&#12289;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#24314;&#35758;&#20197;&#21450;&#26356;&#24555;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#34892;&#21160;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#65292;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#30340;&#25351;&#23548;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.06075</link><description>&lt;p&gt;
&#23454;&#29616;&#25968;&#23383;&#21270;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#25112;&#20105;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Scaling Artificial Intelligence for Digital Wargaming in Support of Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06075
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20915;&#31574;&#25903;&#25345;&#19979;&#30340;&#25112;&#20105;&#28216;&#25103;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35268;&#27169;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#12289;&#25913;&#21892;&#20915;&#31574;&#36895;&#24230;&#21644;&#36136;&#37327;&#12289;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#24314;&#35758;&#20197;&#21450;&#26356;&#24555;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#34892;&#21160;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#65292;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#30340;&#25351;&#23548;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#30001;&#25216;&#26415;&#39537;&#21160;&#30340;&#21464;&#38761;&#26102;&#20195;&#65292;&#25105;&#20204;&#26356;&#38656;&#35201;&#31215;&#26497;&#25237;&#36164;&#20110;&#24320;&#21457;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#25903;&#25345;&#20915;&#31574;&#30340;&#25112;&#20105;&#28216;&#25103;&#12290;&#36890;&#36807;&#25512;&#36827;AI&#25216;&#26415;&#31995;&#32479;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#33021;&#22815;&#25552;&#39640;&#20840;&#22495;&#24847;&#35782;&#65292;&#25913;&#21892;&#20915;&#31574;&#21608;&#26399;&#30340;&#36895;&#24230;&#21644;&#36136;&#37327;&#65292;&#25552;&#20379;&#21019;&#26032;&#34892;&#21160;&#30340;&#24314;&#35758;&#65292;&#26356;&#36805;&#36895;&#22320;&#24212;&#23545;&#23545;&#25163;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#21152;&#24555;AI&#30340;&#24320;&#21457;&#65292;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#24212;&#23545;&#29616;&#20195;&#25361;&#25112;&#21644;&#22256;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#25361;&#25112;&#30446;&#21069;&#38656;&#35201;&#20154;&#31867;&#26234;&#33021;&#65292;&#24182;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#35797;&#22270;&#36229;&#36234;&#20154;&#31867;&#26234;&#33021;-&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#32780;&#26159;&#20197;&#26426;&#22120;&#36895;&#24230;&#22686;&#24378;&#21644;&#26356;&#22909;&#22320;&#25351;&#23548;&#20154;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this unprecedented era of technology-driven transformation, it becomes more critical than ever that we aggressively invest in developing robust artificial intelligence (AI) for wargaming in support of decision-making. By advancing AI-enabled systems and pairing these with human judgment, we will be able to enhance all-domain awareness, improve the speed and quality of our decision cycles, offer recommendations for novel courses of action, and more rapidly counter our adversary's actions. It therefore becomes imperative that we accelerate the development of AI to help us better address the complexity of modern challenges and dilemmas that currently requires human intelligence and, if possible, attempt to surpass human intelligence--not to replace humans, but to augment and better inform human decision-making at machine speed. Although deep reinforcement learning continues to show promising results in intelligent agent behavior development for the long-horizon, complex tasks typically
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#36941;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22312;&#21019;&#26032;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#20013;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25214;&#21040;&#22810;&#31181;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#21644;&#28548;&#28165;&#38382;&#39064;&#38472;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.06053</link><description>&lt;p&gt;
&#38543;&#26426;&#24615;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#39064;-&#35299;&#20915;&#31354;&#38388;&#20013;&#36827;&#34892;&#35821;&#20041;&#36941;&#21382;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Randomness Is All You Need: Semantic Traversal of Problem-Solution Spaces with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06053
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#36941;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22312;&#21019;&#26032;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#20013;&#25506;&#32034;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25214;&#21040;&#22810;&#31181;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#21644;&#28548;&#28165;&#38382;&#39064;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#24605;&#36335;&#25968;&#25454;&#24211;&#23545;&#21019;&#26032;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#39046;&#22495;&#36827;&#34892;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#28201;&#24230;&#32423;&#21035;&#19979;&#35821;&#20041;&#22320;&#36941;&#21382;&#21452;&#21521;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#26641;&#65292;&#25105;&#20204;&#22312;&#35299;&#20915;&#26041;&#26696;&#32534;&#36753;&#36317;&#31163;&#19978;&#23454;&#29616;&#20102;&#39640;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#19982;&#21407;&#22987;&#38382;&#39064;&#35821;&#20041;&#19978;&#30340;&#25509;&#36817;&#12290;&#38500;&#20102;&#25214;&#21040;&#32473;&#23450;&#38382;&#39064;&#30340;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#21644;&#28548;&#28165;&#21407;&#22987;&#38382;&#39064;&#38472;&#36848;&#12290;&#20316;&#20026;&#36827;&#19968;&#27493;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#23581;&#35797;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;Slack bot&#65292;&#20316;&#20026;&#21019;&#26032;&#21161;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to exploring innovation problem and solution domains using LLM fine-tuning with a custom idea database. By semantically traversing the bi-directional problem and solution tree at different temperature levels we achieve high diversity in solution edit distance while still remaining close to the original problem statement semantically. In addition to finding a variety of solutions to a given problem, this method can also be used to refine and clarify the original problem statement. As further validation of the approach, we implemented a proof-of-concept Slack bot to serve as an innovation assistant.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.06049</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits of Large Language Models in Debating Humans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06049
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;&#23558;&#23427;&#20204;&#20316;&#20026;&#20154;&#24037;&#20195;&#34920;&#21644;&#26367;&#20195;&#21697;&#36827;&#34892;&#31038;&#20250;&#23398;&#23454;&#39564;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#19968;&#20010;&#20196;&#20154;&#28608;&#21160;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#36825;&#20010;&#24819;&#27861;&#26377;&#22810;&#21487;&#34892;&#21602;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#27880;&#20876;&#30340;&#30740;&#31350;&#26469;&#27979;&#35797;&#29616;&#38454;&#27573;LLMs&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#23558;&#30495;&#23454;&#30340;&#20154;&#31867;&#19982;&#25198;&#28436;&#20154;&#31867;&#30340;LLM&#20195;&#29702;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#36777;&#35770;&#20026;&#22522;&#30784;&#30340;&#24847;&#35265;&#20849;&#35782;&#24418;&#25104;&#22312;&#19977;&#31181;&#29615;&#22659;&#19979;&#30340;&#24773;&#20917;&#65306;&#20165;&#20154;&#31867;&#12289;&#20195;&#29702;&#21644;&#20154;&#31867;&#12289;&#20165;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;LLM&#20195;&#29702;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#21542;&#19982;&#20154;&#31867;&#30456;&#20284;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#33021;&#22815;&#34701;&#20837;&#24182;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#65292;&#26368;&#32456;&#34892;&#20026;&#19982;&#20154;&#31867;&#26377;&#25152;&#20559;&#31163;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#39044;&#35745;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#24517;&#39035;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day LLMs with a pre-registered study integrating real people with LLM agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how LLM agents influence humans, and how capable they are in debating like humans. We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.
&lt;/p&gt;</description></item><item><title>2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;</title><link>https://arxiv.org/abs/2402.06046</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#20107;&#25925;&#30340;&#35299;&#21078;&#65306;&#20174;Cruise&#34892;&#20154;&#25302;&#25341;&#20107;&#25925;&#20013;&#21560;&#21462;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Anatomy of a Robotaxi Crash: Lessons from the Cruise Pedestrian Dragging Mishap
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06046
&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#19968;&#36215;GM Cruise&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#34892;&#20154;&#30456;&#25758;&#30340;&#20107;&#25925;&#32473;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#35813;&#20844;&#21496;&#22312;&#22788;&#29702;&#20107;&#25925;&#21518;&#30340;&#22833;&#35823;&#12290;&#36825;&#19968;&#20107;&#20214;&#30340;&#35299;&#21078;&#25552;&#20379;&#20102;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#21453;&#24212;&#26041;&#38754;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;10&#26376;&#65292;&#22312;&#26087;&#37329;&#23665;&#65292;&#19968;&#36742;&#36890;&#29992;&#27773;&#36710;Cruise&#30340;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#19982;&#19968;&#21517;&#34892;&#20154;&#30456;&#25758;&#65292;&#36896;&#25104;&#20102;&#20005;&#37325;&#30340;&#20260;&#23475;&#65292;&#21516;&#26102;&#20063;&#23545;&#35813;&#20844;&#21496;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21160;&#33633;&#65292;&#36825;&#24456;&#21487;&#33021;&#20250;&#23545;&#25972;&#20010;&#34892;&#19994;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#38382;&#39064;&#19981;&#20165;&#20165;&#28304;&#20110;&#20107;&#25925;&#26412;&#36523;&#65292;&#36824;&#21253;&#25324;Cruise&#22312;&#22788;&#29702;&#26426;&#22120;&#20154;&#20986;&#31199;&#36710;&#25758;&#21040;&#34892;&#20154;&#21518;&#34987;&#25302;&#34892;&#30340;&#36807;&#31243;&#20013;&#30340;&#22833;&#35823;&#12290;&#20004;&#20221;&#22806;&#37096;&#35843;&#26597;&#25253;&#21578;&#25552;&#20379;&#20102;&#25551;&#36848;&#20107;&#20214;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#24182;&#20174;&#30417;&#31649;&#20114;&#21160;&#30340;&#35282;&#24230;&#25209;&#35780;&#20102;&#20844;&#21496;&#30340;&#21453;&#24212;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#28508;&#22312;&#30340;&#23433;&#20840;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25253;&#21578;&#26448;&#26009;&#26469;&#24378;&#35843;&#20855;&#20307;&#30340;&#20107;&#23454;&#21644;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#23558;&#25253;&#21578;&#26448;&#26009;&#30340;&#19981;&#21516;&#37096;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#22312;&#25216;&#26415;&#12289;&#25805;&#20316;&#23433;&#20840;&#23454;&#36341;&#21644;&#32452;&#32455;&#23545;&#20107;&#20214;&#30340;&#21453;&#24212;&#26041;&#38754;&#21487;&#33021;&#21487;&#20197;&#23398;&#21040;&#30340;&#23433;&#20840;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
An October 2023 crash between a GM Cruise robotaxi and a pedestrian in San Francisco resulted not only in a severe injury, but also dramatic upheaval at that company that will likely have lasting effects throughout the industry. The issues stem not just from the crash facts themselves, but also how Cruise mishandled dealing with their robotaxi dragging a pedestrian under the vehicle after the initial post-crash stop. A pair of external investigation reports provide raw material describing the incident and critique the company response from a regulatory interaction point of view, but did not include potential safety recommendations in scope. We use that report material to highlight specific facts and relationships between events by tying together different pieces of the report material. We then explore safety lessons that might be learned with regard to technology, operational safety practices, and organizational reaction to incidents.
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06038</link><description>&lt;p&gt;
&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#65288;Positive Unlabeled&#65289;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contrastive Approach to Prior Free Positive Unlabeled Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#20808;&#39564;&#27491;&#26080;&#26631;&#23398;&#20064;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#26080;&#26631;&#65288;Positive Unlabeled&#65289;&#23398;&#20064;&#26159;&#25351;&#22312;&#32473;&#23450;&#23569;&#37327;&#26631;&#35760;&#30340;&#27491;&#26679;&#26412;&#21644;&#19968;&#32452;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#33021;&#26159;&#27491;&#20363;&#25110;&#36127;&#20363;&#65289;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19968;&#20010;&#20108;&#20998;&#31867;&#22120;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#26080;&#26631;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#35777;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#23884;&#20837;&#30340;&#27987;&#24230;&#29305;&#24615;&#23545;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#20266;&#26631;&#31614;&#22788;&#29702;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#27491;&#26080;&#26631;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36731;&#26494;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27491;&#26080;&#26631;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#31867;&#20808;&#39564;&#30340;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#32780;&#22823;&#22810;&#25968;&#27491;&#26080;&#26631;&#23398;&#20064;&#31639;&#27861;&#21017;&#22833;&#36133;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25512;&#21160;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#33324;&#21270;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled (PU) learning refers to the task of learning a binary classifier given a few labeled positive samples, and a set of unlabeled samples (which could be positive or negative). In this paper, we propose a novel PU learning framework, that starts by learning a feature space through pretext-invariant representation learning and then applies pseudo-labeling to the unlabeled examples, leveraging the concentration property of the embeddings. Overall, our proposed approach handily outperforms state-of-the-art PU learning methods across several standard PU benchmark datasets, while not requiring a-priori knowledge or estimate of class prior. Remarkably, our method remains effective even when labeled data is scant, where most PU learning algorithms falter. We also provide simple theoretical analysis motivating our proposed algorithms and establish generalization guarantee for our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI&#12290;&#23454;&#39564;&#35777;&#26126;MPGD&#22312;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.06034</link><description>&lt;p&gt;
&#20351;&#29992;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI
&lt;/p&gt;
&lt;p&gt;
Optimizing Predictive AI in Physical Design Flows with Mini Pixel Batch Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#39044;&#27979;AI&#12290;&#23454;&#39564;&#35777;&#26126;MPGD&#22312;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29190;&#28856;&#24335;&#30340;&#39044;&#27979;AI&#22312;&#29616;&#20195;&#33455;&#29255;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#32780;&#26377;&#25928;&#30340;&#35780;&#20272;&#21644;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26694;&#26550;&#36890;&#24120;&#21253;&#25324;&#26368;&#23567;&#21270;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35748;&#20026;MSE&#30340;&#24179;&#22343;&#25928;&#26524;&#23548;&#33268;&#27169;&#22411;&#35757;&#32451;&#21644;&#37096;&#32626;&#20004;&#26041;&#38754;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#32780;&#33391;&#22909;&#30340;MSE&#34892;&#20026;&#19981;&#33021;&#20445;&#35777;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#33021;&#30001;&#20110;&#23569;&#37327;&#39044;&#27979;&#35823;&#24046;&#32780;&#21463;&#25439;&#30340;&#29289;&#29702;&#35774;&#35745;&#27969;&#31243;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36855;&#20320;&#20687;&#32032;&#25209;&#37327;&#26799;&#24230;&#19979;&#38477;&#65288;MPGD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26465;&#30446;&#65292;&#21487;&#33021;&#25552;&#20379;&#26356;&#24555;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#20195;&#34920;&#24615;&#22522;&#20934;&#22871;&#20214;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MPGD&#22312;&#20351;&#29992;CNN&#25110;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#36827;&#34892;&#21508;&#31181;&#29289;&#29702;&#35774;&#35745;&#39044;&#27979;&#20219;&#21153;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploding predictive AI has enabled fast yet effective evaluation and decision-making in modern chip physical design flows. State-of-the-art frameworks typically include the objective of minimizing the mean square error (MSE) between the prediction and the ground truth. We argue the averaging effect of MSE induces limitations in both model training and deployment, and good MSE behavior does not guarantee the capability of these models to assist physical design flows which are likely sabotaged due to a small portion of prediction error. To address this, we propose mini-pixel batch gradient descent (MPGD), a plug-and-play optimization algorithm that takes the most informative entries into consideration, offering probably faster and better convergence. Experiments on representative benchmark suits show the significant benefits of MPGD on various physical design prediction tasks using CNN or Graph-based models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20540;&#27861;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#22312;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.06030</link><description>&lt;p&gt;
&#21338;&#24328;&#35770;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic Counterfactual Explanation for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20540;&#27861;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#28040;&#38500;&#20102;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#19982;&#20854;&#20182;&#27969;&#34892;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#22312;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#26102;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#23545;&#29992;&#25143;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#36825;&#20351;&#24471;&#29702;&#35299;&#20854;&#39044;&#27979;&#32972;&#21518;&#30340;&#25512;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CFE&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#35745;&#31639;GNNs&#30340;CFE&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#39069;&#22806;&#30340;&#22270;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20540;&#30340;&#12289;&#38750;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;CFE&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#39069;&#22806;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#35745;&#31639;Shapley&#20540;&#31561;&#20854;&#20182;&#27969;&#34892;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#38656;&#35201;&#26356;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35782;&#21035;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#19982;Shapley&#20540;&#30456;&#27604;&#65292;&#35745;&#31639;Banzhaf&#20540;&#21487;&#20197;&#23454;&#29616;&#22235;&#20493;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#38408;&#20540;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#22833;&#26799;&#24230;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06026</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20197;&#32531;&#35299;&#24179;&#26495;&#22369;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Quantum neural network with ensemble learning to mitigate barren plateaus and cost function concentration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#28040;&#22833;&#26799;&#24230;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24555;&#36895;&#21457;&#23637;&#25215;&#35834;&#22312;&#31185;&#23398;&#21644;&#25216;&#26415;&#39046;&#22495;&#20135;&#29983;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#21069;&#27839;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#22411;&#65292;&#20294;&#25345;&#32493;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#28040;&#22833;&#26799;&#24230;&#65288;VG&#65289;&#21644;&#25104;&#26412;&#20989;&#25968;&#38598;&#20013;&#65288;CFC&#65289;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#25104;&#21151;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;VG&#21644;CFC&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#65292;&#25512;&#23815;&#21516;&#26102;&#37096;&#32626;&#22810;&#20010;&#28145;&#24230;&#20026;1&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#21333;&#19968;&#28145;&#24230;&#20026;L&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#26500;&#24314;&#30340;QNN&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#23637;&#24320;&#65292;&#20026;&#20102;&#23545;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#21069;&#26223;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of quantum computers promises transformative impacts across diverse fields of science and technology. Quantum neural networks (QNNs), as a forefront application, hold substantial potential. Despite the multitude of proposed models in the literature, persistent challenges, notably the vanishing gradient (VG) and cost function concentration (CFC) problems, impede their widespread success. In this study, we introduce a novel approach to quantum neural network construction, specifically addressing the issues of VG and CFC. Our methodology employs ensemble learning, advocating for the simultaneous deployment of multiple quantum circuits with a depth equal to $1$, a departure from the conventional use of a single quantum circuit with depth $L$. We assess the efficacy of our proposed model through a comparative analysis with a conventionally constructed QNN. The evaluation unfolds in the context of a classification problem, yielding valuable insights into the potential a
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#23545;DRL&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#21021;&#22987;&#24341;&#23548;&#65292;&#24182;&#20419;&#36827;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#39640;&#25928;&#21487;&#38752;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.06023</link><description>&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24555;&#36895;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision Theory-Guided Deep Reinforcement Learning for Fast Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06023
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#23545;DRL&#26234;&#33021;&#20307;&#30340;&#26377;&#25928;&#21021;&#22987;&#24341;&#23548;&#65292;&#24182;&#20419;&#36827;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#39640;&#25928;&#21487;&#38752;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#20915;&#31574;&#29702;&#35770;&#24341;&#23548;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DT-guided DRL&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22266;&#26377;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20915;&#31574;&#29702;&#35770;&#21407;&#21017;&#65292;DT-guided DRL&#22686;&#24378;&#20102;&#26234;&#33021;&#20307;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#21021;&#22987;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#26356;&#39640;&#25928;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#32972;&#26223;&#65306;&#26438;&#36710;&#21644;&#36855;&#23467;&#23548;&#33322;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#29702;&#35770;&#30340;&#25972;&#21512;&#19981;&#20165;&#26377;&#21161;&#20110;&#23545;DRL&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#25928;&#30340;&#21021;&#22987;&#24341;&#23548;&#65292;&#36824;&#20419;&#36827;&#20102;&#22312;&#20855;&#26377;&#22823;&#35268;&#27169;&#21644;&#22797;&#26434;&#29366;&#24577;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#26356;&#26377;&#32467;&#26500;&#21644;&#30693;&#24773;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24120;&#35268;&#30340;DRL&#30456;&#27604;&#65292;DT-guided DRL&#33021;&#22815;&#25552;&#20379;&#26174;&#33879;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#65292;DT-guided DRL&#30340;&#32047;&#31215;&#22870;&#21169;&#22686;&#21152;&#20102;184%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach, Decision Theory-guided Deep Reinforcement Learning (DT-guided DRL), to address the inherent cold start problem in DRL. By integrating decision theory principles, DT-guided DRL enhances agents' initial performance and robustness in complex environments, enabling more efficient and reliable convergence during learning. Our investigation encompasses two primary problem contexts: the cart pole and maze navigation challenges. Experimental results demonstrate that the integration of decision theory not only facilitates effective initial guidance for DRL agents but also promotes a more structured and informed exploration strategy, particularly in environments characterized by large and intricate state spaces. The results of experiment demonstrate that DT-guided DRL can provide significantly higher rewards compared to regular DRL. Specifically, during the initial phase of training, the DT-guided DRL yields up to an 184% increase in accumulated reward. Mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#28151;&#21512;&#31209;&#21387;&#32553;&#31574;&#30053;&#26469;&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#20302;&#31209;&#26435;&#37325;&#24352;&#37327;&#36817;&#20284;&#21644;&#23618;&#38388;&#35823;&#24046;&#34917;&#20607;&#25216;&#26415;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#36991;&#20813;&#20102;&#27973;&#23618;&#23616;&#37096;&#26368;&#23567;&#20540;&#38519;&#38449;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06004</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#35270;&#35273;Transformer&#65306;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#28151;&#21512;&#31209;&#21387;&#32553;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#28151;&#21512;&#31209;&#21387;&#32553;&#31574;&#30053;&#26469;&#25552;&#39640;&#35270;&#35273;Transformer&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#36873;&#25321;&#24615;&#20302;&#31209;&#26435;&#37325;&#24352;&#37327;&#36817;&#20284;&#21644;&#23618;&#38388;&#35823;&#24046;&#34917;&#20607;&#25216;&#26415;&#26469;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#31181;&#31574;&#30053;&#36991;&#20813;&#20102;&#27973;&#23618;&#23616;&#37096;&#26368;&#23567;&#20540;&#38519;&#38449;&#65292;&#21516;&#26102;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#19981;&#26029;&#21047;&#26032;&#26368;&#26032;&#35760;&#24405;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#24341;&#25806;&#19978;&#30340;&#23454;&#38469;&#37096;&#32626;&#24448;&#24448;&#21463;&#21040;&#26174;&#33879;&#30340;&#20869;&#23384;&#24102;&#23485;&#21644;&#65288;&#33455;&#29255;&#20869;&#65289;&#20869;&#23384;&#21344;&#29992;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#28608;&#27963;&#24863;&#30693;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#23618;&#30340;&#36873;&#25321;&#24615;&#20302;&#31209;&#26435;&#37325;&#24352;&#37327;&#36817;&#20284;&#26469;&#20943;&#23569;ViTs&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#26435;&#37325;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24352;&#37327;&#20043;&#21644;&#65292;&#21516;&#26102;&#23558;&#36755;&#20837;&#28608;&#27963;&#19982;&#21407;&#22987;&#26435;&#37325;&#24352;&#37327;&#30340;&#20056;&#31215;&#19982;&#36755;&#20837;&#28608;&#27963;&#19982;&#36817;&#20284;&#24352;&#37327;&#20043;&#21644;&#30340;&#20056;&#31215;&#20043;&#38388;&#30340;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#36890;&#36807;&#37319;&#29992;&#26377;&#25928;&#30340;&#36880;&#23618;&#35823;&#24046;&#34917;&#20607;&#25216;&#26415;&#65292;&#21033;&#29992;&#23618;&#36755;&#20986;&#25439;&#22833;&#30340;&#26799;&#24230;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#36817;&#20284;&#12290;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#22312;&#36991;&#20813;&#38519;&#20837;&#27973;&#23618;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#21516;&#26102;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Vision Transformers (ViTs) increasingly set new benchmarks in computer vision, their practical deployment on inference engines is often hindered by their significant memory bandwidth and (on-chip) memory footprint requirements. This paper addresses this memory limitation by introducing an activation-aware model compression methodology that uses selective low-rank weight tensor approximations of different layers to reduce the parameter count of ViTs. The key idea is to decompose the weight tensors into a sum of two parameter-efficient tensors while minimizing the error between the product of the input activations with the original weight tensor and the product of the input activations with the approximate tensor sum. This approximation is further refined by adopting an efficient layer-wise error compensation technique that uses the gradient of the layer's output loss. The combination of these techniques achieves excellent results while it avoids being trapped in a shallow local minim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.05980</link><description>&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#65311;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do Large Code Models Understand Programming Concepts? A Black-box Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21453;&#20107;&#23454;&#20998;&#26512;&#26694;&#26550;&#35780;&#20272;&#20102;&#21313;&#20010;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#31181;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#65292;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#25104;&#21151;&#20063;&#20351;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#32534;&#30721;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#24037;&#20316;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#34917;&#20840;&#21644;&#32534;&#36753;&#31561;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20026;&#20160;&#20040;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#24213;&#23618;&#31243;&#24207;&#30340;&#36923;&#36753;&#32467;&#26500;&#29702;&#35299;&#31243;&#24230;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#32534;&#31243;&#27010;&#24565;&#35859;&#35789;&#30340;&#21453;&#20107;&#23454;&#20998;&#26512;&#65288;CACP&#65289;&#20316;&#20026;&#19968;&#31181;&#21453;&#20107;&#23454;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#32534;&#31243;&#27010;&#24565;&#12290;&#21482;&#36890;&#36807;&#40657;&#30418;&#35775;&#38382;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;CACP&#35780;&#20272;&#20102;&#21313;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#20195;&#30721;&#27169;&#22411;&#23545;&#22235;&#20010;&#19981;&#21516;&#32534;&#31243;&#27010;&#24565;&#30340;&#29702;&#35299;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#27169;&#22411;&#32570;&#20047;&#23545;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#31561;&#27010;&#24565;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#20351;&#29992;&#26465;&#27454;&#30340;&#26631;&#20934;&#21270;&#21450;&#20854;&#29992;&#20110;&#36127;&#36131;&#20219;&#25480;&#26435;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#12290;&#37319;&#29992;&#20102;&#23450;&#24615;&#35775;&#35848;&#12289;&#35768;&#21487;&#35777;&#26465;&#27454;&#30340;&#32858;&#31867;&#21644;&#35768;&#21487;&#35777;&#37319;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#23398;&#12290;&#32467;&#35770;&#26159;&#36127;&#36131;&#20219;AI&#35768;&#21487;&#35777;&#38656;&#35201;&#26631;&#20934;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05979</link><description>&lt;p&gt;
&#20851;&#20110;&#34892;&#20026;&#20351;&#29992;&#26465;&#27454;&#30340;&#26631;&#20934;&#21270;&#21450;&#20854;&#29992;&#20110;&#36127;&#36131;&#20219;&#25480;&#26435;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Standardization of Behavioral Use Clauses and Their Adoption for Responsible Licensing of AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34892;&#20026;&#20351;&#29992;&#26465;&#27454;&#30340;&#26631;&#20934;&#21270;&#21450;&#20854;&#29992;&#20110;&#36127;&#36131;&#20219;&#25480;&#26435;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#12290;&#37319;&#29992;&#20102;&#23450;&#24615;&#35775;&#35848;&#12289;&#35768;&#21487;&#35777;&#26465;&#27454;&#30340;&#32858;&#31867;&#21644;&#35768;&#21487;&#35777;&#37319;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#23398;&#12290;&#32467;&#35770;&#26159;&#36127;&#36131;&#20219;AI&#35768;&#21487;&#35777;&#38656;&#35201;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;AI&#30340;&#30095;&#24573;&#25110;&#24694;&#24847;&#20351;&#29992;&#30340;&#24551;&#34385;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#22686;&#21152;&#20102;&#23545;&#24110;&#21161;&#31649;&#29702;&#25216;&#26415;&#39118;&#38505;&#30340;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;2018&#24180;&#65292;&#25552;&#20986;&#20102;&#24102;&#26377;&#34892;&#20026;&#20351;&#29992;&#26465;&#27454;&#65288;&#36890;&#24120;&#31216;&#20026;&#36127;&#36131;&#20219;AI&#35768;&#21487;&#35777;&#65289;&#30340;&#35768;&#21487;&#35777;&#65292;&#20026;&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#21457;&#24067;AI&#36164;&#20135;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25351;&#23450;&#20854;&#29992;&#25143;&#20197;&#20943;&#36731;&#36127;&#38754;&#24212;&#29992;&#12290;&#25130;&#33267;2023&#24180;&#24213;&#65292;&#22823;&#32422;&#26377;40,000&#20010;&#36719;&#20214;&#21644;&#27169;&#22411;&#23384;&#20648;&#24211;&#37319;&#29992;&#20102;&#36127;&#36131;&#20219;AI&#35768;&#21487;&#35777;&#12290;&#37319;&#29992;&#34892;&#20026;&#20351;&#29992;&#26465;&#27454;&#30340;&#33879;&#21517;&#27169;&#22411;&#21253;&#25324;BLOOM&#65288;&#35821;&#35328;&#65289;&#21644;LLaMA2&#65288;&#35821;&#35328;&#65289;&#12289;Stable Diffusion&#65288;&#22270;&#20687;&#65289;&#21644;GRID&#65288;&#26426;&#22120;&#20154;&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#35768;&#21487;&#35777;&#20026;&#20309;&#20197;&#21450;&#22914;&#20309;&#34987;&#37319;&#29992;&#65292;&#20197;&#21450;&#20026;&#20309;&#20197;&#21450;&#22914;&#20309;&#34987;&#25913;&#32534;&#20197;&#36866;&#24212;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23450;&#24615;&#35775;&#35848;&#12289;&#35768;&#21487;&#35777;&#26465;&#27454;&#30340;&#32858;&#31867;&#21644;&#35768;&#21487;&#35777;&#37319;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#30340;&#28151;&#21512;&#26041;&#27861;&#23398;&#12290;&#26681;&#25454;&#36825;&#20123;&#35777;&#25454;&#65292;&#25105;&#20204;&#35748;&#20026;&#36127;&#36131;&#20219;AI&#35768;&#21487;&#35777;&#38656;&#35201;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing concerns over negligent or malicious uses of AI have increased the appetite for tools that help manage the risks of the technology. In 2018, licenses with behaviorial-use clauses (commonly referred to as Responsible AI Licenses) were proposed to give developers a framework for releasing AI assets while specifying their users to mitigate negative applications. As of the end of 2023, on the order of 40,000 software and model repositories have adopted responsible AI licenses licenses. Notable models licensed with behavioral use clauses include BLOOM (language) and LLaMA2 (language), Stable Diffusion (image), and GRID (robotics). This paper explores why and how these licenses have been adopted, and why and how they have been adapted to fit particular use cases. We use a mixed-methods methodology of qualitative interviews, clustering of license clauses, and quantitative analysis of license adoption. Based on this evidence we take the position that responsible AI licenses need standa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#38115;&#21066;&#36807;&#31243;&#20013;&#25554;&#20837;&#29289;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;&#20004;&#20010;&#25551;&#36848;&#31526;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05978</link><description>&lt;p&gt;
&#32467;&#21512;&#24418;&#29366;&#21644;&#36718;&#24275;&#29305;&#24449;&#26469;&#25552;&#39640;&#38115;&#21066;&#36807;&#31243;&#20013;&#30340;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining shape and contour features to improve tool wear monitoring in milling processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#65292;&#29992;&#20110;&#38115;&#21066;&#36807;&#31243;&#20013;&#25554;&#20837;&#29289;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;&#20004;&#20010;&#25551;&#36848;&#31526;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#29366;&#25551;&#36848;&#31526;&#21644;&#36718;&#24275;&#25551;&#36848;&#31526;&#32452;&#21512;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#26681;&#25454;&#30952;&#25439;&#31243;&#24230;&#23545;&#38115;&#21066;&#36807;&#31243;&#20013;&#30340;&#25554;&#20837;&#29289;&#36827;&#34892;&#20998;&#31867;&#65292;&#37319;&#29992;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25551;&#36848;&#30952;&#25439;&#21306;&#22495;&#30340;&#24418;&#29366;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25551;&#36848;&#31526;ShapeFeat&#65292;&#24182;&#20351;&#29992;BORCHIZ&#26041;&#27861;&#23545;&#20854;&#36718;&#24275;&#36827;&#34892;&#34920;&#24449;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#35843;&#26597;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#27861;&#23558;BORCHIZ&#21644;ShapeFeat&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20108;&#20803;&#20998;&#31867;&#23558;&#30952;&#25439;&#20998;&#20026;&#39640;&#25110;&#20302;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;91.44%&#65292;&#19977;&#20010;&#30446;&#26631;&#31867;&#21035;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#30952;&#25439;&#65289;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;82.90%&#12290;&#36825;&#20123;&#32467;&#26524;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#20004;&#20010;&#25551;&#36848;&#31526;&#30340;&#32467;&#26524;&#65292;&#20854;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;88.70%&#21644;80.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new system based on combinations of a shape descriptor and a contour descriptor has been proposed for classifying inserts in milling processes according to their wear level following a computer vision based approach. To describe the wear region shape we have proposed a new descriptor called ShapeFeat and its contour has been characterized using the method BORCHIZ that, to the best of our knowledge, achieves the best performance for tool wear monitoring following a computer vision-based approach. Results show that the combination of BORCHIZ with ShapeFeat using a late fusion method improves the classification performance significantly, obtaining an accuracy of 91.44% in the binary classification (i.e. the classification of the wear as high or low) and 82.90% using three target classes (i.e. classification of the wear as high, medium or low). These results outperform the ones obtained by both descriptors used on their own, which achieve accuracies of 88.70 and 80.67% for
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#20999;&#21066;&#24037;&#20855;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#36890;&#36807;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#21028;&#26029;&#27599;&#20010;&#21306;&#22495;&#30340;&#30952;&#25439;&#31243;&#24230;&#65292;&#20174;&#32780;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#21644;&#20992;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;</title><link>https://arxiv.org/abs/2402.05977</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#32441;&#29702;&#30340;&#22312;&#32447;&#12289;&#33258;&#21160;&#21644;&#20302;&#25104;&#26412;&#31995;&#32479;&#36827;&#34892;&#20992;&#20855;&#30952;&#25439;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Tool wear monitoring using an online, automatic and low cost system based on local texture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#20999;&#21066;&#24037;&#20855;&#30340;&#30952;&#25439;&#30417;&#27979;&#12290;&#36890;&#36807;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#21028;&#26029;&#27599;&#20010;&#21306;&#22495;&#30340;&#30952;&#25439;&#31243;&#24230;&#65292;&#20174;&#32780;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#21644;&#20992;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30340;&#22312;&#32447;&#12289;&#20302;&#25104;&#26412;&#21644;&#24555;&#36895;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#36793;&#32536;&#36718;&#24275;&#38115;&#21066;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#20999;&#21066;&#24037;&#20855;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#30952;&#25439;&#31243;&#24230;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;254&#24352;&#36793;&#32536;&#36718;&#24275;&#20999;&#21066;&#22836;&#22270;&#20687;&#32452;&#25104;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#26681;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#19988;&#20855;&#26377;&#36275;&#22815;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#26377;&#20992;&#29255;&#37117;&#34987;&#20998;&#21106;&#65292;&#24182;&#19988;&#20854;&#20999;&#21066;&#36793;&#32536;&#34987;&#35009;&#21098;&#65292;&#33719;&#24471;&#20102;577&#24352;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#65306;301&#24352;&#21487;&#29992;&#21644;276&#24352;&#21487;&#20002;&#24323;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#65288;1&#65289;&#23558;&#20999;&#21066;&#36793;&#32536;&#22270;&#20687;&#20998;&#20026;&#19981;&#21516;&#30340;&#21306;&#22495;&#65292;&#31216;&#20026;&#30952;&#25439;&#26001;&#22359;&#65288;WP&#65289;&#65292;&#65288;2&#65289;&#20351;&#29992;&#22522;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#23616;&#37096;&#20108;&#20540;&#27169;&#24335;&#65288;LBP&#65289;&#30340;&#32441;&#29702;&#25551;&#36848;&#31526;&#26469;&#34920;&#24449;&#27599;&#20010;&#21306;&#22495;&#26159;&#30952;&#25439;&#36824;&#26159;&#21487;&#29992;&#65292;&#24182;&#65288;3&#65289;&#26681;&#25454;&#36825;&#20123;WP&#30340;&#29366;&#24577;&#26469;&#30830;&#23450;&#20999;&#21066;&#36793;&#32536;&#65288;&#22240;&#27492;&#20063;&#26159;&#20992;&#20855;&#65289;&#26159;&#21542;&#21487;&#26381;&#24441;&#25110;&#21487;&#20002;&#24323;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#26001;&#22359;&#20998;&#21106;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we propose a new online, low cost and fast approach based on computer vision and machine learning to determine whether cutting tools used in edge profile milling processes are serviceable or disposable based on their wear level. We created a new dataset of 254 images of edge profile cutting heads which is, to the best of our knowledge, the first publicly available dataset with enough quality for this purpose. All the inserts were segmented and their cutting edges were cropped, obtaining 577 images of cutting edges: 301 functional and 276 disposable. The proposed method is based on (1) dividing the cutting edge image in different regions, called Wear Patches (WP), (2) characterising each one as worn or serviceable using texture descriptors based on different variants of Local Binary Patterns (LBP) and (3) determine, based on the state of these WP, if the cutting edge (and, therefore, the tool) is serviceable or disposable. We proposed and assessed five different patch divis
&lt;/p&gt;</description></item><item><title>RankSum&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#23545;&#21477;&#23376;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#34701;&#21512;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30417;&#30563;&#20449;&#21495;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.05976</link><description>&lt;p&gt;
RankSum&#65306;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#34701;&#21512;&#30340;&#26080;&#30417;&#30563;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RankSum An unsupervised extractive text summarization based on rank fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05976
&lt;/p&gt;
&lt;p&gt;
RankSum&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#23545;&#21477;&#23376;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#34701;&#21512;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#30417;&#30563;&#20449;&#21495;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ranksum&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25490;&#21517;&#34701;&#21512;&#30340;&#26080;&#30417;&#30563;&#21333;&#25991;&#26723;&#25277;&#21462;&#24335;&#25991;&#26412;&#25688;&#35201;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20026;&#27599;&#20010;&#21477;&#23376;&#25552;&#21462;&#30340;&#22235;&#20010;&#22810;&#32500;&#24230;&#21477;&#23376;&#29305;&#24449;&#36827;&#34892;&#21477;&#23376;&#26174;&#33879;&#24615;&#25490;&#21517;&#65306;&#20027;&#39064;&#20449;&#24687;&#12289;&#35821;&#20041;&#20869;&#23481;&#12289;&#37325;&#35201;&#20851;&#38190;&#35789;&#21644;&#20301;&#32622;&#12290;Ranksum&#26681;&#25454;&#27599;&#20010;&#29305;&#24449;&#29983;&#25104;&#30340;&#21477;&#23376;&#26174;&#33879;&#24615;&#25490;&#21517;&#36827;&#34892;&#21152;&#26435;&#34701;&#21512;&#65292;&#20197;&#30830;&#23450;&#21477;&#23376;&#30340;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;&#34701;&#21512;&#26435;&#37325;&#26159;&#23436;&#20840;&#26080;&#30417;&#30563;&#29983;&#25104;&#30340;&#65292;&#38656;&#35201;&#26631;&#35760;&#30340;&#25991;&#26723;&#38598;&#21512;&#26469;&#23398;&#20064;&#34701;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#21457;&#29616;&#34701;&#21512;&#26435;&#37325;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23558;Ranksum&#35270;&#20026;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#23450;&#20027;&#39064;&#25490;&#21517;&#65292;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#65292;&#32780;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#26469;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#12290;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#26469;&#29983;&#25104;&#25490;&#21517;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#36830;&#20307;&#32593;&#32476;&#20135;&#29983;&#25277;&#35937;&#21270;&#30340;&#21477;&#23376;&#34920;&#31034;&#65292;&#28982;&#21518;&#24418;&#25104;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Ranksum, an approach for extractive text summarization of single documents based on the rank fusion of four multi-dimensional sentence features extracted for each sentence: topic information, semantic content, significant keywords, and position. The Ranksum obtains the sentence saliency rankings corresponding to each feature in an unsupervised way followed by the weighted fusion of the four scores to rank the sentences according to their significance. The scores are generated in completely unsupervised way, and a labeled document set is required to learn the fusion weights. Since we found that the fusion weights can generalize to other datasets, we consider the Ranksum as an unsupervised approach. To determine topic rank, we employ probabilistic topic models whereas semantic information is captured using sentence embeddings. To derive rankings using sentence embeddings, we utilize Siamese networks to produce abstractive sentence representation and then we form
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05975</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#31867;&#21644;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Brain Tumor Classification and Segmentation Using a Multiscale Convolutional Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#33041;&#32959;&#30244;&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#12290;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#21253;&#25324;&#22810;&#23610;&#24230;&#26041;&#27861;&#22312;&#20869;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30340;&#19968;&#20010;&#21306;&#21035;&#26159;&#36755;&#20837;&#22270;&#20687;&#22312;&#19981;&#21516;&#22788;&#29702;&#36335;&#24452;&#19978;&#20197;&#19977;&#20010;&#31354;&#38388;&#23610;&#24230;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#26426;&#21046;&#26159;&#21463;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#20869;&#22312;&#25805;&#20316;&#30340;&#21551;&#31034;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#21253;&#21547;&#19977;&#31181;&#31867;&#22411;&#32959;&#30244;&#65288;&#33041;&#33180;&#30244;&#12289;&#33014;&#36136;&#30244;&#21644;&#22402;&#20307;&#30244;&#65289;&#30340;MRI&#22270;&#20687;&#65292;&#21253;&#25324;&#30690;&#29366;&#38754;&#12289;&#20896;&#29366;&#38754;&#21644;&#36724;&#38754;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#22788;&#29702;&#36755;&#20837;&#22270;&#20687;&#20107;&#20808;&#31227;&#38500;&#22836;&#39592;&#25110;&#26894;&#39592;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;233&#21517;&#24739;&#32773;3064&#24352;&#20999;&#29255;&#30340;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#19982;&#20043;&#21069;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#22320;&#33719;&#24471;&#20102;0.973&#30340;&#32959;&#30244;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#39640;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a fully automatic brain tumor segmentation and classification model using a Deep Convolutional Neural Network that includes a multiscale approach. One of the differences of our proposal with respect to previous works is that input images are processed in three spatial scales along different processing pathways. This mechanism is inspired in the inherent operation of the Human Visual System. The proposed neural model can analyze MRI images containing three types of tumors: meningioma, glioma, and pituitary tumor, over sagittal, coronal, and axial views and does not need preprocessing of input images to remove skull or vertebral column parts in advance. The performance of our method on a publicly available MRI image dataset of 3064 slices from 233 patients is compared with previously classical machine learning and deep learning published methods. In the comparison, our method remarkably obtained a tumor classification accuracy of 0.973, higher than the other app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#21644;&#31934;&#32454;&#35774;&#35745;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#20272;&#35745;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.05970</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Modeling Spatio-temporal Dynamical Systems with Neural Discrete Learning and Levels-of-Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#31163;&#25955;&#23398;&#20064;&#21644;&#19987;&#23478;&#32423;&#21035;&#24314;&#27169;&#31354;&#26102;&#21160;&#21147;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#21644;&#31934;&#32454;&#35774;&#35745;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#26377;&#25928;&#22320;&#24314;&#27169;&#21644;&#20272;&#35745;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#19968;&#31995;&#21015;&#35266;&#27979;&#65288;&#22914;&#35270;&#39057;&#24103;&#65289;&#30340;&#31354;&#26102;&#21160;&#24577;&#31995;&#32479;&#20013;&#29366;&#24577;&#21464;&#21270;&#30340;&#24314;&#27169;&#21644;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20256;&#32479;&#30340;&#25968;&#20540;&#27169;&#25311;&#31995;&#32479;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21021;&#22987;&#35774;&#32622;&#21644;&#26500;&#24314;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;PDE&#27169;&#22411;&#30340;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#26159;&#22855;&#24322;&#22330;&#26223;&#21644;&#32570;&#20047;&#23616;&#37096;&#27934;&#23519;&#21147;&#30340;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#26356;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#19987;&#23478;&#27169;&#22359;&#8212;&#8212;&#20809;&#27969;&#20272;&#35745;&#32452;&#20214;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#25429;&#25417;&#19968;&#33324;&#29289;&#29702;&#36807;&#31243;&#30340;&#28436;&#21270;&#35268;&#24459;&#12290;&#20026;&#20102;&#22686;&#24378;&#23616;&#37096;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#29289;&#29702;&#27969;&#27700;&#32447;&#65292;&#22240;&#20026;&#23616;&#37096;&#29305;&#24449;&#21487;&#33021;&#21463;&#21040;&#21508;&#31181;&#20869;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#19982;&#23439;&#35266;&#23646;&#24615;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of modeling and estimating changes in the state of the spatio-temporal dynamical systems based on a sequence of observations like video frames. Traditional numerical simulation systems depend largely on the initial settings and correctness of the constructed partial differential equations (PDEs). Despite recent efforts yielding significant success in discovering data-driven PDEs with neural networks, the limitations posed by singular scenarios and the absence of local insights prevent them from performing effectively in a broader real-world context. To this end, this paper propose the universal expert module -- that is, optical flow estimation component, to capture the evolution laws of general physical processes in a data-driven fashion. To enhance local insight, we painstakingly design a finer-grained physical pipeline, since local characteristics may be influenced by various internal contextual information, which may contradict the macroscopic pro
&lt;/p&gt;</description></item><item><title>&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#21487;&#33021;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#26397;&#20027;&#27969;&#37319;&#29992;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#31561;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.05968</link><description>&lt;p&gt;
&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#20248;&#20808;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Priorities Under the European Union Artificial Intelligence Act
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05968
&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#21487;&#33021;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#26397;&#20027;&#27969;&#37319;&#29992;&#26041;&#21521;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#31561;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30417;&#31649;&#26102;&#20195;&#24050;&#32463;&#26469;&#20020;&#65292;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AI Act&#65289;&#24341;&#39046;&#30528;&#28526;&#27969;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#23558;&#22914;&#20309;&#24433;&#21709;&#20197;&#25968;&#25454;&#38544;&#31169;&#20026;&#20248;&#20808;&#24182;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#20854;&#19982;&#38598;&#20013;&#24335;&#23398;&#20064;&#30340;&#20986;&#21457;&#28857;&#26681;&#26412;&#19981;&#21516;&#12290;&#25105;&#20204;&#30456;&#20449;AI&#27861;&#26696;&#21644;&#26410;&#26469;&#30340;&#30417;&#31649;&#21487;&#33021;&#26159;&#25512;&#21160;FL&#36208;&#21521;&#20027;&#27969;&#37319;&#29992;&#30340;&#32570;&#22833;&#20652;&#21270;&#21058;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#33021;&#21457;&#29983;&#22312;FL&#31038;&#21306;&#37325;&#26032;&#20248;&#20808;&#32771;&#34385;&#20854;&#30740;&#31350;&#37325;&#28857;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#25105;&#20204;&#30340;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#36328;&#23398;&#31185;&#20998;&#26512;&#65288;&#27861;&#24459;&#21644;&#26426;&#22120;&#23398;&#20064;&#65289;&#65292;&#20998;&#26512;&#20102;AI&#27861;&#26696;&#23545;FL&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25903;&#25345;&#25105;&#20204;&#20027;&#35201;&#35266;&#28857;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25968;&#25454;&#27835;&#29702;&#38382;&#39064;&#21644;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#29983;&#21629;&#21608;&#26399;&#30417;&#35270;&#20013;&#24615;&#33021;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#12290;&#32508;&#21512;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;FL&#26377;&#30528;&#24040;&#22823;&#30340;&#26426;&#20250;&#65292;
&lt;/p&gt;
&lt;p&gt;
The age of AI regulation is upon us, with the European Union Artificial Intelligence Act (AI Act) leading the way. Our key inquiry is how this will affect Federated Learning (FL), whose starting point of prioritizing data privacy while performing ML fundamentally differs from that of centralized learning. We believe the AI Act and future regulations could be the missing catalyst that pushes FL toward mainstream adoption. However, this can only occur if the FL community reprioritizes its research focus. In our position paper, we perform a first-of-its-kind interdisciplinary analysis (legal and ML) of the impact the AI Act may have on FL and make a series of observations supporting our primary position through quantitative and qualitative analysis. We explore data governance issues and the concern for privacy. We establish new challenges regarding performance and energy efficiency within lifecycle monitoring. Taken together, our analysis suggests there is a sizable opportunity for FL to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#20197;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05963</link><description>&lt;p&gt;
&#33410;&#20461;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#27169;&#22411;&#65306;&#20351;&#29992;&#29420;&#29305;&#32463;&#21382;&#30340;&#39640;&#25928;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#20197;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29992;&#20110;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#26080;&#27169;&#22411;&#25511;&#21046;&#31574;&#30053;&#21512;&#25104;&#20013;&#65292;&#23545;&#22238;&#25918;&#32531;&#20914;&#22120;&#30340;&#39640;&#25928;&#21033;&#29992;&#22312;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#29420;&#29305;&#26679;&#26412;&#24182;&#23558;&#20854;&#28155;&#21152;&#21040;&#22238;&#25918;&#32531;&#20914;&#22120;&#20013;&#65292;&#26088;&#22312;&#20943;&#23567;&#32531;&#20914;&#22120;&#30340;&#22823;&#23567;&#24182;&#20445;&#25345;&#26679;&#26412;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22312;&#38543;&#26426;&#25506;&#32034;&#30340;&#21021;&#22987;&#38454;&#27573;&#36935;&#21040;&#30340;&#32463;&#21382;&#20013;&#36873;&#25321;&#19968;&#32452;&#37325;&#35201;&#30340;&#29366;&#24577;&#21464;&#37327;&#30340;&#37325;&#35201;&#23376;&#38598;&#65292;&#26681;&#25454;&#25152;&#36873;&#37325;&#35201;&#29366;&#24577;&#21464;&#37327;&#23558;&#29366;&#24577;&#31354;&#38388;&#21010;&#20998;&#20026;&#19968;&#32452;&#25277;&#35937;&#29366;&#24577;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#22120;&#36873;&#25321;&#20855;&#26377;&#29420;&#29305;&#29366;&#24577;-&#22870;&#21169;&#32452;&#21512;&#30340;&#32463;&#21382;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#23558;&#25152;&#25552;&#20986;&#30340;&#29420;&#29305;&#32463;&#21382;&#26041;&#27861;&#32435;&#20837;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient utilization of the replay buffer plays a significant role in the off-policy actor-critic reinforcement learning (RL) algorithms used for model-free control policy synthesis for complex dynamical systems. We propose a method for achieving sample efficiency, which focuses on selecting unique samples and adding them to the replay buffer during the exploration with the goal of reducing the buffer size and maintaining the independent and identically distributed (IID) nature of the samples. Our method is based on selecting an important subset of the set of state variables from the experiences encountered during the initial phase of random exploration, partitioning the state space into a set of abstract states based on the selected important state variables, and finally selecting the experiences with unique state-reward combination by using a kernel density estimator. We formally prove that the off-policy actor-critic algorithm incorporating the proposed method for unique experience
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#22788;&#29702;&#29615;&#22659;&#20449;&#24687;&#32780;&#19981;&#20381;&#36182;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#32467;&#21512;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#65292;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#31561;&#25928;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05959</link><description>&lt;p&gt;
&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Nature-Inspired Local Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#28982;&#21551;&#21457;&#30340;&#23616;&#37096;&#20256;&#25773;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#32447;&#22788;&#29702;&#29615;&#22659;&#20449;&#24687;&#32780;&#19981;&#20381;&#36182;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#32467;&#21512;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#65292;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#65292;&#24182;&#19988;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#31561;&#25928;&#20110;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#21253;&#25324;&#26368;&#36817;&#22312;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#37117;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#30028;&#20013;&#30340;&#26234;&#33021;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#21482;&#38656;&#36890;&#36807;&#23545;&#29615;&#22659;&#20449;&#24687;&#30340;&#22312;&#32447;&#22788;&#29702;&#21363;&#21487;&#20135;&#29983;&#12290;&#29305;&#21035;&#26159;&#65292;&#33258;&#28982;&#23398;&#20064;&#36807;&#31243;&#20381;&#36182;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#23398;&#20064;&#30456;&#20114;&#20132;&#32455;&#20197;&#23562;&#37325;&#26102;&#31354;&#23616;&#37096;&#24615;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#31181;&#29305;&#24615;&#26469;&#33258;&#20110;&#23545;&#23398;&#20064;&#30340;&#39044;&#31639;&#27861;&#35270;&#35282;&#65292;&#35813;&#35270;&#35282;&#21463;&#21040;&#20102;&#29702;&#35770;&#29289;&#29702;&#23398;&#30456;&#20851;&#30740;&#31350;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20256;&#25773;&#36895;&#24230;&#36235;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#25152;&#24471;&#21040;&#30340;&#8220;&#23398;&#20064;&#27861;&#21017;&#8221;&#30340;&#31639;&#27861;&#35299;&#37322;&#65288;&#37319;&#29992;&#21704;&#23494;&#39039;&#26041;&#31243;&#32467;&#26500;&#65289;&#23558;&#24402;&#32467;&#20026;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#36825;&#20026;&#22522;&#20110;&#20840;&#38754;&#22312;&#32447;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#30340;&#36947;&#36335;&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#34987;&#25552;&#20986;&#30340;&#26102;&#31354;&#23616;&#37096;&#31639;&#27861;&#21462;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived "laws of learning", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algori
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.05952</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#36827;&#22270;&#34920;&#31034;&#23398;&#20064;&#65306;&#25216;&#26415;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#20998;&#26512;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#21512;&#20316;&#21033;&#29992;LLM&#30340;&#20808;&#36827;&#35821;&#35328;&#33021;&#21147;&#26469;&#25913;&#36827;&#22270;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;GRL&#30340;&#33539;&#22260;&#21644;&#28508;&#21147;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#23558;LLM&#38598;&#25104;&#21040;&#22270;&#39046;&#22495;&#20013;&#65292;&#20294;&#32570;&#20047;&#19968;&#20221;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#35299;&#36825;&#20123;&#27169;&#22411;&#20026;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20174;&#26032;&#30340;&#25216;&#26415;&#35282;&#24230;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#26368;&#36817;&#30340;&#25991;&#29486;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#32452;&#32455;&#32773;&#65292;&#20197;&#21450;&#20004;&#20010;&#25805;&#20316;&#25216;&#26415;&#65292;&#21253;&#25324;&#38598;&#25104;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#25581;&#31034;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additio
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05946</link><description>&lt;p&gt;
&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#65306;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24322;&#24120;&#20107;&#20214;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling Latent Causal Rules: A Temporal Point Process Approach for Abnormal Event Explanation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#22240;&#26524;&#35268;&#24459;&#26469;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#65292;&#20197;&#24110;&#21161;&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#20248;&#21270;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#35268;&#21017;&#21457;&#29616;&#21644;&#26681;&#22240;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#31995;&#32479;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#29702;&#35299;&#24322;&#24120;&#20107;&#20214;&#32972;&#21518;&#30340;&#22240;&#26524;&#21407;&#22240;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20363;&#22914;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#31361;&#28982;&#21464;&#21270;&#12290;&#25581;&#31034;&#22240;&#26524;&#21407;&#22240;&#26377;&#21161;&#20110;&#24555;&#36895;&#35786;&#26029;&#21644;&#31934;&#30830;&#27835;&#30103;&#35268;&#21010;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#25581;&#31034;&#35299;&#37322;&#35266;&#23519;&#20107;&#20214;&#30340;&#8220;&#22914;&#26524;-&#37027;&#20040;&#8221;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#28857;&#36807;&#31243;&#26469;&#24314;&#27169;&#25152;&#20851;&#27880;&#20107;&#20214;&#65292;&#24182;&#21457;&#29616;&#19968;&#32452;&#28508;&#22312;&#35268;&#21017;&#26469;&#35299;&#37322;&#20107;&#20214;&#30340;&#21457;&#29983;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#22312;E&#27493;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#27599;&#20010;&#20107;&#20214;&#34987;&#27599;&#20010;&#21457;&#29616;&#30340;&#35268;&#21017;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;M&#27493;&#20013;&#65292;&#25105;&#20204;&#26356;&#26032;&#35268;&#21017;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#22686;&#24378;&#21487;&#33021;&#24615;&#20989;&#25968;&#30340;&#19979;&#30028;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20197;&#24494;&#20998;&#30340;&#26041;&#24335;&#20248;&#21270;&#35268;&#21017;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#35268;&#21017;&#21644;&#35782;&#21035;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-stakes systems such as healthcare, it is critical to understand the causal reasons behind unusual events, such as sudden changes in patient's health. Unveiling the causal reasons helps with quick diagnoses and precise treatment planning. In this paper, we propose an automated method for uncovering "if-then" logic rules to explain observational events. We introduce temporal point processes to model the events of interest, and discover the set of latent rules to explain the occurrence of events. To achieve this, we employ an Expectation-Maximization (EM) algorithm. In the E-step, we calculate the likelihood of each event being explained by each discovered rule. In the M-step, we update both the rule set and model parameters to enhance the likelihood function's lower bound. Notably, we optimize the rule set in a differential manner. Our approach demonstrates accurate performance in both discovering rules and identifying root causes. We showcase its promising results using syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65288;SupCBM&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#19988;&#21482;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.05945</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#12289;&#20998;&#23618;&#27010;&#24565;&#23398;&#20064;&#28040;&#38500;&#30828;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Information Leakage in Hard Concept Bottleneck Models with Supervised, Hierarchical Concept Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65288;SupCBM&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#19988;&#21482;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#29305;&#24449;&#21644;&#26631;&#31614;&#19982;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#24178;&#39044;&#30340;&#39044;&#27979;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;CBMs&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#21363;&#22312;&#27010;&#24565;&#34920;&#31034;&#20026;&#27010;&#29575;&#25110;&#20108;&#36827;&#21046;&#29366;&#24577;&#26102;&#65292;&#36229;&#20986;&#27010;&#24565;&#30340;&#24847;&#22270;&#20449;&#24687;&#27844;&#28431;&#21040;&#21518;&#32493;&#30340;&#26631;&#31614;&#39044;&#27979;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26080;&#27861;&#21306;&#20998;&#30340;&#27010;&#24565;&#26469;&#38169;&#35823;&#20998;&#31867;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21066;&#24369;&#20102;CBMs&#30340;&#35299;&#37322;&#21644;&#24178;&#39044;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#27010;&#24565;&#39044;&#27979;&#20013;&#24341;&#20837;&#26631;&#31614;&#30417;&#30563;&#21644;&#26500;&#24314;&#20998;&#23618;&#27010;&#24565;&#38598;&#26469;&#32531;&#35299;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CBMs&#33539;&#20363;&#65292;&#21363;SupCBM&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#30340;&#27010;&#24565;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#24178;&#39044;&#30697;&#38453;&#23454;&#29616;&#26631;&#31614;&#39044;&#27979;&#12290;SupCBM&#23558;&#37325;&#28857;&#25918;&#22312;&#19982;&#39044;&#27979;&#26631;&#31614;&#26368;&#30456;&#20851;&#30340;&#27010;&#24565;&#19978;&#65292;&#24182;&#19988;&#20165;&#22312;&#19981;&#21516;&#30340;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck Models (CBMs) aim to deliver interpretable and interventionable predictions by bridging features and labels with human-understandable concepts. While recent CBMs show promising potential, they suffer from information leakage, where unintended information beyond the concepts (either when concepts are represented with probabilities or binary states) are leaked to the subsequent label prediction. Consequently, distinct classes are falsely classified via indistinguishable concepts, undermining the interpretation and intervention of CBMs.   This paper alleviates the information leakage issue by introducing label supervision in concept predication and constructing a hierarchical concept set. Accordingly, we propose a new paradigm of CBMs, namely SupCBM, which achieves label predication via predicted concepts and a deliberately-designed intervention matrix. SupCBM focuses on concepts that are mostly relevant to the predicted label and only distinguishes classes when differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512; IndRNN &#21644; LSTM &#30340;&#29305;&#28857;&#65292;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#12290;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#36739;&#20302;&#30340; MAE &#21644; RMSE &#20540;&#12290;</title><link>https://arxiv.org/abs/2402.05943</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;&#30340;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A hybrid IndRNNLSTM approach for real-time anomaly detection in software-defined networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; IndRNNLSTM &#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512; IndRNN &#21644; LSTM &#30340;&#29305;&#28857;&#65292;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#25552;&#20379;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#12290;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#36739;&#20302;&#30340; MAE &#21644; RMSE &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#20351;&#29992;&#25968;&#25454;&#27969;&#39044;&#27979;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#38382;&#39064;&#24402;&#31867;&#20026;&#26102;&#24207;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#29305;&#24449;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#33021;&#22815;&#33258;&#21160;&#36873;&#25321;&#29305;&#24449;&#20855;&#26377;&#37325;&#35201;&#30340;&#29305;&#28857;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110; RNN &#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;LSTM &#21644; GRU &#26041;&#27861;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#30456;&#20851;&#23454;&#20307;&#65307;&#32780; IndRNN &#26041;&#27861;&#21017;&#33021;&#22815;&#23398;&#20064;&#26102;&#24207;&#20013;&#30340;&#38750;&#30456;&#20851;&#23454;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#23581;&#35797;&#20351;&#29992; IndRNN &#21644; LSTM &#30340;&#32452;&#21512;&#26469;&#23398;&#20064;&#30456;&#20851;&#21644;&#38750;&#30456;&#20851;&#29305;&#24449;&#12290;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36824;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#29305;&#24449;&#35270;&#35282;&#65307;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20351;&#29992;&#20102;&#22235;&#31181;&#29305;&#24449;&#36873;&#25321;&#27169;&#22411;&#65306;Filter&#12289;Wrapper&#12289;Embedded &#21644; Autoencoder&#12290;&#25552;&#20986;&#30340; IndRNNLSTM &#31639;&#27861;&#19982; Embedded &#30340;&#32452;&#21512;&#33021;&#22815;&#22312; NSL-KDD &#25968;&#25454;&#38598;&#19978;&#23454;&#29616; MAE=1.22 &#21644; RMSE=9.92&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in SDN using data flow prediction is a difficult task. This problem is included in the category of time series and regression problems. Machine learning approaches are challenging in this field due to the manual selection of features. On the other hand, deep learning approaches have important features due to the automatic selection of features. Meanwhile, RNN-based approaches have been used the most. The LSTM and GRU approaches learn dependent entities well; on the other hand, the IndRNN approach learns non-dependent entities in time series. The proposed approach tried to use a combination of IndRNN and LSTM approaches to learn dependent and non-dependent features. Feature selection approaches also provide a suitable view of features for the models; for this purpose, four feature selection models, Filter, Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM algorithm, in combination with Embedded, was able to achieve MAE=1.22 and RMSE=9.92 on NSL-KDD 
&lt;/p&gt;</description></item><item><title>&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#30456;&#20114;&#21512;&#20316;&#26469;&#20256;&#36882;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24357;&#34917;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#38480;&#24615;&#12290;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.05942</link><description>&lt;p&gt;
&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#65306;&#19968;&#31181;&#23398;&#20064;&#32773;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cooperative Knowledge Distillation: A Learner Agnostic Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05942
&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20010;&#27169;&#22411;&#30456;&#20114;&#21512;&#20316;&#26469;&#20256;&#36882;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24357;&#34917;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#38480;&#24615;&#12290;&#19981;&#21516;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#23384;&#22312;&#20197;&#19979;&#33267;&#23569;&#19968;&#31181;&#20851;&#38190;&#38480;&#21046;&#65292;&#38480;&#21046;&#20854;&#20351;&#29992;&#33539;&#22260;&#21644;&#26041;&#21521;&#65306;&#26080;&#35770;&#35813;&#30693;&#35782;&#26159;&#21542;&#26377;&#29992;&#65292;&#25152;&#26377;&#30693;&#35782;&#37117;&#20174;&#25945;&#24072;&#20256;&#36882;&#32473;&#23398;&#29983;&#65307;&#23398;&#29983;&#26159;&#36825;&#31181;&#20132;&#27969;&#20013;&#21807;&#19968;&#23398;&#20064;&#30340;&#19968;&#26041;&#65307;&#20856;&#22411;&#30340;&#33976;&#39311;&#21482;&#20174;&#19968;&#20010;&#25945;&#24072;&#21521;&#19968;&#20010;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#21363;&#21512;&#20316;&#33976;&#39311;&#65292;&#20854;&#20013;&#35768;&#22810;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#20805;&#24403;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#26041;&#24335;&#22914;&#19979;&#65306;&#19968;&#20010;&#27169;&#22411;&#65288;&#23398;&#29983;&#65289;&#35782;&#21035;&#20854;&#24615;&#33021;&#20013;&#30340;&#29305;&#23450;&#32570;&#38519;&#65292;&#24182;&#25628;&#32034;&#21478;&#19968;&#20010;&#27169;&#22411;&#65288;&#25945;&#24072;&#65289;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#24212;&#20107;&#23454;&#24773;&#20917;&#30340;&#34394;&#25311;&#23454;&#20363;&#26469;&#32534;&#30721;&#25152;&#23398;&#30693;&#35782;&#12290;&#30001;&#20110;&#19981;&#21516;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#22240;&#27492;&#21512;&#20316;&#33976;&#39311;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.05941</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#19982;&#36890;&#36807;LLMs&#36827;&#34892;&#35270;&#35273;&#22686;&#24378;&#30340;&#39118;&#26684;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Character-based Outfit Generation with Vision-augmented Style Extraction via LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#29983;&#25104;&#26381;&#35013;&#32452;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#35013;&#29983;&#25104;&#38382;&#39064;&#28041;&#21450;&#26681;&#25454;&#29992;&#25143;&#30340;&#20852;&#36259;&#25512;&#33616;&#19968;&#20010;&#23436;&#25972;&#30340;&#26381;&#35013;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38170;&#23450;&#21830;&#21697;&#25110;&#25351;&#23450;&#26597;&#35810;&#39118;&#26684;&#26469;&#25512;&#33616;&#29289;&#21697;&#65292;&#20294;&#19981;&#32771;&#34385;&#29992;&#25143;&#23545;&#30005;&#24433;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#20013;&#33879;&#21517;&#20154;&#29289;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#20154;&#29289;&#30340;&#26381;&#35013;&#29983;&#25104;&#65288;COG&#65289;&#38382;&#39064;&#65292;&#26088;&#22312;&#20934;&#30830;&#35299;&#37322;&#20154;&#29289;&#20449;&#24687;&#65292;&#24182;&#26681;&#25454;&#29992;&#25143;&#30340;&#35268;&#33539;&#65288;&#22914;&#24180;&#40836;&#21644;&#24615;&#21035;&#65289;&#29983;&#25104;&#23436;&#25972;&#30340;&#26381;&#35013;&#32452;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;LVA-COG&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#29992;&#25143;&#30340;&#20852;&#36259;&#65288;&#20363;&#22914;&#20154;&#29289;&#20449;&#24687;&#65289;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24182;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#20934;&#30830;&#29702;&#35299;&#29992;&#25143;&#30340;&#21916;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#23545;&#36830;&#36143;&#26381;&#35013;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;&#20107;&#23454;&#25110;&#21453;&#20107;&#23454;&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;LLMs&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#25972;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The outfit generation problem involves recommending a complete outfit to a user based on their interests. Existing approaches focus on recommending items based on anchor items or specific query styles but do not consider customer interests in famous characters from movie, social media, etc. In this paper, we define a new Character-based Outfit Generation (COG) problem, designed to accurately interpret character information and generate complete outfit sets according to customer specifications such as age and gender. To tackle this problem, we propose a novel framework LVA-COG that leverages Large Language Models (LLMs) to extract insights from customer interests (e.g., character information) and employ prompt engineering techniques for accurate understanding of customer preferences. Additionally, we incorporate text-to-image models to enhance the visual understanding and generation (factual or counterfactual) of cohesive outfits. Our framework integrates LLMs with text-to-image models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20117;&#19979;&#29028;&#30719;&#30340;&#20260;&#23475;&#35760;&#24405;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#21457;&#29616;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05940</link><description>&lt;p&gt;
&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#39118;&#38505;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Causal Relationship Network of Risk Factors Impacting Workday Loss in Underground Coal Mines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#20117;&#19979;&#29028;&#30719;&#30340;&#20260;&#23475;&#35760;&#24405;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#21457;&#29616;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#24314;&#31435;&#20117;&#19979;&#29028;&#30719;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#32593;&#32476;&#12290;&#20998;&#26512;&#21033;&#29992;&#20102;&#20174;&#22269;&#23478;&#32844;&#19994;&#23433;&#20840;&#19982;&#20581;&#24247;&#30740;&#31350;&#25152;&#65288;NIOSH&#65289;&#33719;&#24471;&#30340;&#25968;&#25454;&#12290;&#20174;NIOSH&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#20102;&#26469;&#33258;1990&#24180;&#33267;2020&#24180;&#30340;&#20849;&#35745;101,010&#20221;&#20260;&#23475;&#35760;&#24405;&#65292;&#28085;&#30422;&#20102;3,982&#20010;&#29420;&#31435;&#30340;&#20117;&#19979;&#29028;&#30719;&#12290;&#21033;&#29992;&#19968;&#31181;&#21517;&#20026;&#32676;&#32452;&#36138;&#23146;&#31561;&#20215;&#25628;&#32034;&#65288;GGES&#65289;&#30340;&#26032;&#39062;&#22240;&#26524;AI&#26041;&#27861;&#36827;&#34892;&#20102;&#22240;&#26524;&#20851;&#31995;&#30340;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#24178;&#39044;&#35745;&#31639;&#35843;&#25972;&#65288;IDA&#65289;&#24471;&#20998;&#23545;&#27599;&#20010;&#21464;&#37327;&#23545;&#24037;&#20316;&#26102;&#38388;&#25439;&#22833;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#20351;&#29992;10&#25240;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#21033;&#29992;&#25509;&#37051;&#28857;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#12289;&#25509;&#37051;&#28857;&#21484;&#22238;&#29575;&#65288;AR&#65289;&#12289;&#31661;&#22836;&#22836;&#37096;&#31934;&#30830;&#24230;&#65288;AHP&#65289;&#21644;&#31661;&#22836;&#22836;&#37096;&#21484;&#22238;&#29575;&#65288;AHR&#65289;&#31561;&#24615;&#33021;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;2006&#24180;&#20043;&#21518;&#65292;&#20851;&#38190;&#30340;&#22240;&#26524;&#20851;&#31995;&#21253;&#25324;&#39118;&#28304;&#21644;&#24037;&#20316;&#29366;&#24577;&#31561;&#19981;&#21516;&#22240;&#32032;&#20043;&#38388;&#30340;&#20316;&#29992;&#26377;&#25152;     changed
&lt;/p&gt;
&lt;p&gt;
This study aims to establish the causal relationship network between various factors leading to workday loss in underground coal mines using a novel causal artificial intelligence (AI) method. The analysis utilizes data obtained from the National Institute for Occupational Safety and Health (NIOSH). A total of 101,010 injury records from 3,982 unique underground coal mines spanning the years from 1990 to 2020 were extracted from the NIOSH database. Causal relationships were analyzed and visualized using a novel causal AI method called Grouped Greedy Equivalence Search (GGES). The impact of each variable on workday loss was assessed through intervention do-calculus adjustment (IDA) scores. Model training and validation were performed using the 10-fold cross-validation technique. Performance metrics, including adjacency precision (AP), adjacency recall (AR), arrowhead precision (AHP), and arrowhead recall (AHR), were utilized to evaluate the models. Findings revealed that after 2006, key
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.05894</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#33976;&#39311;&#20013;&#36935;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Meets Graph Neural Network in Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#26399;&#23398;&#26415;&#30028;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#25991;&#26412;-&#23646;&#24615;&#22270;&#65288;TAG&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#28508;&#21147;&#26377;&#25152;&#25259;&#38706;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#21463;&#21040;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#39640;&#65292;&#25512;&#29702;&#36807;&#31243;&#20013;&#24310;&#36831;&#38271;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34429;&#28982;&#36731;&#37327;&#19988;&#25797;&#38271;&#23398;&#20064;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;TAG&#22797;&#26434;&#35821;&#20041;&#30340;&#25226;&#25569;&#26377;&#25152;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;TAG&#20013;&#33410;&#28857;&#20998;&#31867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#31216;&#20026;&#35821;&#35328;&#22270;&#30693;&#35782;&#33976;&#39311;&#65288;LinguGKD&#65289;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;GNNs&#20316;&#20026;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#20854;&#20013;&#21253;&#25324;&#23545;LLM&#36827;&#34892;TAG&#23450;&#21521;&#25351;&#23548;&#35843;&#25972;&#20197;&#24212;&#23545;&#35774;&#35745;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#31034;&#65292;&#28982;&#21518;&#23545;&#23618;&#27425;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the t
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#33021;&#22815;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#21462;&#20915;&#20110;&#20854;&#23545;&#28216;&#25103;&#20249;&#20276;&#30340;&#20449;&#20219;&#31243;&#24230;&#12289;&#28216;&#25103;&#26694;&#26550;&#23545;&#20110;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#21487;&#33021;&#23384;&#22312;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;</title><link>https://arxiv.org/abs/2402.05786</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#65306;&#20419;&#36827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompting Fairness: Artificial Intelligence as Game Players
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05786
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#28216;&#25103;&#29609;&#23478;&#33021;&#22815;&#23637;&#29616;&#20986;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#21462;&#20915;&#20110;&#20854;&#23545;&#28216;&#25103;&#20249;&#20276;&#30340;&#20449;&#20219;&#31243;&#24230;&#12289;&#28216;&#25103;&#26694;&#26550;&#23545;&#20110;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#24433;&#21709;&#20197;&#21450;&#20854;&#21487;&#33021;&#23384;&#22312;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#31038;&#20250;&#31185;&#23398;&#23478;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#27979;&#37327;&#20844;&#24179;&#24615;&#30340;&#21151;&#21033;&#20027;&#20041;&#28216;&#25103;&#65292;&#27604;&#22914;&#29420;&#35009;&#32773;&#28216;&#25103;&#12290;&#36825;&#20123;&#28216;&#25103;&#19981;&#20165;&#35753;&#25105;&#20204;&#20102;&#35299;&#20102;&#20154;&#31867;&#23545;&#20844;&#24179;&#24615;&#30340;&#30475;&#27861;&#65292;&#36824;&#25581;&#31034;&#20102;&#20844;&#24179;&#24615;&#12289;&#21033;&#20182;&#20027;&#20041;&#21644;&#36138;&#23146;&#22312;&#20309;&#31181;&#26465;&#20214;&#19979;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;&#23613;&#31649;&#36825;&#20123;&#28216;&#25103;&#20256;&#32479;&#19978;&#20851;&#27880;&#20154;&#31867;&#65292;&#20294;&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29609;&#36825;&#20123;&#28216;&#25103;&#12290;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25104;&#20026;&#20154;&#38469;&#20114;&#21160;&#20013;&#30340;&#24120;&#24577;&#65292;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#22312;&#28216;&#25103;&#20013;&#26174;&#31034;&#30340;&#20844;&#24179;&#24615;&#21487;&#20197;&#35753;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20915;&#31574;&#36807;&#31243;&#26377;&#25152;&#20102;&#35299;&#12290;&#36890;&#36807;101&#36718;&#30340;&#29420;&#35009;&#32773;&#28216;&#25103;&#65292;&#25105;&#24471;&#20986;&#32467;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20855;&#26377;&#24378;&#28872;&#30340;&#20844;&#24179;&#24847;&#35782;&#65292;&#36825;&#19982;&#20854;&#35748;&#20026;&#23427;&#30340;&#28216;&#25103;&#20249;&#20276;&#26159;&#21542;&#20540;&#24471;&#20449;&#20219;&#26377;&#20851;&#65307;&#35774;&#32622;&#23545;&#28216;&#25103;&#30340;&#26694;&#26550;&#22312;&#20154;&#24037;&#26234;&#33021;&#32473;&#20104;&#25509;&#21463;&#32773;&#30340;&#25968;&#37327;&#19978;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#32780;&#19988;&#26377;&#21487;&#33021;&#23384;&#22312;&#35777;&#25454;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#20063;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#26377;&#21388;&#24694;&#19981;&#24179;&#31561;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades. These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05650</link><description>&lt;p&gt;
&#12298;&#23721;&#30707;&#32534;&#30721;&#65292;&#19981;&#26159;&#24320;&#21457;-&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05650
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#22411;AI&#22240;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#34920;&#29616;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#25191;&#34892;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#65292;&#24182;&#21462;&#20195;&#20154;&#31867;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#23545;&#36825;&#20123;LLM&#25216;&#26415;&#22312;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#28145;&#20837;&#35843;&#26597;&#12290;&#22312;&#19968;&#39033;&#26377;109&#21517;&#21442;&#19982;&#32773;&#30340;&#21463;&#25511; 2x2 &#21463;&#35797;&#32773;&#38388;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;ChatGPT&#21512;&#20316;&#22312;&#32534;&#30721;&#20219;&#21153;&#21644;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#20013;&#30340;&#25928;&#29992;&#31243;&#24230;&#20197;&#21450;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;ChatGPT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#32534;&#30721;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#22312;&#25903;&#25345;&#20856;&#22411;&#30340;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#20102;&#21442;&#19982;&#32773;&#19982;ChatGPT&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#25214;&#21040;&#20102;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 $\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the i
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2402.05391</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05391
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;KG4MM&#21644;MM4KG&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20219;&#21153;&#23450;&#20041;&#12289;&#26500;&#24314;&#36827;&#23637;&#12289;&#35780;&#20272;&#22522;&#20934;&#20197;&#21450;&#20851;&#38190;&#30740;&#31350;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#25512;&#21160;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#35821;&#20041;&#32593;&#32476;&#31038;&#21306;&#23545;&#22810;&#27169;&#24577;&#32500;&#24230;&#30340;&#25506;&#32034;&#20026;&#21019;&#26032;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;300&#22810;&#31687;&#25991;&#31456;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#24863;&#30693;&#30740;&#31350;&#65306;&#20197;&#30693;&#35782;&#22270;&#35889;&#25903;&#25345;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;KG&#39537;&#21160;&#22810;&#27169;&#24577;&#65288;KG4MM&#65289;&#23398;&#20064;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#30740;&#31350;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MM4KG&#65289;&#39046;&#22495;&#12290;&#25105;&#20204;&#20174;&#23450;&#20041;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#24320;&#22987;&#65292;&#28982;&#21518;&#25506;&#32034;&#23427;&#20204;&#30340;&#26500;&#24314;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#31867;&#21035;&#65306;KG&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#31572;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#22914;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#31361;&#20986;&#20102;&#20855;&#20307;&#30340;&#30740;&#31350;&#36712;&#36857;&#12290;&#23545;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23450;&#20041;&#12289;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#20986;&#36827;&#34892;&#30456;&#20851;&#30740;&#31350;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;cu
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss cu
&lt;/p&gt;</description></item><item><title>LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05130</link><description>&lt;p&gt;
LB-KBQA: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05130
&lt;/p&gt;
&lt;p&gt;
LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22240;&#20854;&#26032;&#20852;&#30340;&#33021;&#21147;&#32780;&#36171;&#20104;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21147;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20856;&#22411;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#20856;&#22411;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;LLM&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#38556;&#30861;&#65292;&#36825;&#28304;&#33258;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#24847;&#22270;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#21463;&#21040;&#26377;&#38480;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;BERT&#30340;&#26032;&#22411;KBQA&#31995;&#32479;&#65288;LB-KBQA&#65289;&#12290;&#22312;&#29983;&#25104;&#24335;AI&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#8230;&#8230;&#65288;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05027</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20013;&#23454;&#29616;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#20449;&#24687;&#27969;&#23454;&#29616;&#20102;&#35266;&#23519;&#37051;&#22495;&#22823;&#23567;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#21160;&#20316;&#30340;&#36136;&#37327;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#29615;&#22659;&#32473;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#20998;&#25955;&#24335;&#26041;&#27861;&#20013;&#65292;Agent&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25805;&#20316;&#65292;&#24182;&#26681;&#25454;&#37096;&#20998;&#25110;&#36807;&#26102;&#30340;&#35266;&#23519;&#20570;&#20986;&#20915;&#31574;&#12290;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#30340;&#22823;&#23567;&#38480;&#21046;&#20102;&#22312;&#19981;&#21516;&#22270;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24433;&#21709;&#21040;Agent&#30340;&#21453;&#24212;&#24615;&#12289;&#36873;&#25321;&#30340;&#21160;&#20316;&#36136;&#37327;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;&#25972;&#20010;&#22270;&#20013;&#36827;&#34892;&#36830;&#32493;&#30340;&#20449;&#24687;&#27969;&#35299;&#20915;&#20102;&#35266;&#23519;&#21040;&#30340;&#37051;&#22495;&#22823;&#23567;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#65292;&#23427;&#19982;&#29615;&#22659;&#30340;&#27493;&#39588;&#36845;&#20195;&#65292;&#24182;&#20801;&#35768;&#33410;&#28857;&#36890;&#36807;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#28040;&#24687;&#26469;&#21019;&#24314;&#22270;&#30340;&#20840;&#23616;&#34920;&#31034;&#12290;&#26681;&#25454;Agent&#22312;&#22270;&#20013;&#30340;&#20301;&#32622;&#65292;Agent&#25509;&#25910;&#21040;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#22270;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#24182;&#19982;&#36873;&#25321;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03843</link><description>&lt;p&gt;
&#20809;&#23398;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method for optical steel rope non-destructive damage detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#29992;&#20110;&#22312;&#39640;&#28023;&#25300;&#29615;&#22659;&#20013;&#23545;&#38050;&#19997;&#32499;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#30830;&#25552;&#21462;&#38050;&#19997;&#32499;&#30340;&#20998;&#21106;&#27169;&#22411;&#21644;&#19968;&#31181;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#38050;&#19997;&#32499;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#26174;&#33879;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#28023;&#25300;&#29615;&#22659;&#65288;&#31354;&#20013;&#21514;&#32034;&#36947;&#65289;&#20013;&#30340;&#38050;&#19997;&#32499;&#38750;&#30772;&#22351;&#24615;&#25439;&#20260;&#26816;&#27979;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RGBD-UNet&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#22797;&#26434;&#32972;&#26223;&#20013;&#25552;&#21462;&#38050;&#19997;&#32499;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20986;&#30340;CMA&#27169;&#22359;&#21487;&#20197;&#22788;&#29702;&#21644;&#32467;&#21512;&#39068;&#33394;&#21644;&#28145;&#24230;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;VovNetV3.5&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#38050;&#19997;&#32499;&#12290;&#23427;&#23558;VovNet&#26550;&#26500;&#19982;DBB&#27169;&#22359;&#32467;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32972;&#26223;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#21516;&#22330;&#26223;&#20013;&#38050;&#19997;&#32499;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#21106;&#21644;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#22312;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#27492;&#31639;&#27861;&#30340;&#20256;&#24863;&#22120;&#35782;&#21035;&#24615;&#33021;&#65288;h&#65289;&#26126;&#26174;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for non-destructive damage detection for steel ropes in high-altitude environments (aerial ropeway). The algorithm comprises two key components: First, a segmentation model named RGBD-UNet is designed to accurately extract steel ropes from complex backgrounds. This model is equipped with the capability to process and combine color and depth information through the proposed CMA module. Second, a detection model named VovNetV3.5 is developed to differentiate between normal and abnormal steel ropes. It integrates the VovNet architecture with a DBB module to enhance performance. Besides, a novel background augmentation method is proposed to enhance the generalization ability of the segmentation model. Datasets containing images of steel ropes in different scenarios are created for the training and testing of both the segmentation and detection models. Experiments demonstrate a significant improvement over baseline models. On the proposed dataset, the h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03247</link><description>&lt;p&gt;
HEANA: &#19968;&#31181;&#20855;&#26377;&#28789;&#27963;&#25968;&#25454;&#27969;&#30340;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#33021;&#37327;&#39640;&#25928;&#30340;CNN&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible Dataflows for Energy-Efficient CNN Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03247
&lt;/p&gt;
&lt;p&gt;
HEANA&#26159;&#19968;&#31181;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#25552;&#39640;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#28789;&#27963;&#24615;&#12290;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#20018;&#25200;&#12289;&#19981;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HEANA&#30340;&#26032;&#22411;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#21152;&#36895;&#25972;&#25968;&#37327;&#21270;CNN&#30340;&#25512;&#29702;&#12290;HEANA&#37319;&#29992;&#28151;&#21512;&#26102;&#24133;&#27169;&#25311;&#20809;&#23398;&#20056;&#27861;&#22120;(TAOMs)&#65292;&#22686;&#24378;&#20102;HEANA&#23545;&#22810;&#31181;&#25968;&#25454;&#27969;&#30340;&#25903;&#25345;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#35889;&#26080;&#25439;&#30340;TAOMs&#25490;&#21015;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#29616;&#26377;&#20809;&#23398;&#21152;&#36895;&#22120;&#23384;&#22312;&#30340;&#27874;&#38271;&#24182;&#34892;&#24615;&#21463;&#21040;&#21508;&#31181;&#20018;&#25200;&#24433;&#21709;&#12289;&#26080;&#27861;&#25903;&#25345;&#22810;&#31181;&#25968;&#25454;&#27969;&#20197;&#21450;&#26410;&#20805;&#20998;&#21033;&#29992;&#20809;&#30005;&#25506;&#27979;&#22120;&#36827;&#34892;&#21407;&#20301;&#32047;&#31215;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several photonic microring resonators (MRRs) based analog accelerators have been proposed to accelerate the inference of integer-quantized CNNs with remarkably higher throughput and energy efficiency compared to their electronic counterparts. However, the existing analog photonic accelerators suffer from three shortcomings: (i) severe hampering of wavelength parallelism due to various crosstalk effects, (ii) inflexibility of supporting various dataflows other than the weight-stationary dataflow, and (iii) failure in fully leveraging the ability of photodetectors to perform in-situ accumulations. These shortcomings collectively hamper the performance and energy efficiency of prior accelerators. To tackle these shortcomings, we present a novel Hybrid timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid time-amplitude analog optical multipliers (TAOMs) that increase the flexibility of HEANA to support multiple dataflows. A spectrally hitless arrangement of TAOMs s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.02420</link><description>&lt;p&gt;
2024&#24180;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Factuality of Large Language Models in the Year 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#32842;&#22825;&#26041;&#38754;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22320;&#26041;&#30452;&#25509;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#65292;&#20351;&#20154;&#20204;&#20174;&#25628;&#32034;&#12289;&#25552;&#21462;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#35299;&#33073;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;LLM&#30495;&#23454;&#24615;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#20986;&#20027;&#35201;&#25361;&#25112;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#20998;&#26512;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#38754;&#20020;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.01703</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver Interaction in Los Angeles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#27931;&#26441;&#30710;&#35686;&#23519;&#19982;&#21496;&#26426;&#30340;&#20114;&#21160;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#21253;&#25324;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#22797;&#26434;&#21644;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#24220;&#23448;&#21592;&#19982;&#24066;&#27665;&#20043;&#38388;&#30340;&#20114;&#21160;&#24433;&#21709;&#20844;&#20849;&#31119;&#31049;&#21644;&#27665;&#20027;&#31038;&#20250;&#30340;&#27491;&#24403;&#24615;&#12290;&#35686;&#23519;&#26159;&#22269;&#23478;&#26368;&#26174;&#32780;&#26131;&#35265;&#12289;&#26368;&#25509;&#35302;&#24066;&#27665;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#20132;&#36890;&#31449;&#20572;&#26399;&#38388;&#65292;&#20182;&#20204;&#27599;&#24180;&#19982;&#20844;&#20247;&#20114;&#21160;&#36229;&#36807;2000&#19975;&#27425;&#12290;&#22914;&#20170;&#65292;&#36825;&#20123;&#20114;&#21160;&#32463;&#24120;&#34987;&#25140;&#22312;&#36523;&#19978;&#30340;&#25668;&#20687;&#26426;&#35760;&#24405;&#19979;&#26469;&#65292;&#36825;&#34987;&#35270;&#20026;&#25552;&#39640;&#35686;&#23519;&#38382;&#36131;&#21046;&#21644;&#25913;&#21892;&#35686;&#27665;&#20114;&#21160;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20998;&#26512;&#36825;&#20123;&#22797;&#26434;&#32780;&#26377;&#20105;&#35758;&#30340;&#35686;&#27665;&#20114;&#21160;&#65292;&#36825;&#20123;&#35760;&#24405;&#30340;&#21450;&#26102;&#20998;&#26512;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35282;&#24230;&#12289;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#26469;&#33258;&#36825;&#20123;&#36523;&#19978;&#25668;&#20687;&#26426;&#35760;&#24405;&#30340;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#25991;&#23383;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#19982;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#26368;&#30456;&#20851;&#30340;&#27807;&#36890;&#26041;&#38754;&#65292;&#21253;&#25324;&#20849;&#21516;&#24863;&#30693;&#20114;&#21160;&#30340;&#26631;&#24535;&#26631;&#35760;&#20197;&#21450;&#20855;&#26377;&#36825;&#20123;&#26631;&#35760;&#30340;&#31526;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between the government officials and civilians affect public wellbeing and the state legitimacy that is necessary for the functioning of democratic society. Police officers, the most visible and contacted agents of the state, interact with the public more than 20 million times a year during traffic stops. Today, these interactions are regularly recorded by body-worn cameras (BWCs), which are lauded as a means to enhance police accountability and improve police-public interactions. However, the timely analysis of these recordings is hampered by a lack of reliable automated tools that can enable the analysis of these complex and contested police-public interactions. This article proposes an approach to developing new multi-perspective, multimodal machine learning (ML) tools to analyze the audio, video, and transcript information from this BWC footage. Our approach begins by identifying the aspects of communication most salient to different stakeholders, including both commun
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2401.17350</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#20248;&#21270;&#26102;&#38388;&#24207;&#21015;&#20379;&#24212;&#21830;&#20998;&#37197;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Timeseries Suppliers Allocation Risk Optimization via Deep Black Litterman Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17350
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#40657;&#30707;&#36125;&#33713;&#26364;&#27169;&#22411;&#21644;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;BL&#27169;&#22411;&#21644;Perspective&#30697;&#38453;&#65292;&#20197;&#20248;&#21270;&#20379;&#24212;&#21830;&#36873;&#25321;&#21644;&#35746;&#21333;&#20998;&#37197;&#65292;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#20379;&#24212;&#21830;&#20851;&#31995;&#32593;&#32476;&#65292;&#22686;&#24378;&#20102;&#23545;&#22797;&#26434;&#20379;&#24212;&#21830;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;Masked Ranking&#26426;&#21046;&#35299;&#20915;&#20102;&#38646;&#38454;&#24773;&#20917;&#19979;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20379;&#24212;&#21830;&#25490;&#24207;&#25928;&#29575;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;DBLM&#22312;&#25552;&#20379;&#20934;&#30830;&#39044;&#27979;&#21644;&#31934;&#30830;&#32622;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#20998;&#36776;&#29575;&#24773;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the BL model and the Perspective Matrix to optimize supplier selection and order allocation, focusing on both temporal and spatial dynamics. Our development of a Supplier Relationship Network, using a Spatio-Temporal Graph Neural Network, enhances the understanding of complex supplier interdependencies. Additionally, we address credibility issues in zero-order scenarios with a Masked Ranking Mechanism, improving supplier ranking efficiency. Our model demonstrates superior results on two datasets compared to the traditional models. Our evaluations using real-world datasets highlight DBLM's superiority in providing accurate predictions and precise confidence intervals, particularly in high-resolution scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2401.03630</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#23578;&#26410;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;
Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#24341;&#21457;&#30340;&#29190;&#28856;&#24615;&#24433;&#21709;&#65292;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#26041;&#38754;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20998;&#20139;&#35265;&#35299;&#12290;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#19981;&#21516;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#23427;&#23558;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#21644;&#35268;&#21010;&#30340;&#22256;&#38590;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#20419;&#36827;&#25152;&#38656;&#30340;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#35299;&#20915;MAPF&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#38556;&#30861;&#29289;&#30340;&#31354;&#25151;&#38388;&#22320;&#22270;&#19978;&#30340;&#28608;&#21169;&#24615;&#25104;&#21151;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#23545;&#26631;&#20934;MAPF&#22522;&#20934;&#27979;&#35797;&#20013;&#36739;&#38590;&#30340;&#25151;&#38388;&#22320;&#22270;&#21644;&#36855;&#23467;&#22320;&#22270;&#30340;&#35268;&#21010;&#22833;&#36133;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#30452;&#25509;&#20351;&#29992;LLM&#35299;&#20915;MAPF&#23578;&#26410;&#25104;&#21151;&#30340;&#31435;&#22330;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#26469;&#25903;&#25745;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to supp
&lt;/p&gt;</description></item><item><title>LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.16044</link><description>&lt;p&gt;
LLMLight: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLMLight: Large Language Models as Traffic Signal Control Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16044
&lt;/p&gt;
&lt;p&gt;
LLMLight&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#21161;&#20808;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26500;&#24314;&#19987;&#20026;TSC&#20219;&#21153;&#23450;&#21046;&#30340;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;LightGPT&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;LLMLight&#30340;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#22478;&#24066;&#20132;&#36890;&#31649;&#29702;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#20248;&#21270;&#36947;&#36335;&#32593;&#32476;&#25928;&#29575;&#21644;&#20943;&#23569;&#25317;&#22581;&#12290;&#20256;&#32479;&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#20132;&#36890;&#24037;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#24448;&#24448;&#22312;&#21508;&#31181;&#20132;&#36890;&#22330;&#26223;&#20013;&#23384;&#22312;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLMLight&#65292;&#36825;&#26159;&#19968;&#20010;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;TSC&#20915;&#31574;&#20195;&#29702;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21521;LLM&#25552;&#20379;&#35814;&#32454;&#30340;&#23454;&#26102;&#20132;&#36890;&#29366;&#20917;&#35828;&#26126;&#20316;&#20026;&#25351;&#23548;&#65292;&#20511;&#21161;LLM&#30340;&#20808;&#36827;&#27867;&#21270;&#33021;&#21147;&#65292;LLMLight&#23454;&#29616;&#20102;&#31867;&#20284;&#20154;&#31867;&#30452;&#35273;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#20132;&#36890;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;LightGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;TSC&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#39592;&#24178;LLM&#12290;&#36890;&#36807;&#23398;&#20064;&#32454;&#24494;&#30340;&#20132;&#36890;&#27169;&#24335;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;LightGPT&#22312;&#32463;&#27982;&#25104;&#26412;&#26041;&#38754;&#25552;&#21319;&#20102;LLMLight&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;LLMLight&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional methods in TSC, primarily based on transportation engineering and reinforcement learning (RL), often exhibit limitations in generalization across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15842</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#30340;LLM&#30340;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#65288;&#20316;&#20026;&#36719;&#26631;&#31614;&#65289;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;LLM&#20805;&#24403;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#20026;&#20102;&#20174;LLM&#30340;&#36755;&#20986;&#27010;&#29575;&#20013;&#23398;&#20064;&#32780;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#20197;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#20026;&#20102;&#39564;&#35777;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;6,684&#20010;&#23398;&#29983;&#23545;&#31185;&#23398;&#38382;&#39064;&#30340;&#20889;&#20316;&#22238;&#31572;&#21644;&#19977;&#20010;&#20154;&#24037;&#19987;&#23478;&#35780;&#20998;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;7T&#12290;&#25105;&#20204;&#23558;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;TinyBERT&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.15122</link><description>&lt;p&gt;
&#25193;&#23637;&#23601;&#26159;&#19968;&#20999;&#65306;&#20351;&#29992;JAX&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#35270;&#39057;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#26368;&#20248;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36816;&#34892;&#24517;&#35201;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#38750;&#24120;&#22256;&#38590;&#12290;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#36827;&#34892;&#20998;&#24067;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#19978;&#25910;&#38598;&#32463;&#39564;&#20174;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#19988;&#30495;&#23454;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#30495;&#23454;&#39550;&#39542;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33021;&#21147;&#38598;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36127;&#26679;&#26412;&#21644;&#26368;&#36817;&#37051;&#23545;&#65292;&#28040;&#38500;&#23545;&#26631;&#35760;&#30340;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.14001</link><description>&lt;p&gt;
&#20351;&#29992;Siamese&#32593;&#32476;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Face Recognition Method using Siamese Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14001
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#26080;&#30417;&#30563;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36127;&#26679;&#26412;&#21644;&#26368;&#36817;&#37051;&#23545;&#65292;&#28040;&#38500;&#23545;&#26631;&#35760;&#30340;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#37096;&#39564;&#35777;&#31995;&#32479;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#36890;&#24120;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#28982;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#22823;&#37327;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;Siamese&#32593;&#32476;&#36827;&#34892;&#20154;&#33080;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#26631;&#35760;&#30340;&#20154;&#33080;&#22270;&#20687;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#21644;&#26368;&#36817;&#37051;&#23545;&#26469;&#24314;&#31435;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#35757;&#32451;&#23545;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#19968;&#20010;VGG&#32534;&#30721;&#22120;&#65292;&#20316;&#20026;&#21452;&#20998;&#25903;Siamese&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36991;&#20813;&#23545;&#26631;&#35760;&#30340;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#19979;&#29983;&#25104;&#35757;&#32451;&#23545;&#30340;&#26041;&#27861;&#12290;&#27491;&#35757;&#32451;&#25968;&#25454;&#26159;&#26681;&#25454;&#20854;&#19982;&#25351;&#23450;&#38170;&#28857;&#20043;&#38388;&#30340;&#26368;&#39640;&#20313;&#24358;&#30456;&#20284;&#24230;&#24471;&#20998;&#22312;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#30340;&#65292;&#32780;&#36127;&#35757;&#32451;&#25968;&#25454;&#21017;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#30830;&#23450;&#65292;&#20294;&#20174;&#19968;&#20010;&#21478;&#22806;&#30340;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving state-of-the-art results in face verification systems typically hinges on the availability of labeled face training data, a resource that often proves challenging to acquire in substantial quantities. In this research endeavor, we proposed employing Siamese networks for face recognition, eliminating the need for labeled face images. We achieve this by strategically leveraging negative samples alongside nearest neighbor counterparts, thereby establishing positive and negative pairs through an unsupervised methodology. The architectural framework adopts a VGG encoder, trained as a double branch siamese network. Our primary aim is to circumvent the necessity for labeled face image data, thus proposing the generation of training pairs in an entirely unsupervised manner. Positive training data are selected within a dataset based on their highest cosine similarity scores with a designated anchor, while negative training data are culled in a parallel fashion, though drawn from an al
&lt;/p&gt;</description></item><item><title>LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17943</link><description>&lt;p&gt;
LayerCollapse: &#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LayerCollapse: Adaptive compression of neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17943
&lt;/p&gt;
&lt;p&gt;
LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19981;&#26029;&#22686;&#38271;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36229;&#21442;&#25968;&#21270;&#30340;Transformer&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#19994;&#32489;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#21547;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerCollapse&#65292;&#19968;&#31181;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#24418;&#24335;&#65292;&#29992;&#20110;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20801;&#35768;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#21518;&#21387;&#32553;&#65292;&#24182;&#23545;&#24615;&#33021;&#20135;&#29983;&#26377;&#38480;&#30340;&#24433;&#21709;&#12290;LayerCollapse&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#30340;&#28608;&#27963;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#35843;&#33410;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#38477;&#20302;&#21040;&#30456;&#24212;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;LayerCollapse&#30340;&#21387;&#32553;&#33021;&#21147;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#38453;&#21015;&#20013;&#20248;&#21270;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35777;&#26126;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;NP&#22256;&#38590;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2311.16594</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#20013;&#30340;&#25925;&#38556;&#23450;&#20301;&#30417;&#27979;&#22120;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Monitor Placement for Fault Localization in Deep Neural Network Accelerators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24182;&#34892;&#38453;&#21015;&#20013;&#20248;&#21270;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35777;&#26126;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#65292;&#24182;&#35299;&#20915;&#20102;NP&#22256;&#38590;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#24615;&#21644;&#39640;&#25928;&#25968;&#25454;&#37325;&#29992;&#20351;&#24471;&#24182;&#34892;&#38453;&#21015;&#31995;&#32479;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#36873;&#25321;&#12290;&#25552;&#39640;DNN&#21152;&#36895;&#22120;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#30828;&#20214;&#25925;&#38556;&#21487;&#33021;&#20250;&#38477;&#20302;DNN&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;&#30001;&#20110;&#24182;&#34892;&#38453;&#21015;&#21033;&#29992;&#22823;&#37327;&#22788;&#29702;&#20803;&#20214;&#65288;PE&#65289;&#36827;&#34892;&#24182;&#34892;&#22788;&#29702;&#65292;&#20294;&#24403;&#19968;&#20010;PE&#25925;&#38556;&#26102;&#65292;&#38169;&#35823;&#20250;&#20256;&#25773;&#24182;&#24433;&#21709;&#19979;&#28216;PE&#30340;&#36755;&#20986;&#12290;&#30001;&#20110;PE&#30340;&#25968;&#37327;&#36739;&#22823;&#65292;&#20351;&#29992;&#22522;&#20110;&#30828;&#20214;&#30340;&#36816;&#34892;&#26102;&#30417;&#27979;&#30340;&#25104;&#26412;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#24182;&#34892;&#38453;&#21015;&#20013;&#30828;&#20214;&#30417;&#27979;&#22120;&#37096;&#32626;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#23450;&#20301;&#21333;&#20010;&#25925;&#38556;&#30340;PE&#25152;&#38656;&#30340;&#30417;&#27979;&#22120;&#25968;&#37327;&#20026;$2N-1$&#65292;&#24182;&#23548;&#20986;&#20102;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31532;&#20108;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#30417;&#27979;&#22120;&#37096;&#32626;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#32473;&#23450;&#25968;&#37327;&#30340;&#30417;&#27979;&#22120;&#26368;&#23567;&#21270;&#20505;&#36873;&#25925;&#38556;PE&#38598;&#21512;&#65292;&#35813;&#38382;&#39064;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24179;&#34913;&#23454;&#29616;&#30828;&#20214;&#30417;&#27979;&#30340;&#25928;&#30410;&#19982;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systolic arrays are a prominent choice for deep neural network (DNN) accelerators because they offer parallelism and efficient data reuse. Improving the reliability of DNN accelerators is crucial as hardware faults can degrade the accuracy of DNN inferencing. Systolic arrays make use of a large number of processing elements (PEs) for parallel processing, but when one PE is faulty, the error propagates and affects the outcomes of downstream PEs. Due to the large number of PEs, the cost associated with implementing hardware-based runtime monitoring of every single PE is infeasible. We present a solution to optimize the placement of hardware monitors within systolic arrays. We first prove that $2N-1$ monitors are needed to localize a single faulty PE and we also derive the monitor placement. We show that a second placement optimization problem, which minimizes the set of candidate faulty PEs for a given number of monitors, is NP-hard. Therefore, we propose a heuristic approach to balance 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#22312;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#20219;&#21153;&#24182;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2311.15960</link><description>&lt;p&gt;
&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65306;&#36890;&#36807;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Program Machine Policy: Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#22312;&#38598;&#25104;&#31243;&#24207;&#21512;&#25104;&#21644;&#29366;&#24577;&#26426;&#30340;&#22522;&#30784;&#19978;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#20219;&#21153;&#24182;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32534;&#31243;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#23558;&#20854;&#35270;&#20026;&#21512;&#25104;&#21487;&#35299;&#37322;&#30340;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23616;&#38480;&#20110;&#30701;&#26399;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#29366;&#24577;&#26426;&#34920;&#31034;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#21487;&#20197;&#24402;&#32435;&#25512;&#24191;&#21040;&#38271;&#26399;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#23427;&#22312;&#33719;&#21462;&#22810;&#26679;&#21644;&#22797;&#26434;&#34892;&#20026;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31243;&#24207;&#26426;&#22120;&#31574;&#30053;&#65288;POMP&#65289;&#65292;&#20197;&#26725;&#25509;&#32534;&#31243;&#24335;&#24378;&#21270;&#23398;&#20064;&#21644;&#29366;&#24577;&#26426;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#20801;&#35768;&#34920;&#31034;&#22797;&#26434;&#34892;&#20026;&#24182;&#35299;&#20915;&#38271;&#26399;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#32034;&#19968;&#32452;&#26377;&#25928;&#12289;&#22810;&#26679;&#19988;&#20860;&#23481;&#30340;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31243;&#24207;&#29992;&#20316;&#29366;&#24577;&#26426;&#30340;&#27169;&#24335;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#36716;&#31227;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (deep RL) excels in various domains but lacks generalizability and interpretability. On the other hand, programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes the Program Machine Policy (POMP), which bridges the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, and compatible programs. Then, we use these programs as modes of a state machine and learn a transition func
&lt;/p&gt;</description></item><item><title>MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.13668</link><description>&lt;p&gt;
MAIRA-1&#65306;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAIRA-1: A specialised large multimodal model for radiology report generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13668
&lt;/p&gt;
&lt;p&gt;
MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20351;&#22810;&#27169;&#24577;&#27169;&#22411;&#33719;&#24471;&#22270;&#20687;&#29702;&#35299;&#21644;&#25551;&#36848;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65288;MAIRA-1&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#22522;&#20110;Vicuna-7B&#30340;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;&#29305;&#21035;&#22320;&#65292;MAIRA-1&#22312;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#40784;&#30340;RadCliQ&#24230;&#37327;&#21644;&#32771;&#34385;&#30340;&#25152;&#26377;&#35789;&#27719;&#24230;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25163;&#21160;&#23457;&#26680;&#26174;&#31034;&#20986;&#20102;&#20135;&#29983;&#25253;&#21578;&#30340;&#27969;&#30021;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#25152;&#26410;&#25429;&#25417;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#26356;&#22810;&#20449;&#24687;&#21644;&#36164;&#28304;&#21487;&#22312;&#39033;&#30446;&#32593;&#31449;&#19978;&#25214;&#21040;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.12944</link><description>&lt;p&gt;
DroneOptiNet: &#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution Mechanism for 5G and Beyond Solar Small Cell Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;5G&#21450;&#20854;&#21518;&#22826;&#38451;&#33021;&#23567;&#22411;&#34562;&#31389;&#32593;&#32476;&#30340;&#26368;&#20339;&#26080;&#20154;&#26426;&#36127;&#36733;&#37325;&#20998;&#37197;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#65292;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#20195;&#21450;&#20854;&#21518;&#30340;&#34562;&#31389;&#32593;&#32476;&#23545;&#21151;&#29575;&#38656;&#27714;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38480;&#21046;&#65292;&#38656;&#35201;&#33021;&#22815;&#39640;&#25928;&#21033;&#29992;&#33021;&#28304;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20351;&#29992;&#26080;&#20154;&#26426;&#19978;&#30340;&#31354;&#20013;&#22522;&#31449;&#65288;BS&#65289;&#36827;&#34892;&#21487;&#38752;&#23433;&#20840;&#30340;&#30005;&#21147;&#20877;&#20998;&#37197;&#30340;&#29992;&#25143;&#36127;&#36733;&#36716;&#31227;&#26041;&#27861;&#65292;&#20197;&#36328;&#36234;&#30001;&#32511;&#33394;&#23567;&#22411;&#34562;&#31389;BS&#32452;&#25104;&#30340;&#24494;&#32593;&#32593;&#32476;&#12290;&#26681;&#25454;&#29992;&#25143;&#23494;&#24230;&#21644;&#31354;&#20013;&#22522;&#31449;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#23558;&#31354;&#20013;&#22522;&#31449;&#20174;&#39640;&#33021;&#32791;&#23567;&#21306;&#36801;&#31227;&#21040;&#20302;&#33021;&#32791;&#23567;&#21306;&#65292;&#26469;&#28385;&#36275;&#33021;&#37327;&#19981;&#36275;&#30340;&#23567;&#21306;&#30340;&#33021;&#37327;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#26080;&#20154;&#26426;&#26694;&#26550;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#19982;&#29420;&#29305;&#30340;&#25104;&#26412;&#20989;&#25968;&#32467;&#21512;&#65292;&#20351;&#29992;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26377;&#25928;&#22320;&#31649;&#29702;&#26080;&#20154;&#26426;&#21644;&#22522;&#31449;&#30340;&#33021;&#37327;&#21644;&#36127;&#36733;&#37325;&#20998;&#37197;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20943;&#23569;&#20102;&#22522;&#31449;&#20572;&#30005;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#33268;&#30340;&#21534;&#21520;&#37327;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#23637;&#31034;&#20102;&#20854;&#25552;&#21319;&#26080;&#32447;&#32593;&#32476;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The power requirements posed by the fifth-generation and beyond cellular networks are an important constraint in network deployment and require energy-efficient solutions. In this work, we propose a novel user load transfer approach using airborne base stations (BS) mounted on drones for reliable and secure power redistribution across the micro-grid network comprising green small cell BSs. Depending on the user density and the availability of an aerial BS, the energy requirement of a cell with an energy deficit is accommodated by migrating the aerial BS from a high-energy to a low-energy cell. The proposed hybrid drone-based framework integrates long short-term memory with unique cost functions using an evolutionary neural network for drones and BSs and efficiently manages energy and load redistribution. The proposed algorithm reduces power outages at BSs and maintains consistent throughput stability, thereby demonstrating its capability to boost the reliability and robustness of wirel
&lt;/p&gt;</description></item><item><title>AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09830</link><description>&lt;p&gt;
AutoPlanBench: &#20174;PDDL&#33258;&#21160;&#29983;&#25104;LLM&#35268;&#21010;&#22120;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09830
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#36923;&#36753;-&#27010;&#29575;&#27169;&#22411;&#65289;&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPlanBench&#65292;&#19968;&#31181;&#23558;PDDL&#20013;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#20182;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#22256;&#38590;&#65292;&#21457;&#29616;&#38169;&#35823;&#36890;&#24120;&#28304;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#12290;&#22522;&#20110;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;StructChem&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09656</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#21270;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Structured Chemistry Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#22256;&#38590;&#65292;&#21457;&#29616;&#38169;&#35823;&#36890;&#24120;&#28304;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#12290;&#22522;&#20110;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;StructChem&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#20013;&#28041;&#21450;&#30340;&#31616;&#21333;&#21270;&#23398;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#23376;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#22797;&#26434;&#30340;&#21270;&#23398;&#38382;&#39064;&#19981;&#20165;&#38656;&#35201;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#20851;&#20110;&#19981;&#21516;&#27010;&#24565;&#65288;&#20363;&#22914;&#28201;&#24230;&#21464;&#21270;&#65289;&#30340;&#20016;&#23500;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#20687;GPT-4&#36825;&#26679;&#20808;&#36827;&#30340;LLMs&#20063;&#24456;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#38169;&#35823;&#36890;&#24120;&#19981;&#26159;&#30001;&#20110;LLMs&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#26159;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#26469;&#24341;&#23548;LLMs&#24341;&#21457;&#27491;&#30830;&#30340;&#30693;&#35782;&#65292;&#23558;&#30693;&#35782;&#34701;&#20837;&#36880;&#27493;&#25512;&#29702;&#20013;&#65292;&#24182;&#36845;&#20195;&#25913;&#36827;&#32467;&#26524;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36136;&#37327;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#32467;&#26500;&#21270;&#21270;&#23398;&#65288;StructChem&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#25152;&#38656;&#30340;&#25351;&#23548;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.06623</link><description>&lt;p&gt;
VT-Former: &#22522;&#20110;Transformer&#30340;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;&#36947;&#36335;&#23433;&#20840;&#21644;&#20132;&#36890;&#31649;&#29702;&#24050;&#25104;&#20026;&#29616;&#20195;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#22312;&#20844;&#36335;&#21644;&#36947;&#36335;&#23433;&#20840;&#30340;&#20247;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#20132;&#36890;&#31649;&#29702;&#12289;&#20107;&#25925;&#39044;&#38450;&#12289;&#24037;&#22320;&#23433;&#20840;&#21644;&#33021;&#28304;&#20248;&#21270;&#31561;&#21508;&#31181;&#29992;&#20363;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#21450;&#30417;&#25511;&#25668;&#20687;&#22836;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#22686;&#21152;&#37096;&#32626;&#25512;&#21160;&#19979;&#65292;&#26234;&#33021;&#31649;&#29702;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;VT-Former&#12290;&#38500;&#20102;&#21033;&#29992;Transformer&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#27169;&#24335;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27880;&#24847;&#21147;&#20998;&#35789;&#65288;GAT&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing roadway safety and traffic management has become an essential focus area for a broad range of modern cyber-physical systems and intelligent transportation systems. Vehicle Trajectory Prediction is a pivotal element within numerous applications for highway and road safety. These applications encompass a wide range of use cases, spanning from traffic management and accident prevention to enhancing work-zone safety and optimizing energy conservation. The ability to implement intelligent management in this context has been greatly advanced by the developments in the field of Artificial Intelligence (AI), alongside the increasing deployment of surveillance cameras across road networks. In this paper, we introduce a novel transformer-based approach for vehicle trajectory prediction for highway safety and surveillance, denoted as VT-Former. In addition to utilizing transformers to capture long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module has been proposed
&lt;/p&gt;</description></item><item><title>LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.14894</link><description>&lt;p&gt;
&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;-- &#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#20855;&#26377;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14894
&lt;/p&gt;
&lt;p&gt;
LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#36817;&#24180;&#26469;&#26368;&#34987;&#24191;&#27867;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20043;&#19968;&#12290;&#23427;&#20063;&#26159;&#26368;&#20998;&#25955;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#19987;&#27880;&#20110;&#35299;&#37322;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#20351;&#24471;&#19968;&#27425;&#24615;&#20197;&#32039;&#20945;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#33719;&#24471;&#23436;&#25972;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#12290;&#23427;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#20801;&#35768;&#26012;&#20132;&#21644;&#38598;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;XAI&#26041;&#27861;&#65292;&#22914;SHAP&#25110;LIME&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21453;&#65292;&#23427;&#19981;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36873;&#25321;&#20197;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#20986;&#29616;&#30340;&#30495;&#23454;&#25968;&#25454;&#30340;&#23616;&#37096;&#27010;&#24565;&#65292;&#36825;&#20123;&#23616;&#37096;&#27010;&#24565;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#24418;&#25104;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#37319;&#29992;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#38382;&#39064;&#30340;&#27714;&#35299;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2306.02611</link><description>&lt;p&gt;
&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#21487;&#20197;&#34987;&#35777;&#26126;&#26159;&#26377;&#24110;&#21161;&#30340;
&lt;/p&gt;
&lt;p&gt;
Stochastic Population Update Can Provably Be Helpful in Multi-Objective Evolutionary Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#22312;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#37319;&#29992;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#38382;&#39064;&#30340;&#27714;&#35299;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#22240;&#20854;&#22522;&#20110;&#31181;&#32676;&#30340;&#25628;&#32034;&#29305;&#24615;&#65292;&#24050;&#34987;&#24191;&#27867;&#19988;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#31181;&#32676;&#26356;&#26032;&#26159;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEAs&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#24120;&#20197;&#36138;&#23146;&#12289;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#19979;&#19968;&#20195;&#31181;&#32676;&#26159;&#36890;&#36807;&#20174;&#24403;&#21069;&#31181;&#32676;&#21644;&#26032;&#29983;&#25104;&#30340;&#35299;&#20013;&#36873;&#25321;&#26368;&#20248;&#35299;&#24418;&#25104;&#30340;&#65288;&#26080;&#35770;&#20351;&#29992;&#30340;&#36873;&#25321;&#26631;&#20934;&#26159;Pareto&#25903;&#37197;&#12289;&#25317;&#25380;&#24230;&#36824;&#26159;&#25351;&#26631;&#31561;&#65289;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#38543;&#26426;&#31181;&#32676;&#26356;&#26032;&#23545;&#20110;MOEAs&#30340;&#25628;&#32034;&#26159;&#26377;&#30410;&#30340;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#30830;&#23450;&#24615;&#31181;&#32676;&#26356;&#26032;&#26426;&#21046;&#26367;&#25442;&#20026;&#38543;&#26426;&#26426;&#21046;&#65292;&#21487;&#20197;&#25351;&#25968;&#32423;&#20943;&#23569;&#20004;&#20010;&#24050;&#32463;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;MOEAs&#65288;SMS-EMOA&#21644;NSGA-II&#65289;&#22312;&#35299;&#20915;&#20004;&#20010;&#21452;&#30446;&#26631;&#38382;&#39064;&#65288;OneJumpZeroJump&#21644;&#21452;&#30446;&#26631;RealRoyalRoad&#65289;&#19978;&#30340;&#39044;&#35745;&#36816;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have been widely and successfully applied to solve multi-objective optimization problems, due to their nature of population-based search. Population update, a key component in multi-objective EAs (MOEAs), is usually performed in a greedy, deterministic manner. That is, the next-generation population is formed by selecting the best solutions from the current population and newly-generated solutions (irrespective of the selection criteria used such as Pareto dominance, crowdedness and indicators). In this paper, we question this practice. We analytically present that stochastic population update can be beneficial for the search of MOEAs. Specifically, we prove that the expected running time of two well-established MOEAs, SMS-EMOA and NSGA-II, for solving two bi-objective problems, OneJumpZeroJump and bi-objective RealRoyalRoad, can be exponentially decreased if replacing its deterministic population update mechanism by a stochastic one. Empirical studies als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2305.13179</link><description>&lt;p&gt;
&#23558;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#25945;&#32473;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Teaching Probabilistic Logical Reasoning to Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#30340;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#30830;&#23450;&#30340;&#25512;&#29702;&#35268;&#21017;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#26041;&#38754;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#65292;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#65292;&#23427;&#22312;&#24494;&#35843;&#38454;&#27573;&#21033;&#29992;&#27010;&#29575;&#36923;&#36753;&#35268;&#21017;&#20316;&#20026;&#32422;&#26463;&#65292;&#32780;&#19981;&#20381;&#36182;&#36825;&#20123;&#35268;&#21017;&#22312;&#25512;&#29702;&#38454;&#27573;&#12290;&#20026;&#20102;&#35780;&#20272;PCT&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#30456;&#20851;&#35821;&#26009;&#24211;&#65292;&#24182;&#39069;&#22806;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#20043;&#21069;&#30340;&#27979;&#35797;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20102;&#23454;&#20363;&#29305;&#23450;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;PCT&#25552;&#39640;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26356;&#26126;&#30830;&#21644;&#21487;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;PCT&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19981;&#30830;&#23450;&#30340;&#25991;&#26412;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2305.12707</link><description>&lt;p&gt;
&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#21450;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#19982;&#27492;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;LLMs&#33021;&#21147;&#26159;&#23427;&#20204;&#33021;&#22815;&#24418;&#25104;&#19981;&#21516;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20294;&#36825;&#22312;&#28041;&#21450;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#26102;&#24341;&#21457;&#20102;&#25285;&#24551;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#26088;&#22312;&#25581;&#31034;&#24433;&#21709;&#20854;&#20851;&#32852;&#20449;&#24687;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#23545;&#23637;&#31034;&#26356;&#30701;&#30340;&#20849;&#29616;&#36317;&#31163;&#25110;&#26356;&#39640;&#30340;&#20849;&#29616;&#39057;&#29575;&#26102;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#32852;&#24120;&#35782;&#30693;&#35782;&#19982;PII&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#23613;&#31649;&#20934;&#30830;&#39044;&#27979;PII&#30340;&#27604;&#20363;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;LLMs&#20173;&#28982;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capab
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>https://arxiv.org/abs/2302.05262</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Data Augmentation and Loss Functions in Semantic Image Segmentation for Drilling Tool Wear Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#38075;&#23380;&#24037;&#20855;&#30952;&#25439;&#26816;&#27979;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#25439;&#22833;&#20989;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#24037;&#20855;&#30952;&#25439;&#30417;&#27979;&#23545;&#20110;&#36136;&#37327;&#25511;&#21046;&#21644;&#25104;&#26412;&#38477;&#20302;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#27969;&#31243;&#65292;&#24212;&#29992;&#20110;&#20999;&#21106;&#25554;&#20837;&#29289;&#30340;&#26174;&#24494;&#22270;&#20687;&#65292;&#29992;&#20110;&#30952;&#25439;&#26816;&#27979;&#12290;&#30952;&#25439;&#21306;&#22495;&#20998;&#20026;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#20004;&#31181;&#30952;&#25439;&#31867;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#30952;&#25439;&#31867;&#21035;&#65292;&#21487;&#20197;&#25226;&#38382;&#39064;&#23450;&#20041;&#20026;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#12290;&#38500;&#20102;&#27604;&#36739;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#38382;&#39064;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#20132;&#21449;&#29109;&#12289;&#28966;&#28857;&#20132;&#21449;&#29109;&#21644;&#22522;&#20110;IoU&#30340;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#19981;&#21516;&#23610;&#23544;&#30340;&#22270;&#20687;&#22359;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24230;&#22686;&#24378;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20108;&#20803;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tool wear monitoring is crucial for quality control and cost reduction in manufacturing processes, of which drilling applications are one example. In this paper, we present a U-Net based semantic image segmentation pipeline, deployed on microscopy images of cutting inserts, for the purpose of wear detection. The wear area is differentiated in two different types, resulting in a multiclass classification problem. Joining the two wear types in one general wear class, on the other hand, allows the problem to be formulated as a binary classification task. Apart from the comparison of the binary and multiclass problem, also different loss functions, i. e., Cross Entropy, Focal Cross Entropy, and a loss based on the Intersection over Union (IoU), are investigated. Furthermore, models are trained on image tiles of different sizes, and augmentation techniques of varying intensities are deployed. We find, that the best performing models are binary models, trained on data with moderate augmentat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#37325;&#26032;&#23450;&#20041;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2210.11846</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#27010;&#36848;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.11846
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#37325;&#26032;&#23450;&#20041;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#65292;&#20197;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#21644;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#32570;&#20047;&#36879;&#26126;&#24230;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#38754;&#21521;&#38750;&#19987;&#23478;&#30340;&#35299;&#37322;&#23545;&#29992;&#25143;&#30340;&#20449;&#20219;&#21644;&#20154;&#26426;&#21327;&#20316;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#22823;&#37096;&#20998;&#38754;&#21521;AI&#30340;&#35299;&#37322;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#24320;&#21457;&#32773;&#21644;&#19987;&#23478;&#29992;&#25143;&#30340;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#25552;&#20379;&#29992;&#25143;&#20851;&#20110;&#22914;&#20309;&#25913;&#21464;&#36755;&#20837;&#20197;&#25913;&#21464;&#40657;&#30418;&#27169;&#22411;&#36755;&#20986;&#30340;&#23616;&#37096;&#35299;&#37322;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#21451;&#22909;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#24314;&#35758;&#65292;&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;AI&#31995;&#32479;&#36755;&#20986;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24456;&#23569;&#26377;&#24212;&#29992;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24378;&#22823;&#35299;&#37322;&#26041;&#27861;&#20195;&#34920;&#19981;&#36275;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24403;&#21069;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
While AI algorithms have shown remarkable success in various fields, their lack of transparency hinders their application to real-life tasks. Although explanations targeted at non-experts are necessary for user trust and human-AI collaboration, the majority of explanation methods for AI are focused on developers and expert users. Counterfactual explanations are local explanations that offer users advice on what can be changed in the input for the output of the black-box model to change. Counterfactuals are user-friendly and provide actionable advice for achieving the desired output from the AI system. While extensively researched in supervised learning, there are few methods applying them to reinforcement learning (RL). In this work, we explore the reasons for the underrepresentation of a powerful explanation method in RL. We start by reviewing the current work in counterfactual explanations in supervised learning. Additionally, we explore the differences between counterfactual explana
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#19982;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2209.09441</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Locally Constrained Representations in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#19982;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#20855;&#26377;&#19968;&#23450;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20174;&#29615;&#22659;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#32431;&#31929;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#29366;&#24577;&#19979;&#21487;&#33021;&#24046;&#24322;&#24040;&#22823;&#65292;&#36825;&#21462;&#20915;&#20110;&#20540;&#20989;&#25968;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#24182;&#19981;&#19968;&#23450;&#38656;&#35201;&#19982;&#24403;&#21069;&#20219;&#21153;&#38750;&#24120;&#30456;&#20851;&#12290;&#20165;&#20381;&#36182;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#31034;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#27493;&#38271;&#20013;&#24046;&#24322;&#24456;&#22823;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#26377;&#19968;&#20010;&#21464;&#21270;&#30340;&#30446;&#26631;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#23558;&#21462;&#20915;&#20110;&#24403;&#21069;&#20540;/&#31574;&#30053;&#30340;&#22909;&#22351;&#12290;&#22240;&#27492;&#65292;&#23558;&#34920;&#31034;&#19982;&#20027;&#35201;&#20219;&#21153;&#35299;&#32806;&#21487;&#20197;&#20351;&#20854;&#19981;&#20165;&#20851;&#27880;&#20110;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#65292;&#36824;&#20851;&#27880;&#29615;&#22659;&#21160;&#24577;&#29305;&#24449;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#32422;&#26463;&#34920;&#31034;&#65292;&#20854;&#20013;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#36843;&#20351;&#29366;&#24577;&#34920;&#31034;&#33021;&#22815;&#30001;&#30456;&#37051;&#29366;&#24577;&#30340;&#34920;&#31034;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#40723;&#21169;&#34920;&#31034;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#29615;&#22659;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Reinforcement Learning (RL) heavily relies on the ability to learn robust representations from the observations of the environment. In most cases, the representations learned purely by the reinforcement learning loss can differ vastly across states depending on how the value functions change. However, the representations learned need not be very specific to the task at hand. Relying only on the RL objective may yield representations that vary greatly across successive time steps. In addition, since the RL loss has a changing target, the representations learned would depend on how good the current values/policies are. Thus, disentangling the representations from the main task would allow them to focus not only on the task-specific features but also the environment dynamics. To this end, we propose locally constrained representations, where an auxiliary loss forces the state representations to be predictable by the representations of the neighboring states. This encourages
&lt;/p&gt;</description></item><item><title>ALEXSIS-PT&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20026;LS&#31995;&#32479;&#30340;&#25913;&#36827;&#21644;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;BERTimbau&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.09034</link><description>&lt;p&gt;
ALEXSIS-PT&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.09034
&lt;/p&gt;
&lt;p&gt;
ALEXSIS-PT&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20026;LS&#31995;&#32479;&#30340;&#25913;&#36827;&#21644;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;BERTimbau&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#26159;&#33258;&#21160;&#26367;&#25442;&#22797;&#26434;&#35789;&#27719;&#20026;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#35789;&#27719;&#30340;&#20219;&#21153;&#65292;&#20351;&#25991;&#26412;&#23545;&#21508;&#31181;&#30446;&#26631;&#20154;&#32676;&#65288;&#22914;&#20302;&#35782;&#23383;&#29575;&#30340;&#20010;&#20307;&#12289;&#23398;&#20064;&#38556;&#30861;&#20010;&#20307;&#12289;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#26356;&#26131;&#20110;&#35775;&#38382;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;LS&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#21253;&#21547;&#22797;&#26434;&#35789;&#27719;&#21450;&#20854;&#20505;&#36873;&#26367;&#20195;&#35789;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LS&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ALEXSIS-PT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;LS&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;387&#20010;&#22797;&#26434;&#35789;&#27719;&#30340;9605&#20010;&#20505;&#36873;&#26367;&#20195;&#35789;&#12290;ALEXSIS-PT&#26159;&#25353;&#29031;ALEXSIS&#21327;&#35758;&#32534;&#21046;&#30340;&#65292;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#65292;&#20026;&#36328;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;ALEXSIS-PT&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#25253;&#32440;&#25991;&#31456;&#30340;LS&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#31181;&#26367;&#20195;&#29983;&#25104;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;mDistilBERT&#12289;mBERT&#12289;XLM-R&#21644;BERTimbau&#12290;BERTimbau&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;</title><link>http://arxiv.org/abs/2401.10816</link><description>&lt;p&gt;
Co-Pilot for Health: &#20010;&#24615;&#21270;&#31639;&#27861;AI&#24341;&#23548;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve Health Outcomes. (arXiv:2401.10816v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#65292;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#65292;&#33021;&#22815;&#25552;&#39640;&#21442;&#19982;&#32773;&#30340;&#26085;&#24120;&#27963;&#21160;&#27700;&#24179;&#21644;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#26102;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#33258;&#21160;&#22609;&#36896;&#22823;&#22411;&#20154;&#32676;&#30340;&#20581;&#24247;&#34892;&#20026;&#65292;&#36328;&#21487;&#31359;&#25140;&#35774;&#22791;&#31867;&#22411;&#21644;&#30142;&#30149;&#29366;&#20917;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#26469;&#25913;&#21892;&#20840;&#29699;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26469;&#33258;&#21487;&#31359;&#25140;&#20581;&#36523;&#35774;&#22791;&#30340;&#31934;&#32454;&#20581;&#24247;&#34892;&#20026;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#24179;&#21488;&#65292;&#29992;&#20110;&#25968;&#23383;&#31639;&#27861;&#24341;&#23548;&#12290;&#22312;&#27492;&#25105;&#20204;&#25551;&#36848;&#20102;&#35813;&#24179;&#21488;&#22312;&#26032;&#21152;&#22369;&#38024;&#23545;$n=84,764$&#20010;&#20010;&#20307;&#30340;12&#21608;&#26399;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#24341;&#23548;&#30340;&#26377;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#32479;&#35745;&#39564;&#35777;&#20102;&#30446;&#26631;&#32452;&#20013;&#25509;&#21463;&#27492;&#31867;AI&#20248;&#21270;&#26085;&#24120;&#24341;&#23548;&#30340;&#21442;&#19982;&#32773;&#30456;&#36739;&#20110;&#25511;&#21046;&#32452;&#20013;&#26410;&#25509;&#21463;&#20219;&#20309;&#24341;&#23548;&#30340;&#21305;&#37197;&#21442;&#19982;&#32773;&#65292;&#20854;&#27599;&#22825;&#30340;&#27493;&#25968;&#22686;&#21152;&#20102;6.17%&#65288;$p = 3.09\times10^{-4}$&#65289;&#65292;&#27599;&#21608;&#20013;&#31561;&#33267;&#21095;&#28872;&#36816;&#21160;&#65288;MVPA&#65289;&#20998;&#38047;&#22686;&#21152;&#20102;7.61%&#65288;$p = 1.16\times10^{-2}$&#65289;&#12290;&#27492;&#22806;&#65292;&#27492;&#31867;&#24341;&#23548;&#38750;&#24120;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to shape health behaviors of large populations automatically, across wearable types and disease conditions at scale has tremendous potential to improve global health outcomes. We designed and implemented an AI driven platform for digital algorithmic nudging, enabled by a Graph-Neural Network (GNN) based Recommendation System, and granular health behavior data from wearable fitness devices. Here we describe the efficacy results of this platform with its capabilities of personalized and contextual nudging to $n=84,764$ individuals over a 12-week period in Singapore. We statistically validated that participants in the target group who received such AI optimized daily nudges increased daily physical activity like step count by 6.17% ($p = 3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched participants in control group who did not receive any nudges. Further, such nudges were very well r
&lt;/p&gt;</description></item><item><title>SEINE&#26159;&#19968;&#31181;&#29992;&#20110;&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#26680;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09773</link><description>&lt;p&gt;
SEINE:&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#19982;&#20132;&#20114;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09773
&lt;/p&gt;
&lt;p&gt;
SEINE&#26159;&#19968;&#31181;&#29992;&#20110;&#26680;&#23454;&#20363;&#20998;&#21106;&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#36890;&#36807;&#32771;&#34385;&#26680;&#32467;&#26500;&#30340;&#30456;&#20851;&#24615;&#21644;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#26680;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#29983;&#29289;&#20998;&#26512;&#21644;&#30284;&#30151;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39318;&#20808;&#65292;&#21980;&#26579;&#26680;&#20869;&#21644;&#26680;&#22806;&#21306;&#22495;&#30340;&#35270;&#35273;&#21576;&#29616;&#30456;&#20284;&#65292;&#24120;&#24120;&#23548;&#33268;&#27424;&#20998;&#21106;&#12290;&#20854;&#27425;&#65292;&#24403;&#21069;&#26041;&#27861;&#32570;&#20047;&#23545;&#26680;&#32467;&#26500;&#30340;&#25506;&#32034;&#65292;&#23548;&#33268;&#20998;&#27573;&#23454;&#20363;&#30340;&#30862;&#29255;&#21270;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SEINE&#30340;&#32467;&#26500;&#32534;&#30721;&#21644;&#20132;&#20114;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#24320;&#21457;&#20102;&#26680;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#26680;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#27599;&#20010;&#20998;&#21106;&#23454;&#20363;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEINE&#24341;&#20837;&#20102;&#22522;&#20110;&#36718;&#24275;&#30340;&#32467;&#26500;&#32534;&#30721;&#65288;SE&#65289;&#65292;&#20197;&#32771;&#34385;&#26680;&#32467;&#26500;&#21644;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#23545;&#26680;&#32467;&#26500;&#30340;&#21512;&#29702;&#34920;&#31034;&#12290;&#22522;&#20110;&#32534;&#30721;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32467;&#26500;&#24341;&#23548;&#30340;&#27880;&#24847;&#21147;&#65288;SGA&#65289;&#65292;&#23427;&#20197;&#28165;&#26224;&#30340;&#26680;&#20316;&#20026;&#21407;&#22411;&#65292;&#20197;&#22686;&#24378;&#27880;&#24847;&#21147;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclei instance segmentation in histopathological images is of great importance for biological analysis and cancer diagnosis but remains challenging for two reasons. (1) Similar visual presentation of intranuclear and extranuclear regions of chromophobe nuclei often causes under-segmentation, and (2) current methods lack the exploration of nuclei structure, resulting in fragmented instance predictions. To address these problems, this paper proposes a structure encoding and interaction network, termed SEINE, which develops the structure modeling scheme of nuclei and exploits the structure similarity between nuclei to improve the integrality of each segmented instance. Concretely, SEINE introduces a contour-based structure encoding (SE) that considers the correlation between nuclei structure and semantics, realizing a reasonable representation of the nuclei structure. Based on the encoding, we propose a structure-guided attention (SGA) that takes the clear nuclei as prototypes to enhance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04846</link><description>&lt;p&gt;
&#21463;&#36807;&#33391;&#22909;&#25945;&#32946;&#30340;&#26234;&#33021;&#30340;&#20869;&#22312;&#21892;&#33391;
&lt;/p&gt;
&lt;p&gt;
The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#24378;&#35843;&#20102;&#25484;&#25569;&#29305;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26680;&#24515;&#26159;&#8220;&#38598;&#20307;&#22914;&#19968;&#20307;&#8221;&#21644;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#25506;&#35752;&#20351;&#19968;&#20010;&#26234;&#33021;&#20307;&#21464;&#24471;&#26234;&#33021;&#30340;&#22240;&#32032;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#20307;&#36824;&#26159;&#35745;&#31639;&#26426;&#19978;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#33021;&#22815;&#34920;&#24449;&#21644;&#25511;&#21046;&#22810;&#20010;&#20445;&#23432;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#21516;&#23376;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26234;&#33021;&#30340;&#26412;&#36136;&#23558;&#34987;&#21457;&#29616;&#26159;&#40644;&#37329;&#27861;&#21017;&#8212;&#8212;&#8220;&#38598;&#20307;&#34892;&#21160;&#22914;&#19968;&#20307;&#8221;&#25110;&#8220;&#20102;&#35299;&#23616;&#37096;&#34892;&#21160;&#30340;&#25972;&#20307;&#32467;&#26524;&#8221;&#12290;&#38598;&#20307;&#30340;&#27969;&#21160;&#26159;&#30001;&#25484;&#25511;&#30528;&#23569;&#37327;&#23383;&#31526;&#20018;&#30340;&#25805;&#32437;&#32773;&#20915;&#23450;&#30340;&#65292;&#26681;&#25454;&#23545;&#31216;&#24615;&#30830;&#23450;&#30340;&#26368;&#23567;&#20316;&#29992;&#36335;&#24452;&#30340;&#27979;&#22320;&#32447;&#36816;&#21160;&#12290;&#25511;&#21046;&#38598;&#20307;&#20445;&#23432;&#31995;&#32479;&#26159;&#22256;&#38590;&#30340;&#65292;&#21382;&#21490;&#19978;&#19968;&#30452;&#36890;&#36807;&#20026;&#31995;&#32479;&#28155;&#21152;&#26174;&#33879;&#40655;&#24615;&#26469;&#31283;&#23450;&#26399;&#26395;&#30340;&#26368;&#22823;&#24615;&#33021;&#30340;&#20122;&#31283;&#24179;&#34913;&#29366;&#24577;&#65292;&#20294;&#36825;&#20250;&#22312;&#36807;&#31243;&#20013;&#38477;&#20302;&#25110;&#30772;&#22351;&#23427;&#20204;&#12290;&#26377;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper will examine what makes a being intelligent, whether that be a biological being or an artificial silicon being on a computer. Special attention will be paid to the being having the ability to characterize and control a collective system of many identical conservative sub-systems conservatively interacting. The essence of intelligence will be found to be the golden rule -- "the collective acts as one" or "knowing the global consequences of local actions". The flow of the collective is a small set of twinkling textures, that are governed by a puppeteer who is pulling a small number of strings according to a geodesic motion of least action, determined by the symmetries. Controlling collective conservative systems is difficult and has historically been done by adding significant viscosity to the system to stabilize the desirable meta stable equilibriums of maximum performance, but it degrades or destroys them in the process. There is an alternative. Once the optimum twinkling te
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07665</link><description>&lt;p&gt;
&#28145;&#24230;&#22238;&#28335;&#23545;&#22240;&#26524;&#19968;&#33268;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#12290;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#22791;&#20102;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#31526;&#21512;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#21487;&#20197;&#36890;&#36807;&#22238;&#31572;&#22312;&#25913;&#21464;&#24773;&#20917;&#19979;&#20250;&#35266;&#23519;&#21040;&#20160;&#20040;&#26469;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#26465;&#20214;&#26159;&#26681;&#25454;&#23454;&#38469;&#35266;&#23519;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20171;&#20837;&#24335;&#35299;&#37322;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22238;&#28335;&#21407;&#21017;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25345;&#25152;&#26377;&#22240;&#26524;&#23450;&#24459;&#23436;&#25972;&#24615;&#30340;&#26367;&#20195;&#21746;&#23398;&#65292;&#20294;&#20854;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22312;&#30001;&#28145;&#24230;&#29983;&#25104;&#32452;&#20214;&#32452;&#25104;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#35745;&#31639;&#22238;&#28335;&#21453;&#20107;&#23454;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#20998;&#37197;&#26045;&#21152;&#20102;&#26465;&#20214;&#65292;&#36890;&#36807;&#22312;&#22240;&#26524;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#19968;&#20010;&#21487;&#34892;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#19982;&#21453;&#20107;&#23454;&#35299;&#37322;&#39046;&#22495;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#12289;&#27169;&#22359;&#21270;&#21644;&#36981;&#23432;&#22240;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.05207</link><description>&lt;p&gt;
&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12289;&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#26469;&#25552;&#39640;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#26816;&#27979;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#21644;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#22312;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#21644;AU&#22495;&#20998;&#31163;&#19982;&#37325;&#24314;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;&#20309;&#23558;&#22823;&#37327;&#30340;&#22312;&#37326;&#38750;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#24341;&#20837;&#30417;&#30563;&#24335;&#38754;&#37096;&#21160;&#20316;&#21333;&#20301;&#65288;AU&#65289;&#26816;&#27979;&#26694;&#26550;&#20013;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AU&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#21516;&#26500;&#38754;&#37096;&#25552;&#21462;&#27169;&#22359;&#30340;&#21442;&#25968;&#65292;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#23398;&#20064;AU&#22495;&#20998;&#31163;&#21644;&#37325;&#24314;&#20197;&#21450;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#29305;&#24449;&#23545;&#40784;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25237;&#24433;&#22120;&#21644;&#25913;&#36827;&#30340;&#23545;&#27604;&#25439;&#22833;&#28155;&#21152;&#20102;&#22235;&#20010;&#39069;&#22806;&#30340;&#20013;&#38388;&#30417;&#30563;&#22120;&#26469;&#20419;&#36827;&#29305;&#24449;&#37325;&#24314;&#30340;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#37326;&#22806;AU&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#21644;Residual Pixel Attention&#23618;&#65292;&#21033;&#29992;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11641</link><description>&lt;p&gt;
Attentive VQ-VAE&#65306;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attentive VQ-VAE. (arXiv:2309.11641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#21644;Residual Pixel Attention&#23618;&#65292;&#21033;&#29992;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25972;&#21512;Attentive Residual Encoder&#65288;AREN&#65289;&#21644;Residual Pixel Attention&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;VQ-VAE&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#23454;&#29992;&#30340;&#21442;&#25968;&#27700;&#24179;&#30340;&#21516;&#26102;&#25913;&#36827;VQ-VAE&#30340;&#24615;&#33021;&#12290;AREN&#32534;&#30721;&#22120;&#34987;&#35774;&#35745;&#25104;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#22810;&#20010;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#36866;&#24212;&#19981;&#21516;&#30340;&#26550;&#26500;&#22797;&#26434;&#24615;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#23558;&#20687;&#32032;&#38388;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;AREN&#32534;&#30721;&#22120;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#28508;&#22312;&#21521;&#37327;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#32534;&#30721;&#32423;&#21035;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#23618;&#37319;&#29992;&#26368;&#23567;&#21442;&#25968;&#26041;&#27861;&#65292;&#30830;&#20445;&#21482;&#26377;&#22312;&#20854;&#20182;&#20687;&#32032;&#30340;&#30456;&#20851;&#20449;&#24687;&#21487;&#29992;&#26102;&#25165;&#20462;&#25913;&#28508;&#22312;&#21521;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06358</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#38382;&#31572;&#20013;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#65292;&#23545;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#30740;&#31350;&#24037;&#20316;&#20173;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#22495;&#27867;&#21270;&#27010;&#24565;&#21364;&#21463;&#21040;&#24456;&#23569;&#20851;&#27880;&#65292;&#22240;&#20026;&#30446;&#26631;&#22495;&#26159;&#26410;&#30693;&#30340;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#21644;&#33719;&#21462;&#26041;&#24335;&#30340;&#22823;&#24133;&#25552;&#39640;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22914;&#20309;&#24433;&#21709;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#8220;&#37326;&#22806;&#29983;&#25104;&#8221;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#29983;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#31572;&#23545;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.04451</link><description>&lt;p&gt;
AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#28431;&#27934;&#65306;&#25506;&#32034;&#38024;&#23545;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning Attacks. (arXiv:2308.04451v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#65292;&#21363;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25915;&#20987;&#21487;&#20197;&#25104;&#21151;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#25968;&#25454;&#34987;&#27602;&#21270;&#65292;&#32780;&#19988;&#19981;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#34987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#27602;&#21270;&#35780;&#20272;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#30340;&#23433;&#20840;&#24615;&#65292;&#21363;&#36890;&#36807;&#23558;&#24694;&#24847;&#26679;&#26412;&#27880;&#20837;&#35757;&#32451;&#25968;&#25454;&#26469;&#29983;&#25104;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#26469;&#27602;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#30340;&#25968;&#25454;&#27602;&#21270;&#65292;AI&#20195;&#30721;&#29983;&#25104;&#22120;&#20063;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#35813;&#25915;&#20987;&#19981;&#20250;&#24433;&#21709;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#20351;&#20854;&#38590;&#20197;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we assess the security of AI code generators via data poisoning, i.e., an attack that injects malicious samples into the training data to generate vulnerable code. We poison the training data by injecting increasing amounts of code containing security vulnerabilities and assess the attack's success on different state-of-the-art models for code generation. Our analysis shows that AI code generators are vulnerable to even a small amount of data poisoning. Moreover, the attack does not impact the correctness of code generated by pre-trained models, making it hard to detect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13566</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#25913;&#36827;&#21508;&#31181;&#21512;&#20316;&#24037;&#20316;&#29615;&#22659;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#20915;&#31574;&#32773;&#19982;&#19981;&#23436;&#32654;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21512;&#36866;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#20415;&#35774;&#35745;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#21327;&#20316;&#24037;&#20855;&#12290;&#19968;&#20123;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#24076;&#26395;&#25913;&#21892;&#20915;&#31574;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#20316;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#20027;&#35201;&#20851;&#27880;&#38169;&#35823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#25215;&#35748;&#21363;&#20351;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#27491;&#30830;&#65292;&#35299;&#37322;&#20063;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#19981;&#23436;&#32654;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24378;&#22823;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#28041;&#21450;136&#21517;&#21442;&#19982;&#32773;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.</title><link>http://arxiv.org/abs/2307.07514</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#19981;&#26159;&#28216;&#25103;&#12290;(arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07514
&lt;/p&gt;
&lt;p&gt;
Shapley values may provide misleading measures of relative feature importance in XAI, challenging their proposed uses in high-stakes application domains.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#20154;&#31867;&#20915;&#31574;&#32773;&#29702;&#35299;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;XAI&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35770;&#35777;&#65292;&#35828;&#26126;Shapley&#20540;&#21487;&#33021;&#20250;&#32473;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#25552;&#20379;&#35823;&#23548;&#65292;&#20351;&#20854;&#20026;&#39044;&#27979;&#20013;&#26080;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#26356;&#39640;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#23545;&#19982;&#39044;&#27979;&#26377;&#20851;&#30340;&#29305;&#24449;&#20998;&#37197;&#36739;&#20302;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#24847;&#20041;&#22312;&#20110;&#23427;&#20204;&#26377;&#25928;&#22320;&#25361;&#25112;&#20102;&#30456;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#22810;&#31181;&#25552;&#35758;&#29992;&#27861;&#65292;&#36825;&#20123;&#29992;&#27861;&#27491;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding complex machine learning (ML) models. One of the hallmarks of XAI are measures of relative feature importance, which are theoretically justified through the use of Shapley values. This paper builds on recent work and offers a simple argument for why Shapley values can provide misleading measures of relative feature importance, by assigning more importance to features that are irrelevant for a prediction, and assigning less importance to features that are relevant for a prediction. The significance of these results is that they effectively challenge the many proposed uses of measures of relative feature importance in a fast-growing range of high-stakes application domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.10890</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#23545;&#31216;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#23398;&#20064;&#22312;&#21019;&#36896;&#24615;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#38899;&#20048;&#39046;&#22495;&#65292;&#24403;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#21644;&#24358;&#12289;&#32441;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#12290;&#29616;&#22312;&#36824;&#19981;&#28165;&#26970;&#20160;&#20040;&#26679;&#30340;&#19968;&#33324;&#24615;&#35745;&#31639;&#21407;&#21017;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#24863;&#30693;&#20445;&#25345;&#19968;&#33268;&#30340;&#20302;&#32500;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23558;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35201;&#27714;&#20808;&#39564;&#27169;&#22411;&#23545;&#28508;&#22312;&#29366;&#24577;&#30340;&#21160;&#24577;&#36827;&#34892;&#25551;&#36848;&#65292;&#24182;&#20197;&#26576;&#31181;&#32676;&#21464;&#25442;&#23545;&#20854;&#36827;&#34892;&#31561;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23545;&#31216;&#24615;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#21333;&#22768;&#36947;&#38899;&#20048;&#38899;&#39057;&#20013;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#38899;&#39640;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#23398;&#20064;&#19968;&#20010;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
&lt;/p&gt;</description></item></channel></rss>