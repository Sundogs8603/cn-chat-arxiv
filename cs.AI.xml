<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.13519</link><description>&lt;p&gt;
&#25945;&#23398;&#35270;&#39057;&#20013;&#20219;&#21153;&#32467;&#26500;&#30340;&#23398;&#20064;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13519
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#12290;&#36890;&#36807;&#20174;&#25972;&#20307;&#19978;&#23398;&#20064;&#34920;&#31034;&#26469;&#39564;&#35777;&#35270;&#39057;&#30340;&#27491;&#30830;&#25191;&#34892;&#65292;&#24182;&#39044;&#27979;&#19979;&#19968;&#27493;&#39588;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#20063;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#38169;&#35823;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22312;&#32447;&#25945;&#23398;&#35270;&#39057;&#25968;&#37327;&#24222;&#22823;&#65292;&#20174;&#35270;&#39057;&#20013;&#23398;&#20064;&#22810;&#27493;&#39588;&#20219;&#21153;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#26159;&#19968;&#20010;&#35825;&#20154;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35270;&#39057;&#27169;&#22411;&#8212;&#8212;VideoTaskformer&#65292;&#19987;&#27880;&#20110;&#34920;&#31034;&#25945;&#23398;&#35270;&#39057;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#30446;&#26631;&#26469;&#23545;VideoTaskformer&#36827;&#34892;&#39044;&#35757;&#32451;&#65306;&#20174;&#25945;&#23398;&#35270;&#39057;&#20013;&#38543;&#26426;&#23631;&#34109;&#30340;&#27493;&#39588;&#39044;&#27979;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#26631;&#31614;&#65288;&#36974;&#30422;&#27493;&#39588;&#24314;&#27169;&#65289;&#12290;&#19982;&#20808;&#21069;&#23398;&#20064;&#23616;&#37096;&#27493;&#39588;&#34920;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20840;&#23616;&#23398;&#20064;&#65292;&#21033;&#29992;&#25972;&#20010;&#21608;&#22260;&#20219;&#21153;&#30340;&#35270;&#39057;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#20174;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#39564;&#35777;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#35270;&#39057;&#26159;&#21542;&#27491;&#30830;&#25191;&#34892;&#32473;&#23450;&#30340;&#20219;&#21153;&#65292;&#20197;&#21450;&#39044;&#27979;&#22312;&#32473;&#23450;&#27493;&#39588;&#20043;&#21518;&#21487;&#33021;&#37319;&#21462;&#21738;&#20123;&#27493;&#39588;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#26816;&#27979;&#25945;&#23398;&#35270;&#39057;&#20013;&#30340;&#38169;&#35823;&#65292;&#20197;&#39564;&#35777;&#26159;&#21542;&#23384;&#22312;&#24322;&#24120;&#27493;&#39588;&#24182;&#26816;&#26597;&#27493;&#39588;&#26159;&#21542;&#25353;&#27491;&#30830;&#30340;&#39034;&#24207;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#12289;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#12289;&#20197;&#21450;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#27169;&#22411;&#22312;&#26410;&#35265;&#31867;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13518</link><description>&lt;p&gt;
&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#19977;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Three ways to improve feature alignment for open vocabulary detection. (arXiv:2303.13518v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13518
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#20013;&#29305;&#24449;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#12289;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#12289;&#20197;&#21450;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#27169;&#22411;&#22312;&#26410;&#35265;&#31867;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#20197;&#20351;&#26816;&#27979;&#22120;&#22312;&#26410;&#35265;&#31867;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#29305;&#24449;&#37329;&#23383;&#22612;&#21644;&#26816;&#27979;&#22836;&#37096;&#65292;&#36825;&#30772;&#22351;&#20102;&#39044;&#35757;&#32451;&#26399;&#38388;&#24314;&#31435;&#30340;&#35270;&#35273;-&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#19988;&#38590;&#20197;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#24536;&#35760;&#26410;&#35265;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#22686;&#24378;&#25991;&#26412;&#23884;&#20837;&#65292;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#21040;&#35757;&#32451;&#26399;&#38388;&#35265;&#21040;&#30340;&#23569;&#37327;&#31867;&#21035;&#65292;&#24182;&#21516;&#26102;&#33410;&#30465;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#20854;&#27425;&#65292;&#20462;&#25913;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#26816;&#27979;&#22836;&#37096;&#65292;&#21253;&#25324;&#21487;&#35757;&#32451;&#38376;&#25511;&#24555;&#25463;&#26041;&#24335;&#65292;&#36825;&#40723;&#21169;&#35270;&#35273;-&#25991;&#26412;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#30830;&#20445;&#22312;&#26816;&#27979;&#35757;&#32451;&#24320;&#22987;&#26102;&#23454;&#29616;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#33258;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26356;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#25913;&#21892;&#26080;&#20154;&#31867;&#27880;&#37322;&#31867;&#21035;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core problem in zero-shot open vocabulary detection is how to align visual and text features, so that the detector performs well on unseen classes. Previous approaches train the feature pyramid and detection head from scratch, which breaks the vision-text feature alignment established during pretraining, and struggles to prevent the language model from forgetting unseen classes.  We propose three methods to alleviate these issues. Firstly, a simple scheme is used to augment the text embeddings which prevents overfitting to a small number of classes seen during training, while simultaneously saving memory and computation. Secondly, the feature pyramid network and the detection head are modified to include trainable gated shortcuts, which encourages vision-text feature alignment and guarantees it at the start of detection training. Finally, a self-training approach is used to leverage a larger corpus of image-text pairs thus improving detection performance on classes with no human an
&lt;/p&gt;</description></item><item><title>MineRL BASALT 2022&#31454;&#36187;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#31639;&#27861;&#26469;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#35299;&#20915;Minecraft&#20013;&#38590;&#20197;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24110;&#21161;&#25512;&#21160;&#27492;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.13512</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#35299;&#20915;&#27169;&#31946;&#20219;&#21153;&#65306;MineRL BASALT 2022&#31454;&#36187;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition. (arXiv:2303.13512v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13512
&lt;/p&gt;
&lt;p&gt;
MineRL BASALT 2022&#31454;&#36187;&#26088;&#22312;&#20419;&#36827;&#24320;&#21457;&#31639;&#27861;&#26469;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#35299;&#20915;Minecraft&#20013;&#38590;&#20197;&#25351;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#24110;&#21161;&#25512;&#21160;&#27492;&#26041;&#21521;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24494;&#35843;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312;NeurIPS 2022&#20030;&#21150;&#20102;MineRL BASALT&#31454;&#36187;&#65292;&#35813;&#31454;&#36187;&#26088;&#22312;&#23547;&#27714;&#29992;&#20110;&#35299;&#20915;Minecraft&#20013;&#38590;&#20197;&#26126;&#30830;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#30340;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#27492;&#31454;&#36187;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#20419;&#36827;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;&#30340;&#28192;&#36947;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31454;&#36187;&#65292;&#24182;&#27010;&#36848;&#20102;&#39030;&#23574;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31454;&#36187;&#30340;&#24433;&#21709;&#21644;&#26410;&#26469;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate research in the direction of fine-tuning foundation models from human feedback, we held the MineRL BASALT Competition on Fine-Tuning from Human Feedback at NeurIPS 2022. The BASALT challenge asks teams to compete to develop algorithms to solve tasks with hard-to-specify reward functions in Minecraft. Through this competition, we aimed to promote the development of algorithms that use human feedback as channels to learn the desired behavior. We describe the competition and provide an overview of the top solutions. We conclude by discussing the impact of the competition and future directions for improvement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13511</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#35774;&#65306;&#29992;&#20110;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20266;&#24433;&#12289;&#22823;&#37327;&#20869;&#23384;&#38656;&#27714;&#21644;&#32531;&#24930;&#30340;&#39118;&#26684;&#20999;&#25442;&#36895;&#24230;&#12290;&#26412;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#26680;&#24515;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#39068;&#33394;&#26144;&#23556;&#30697;&#38453;&#22312;&#27599;&#20010;&#20687;&#32032;&#19978;&#36827;&#34892;&#19968;&#33268;&#30340;&#25805;&#20316;&#65292;&#36991;&#20813;&#20102;&#20266;&#24433;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#20026;&#39068;&#33394;&#24402;&#19968;&#21270;&#21644;&#39118;&#26684;&#21270;&#20004;&#20010;&#38454;&#27573;&#26469;&#24320;&#21457;&#19968;&#20010;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39068;&#33394;&#39118;&#26684;&#20316;&#20026;&#39044;&#35774;&#25552;&#21462;&#65292;&#24182;&#22312;&#24402;&#19968;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#39118;&#26684;&#20999;&#25442;&#12290;&#30001;&#20110;&#23384;&#22312;&#25104;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#39044;&#35774;&#27169;&#22411;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#31070;&#32463;&#39044;&#35774;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21508;&#31181;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25903;&#25345;&#22810;&#20010;&#39118;&#26684;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
&lt;/p&gt;</description></item><item><title>DreamBooth3D&#26159;&#19968;&#31181;&#21487;&#20174;3-6&#24352;&#22270;&#29255;&#20013;&#29983;&#25104;&#20027;&#20307;&#29305;&#23450;3D&#32032;&#26448;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#32032;&#26448;&#12290;</title><link>http://arxiv.org/abs/2303.13508</link><description>&lt;p&gt;
DreamBooth3D&#65306;&#20027;&#20307;&#39537;&#21160;&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamBooth3D: Subject-Driven Text-to-3D Generation. (arXiv:2303.13508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13508
&lt;/p&gt;
&lt;p&gt;
DreamBooth3D&#26159;&#19968;&#31181;&#21487;&#20174;3-6&#24352;&#22270;&#29255;&#20013;&#29983;&#25104;&#20027;&#20307;&#29305;&#23450;3D&#32032;&#26448;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#19968;&#31181;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#32032;&#26448;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DreamBooth3D&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20174;3-6&#20010;&#38543;&#24847;&#25293;&#25668;&#30340;&#20027;&#20307;&#22270;&#20687;&#20010;&#24615;&#21270;&#29983;&#25104;&#25991;&#26412;&#21040;3D&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;(DreamBooth)&#19982;&#25991;&#26412;&#21040;3D&#29983;&#25104;(DreamFusion)&#30340;&#26368;&#26032;&#36827;&#23637;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#36825;&#20123;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#20027;&#20307;&#29305;&#23450;&#30340;3D&#32032;&#26448;&#65292;&#22240;&#20026;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20250;&#36807;&#24230;&#25311;&#21512;&#20027;&#20307;&#22270;&#20687;&#30340;&#36755;&#20837;&#35270;&#35282;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#20248;&#21270;&#31574;&#30053;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#21516;&#26102;&#21033;&#29992;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;3D&#19968;&#33268;&#24615;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#12289;&#20027;&#20307;&#29305;&#23450;&#30340;3D&#32032;&#26448;&#65292;&#20855;&#26377;&#25991;&#26412;&#39537;&#21160;&#30340;&#20462;&#25913;&#65292;&#22914;&#26032;&#39062;&#30340;&#23039;&#21183;&#12289;&#39068;&#33394;&#21644;&#23646;&#24615;&#65292;&#36825;&#20123;&#20462;&#25913;&#22312;&#20027;&#20307;&#30340;&#20219;&#20309;&#36755;&#20837;&#22270;&#20687;&#20013;&#37117;&#27809;&#26377;&#30475;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;TriPlaneNet&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;EG3D&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#29616;&#26377;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.13497</link><description>&lt;p&gt;
TriPlaneNet&#65306;&#19968;&#31181;EG3D&#21453;&#28436;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
TriPlaneNet: An Encoder for EG3D Inversion. (arXiv:2303.13497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;TriPlaneNet&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;EG3D&#29983;&#25104;&#27169;&#22411;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#65292;&#26088;&#22312;&#24357;&#21512;&#29616;&#26377;&#30340;GAN&#21453;&#28436;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;NeRF&#30340;GAN&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#26032;&#39062;&#30340;&#35270;&#35282;&#28210;&#26579;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#20102;&#33021;&#22815;&#37325;&#26032;&#28210;&#26579;&#25110;&#20462;&#25913;&#29616;&#26377;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#65292;&#24517;&#39035;&#35299;&#20915;&#19968;&#20010;&#21453;&#38382;&#39064;&#12290;&#23613;&#31649;&#23545;&#20110;2D GAN&#21453;&#28436;&#32780;&#35328;&#65292;&#36890;&#29992;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;3D GAN&#21453;&#28436;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;3D&#19968;&#33268;&#30340;&#28210;&#26579;&#12290;&#32780;&#20687;StyleGAN&#36825;&#26679;&#30340;&#24555;&#36895;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21487;&#33021;&#20063;&#19981;&#22826;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36523;&#20221;&#20445;&#30041;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#21033;&#29992;&#20026;EG3D&#29983;&#25104;&#27169;&#22411;&#24341;&#20837;&#30340;&#19977;&#24179;&#38754;&#34920;&#31034;&#65292;&#24357;&#21512;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#22312;&#19968;&#20010;&#29992;&#20110;&#28508;&#22312;&#32534;&#30721;&#30340;&#21069;&#39304;&#21367;&#31215;&#32534;&#30721;&#22120;&#19978;&#65292;&#24182;&#25193;&#23637;&#20102;&#19968;&#20010;&#23436;&#20840;&#21367;&#31215;&#30340;&#19977;&#24179;&#38754;&#25968;&#20540;&#20559;&#31227;&#39044;&#27979;&#22120;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#24037;&#20316;&#25152;&#26174;&#31034;&#30340;&#37027;&#26679;&#65292;&#28210;&#26579;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in NeRF-based GANs has introduced a number of approaches for high-resolution and high-fidelity generative modeling of human heads with a possibility for novel view rendering. At the same time, one must solve an inverse problem to be able to re-render or modify an existing image or video. Despite the success of universal optimization-based methods for 2D GAN inversion, those, applied to 3D GANs, may fail to produce 3D-consistent renderings. Fast encoder-based techniques, such as those developed for StyleGAN, may also be less appealing due to the lack of identity preservation. In our work, we introduce a real-time method that bridges the gap between the two approaches by directly utilizing the tri-plane representation introduced for EG3D generative model. In particular, we build upon a feed-forward convolutional encoder for the latent code and extend it with a fully-convolutional predictor of tri-plane numerical offsets. As shown in our work, the renderings are similar in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13496</link><description>&lt;p&gt;
MAE&#39044;&#21069;&#32622;&#35757;&#32451;&#23545;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#39044;&#21069;&#32622;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20159;&#32423;&#39044;&#35757;&#32451;&#35268;&#27169;&#65292;&#24182;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#30340;&#26631;&#20934;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#20351;&#29992;&#25968;&#21313;&#20159;&#24352;&#22270;&#20687;&#30340;&#22823;&#35268;&#27169;&#65288;&#24369;&#65289;&#30417;&#30563;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;MAE&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;&#34429;&#28982;MAE&#25216;&#26415;&#20165;&#34987;&#35777;&#26126;&#33021;&#22815;&#19982;&#27169;&#22411;&#22823;&#23567;&#30456;&#32553;&#25918;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20063;&#21487;&#20197;&#38543;&#25968;&#25454;&#38598;&#22823;&#23567;&#32553;&#25918;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22522;&#20110;MAE&#30340;&#39044;&#21069;&#32622;&#35757;&#32451;&#21487;&#21516;&#26102;&#36866;&#29992;&#20110;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#12290;&#39044;&#21069;&#32622;&#35757;&#32451;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#35268;&#27169;&#65288;&#21442;&#25968;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65288;&#22270;&#20687;&#25968;&#30334;&#19975;&#21040;&#25968;&#21313;&#20159;&#65289;&#19978;&#19968;&#33268;&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#24615;&#21644;&#19979;&#28216;&#36716;&#31227;&#24615;&#33021;&#65292;&#19988;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#20854;&#22312;10&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#35270;&#39057;&#35782;&#21035;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.13494</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#65281;&#65288;&#19981;&#65289;&#19987;&#27880;&#20195;&#29702;&#20154;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attention! Dynamic Epistemic Logic Models of (In)attentive Agents. (arXiv:2303.13494v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#38480;&#21046;&#21644;&#36873;&#25321;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#30340;&#20851;&#38190;&#35748;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#35748;&#30693;&#36923;&#36753;&#65288;DEL&#65289;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#35201;&#20040;&#23436;&#20840;&#19987;&#27880;&#65292;&#35201;&#20040;&#23436;&#20840;&#19981;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#21270;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#20195;&#29702;&#20154;&#20851;&#27880;&#19968;&#20123;&#21407;&#23376;&#20844;&#24335;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#21629;&#39064;&#27880;&#24847;&#21147;&#30340;&#30456;&#24212;&#36923;&#36753;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20844;&#29702;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#26694;&#26550;&#65292;&#20197;&#35299;&#37322;&#26080;&#27880;&#24847;&#30606;&#35270;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the crucial cognitive ability that limits and selects what information we observe. Previous work by Bolander et al. (2016) proposes a model of attention based on dynamic epistemic logic (DEL) where agents are either fully attentive or not attentive at all. While introducing the realistic feature that inattentive agents believe nothing happens, the model does not represent the most essential aspect of attention: its selectivity. Here, we propose a generalization that allows for paying attention to subsets of atomic formulas. We introduce the corresponding logic for propositional attention, and show its axiomatization to be sound and complete. We then extend the framework to account for inattentive agents that, instead of assuming nothing happens, may default to a specific truth-value of what they failed to attend to (a sort of prior concerning the unattended atoms). This feature allows for a more cognitively plausible representation of the inattentional blindness phenomenon
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.13489</link><description>&lt;p&gt;
&#20351;&#29992;&#31034;&#33539;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#19982;&#35268;&#21010;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13489
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#20943;&#23569;&#35797;&#38169;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31034;&#33539;&#65292;&#26412;&#25991;&#32508;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#31034;&#33539;&#26469;&#20419;&#36827;&#23398;&#20064;&#20915;&#31574;&#27169;&#22411;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#35797;&#38169;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20351;&#29992;&#31034;&#33539;&#21487;&#20197;&#35753;&#26234;&#33021;&#20307;&#21463;&#30410;&#20110;&#19987;&#23478;&#30340;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25506;&#32034;&#26368;&#20339;&#34892;&#21160;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#20351;&#29992;&#31034;&#33539;&#30340;&#20248;&#28857;&#65292;&#20197;&#21450;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20915;&#31574;&#21046;&#23450;&#33539;&#24335;&#65288;&#20363;&#22914;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#22312;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#22914;&#20309;&#24212;&#29992;&#31034;&#33539;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#25910;&#38598;&#31034;&#33539;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20030;&#20102;&#19968;&#20010;&#23454;&#38469;&#30340;&#31034;&#33539;&#29983;&#25104;&#21644;&#21033;&#29992;&#31649;&#36947;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;ManiSkill&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#22522;&#30784;&#30340;&#19977;&#32500;&#29289;&#20307;&#21450;&#20851;&#31995;&#30340;&#26694;&#26550;NS3D&#65292;&#21487;&#20197;&#26377;&#25928;&#30340;&#25512;&#29702;&#20986;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#39046;&#20808;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13483</link><description>&lt;p&gt;
NS3D: &#19977;&#32500;&#29289;&#20307;&#21450;&#20851;&#31995;&#30340;&#31070;&#32463;&#31526;&#21495;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations. (arXiv:2303.13483v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#22522;&#30784;&#30340;&#19977;&#32500;&#29289;&#20307;&#21450;&#20851;&#31995;&#30340;&#26694;&#26550;NS3D&#65292;&#21487;&#20197;&#26377;&#25928;&#30340;&#25512;&#29702;&#20986;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#35821;&#20041;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#39046;&#20808;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#22914;&#35270;&#35273;&#23545;&#35805;&#21644;&#20855;&#20307;&#25805;&#20316;&#65292;&#23558;&#19977;&#32500;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#23646;&#24615;&#21644;&#20851;&#31995;&#22522;&#30784;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#39046;&#22495;&#30340;&#21464;&#21270;&#24615;&#23548;&#33268;&#20102;&#20004;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65306;1&#65289;&#26631;&#27880;&#25104;&#26412;&#26114;&#36149;&#65307;2&#65289;&#22797;&#26434;&#30340;&#19977;&#32500;&#22522;&#30784;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#22522;&#30784;&#35201;&#27714;&#26159;&#25968;&#25454;&#39640;&#25928;&#12289;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20219;&#21153;&#20197;&#21450;&#20855;&#26377;&#30475;&#19981;&#21040;&#35821;&#20041;&#24418;&#24335;&#30340;&#22522;&#30784;&#22797;&#26434;&#35821;&#20041;&#65288;&#20363;&#22914;&#65292;&#35270;&#28857;&#38170;&#23450;&#21644;&#22810;&#23545;&#35937;&#24341;&#29992;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NS3D&#65292;&#19968;&#31181;&#19977;&#32500;&#22522;&#30784;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#12290;NS3D&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#27169;&#22411;&#23558;&#35821;&#35328;&#36716;&#21270;&#20026;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#31243;&#24207;&#12290;&#31243;&#24207;&#20013;&#30340;&#19981;&#21516;&#21151;&#33021;&#27169;&#22359;&#34987;&#23454;&#29616;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;NS3D&#36890;&#36807;&#24341;&#20837;&#33021;&#22815;&#26377;&#25928;&#25512;&#29702;&#20986;&#35270;&#28857;&#38170;&#23450;&#12289;&#22810;&#23545;&#35937;&#24341;&#29992;&#21644;&#20854;&#20182;&#19977;&#32500;&#22330;&#26223;&#20013;&#22797;&#26434;&#35821;&#20041;&#30340;&#21151;&#33021;&#27169;&#22359;&#65292;&#25193;&#23637;&#20102;&#20197;&#21069;&#30340;&#31070;&#32463;&#31526;&#21495;&#35270;&#35273;&#25512;&#29702;&#26041;&#27861;&#12290;&#24403;NS3D&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NS3D&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26368;&#20808;&#36827;&#25110;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding object properties and relations in 3D scenes is a prerequisite for a wide range of artificial intelligence tasks, such as visually grounded dialogues and embodied manipulation. However, the variability of the 3D domain induces two fundamental challenges: 1) the expense of labeling and 2) the complexity of 3D grounded language. Hence, essential desiderata for models are to be data-efficient, generalize to different data distributions and tasks with unseen semantic forms, as well as ground complex language semantics (e.g., view-point anchoring and multi-object reference). To address these challenges, we propose NS3D, a neuro-symbolic framework for 3D grounding. NS3D translates language into programs with hierarchical structures by leveraging large language-to-code models. Different functional modules in the programs are implemented as neural networks. Notably, NS3D extends prior neuro-symbolic visual reasoning methods by introducing functional modules that effectively reason ab
&lt;/p&gt;</description></item><item><title>TactoFind&#26159;&#19968;&#20010;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25163;&#25351;&#19978;&#30340;&#35302;&#25511;&#20256;&#24863;&#22120;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2303.13482</link><description>&lt;p&gt;
TactoFind&#65306;&#19968;&#31181;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TactoFind: A Tactile Only System for Object Retrieval. (arXiv:2303.13482v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13482
&lt;/p&gt;
&lt;p&gt;
TactoFind&#26159;&#19968;&#20010;&#32431;&#35302;&#35273;&#29289;&#21697;&#33719;&#21462;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#25163;&#25351;&#19978;&#30340;&#35302;&#25511;&#20256;&#24863;&#22120;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35270;&#35273;&#24863;&#30693;&#12289;&#26410;&#30693;&#29289;&#20307;&#24418;&#29366;&#20197;&#21450;&#29289;&#20307;&#21487;&#20197;&#33258;&#30001;&#31227;&#21160;&#30340;&#22330;&#26223;&#19979;&#36827;&#34892;&#29289;&#21697;&#26816;&#32034;&#30340;&#38382;&#39064;&#12290;&#25104;&#21151;&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#23450;&#20301;&#33258;&#30001;&#29289;&#20307;&#12289;&#35782;&#21035;&#29305;&#23450;&#30340;&#29289;&#20307;&#23454;&#20363;&#24182;&#20351;&#29992;&#35302;&#35273;&#21453;&#39304;&#26469;&#25235;&#21462;&#24050;&#35782;&#21035;&#30340;&#29289;&#20307;&#12290;&#19982;&#25668;&#24433;&#26426;&#21487;&#35266;&#23519;&#25972;&#20010;&#22330;&#26223;&#30340;&#35270;&#35273;&#19981;&#21516;&#65292;&#35302;&#35273;&#20256;&#24863;&#22120;&#26159;&#23616;&#37096;&#30340;&#65292;&#24182;&#19988;&#20165;&#35266;&#23519;&#19982;&#25805;&#32437;&#22120;&#25509;&#35302;&#30340;&#22330;&#26223;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35302;&#35273;&#20256;&#24863;&#22120;&#25910;&#38598;&#20449;&#24687;&#38656;&#35201;&#22312;&#35302;&#25720;&#34920;&#38754;&#26045;&#21152;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#25200;&#20081;&#22330;&#26223;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#35302;&#25720;&#24863;&#30693;&#38656;&#35201;&#36890;&#36807;&#26102;&#38388;&#19978;&#30340;&#31934;&#32454;&#25506;&#32034;&#21644;&#20449;&#24687;&#38598;&#25104;&#26469;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#21033;&#29992;&#25163;&#25351;&#35302;&#25720;&#20256;&#24863;&#22120;&#19978;&#30340;&#31232;&#30095;&#35302;&#35273;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#35270;&#35273;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#23450;&#20301;&#12289;&#35782;&#21035;&#21644;&#25235;&#21462;&#26032;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of object retrieval in scenarios where visual sensing is absent, object shapes are unknown beforehand and objects can move freely, like grabbing objects out of a drawer. Successful solutions require localizing free objects, identifying specific object instances, and then grasping the identified objects, only using touch feedback. Unlike vision, where cameras can observe the entire scene, touch sensors are local and only observe parts of the scene that are in contact with the manipulator. Moreover, information gathering via touch sensors necessitates applying forces on the touched surface which may disturb the scene itself. Reasoning with touch, therefore, requires careful exploration and integration of information over time -- a challenge we tackle. We present a system capable of using sparse tactile feedback from fingertip touch sensors on a dexterous hand to localize, identify and grasp novel objects without any visual feedback. Videos are available at https://ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.13472</link><description>&lt;p&gt;
&#24149;&#21518;&#21046;&#20316;&#65306;&#38754;&#21521;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Plotting Behind the Scenes: Towards Learnable Game Engines. (arXiv:2303.13472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36825;&#20010;&#27169;&#22411;&#34987;&#31216;&#20026;&#21487;&#23398;&#20064;&#28216;&#25103;&#24341;&#25806;(LGE)&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#65292;&#24182;&#19988;&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#25511;&#21046;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#24341;&#25806;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#24320;&#21457;&#25104;&#26412;&#20063;&#26159;&#21313;&#20998;&#24040;&#22823;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#21333;&#30524;&#27880;&#37322;&#35270;&#39057;&#20013;&#35757;&#32451;&#20986;&#31867;&#20284;&#28216;&#25103;&#24341;&#25806;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#35813;&#32467;&#26524;&#34987;&#31216;&#20026;Learnable Game Engine (LGE)&#65292;&#21487;&#20197;&#32500;&#25252;&#22330;&#26223;&#12289;&#29289;&#20307;&#21644;&#20854;&#20013;&#30340;&#20195;&#29702;&#29366;&#24577;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#21487;&#25511;&#21046;&#30340;&#35270;&#35282;&#28210;&#26579;&#29615;&#22659;&#12290;&#31867;&#20284;&#20110;&#28216;&#25103;&#24341;&#25806;&#65292;&#23427;&#27169;&#25311;&#20102;&#28216;&#25103;&#30340;&#36923;&#36753;&#21644;&#24213;&#23618;&#29289;&#29702;&#35268;&#21017;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25351;&#23450;&#39640;&#32423;&#21644;&#20302;&#32423;&#25805;&#20316;&#24207;&#21015;&#26469;&#29609;&#28216;&#25103;&#12290;&#26368;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;LGE&#35299;&#38145;&#20102;&#23548;&#28436;&#27169;&#24335;&#65292;&#29992;&#25143;&#36890;&#36807;&#26631;&#27880;&#39640;&#23618;&#27425;&#30340;&#21160;&#20316;&#21644;&#30446;&#26631;&#26469;&#25511;&#21046;&#20195;&#29702;&#12290;&#36825;&#35201;&#27714;&#23398;&#20064;&#8220;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#8221;&#65292;&#30001;&#25105;&#20204;&#30340;&#21160;&#30011;&#27169;&#22411;&#23553;&#35013;&#65292;&#20197;&#20351;&#29992;&#39640;&#32423;&#32422;&#26463;&#26465;&#20214;&#23548;&#33322;&#22330;&#26223;&#12289;&#19982;&#23545;&#25163;&#23545;&#25112;&#65292;&#35774;&#35745;&#36194;&#24471;&#28216;&#25103;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game engines are powerful tools in computer graphics. Their power comes at the immense cost of their development. In this work, we present a framework to train game-engine-like neural models, solely from monocular annotated videos. The result-a Learnable Game Engine (LGE)-maintains states of the scene, objects and agents in it, and enables rendering the environment from a controllable viewpoint. Similarly to a game engine, it models the logic of the game and the underlying rules of physics, to make it possible for a user to play the game by specifying both high- and low-level action sequences. Most captivatingly, our LGE unlocks the director's mode, where the game is played by plotting behind the scenes, specifying high-level actions and goals for the agents in the form of language and desired states. This requires learning "game AI", encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, devise the strategy to win a point. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.13466</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#65306;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Extracting Physical Rehabilitation Exercise Information from Clinical Notes: a Comparison of Rule-Based and Machine Learning Natural Language Processing Techniques. (arXiv:2303.13466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21330;&#20013;&#24739;&#32773;&#27835;&#30103;&#36807;&#31243;&#30340;&#38203;&#28860;&#20449;&#24687;&#65292;&#24182;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#36825;&#20123;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#22797;&#38203;&#28860;&#22312;&#21330;&#20013;&#21518;&#24739;&#32773;&#30340;&#24247;&#22797;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#21487;&#20197;&#20351;&#24247;&#22797;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#22312;&#39044;&#27979;&#24314;&#27169;&#20026;&#24739;&#32773;&#20998;&#37197;&#27835;&#30103;&#35745;&#21010;&#20043;&#21069;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#24247;&#22797;&#38203;&#28860;&#20449;&#24687;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#26469;&#27880;&#37322;&#21330;&#20013;&#24739;&#32773;&#30340;&#27835;&#30103;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#19982;&#20960;&#20010;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25552;&#21462;&#19968;&#21322;&#30340;&#27010;&#24565;&#26041;&#38754;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#19988;&#27599;&#20010;&#27010;&#24565;&#30340;&#20010;&#20307;&#36816;&#21160;&#25551;&#36848;&#21487;&#20197;&#20998;&#37197;&#20108;&#36827;&#21046;&#26631;&#31614;&#65292;&#24182;&#19988;F&#20540;&#19981;&#20302;&#20110;0.75&#12290;&#22312;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#37096;&#32626;&#21040;&#26080;&#26631;&#31614;&#25991;&#26723;&#20043;&#21069;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#23450;&#21046;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#20934;&#30830;&#25552;&#21462;&#20020;&#24202;&#31508;&#35760;&#20013;&#24247;&#22797;&#27835;&#30103;&#38203;&#28860;&#20449;&#24687;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical rehabilitation plays a crucial role in the recovery process of post-stroke patients. By personalizing therapies for patients leveraging predictive modeling and electronic health records (EHRs), healthcare providers can make the rehabilitation process more efficient. Before predictive modeling can provide decision support for the assignment of treatment plans, automated methods are necessary to extract physical rehabilitation exercise information from unstructured EHRs. We introduce a rule-based natural language processing algorithm to annotate therapeutic procedures for stroke patients and compare it to several small machine learning models. We find that our algorithm outperforms these models in extracting half of the concepts where sufficient data is available, and individual exercise descriptions can be assigned binary labels with an f-score of no less than 0.75 per concept. More research needs to be done before these algorithms can be deployed on unlabeled documents, but cu
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13397</link><description>&lt;p&gt;
DDT&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#20449;&#24687;&#65292;&#20363;&#22914;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#34394;&#25311;&#29616;&#23454;&#12290;&#19982;&#21333;&#19968;&#22270;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#36890;&#36807;&#34701;&#21512;&#20154;&#20307;&#36816;&#21160;&#20808;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687; VIBE &#36825;&#26679;&#30340;&#22810;&#23545;&#22810;&#26041;&#27861;&#23384;&#22312;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#12290;&#32780;&#20687; TCMR &#21644; MPS-Net &#36825;&#26679;&#30340;&#22810;&#23545;&#19968;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#26410;&#26469;&#24103;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#38750;&#22240;&#26524;&#21644;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#12290;DDT &#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#22810;&#23545;&#22810;&#26041;&#27861;&#65292;DDT &#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;M4M&#31639;&#27861;&#24182;&#25552;&#20986;&#20102;E-M4M&#31639;&#27861;&#65292;&#22312;&#21463;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#25490;&#21015;&#22810;&#20010;&#29289;&#20307;&#65292;&#24182;&#19988;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#30340;&#25512;&#21160;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#20110;M4M&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13385</link><description>&lt;p&gt;
&#21487;&#31227;&#21160;&#29289;&#20307;&#25805;&#32437;&#35268;&#21010;&#65306;&#22914;&#20309;&#20915;&#23450;&#21738;&#20123;&#29289;&#20307;&#34987;&#25918;&#32622;&#22312;&#21738;&#37324;&#12289;&#25353;&#20160;&#20040;&#39034;&#24207;&#25918;&#32622;&#20197;&#21450;&#22914;&#20309;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Planning for Manipulation among Movable Objects: Deciding Which Objects Go Where, in What Order, and How. (arXiv:2303.13385v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;M4M&#31639;&#27861;&#24182;&#25552;&#20986;&#20102;E-M4M&#31639;&#27861;&#65292;&#22312;&#21463;&#32422;&#26463;&#30340;&#29615;&#22659;&#20013;&#25490;&#21015;&#22810;&#20010;&#29289;&#20307;&#65292;&#24182;&#19988;&#33021;&#22815;&#30830;&#23450;&#21487;&#34892;&#30340;&#25512;&#21160;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#20110;M4M&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20805;&#28385;&#38556;&#30861;&#29289;&#19988;&#31354;&#38388;&#29421;&#23567;&#30340;&#19977;&#32500;&#24037;&#20316;&#21306;&#22495;&#20013;&#65292;&#26426;&#22120;&#20154;&#22914;&#20309;&#36827;&#34892;&#25195;&#25551;-&#25235;&#21462;-&#25644;&#36816;&#39118;&#26684;&#30340;&#29289;&#21697;&#25805;&#32437;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;M4M&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#30340;&#25277;&#35937;&#65292;&#30830;&#23450;&#20102;&#38656;&#35201;&#31227;&#21160;&#30340;&#29289;&#20307;&#21644;&#23427;&#20204;&#30340;&#30446;&#30340;&#22320;&#65292;&#24182;&#37319;&#29992;&#38750;&#39044;&#25235;&#21462;&#25512;&#24335;&#35268;&#21010;&#22120;&#26469;&#35745;&#31639;&#26426;&#22120;&#20154;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#37325;&#26032;&#25490;&#21015;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21018;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#26816;&#26597;&#34892;&#21160;&#26159;&#21542;&#31526;&#21512;&#32534;&#30721;&#22312;&#38382;&#39064;&#20013;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;M4M&#31639;&#27861;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#36138;&#24515;&#22320;&#36873;&#25321;&#26377;&#25928;&#30340;&#25512;&#21160;&#25805;&#20316;&#65292;&#24182;&#19988;&#22914;&#26524;&#38656;&#35201;&#37325;&#26032;&#25490;&#21015;&#22810;&#20010;&#29289;&#20307;&#65292;&#21017;&#19981;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;M4M&#31639;&#27861;&#20063;&#26080;&#27861;&#32771;&#34385;&#23548;&#33268;&#19981;&#21516;&#29289;&#21697;&#37325;&#26032;&#25490;&#21015;&#21644;&#25512;&#21160;&#30340;&#20854;&#20182;MAPF&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;M4M&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Enhanced-M4M (E-M4M)&#31639;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#22270;&#25628;&#32034;&#30340;&#27714;&#35299;&#22120;&#65292;&#21487;&#25628;&#32034;&#25512;&#21160;&#25805;&#20316;&#30340;&#39034;&#24207;&#21644;&#19981;&#21516;&#30340;MAPF&#35299;&#20915;&#26041;&#26696;&#12290;E-M4M&#31639;&#27861;&#33021;&#22815;&#30830;&#23450;&#22312;&#21463;&#32422;&#26463;&#30340;&#31354;&#38388;&#20013;&#25490;&#21015;&#22810;&#20010;&#29289;&#20307;&#30340;&#21487;&#34892;&#25512;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#20855;&#26377;&#38543;&#26426;&#29289;&#20307;&#37197;&#32622;&#30340;&#27169;&#25311;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35268;&#21010;&#22120;&#65292;&#24182;&#26174;&#31034;E-M4M&#31639;&#27861;&#22312;&#25104;&#21151;&#29575;&#21644;&#35268;&#21010;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;M4M&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are interested in pick-and-place style robot manipulation tasks in cluttered and confined 3D workspaces among movable objects that may be rearranged by the robot and may slide, tilt, lean or topple. A recently proposed algorithm, M4M, determines which objects need to be moved and where by solving a Multi-Agent Pathfinding MAPF abstraction of this problem. It then utilises a nonprehensile push planner to compute actions for how the robot might realise these rearrangements and a rigid body physics simulator to check whether the actions satisfy physics constraints encoded in the problem. However, M4M greedily commits to valid pushes found during planning, and does not reason about orderings over pushes if multiple objects need to be rearranged. Furthermore, M4M does not reason about other possible MAPF solutions that lead to different rearrangements and pushes. In this paper, we extend M4M and present Enhanced-M4M (E-M4M) -- a systematic graph search-based solver that searches over ord
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.13379</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13379
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#25945;&#32946;&#25216;&#26415;&#21019;&#26032;&#26174;&#31034;&#20986;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#25945;&#32946;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#38382;&#39064;&#12289;&#25552;&#20379;&#21453;&#39304;&#21644;&#35780;&#20998;&#65289;&#65292;&#20294;&#23545;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#20123;&#25285;&#24551;&#21487;&#33021;&#20250;&#38459;&#30861;&#26410;&#26469;&#30740;&#31350;&#21644;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#21019;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;118&#31687;&#33258;2017&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#21516;&#34892;&#35780;&#35758;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;LLMs&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#25945;&#32946;&#20219;&#21153;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#31995;&#32479;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#65292;&#36824;&#30830;&#23450;&#20102;LLMs&#21019;&#26032;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13375</link><description>&lt;p&gt;
GPT-4&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Capabilities of GPT-4 on Medical Challenge Problems. (arXiv:2303.13375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#21161;&#20110;&#21307;&#23398;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#21307;&#23398;&#12290;&#25105;&#20204;&#23545;&#19968;&#39033;&#26368;&#20808;&#36827;&#30340;LLM&#8212;&#8212;GPT-4&#22312;&#21307;&#23398;&#33021;&#21147;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;GPT-4&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#27809;&#26377;&#32463;&#36807;&#38024;&#23545;&#21307;&#23398;&#38382;&#39064;&#30340;&#35757;&#32451;&#25110;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#32654;&#22269;&#20020;&#24202;&#33021;&#21147;&#35780;&#20272;&#21644;&#25480;&#26435;&#32771;&#26680;&#35745;&#21010;&#65288;USMLE&#65289;&#30340;&#20004;&#32452;&#23448;&#26041;&#32451;&#20064;&#26448;&#26009;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22312;MultiMedQA&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#38500;&#20102;&#27979;&#37327;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#27979;&#35797;&#38382;&#39064;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25506;&#32034;&#35757;&#32451;&#26399;&#38388;&#20869;&#23481;&#35760;&#24518;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#30740;&#31350;&#27010;&#29575;&#26657;&#20934;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation across various domains, including medicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art LLM, on medical competency examinations and benchmark datasets. GPT-4 is a general-purpose model that is not specialized for medical problems through training or engineered to solve clinical tasks. Our analysis covers two sets of official practice materials for the USMLE, a three-step examination program used to assess clinical competency and grant licensure in the United States. We also evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond measuring model performance, experiments were conducted to investigate the influence of test questions containing both text and images on model performance, probe for memorization of content during training, and study probability calibration, which is of critical importance in high-stakes applications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;ClimaText&#25968;&#25454;&#38598;&#65292;&#24494;&#35843;ClimateBert transformer&#21644;BERT&#27169;&#22411;&#65292;&#25104;&#21151;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#21477;&#23376;&#65292;&#20026;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#21644;&#25237;&#36164;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#26395;&#25903;&#25345;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.13373</link><description>&lt;p&gt;
&#29992;ClimaText&#24494;&#35843;ClimateBERT transformer&#35299;&#26512;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#25259;&#38706;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning ClimateBert transformer with ClimaText for the disclosure analysis of climate-related financial risks. (arXiv:2303.13373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;ClimaText&#25968;&#25454;&#38598;&#65292;&#24494;&#35843;ClimateBert transformer&#21644;BERT&#27169;&#22411;&#65292;&#25104;&#21151;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#21477;&#23376;&#65292;&#20026;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#21644;&#25237;&#36164;&#32773;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#26395;&#25903;&#25345;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37329;&#34701;&#26426;&#26500;&#65292;&#29305;&#21035;&#26159;&#20010;&#20154;&#21644;&#26426;&#26500;&#25237;&#36164;&#32773;&#65292;&#23545;&#20844;&#21496;&#25253;&#21578;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#20026;&#20102;&#35782;&#21035;&#36825;&#20123;&#31867;&#22411;&#30340;&#39118;&#38505;&#65292;&#20844;&#21496;&#21487;&#20197;&#22312;&#36130;&#21153;&#21644;&#38750;&#36130;&#21153;&#25253;&#21578;&#20013;&#30701;&#26399;&#20869;&#25259;&#38706;&#22823;&#37327;&#25991;&#26412;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#21709;&#24212;&#19981;&#26029;&#36890;&#36807;&#30340;&#35268;&#23450;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#23454;&#29616;&#27668;&#20505;&#21464;&#21270;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#24494;&#35843;&#20004;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;BERT&#21644;ClimateBert&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;DistillRoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#20026;&#27668;&#20505;&#25991;&#26412;&#20998;&#31867;&#37327;&#36523;&#23450;&#21046;&#12290;&#36825;&#20004;&#20010;&#31639;&#27861;&#22522;&#20110;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#33021;&#22815;&#23398;&#20064;&#25991;&#26412;&#20013;&#21333;&#35789;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;ClimaText&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#36229;&#36807;2000&#23478;&#20844;&#21496;&#25259;&#38706;&#20854;&#27668;&#20505;&#30456;&#20851;&#39118;&#38505;&#30340;&#25253;&#21578;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#35782;&#21035;ClimaText&#20013;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#21477;&#23376;&#26041;&#38754;&#20248;&#20110;&#21407;&#22987;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#26377;&#28508;&#21147;&#25903;&#25345;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#12289;&#25237;&#36164;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#26356;&#20934;&#30830;&#12289;&#19968;&#33268;&#30340;&#20851;&#20110;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#39118;&#38505;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#19968;&#20010;&#26356;&#21487;&#25345;&#32493;&#30340;&#37329;&#34701;&#31995;&#32479;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there has been a growing demand from financial agents, especially from particular and institutional investors, for companies to report on climate-related financial risks. A vast amount of information, in text format, can be expected to be disclosed in the short term by firms in order to identify these types of risks in their financial and non financial reports, particularly in response to the growing regulation that is being passed on the matter. To this end, this paper applies state-of-the-art NLP techniques to achieve the detection of climate change in text corpora. We use transfer learning to fine-tune two transformer models, BERT and ClimateBert -a recently published DistillRoBERTa-based model that has been specifically tailored for climate text classification-. These two algorithms are based on the transformer architecture which enables learning the contextual relationships between words in a text. We carry out the fine-tuning process of both models on the novel Cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13365</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#38656;&#27714;&#35268;&#33539;&#21270;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#26041;&#27861;&#30340;&#25913;&#36827;&#21560;&#24341;&#20102;&#24320;&#21457;&#20154;&#21592;&#22312;&#38656;&#27714;&#24037;&#31243;&#39046;&#22495;&#33258;&#21160;&#21270;&#38656;&#27714;&#35268;&#33539;&#21270;&#65288;RF&#65289;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#25253;&#21578;&#20102;&#24212;&#29992;NLP&#21644;ML&#22312;&#20943;&#23569;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#21644;&#20998;&#31867;&#29616;&#26377;&#30340;NLP&#21644;ML&#22312;RF&#19978;&#30340;&#24037;&#20316;&#65292;&#35782;&#21035;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#24182;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#36873;&#21462;&#20102;&#26469;&#33258;&#24120;&#29992;&#24211;&#30340;257&#31687;&#35770;&#25991;&#12290;&#36890;&#36807;&#23450;&#20041;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#26469;&#36807;&#28388;&#25628;&#32034;&#32467;&#26524;&#65292;&#24182;&#36873;&#25321;&#20102;47&#39033;&#30456;&#20851;&#30740;&#31350;&#65292;&#26102;&#38388;&#36328;&#24230;&#22312;2012&#24180;&#33267;2022&#24180;&#20043;&#38388;&#12290;&#25105;&#20204;&#21457;&#29616;&#21551;&#21457;&#24335;NLP&#26041;&#27861;&#26159;&#33258;&#21160;RF&#20013;&#26368;&#24120;&#29992;&#30340;NLP&#25216;&#26415;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21017;&#21253;&#25324;&#22522;&#20110;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;NLP&#21644;ML&#25216;&#26415;&#22312;RF&#39046;&#22495;&#20013;&#30340;&#36739;&#22823;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22312;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
&lt;/p&gt;</description></item><item><title>POTTER&#26159;&#19968;&#31181;&#38024;&#23545;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#20219;&#21153;&#30340;&#32431;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#26377;&#25928;&#30340;&#27744;&#21270;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#38598;&#25104;HR&#25968;&#25454;&#27969;&#21487;&#20197;&#29992;&#20110;&#24674;&#22797;&#26356;&#31934;&#30830;&#30340;&#20154;&#20307;&#32593;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.13357</link><description>&lt;p&gt;
POTTER: &#22522;&#20110;&#27744;&#21270;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#39640;&#25928;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery. (arXiv:2303.13357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13357
&lt;/p&gt;
&lt;p&gt;
POTTER&#26159;&#19968;&#31181;&#38024;&#23545;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#20219;&#21153;&#30340;&#32431;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#20854;&#26377;&#25928;&#30340;&#27744;&#21270;&#27880;&#24847;&#21147;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#38598;&#25104;HR&#25968;&#25454;&#27969;&#21487;&#20197;&#29992;&#20110;&#24674;&#22797;&#26356;&#31934;&#30830;&#30340;&#20154;&#20307;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#30446;&#22270;&#20687;&#20013;&#65292;&#21464;&#25442;&#22120;&#26550;&#26500;&#24050;&#32463;&#22312;&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#19978;&#21462;&#24471;SOTA&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#30340;&#25552;&#21319;&#26159;&#20197;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#20026;&#20195;&#20215;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#38469;&#24212;&#29992;&#20013;&#31934;&#30830;&#30340;&#20154;&#20307;&#32593;&#26684;&#37325;&#24314;&#65292;&#38656;&#35201;&#19968;&#31181;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32431;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#21517;&#20026;POTTER&#65288;&#27744;&#21270;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65289;&#65292;&#29992;&#20110;&#21333;&#22270;&#20687;HMR&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27744;&#21270;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#25968;&#25454;&#27969;&#36827;&#34892;HMR&#20219;&#21153;&#12290;&#26469;&#33258;HR&#27969;&#30340;&#39640;&#20998;&#36776;&#29575;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#24674;&#22797;&#26356;&#31934;&#30830;&#30340;&#20154;&#20307;&#32593;&#26684;&#12290;&#25105;&#20204;&#30340;POTTER&#20165;&#38656;&#35201;7&#20010;&#33521;&#20255;&#36798;&#26174;&#21345;&#65292;&#36229;&#36234;&#20102;SOTA&#26041;&#27861;METRO&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures have achieved SOTA performance on the human mesh recovery (HMR) from monocular images. However, the performance gain has come at the cost of substantial memory and computational overhead. A lightweight and efficient model to reconstruct accurate human mesh is needed for real-world applications. In this paper, we propose a pure transformer architecture named POoling aTtention TransformER (POTTER) for the HMR task from single images. Observing that the conventional attention module is memory and computationally expensive, we propose an efficient pooling attention module, which significantly reduces the memory and computational cost without sacrificing performance. Furthermore, we design a new transformer architecture by integrating a High-Resolution (HR) stream for the HMR task. The high-resolution local and global features from the HR stream can be utilized for recovering more accurate human mesh. Our POTTER outperforms the SOTA method METRO by only requiring 7
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#21516;&#26102;&#36824;&#21457;&#29616;&#29616;&#26377;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#23384;&#22312;&#20154;&#24037;&#38382;&#39064;&#65292;&#38656;&#36843;&#20999;&#23547;&#27714;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#35780;&#20272;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.13355</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension. (arXiv:2303.13355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#24182;&#25552;&#20986;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#21516;&#26102;&#36824;&#21457;&#29616;&#29616;&#26377;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#23384;&#22312;&#20154;&#24037;&#38382;&#39064;&#65292;&#38656;&#36843;&#20999;&#23547;&#27714;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#35780;&#20272;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#35821;&#35328;&#33021;&#21147;&#21463;&#21040;&#20102;&#24456;&#22823;&#38480;&#21046;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#20173;&#28982;&#19981;&#24471;&#19981;&#20381;&#38752;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#24320;&#21457;&#36234;&#21335;&#25991;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#12290;&#36825;&#31181;&#30740;&#31350;&#22256;&#38590;&#26159;&#30001;&#20110;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#30340;&#39640;&#36136;&#37327;&#20316;&#21697;&#25968;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#40723;&#21169;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#36827;&#34892;&#26356;&#22810;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#23545;&#24403;&#21069;&#36234;&#21335;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#24369;&#28857;&#21644;&#20248;&#21183;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#20174;&#20998;&#26512;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#23637;&#36234;&#21335;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#21521;&#12290;&#38500;&#20102;&#36825;&#20010;&#20027;&#35201;&#36129;&#29486;&#22806;&#65292;&#25105;&#20204;&#36824;&#25104;&#21151;&#22320;&#25581;&#31034;&#20102;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;&#20013;&#23384;&#22312;&#30340;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#26469;&#36319;&#36394;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20013;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#22914;&#20309;&#26292;&#38706;&#35821;&#35328;&#24314;&#27169;&#30340;&#24369;&#28857;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20132;&#32455;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#35268;&#21010;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32771;&#34385;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;-&#29289;&#20307;&#21644;&#29289;&#20307;-&#29289;&#20307;&#30340;&#20132;&#20114;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#21487;&#21160;&#29289;&#20307;&#30340;&#22797;&#26434;&#38750;&#25569;&#21462;&#24335;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.13352</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#32455;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#35268;&#21010;&#65292;&#35268;&#21010;&#21487;&#21160;&#29289;&#20307;&#30340;&#22797;&#26434;&#38750;&#25569;&#21462;&#24335;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Planning for Complex Non-prehensile Manipulation Among Movable Objects by Interleaving Multi-Agent Pathfinding and Physics-Based Simulation. (arXiv:2303.13352v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20132;&#32455;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#35268;&#21010;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32771;&#34385;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;-&#29289;&#20307;&#21644;&#29289;&#20307;-&#29289;&#20307;&#30340;&#20132;&#20114;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#21487;&#21160;&#29289;&#20307;&#30340;&#22797;&#26434;&#38750;&#25569;&#21462;&#24335;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#28151;&#26434;&#24230;&#29615;&#22659;&#20013;&#65292;&#23454;&#29616;&#29289;&#20307;&#30340;&#25805;&#20316;&#21487;&#33021;&#38656;&#35201;&#32771;&#34385;&#29289;&#20307;&#38388;&#30340;&#28508;&#22312;&#25509;&#35302;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#20174;&#26550;&#23376;&#19978;&#21462;&#19979;&#30446;&#26631;&#29289;&#20307;&#30340; pick-and-place &#25805;&#20316;&#65292;&#20854;&#20013;&#24517;&#39035;&#37325;&#26032;&#25490;&#21015;&#19968;&#20123;&#8220;&#21487;&#21160;&#8221;&#29289;&#20307;&#25165;&#33021;&#35299;&#20915;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#20801;&#35768;&#26426;&#22120;&#20154;&#32771;&#34385;&#38750;&#25569;&#21462;&#30340;&#37325;&#26032;&#25490;&#21015;&#34892;&#21160;&#65292;&#36825;&#20123;&#34892;&#21160;&#20250;&#23548;&#33268;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;-&#29289;&#20307;&#21644;&#29289;&#20307;-&#29289;&#20307;&#30340;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#21487;&#33021;&#21516;&#26102;&#31227;&#21160;&#22810;&#20010;&#29289;&#20307;&#65292;&#29289;&#20307;&#21487;&#33021;&#20542;&#26012;&#65292;&#30456;&#20114;&#38752;&#22312;&#19968;&#36215;&#25110;&#20542;&#20498;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26597;&#35810;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#27169;&#25311;&#22120;&#26469;&#21069;&#21521;&#27169;&#25311;&#36825;&#20123;&#20132;&#20114;&#21160;&#21147;&#23398;&#65292;&#36825;&#20351;&#24471;&#35745;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#35780;&#20272;&#21464;&#24471;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#20351;&#35268;&#21010;&#22120;&#21487;&#34892;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#21160;&#29289;&#20307;&#25805;&#20316;&#39046;&#22495;&#21644;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35753;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340; M4M &#31639;&#27861;&#36845;&#20195;&#22320;&#36827;&#34892;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#29983;&#25104;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#36816;&#21160;&#65292;&#23558;&#29289;&#20307;&#36816;&#36755;&#21040;&#23427;&#20204;&#30340;&#30446;&#26631;&#20301;&#32622;&#65292;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#27169;&#25311;&#22120;&#35780;&#20272;&#29983;&#25104;&#30340;&#27599;&#20010;&#36816;&#21160;&#35745;&#21010;&#20197;&#25214;&#21040;&#26368;&#20248;&#35745;&#21010;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35268;&#21010;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world manipulation problems in heavy clutter require robots to reason about potential contacts with objects in the environment. We focus on pick-and-place style tasks to retrieve a target object from a shelf where some `movable' objects must be rearranged in order to solve the task. In particular, our motivation is to allow the robot to reason over and consider non-prehensile rearrangement actions that lead to complex robot-object and object-object interactions where multiple objects might be moved by the robot simultaneously, and objects might tilt, lean on each other, or topple. To support this, we query a physics-based simulator to forward simulate these interaction dynamics which makes action evaluation during planning computationally very expensive. To make the planner tractable, we establish a connection between the domain of Manipulation Among Movable Objects and Multi-Agent Pathfinding that lets us decompose the problem into two phases our M4M algorithm iterates over. Firs
&lt;/p&gt;</description></item><item><title>&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.13336</link><description>&lt;p&gt;
&#35821;&#38899;&#21512;&#25104;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13336
&lt;/p&gt;
&lt;p&gt;
&#27492;&#25991;&#20171;&#32461;&#20102;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#35821;&#38899;&#21512;&#25104;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#26041;&#21521;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#26368;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#23581;&#35797;&#20102;&#20004;&#20010;&#27963;&#36291;&#20219;&#21153;&#65306;&#25991;&#26412;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#22686;&#24378;&#12290;&#26412;&#25991;&#23545;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#35843;&#26597;&#30340;&#34917;&#20805;&#65292;&#36825;&#20123;&#35843;&#26597;&#35201;&#20040;&#32570;&#20047;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#38899;&#21512;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35201;&#20040;&#24378;&#35843;&#22312;&#22810;&#20010;&#39046;&#22495;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25972;&#20307;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;&#38899;&#39057;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;&#23545;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#30340;&#38454;&#27573;&#65306;&#22768;&#23398;&#27169;&#22411;&#12289;&#22768;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#26576;&#20123;&#20449;&#21495;&#20174;&#36755;&#20837;&#35821;&#38899;&#20013;&#21024;&#38500;&#25110;&#28155;&#21152;&#26469;&#23558;&#21508;&#31181;&#35821;&#38899;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#36824;&#28085;&#30422;&#20102;&#23454;&#39564;&#32467;&#26524;&#30340;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.13326</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decentralized Adversarial Training over Graphs. (arXiv:2303.13326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#28431;&#27934;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#29420;&#31435;&#21333;&#19968;&#20195;&#29702;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20854;&#20013;&#21508;&#20010;&#21333;&#29420;&#30340;&#20195;&#29702;&#20250;&#21463;&#21040;&#31354;&#38388;&#20013;&#19981;&#21516;&#24378;&#24230;&#30340;&#25200;&#21160;&#12290;&#39044;&#26399;&#36890;&#36807;&#38142;&#25509;&#20195;&#29702;&#21644;&#21487;&#33021;&#22312;&#22270;&#19978;&#23454;&#29616;&#30340;&#25915;&#20987;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#21327;&#35843;&#25972;&#20010;&#22242;&#38431;&#30340;&#24378;&#22823;&#21327;&#21516;&#20316;&#29992;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26497;&#23567;-&#26497;&#22823;&#20844;&#24335;&#65292;&#20026;&#22810;&#20195;&#29702;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#26696;&#22312;&#20984;&#21644;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861; DARE-GRAM&#65292;&#21033;&#29992;&#20266;&#36870;&#20302;&#31209;&#24615;&#22312;&#30001;&#20004;&#20010;&#39046;&#22495;&#30340;&#20266;&#36870;&#26684;&#25289;&#22982;&#30697;&#38453;&#29983;&#25104;&#30340;&#36873;&#25321;&#23376;&#31354;&#38388;&#20013;&#23545;&#40784;&#23610;&#24230;&#21644;&#35282;&#24230;&#65292;&#35299;&#20915;&#20102;&#22238;&#24402;&#38382;&#39064;&#20013;&#26631;&#35760;&#28304;&#25968;&#25454;&#38598;&#19982;&#26410;&#26631;&#35760;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13325</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#36870;&#26684;&#25289;&#22982;&#30697;&#38453;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#65306;DARE-GRAM
&lt;/p&gt;
&lt;p&gt;
DARE-GRAM : Unsupervised Domain Adaptation Regression by Aligning Inverse Gram Matrices. (arXiv:2303.13325v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#26041;&#27861; DARE-GRAM&#65292;&#21033;&#29992;&#20266;&#36870;&#20302;&#31209;&#24615;&#22312;&#30001;&#20004;&#20010;&#39046;&#22495;&#30340;&#20266;&#36870;&#26684;&#25289;&#22982;&#30697;&#38453;&#29983;&#25104;&#30340;&#36873;&#25321;&#23376;&#31354;&#38388;&#20013;&#23545;&#40784;&#23610;&#24230;&#21644;&#35282;&#24230;&#65292;&#35299;&#20915;&#20102;&#22238;&#24402;&#38382;&#39064;&#20013;&#26631;&#35760;&#28304;&#25968;&#25454;&#38598;&#19982;&#26410;&#26631;&#35760;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;(DAR)&#26088;&#22312;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#26631;&#35760;&#28304;&#25968;&#25454;&#38598;&#19982;&#26410;&#26631;&#35760;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#28145;&#24230;&#29305;&#24449;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23567;&#28304;&#21644;&#30446;&#26631;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;DAR&#38382;&#39064;&#30340;&#35266;&#28857;&#65292;&#36890;&#36807;&#20998;&#26512;&#28145;&#24230;&#39046;&#22495;&#36866;&#24212;&#32972;&#26223;&#19979;&#32447;&#24615;&#22238;&#24402;&#22120;&#30340;&#38381;&#24335;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;(OLS)&#35299;&#27861;&#65292;&#25552;&#20986;&#20102;&#23545;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23545;&#40784;&#29305;&#24449;&#30340;&#36870;&#26684;&#25289;&#22982;&#30697;&#38453;&#26469;&#36827;&#34892;&#23545;&#40784;&#65292;&#36825;&#26159;&#30001;&#20110;&#36870;Gram&#30697;&#38453;&#23384;&#22312;&#20110;OLS&#35299;&#27861;&#20013;&#24182;&#19988;Gram&#30697;&#38453;&#33021;&#22815;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25152;&#20419;&#20351;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#22320;DAR&#26041;&#27861;&#65292;&#21033;&#29992;&#20266;&#36870;&#20302;&#31209;&#24615;&#22312;&#30001;&#20004;&#20010;&#39046;&#22495;&#30340;&#20266;&#36870;&#26684;&#25289;&#22982;&#30697;&#38453;&#29983;&#25104;&#30340;&#36873;&#25321;&#23376;&#31354;&#38388;&#20013;&#23545;&#40784;&#23610;&#24230;&#21644;&#35282;&#24230;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation Regression (DAR) aims to bridge the domain gap between a labeled source dataset and an unlabelled target dataset for regression problems. Recent works mostly focus on learning a deep feature encoder by minimizing the discrepancy between source and target features. In this work, we present a different perspective for the DAR problem by analyzing the closed-form ordinary least square~(OLS) solution to the linear regressor in the deep domain adaptation context. Rather than aligning the original feature embedding space, we propose to align the inverse Gram matrix of the features, which is motivated by its presence in the OLS solution and the Gram matrix's ability to capture the feature correlations. Specifically, we propose a simple yet effective DAR method which leverages the pseudo-inverse low-rank property to align the scale and angle in a selected subspace generated by the pseudo-inverse Gram matrix of the two domains. We evaluate our method on three doma
&lt;/p&gt;</description></item><item><title>QDP&#26041;&#27861;&#20248;&#21270;&#20102;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#21644;&#25361;&#36873;&#25918;&#32622;&#20301;&#32622;&#31561;&#21442;&#25968;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#26448;&#26009;&#33539;&#22260;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13320</link><description>&lt;p&gt;
QDP&#65306;&#23398;&#20064;&#20018;&#34892;&#20248;&#21270;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#20197;&#36827;&#34892;&#26426;&#22120;&#20154;&#24067;&#26009;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
QDP: Learning to Sequentially Optimise Quasi-Static and Dynamic Manipulation Primitives for Robotic Cloth Manipulation. (arXiv:2303.13320v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13320
&lt;/p&gt;
&lt;p&gt;
QDP&#26041;&#27861;&#20248;&#21270;&#20102;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#21644;&#25361;&#36873;&#25918;&#32622;&#20301;&#32622;&#31561;&#21442;&#25968;&#65292;&#26377;&#21161;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#26448;&#26009;&#33539;&#22260;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#23450;&#20041;&#30340;&#25805;&#20316;&#22522;&#20803;&#34987;&#24191;&#27867;&#29992;&#20110;&#24067;&#26009;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#20725;&#30828;&#24230;&#25110;&#23494;&#24230;&#31561;&#24067;&#26009;&#23646;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#22522;&#20803;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#35299;&#20915;&#20102;&#25361;&#36873;&#21644;&#25918;&#32622;&#20301;&#32622;&#30340;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#20294;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36895;&#24230;&#25110;&#36712;&#36857;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#21364;&#34987;&#24573;&#30053;&#20102;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#36825;&#20123;&#21442;&#25968;&#30340;&#20540;&#23545;&#20110;&#22788;&#29702;&#23478;&#24237;&#24067;&#29289;&#29289;&#21697;&#20013;&#23384;&#22312;&#30340;&#26448;&#26009;&#33539;&#22260;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Quasi-Dynamic Parameterisable&#65288;QDP&#65289;&#26041;&#27861;&#65292;&#23427;&#20248;&#21270;&#20102;&#21442;&#25968;&#65292;&#20363;&#22914;&#20934;&#38745;&#24577;&#21644;&#21160;&#24577;&#25805;&#20316;&#22522;&#20803;&#30340;&#36816;&#21160;&#36895;&#24230;&#20197;&#22806;&#65292;&#36824;&#21253;&#25324;&#25361;&#36873;&#21644;&#25918;&#32622;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20018;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#39034;&#24207;&#35299;&#32806;&#32452;&#25104;&#22522;&#20803;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#24067;&#26009;&#24179;&#25972;&#21644;&#22534;&#21472;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-defined manipulation primitives are widely used for cloth manipulation. However, cloth properties such as its stiffness or density can highly impact the performance of these primitives. Although existing solutions have tackled the parameterisation of pick and place locations, the effect of factors such as the velocity or trajectory of quasi-static and dynamic manipulation primitives has been neglected. Choosing appropriate values for these parameters is crucial to cope with the range of materials present in house-hold cloth objects. To address this challenge, we introduce the Quasi-Dynamic Parameterisable (QDP) method, which optimises parameters such as the motion velocity in addition to the pick and place positions of quasi-static and dynamic manipulation primitives. In this work, we leverage the framework of Sequential Reinforcement Learning to decouple sequentially the parameters that compose the primitives. To evaluate the effectiveness of the method we focus on the task of clo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13314</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Leveraging Foundation Models for Clinical Text Analysis. (arXiv:2303.13314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#22312;&#35780;&#20272;&#20013;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#30149;&#26159;&#20840;&#29699;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#65292;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21487;&#20197;&#20419;&#36827;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#31574;&#30053;&#30340;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#21487;&#29992;&#24615;&#20250;&#23545;&#20449;&#24687;&#25277;&#21462;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#33258;&#30001;&#25991;&#26412;&#20020;&#24202;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#20256;&#26579;&#30149;&#30456;&#20851;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#25968;&#25454;&#23618;&#29992;&#20110;&#20174;&#20020;&#24202;&#25991;&#26412;&#20934;&#22791;&#25968;&#25454;&#38598;&#65292;&#22522;&#30784;&#27169;&#22411;&#23618;&#29992;&#20110;&#23454;&#20307;&#25552;&#21462;&#65292;&#35780;&#20272;&#23618;&#29992;&#20110;&#24615;&#33021;&#20998;&#26512;&#12290;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#20351;&#20854;&#26377;&#21161;&#20110;&#30740;&#31350;&#20854;&#20182;&#20256;&#26579;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infectious diseases are a significant public health concern globally, and extracting relevant information from scientific literature can facilitate the development of effective prevention and treatment strategies. However, the large amount of clinical data available presents a challenge for information extraction. To address this challenge, this study proposes a natural language processing (NLP) framework that uses a pre-trained transformer model fine-tuned on task-specific data to extract key information related to infectious diseases from free-text clinical data. The proposed framework includes three components: a data layer for preparing datasets from clinical texts, a foundation model layer for entity extraction, and an assessment layer for performance analysis. The results of the evaluation indicate that the proposed method outperforms standard methods, and leveraging prior knowledge through the pre-trained transformer model makes it useful for investigating other infectious disea
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.13300</link><description>&lt;p&gt;
&#21019;&#26032;&#25918;&#32531;&#65306;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#30340;&#20943;&#36895;&#21450;&#26032;&#25216;&#26415;&#27010;&#24565;&#21407;&#21019;&#24615;&#30340;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Innovation Slowdown: Decelerating Concept Creation and Declining Originality in New Technological Concepts. (arXiv:2303.13300v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13300
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26234;&#21147;&#30340;&#23616;&#38480;&#24615;&#23548;&#33268;&#25216;&#26415;&#27010;&#24565;&#21019;&#36896;&#25918;&#32531;&#21644;&#21407;&#21019;&#24615;&#19979;&#38477;&#65292;&#22240;&#27492;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20043;&#21069;&#30340;&#27010;&#24565;&#37325;&#29992;&#12289;&#37325;&#32452;&#21644;&#21512;&#25104;&#36827;&#34892;&#26032;&#25216;&#26415;&#27010;&#24565;&#30340;&#21019;&#36896;&#21487;&#33021;&#20250;&#23548;&#33268;&#27010;&#24565;&#31354;&#38388;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#19987;&#21033;&#25991;&#26412;&#20013;&#36229;&#36807;400&#19975;&#20010;&#27010;&#24565;&#32452;&#25104;&#30340;&#22823;&#35268;&#27169;&#25216;&#26415;&#35821;&#20041;&#32593;&#32476;&#36827;&#34892;&#30340;&#32479;&#35745;&#20998;&#26512;&#21457;&#29616;&#65292;&#27010;&#24565;&#21019;&#36896;&#30340;&#27493;&#20240;&#22312;&#25345;&#32493;&#20943;&#32531;&#65292;&#24182;&#19988;&#26032;&#21019;&#36896;&#20986;&#30340;&#27010;&#24565;&#30340;&#21407;&#21019;&#24615;&#26377;&#25152;&#19979;&#38477;&#12290;&#36825;&#20123;&#36235;&#21183;&#21487;&#20197;&#24402;&#22240;&#20110;&#20154;&#31867;&#26234;&#21147;&#22312;&#21019;&#26032;&#36229;&#20986;&#29616;&#26377;&#25216;&#26415;&#30340;&#25299;&#23637;&#31354;&#38388;&#26041;&#38754;&#30340;&#23616;&#38480;&#12290;&#20026;&#20102;&#20445;&#25345;&#21019;&#26032;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#21644;&#23454;&#26045;&#21019;&#36896;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#20197;&#22686;&#24378;&#21019;&#26032;&#36807;&#31243;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#23398;&#20064;&#12289;&#21019;&#36896;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The creation of new technological concepts through design reuses, recombination, and synthesis of prior concepts to create new ones may lead to exponential growth of the concept space over time. However, our statistical analysis of a large-scale technology semantic network consisting of over four million concepts from patent texts found evidence of a persistent deceleration in the pace of concept creation and a decline in the originality of newly created concepts. These trends may be attributed to the limitations of human intelligence in innovating beyond an expanding space of prior art. To sustain innovation, we recommend the development and implementation of creative artificial intelligence that can augment various aspects of the innovation process, including learning, creation, and evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#19981;&#21516;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;PEAR&#25439;&#22833;&#39033;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#36798;&#21040;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#21644;&#21487;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2303.13299</link><description>&lt;p&gt;
&#35770;&#22914;&#20309;&#35299;&#20915;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#35757;&#32451;&#30446;&#26631;&#36798;&#25104;&#35299;&#37322;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#25152;&#23384;&#22312;&#30340;&#19981;&#21516;&#35299;&#37322;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;PEAR&#25439;&#22833;&#39033;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#36798;&#21040;&#27169;&#22411;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#21644;&#21487;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36880;&#28176;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#65292;&#30417;&#25511;&#21644;&#35299;&#37322;&#20854;&#34892;&#20026;&#25104;&#20026;&#24517;&#38656;&#12290;&#21518;&#32493;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21487;&#20026;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;&#29305;&#24449;&#20998;&#37197;&#24471;&#20998;&#65292;&#20197;&#34913;&#37327;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#23616;&#38480;&#26159;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#21738;&#20123;&#29305;&#24449;&#26356;&#37325;&#35201;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#30340;&#35757;&#32451;&#27169;&#22411;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;Post hoc Explainer Agreement Regularization (PEAR)&#25439;&#22833;&#39033;&#20197;&#25552;&#21319;&#35299;&#37322;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27492;&#25439;&#22833;&#39033;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#22312;&#26410;&#30475;&#35265;&#30340;&#25968;&#25454;&#19978;&#33719;&#24471;&#35299;&#37322;&#19968;&#33268;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#27010;&#29575;&#28857;&#20113;&#37197;&#20934;&#26694;&#26550;UDPReg&#65292;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#26469;&#36827;&#34892;&#28857;&#20113;&#30340;&#21305;&#37197;&#65292;&#22312;&#28151;&#21512;&#26435;&#37325;&#32422;&#26463;&#19979;&#20351;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#32423;&#23545;&#24212;&#20851;&#31995;&#39044;&#27979;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#20132;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#23545;&#27604;&#19977;&#31181;&#20998;&#24067;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#26631;&#31614;&#20449;&#24687;&#19979;&#30340;&#33258;&#21160;&#23398;&#20064;&#21644;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.13290</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#27010;&#29575;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration. (arXiv:2303.13290v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#27010;&#29575;&#28857;&#20113;&#37197;&#20934;&#26694;&#26550;UDPReg&#65292;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#26469;&#36827;&#34892;&#28857;&#20113;&#30340;&#21305;&#37197;&#65292;&#22312;&#28151;&#21512;&#26435;&#37325;&#32422;&#26463;&#19979;&#20351;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#32423;&#23545;&#24212;&#20851;&#31995;&#39044;&#27979;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#20132;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#23545;&#27604;&#19977;&#31181;&#20998;&#24067;&#19968;&#33268;&#24615;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#26631;&#31614;&#20449;&#24687;&#19979;&#30340;&#33258;&#21160;&#23398;&#20064;&#21644;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#38754;&#20020;&#30528;&#23616;&#37096;&#37325;&#21472;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UDPReg&#65292;&#19968;&#31181;&#29992;&#20110;&#37096;&#20998;&#37325;&#21472;&#28857;&#20113;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#27010;&#29575;&#37197;&#20934;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#32593;&#32476;&#20174;&#28857;&#20113;&#20013;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#30340;&#21518;&#39564;&#27010;&#29575;&#20998;&#24067;&#12290;&#20026;&#20102;&#22788;&#29702;&#23616;&#37096;&#28857;&#20113;&#37197;&#20934;&#65292;&#25105;&#20204;&#37319;&#29992;Sinkhorn&#31639;&#27861;&#22312;GMM&#30340;&#28151;&#21512;&#26435;&#37325;&#32422;&#26463;&#19979;&#39044;&#27979;&#20998;&#24067;&#32423;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#22522;&#20110;&#20998;&#24067;&#19968;&#33268;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65306;&#33258;&#19968;&#33268;&#24615;&#12289;&#20132;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#23545;&#27604;&#12290;&#33258;&#19968;&#33268;&#24615;&#25439;&#22833;&#36890;&#36807;&#40723;&#21169;Euclidean&#21644;&#29305;&#24449;&#31354;&#38388;&#30340;GMM&#20849;&#20139;&#30456;&#21516;&#30340;&#21518;&#39564;&#20998;&#24067;&#26469;&#24418;&#25104;&#65307;&#20132;&#19968;&#33268;&#24615;&#25439;&#22833;&#28304;&#20110;&#20004;&#20010;&#37096;&#20998;&#37325;&#21472;&#28857;&#20113;&#30340;&#28857;&#23646;&#20110;&#30456;&#21516;&#31751;&#30340;&#20107;&#23454;&#65292;&#23427;&#20204;&#20849;&#20139;&#31751;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep point cloud registration methods face challenges to partial overlaps and rely on labeled data. To address these issues, we propose UDPReg, an unsupervised deep probabilistic registration framework for point clouds with partial overlaps. Specifically, we first adopt a network to learn posterior probability distributions of Gaussian mixture models (GMMs) from point clouds. To handle partial point cloud registration, we apply the Sinkhorn algorithm to predict the distribution-level correspondences under the constraint of the mixing weights of GMMs. To enable unsupervised learning, we design three distribution consistency-based losses: self-consistency, cross-consistency, and local contrastive. The self-consistency loss is formulated by encouraging GMMs in Euclidean and feature spaces to share identical posterior distributions. The cross-consistency loss derives from the fact that the points of two partially overlapping point clouds belonging to the same clusters share the cluster cen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24103;&#32423;&#22810;&#26631;&#31614;&#28436;&#22863;&#25216;&#24039;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21476;&#31581;&#28436;&#22863;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#22768;&#37096;&#29420;&#22863;&#38899;&#20048;&#20013;&#30340;IPT&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13272</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24103;&#32423;&#22810;&#26631;&#31614;&#28436;&#22863;&#25216;&#24039;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism. (arXiv:2303.13272v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24103;&#32423;&#22810;&#26631;&#31614;&#28436;&#22863;&#25216;&#24039;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21476;&#31581;&#28436;&#22863;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#22768;&#37096;&#29420;&#22863;&#38899;&#20048;&#20013;&#30340;IPT&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#22863;&#25216;&#24039;&#65288;IPT&#65289;&#26159;&#38899;&#20048;&#34920;&#28436;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;IPT&#26816;&#27979;&#26041;&#27861;&#20165;&#28041;&#21450;&#21333;&#22768;&#36947;&#38899;&#20048;&#20449;&#21495;&#65292;&#23545;&#20110;&#20855;&#26377;&#37325;&#21472;IPT&#25110;&#28151;&#21512;IPT&#30340;&#22810;&#22768;&#37096;&#29420;&#22863;&#38899;&#20048;&#20013;&#30340;IPT&#26816;&#27979;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#24314;&#27169;&#20026;&#24103;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21476;&#31581;&#65292;&#19968;&#31181;&#20013;&#22269;&#24377;&#25320;&#24358;&#20048;&#22120;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21476;&#31581;_Tech99&#65292;&#20854;&#20013;&#21253;&#21547;&#21476;&#31581;&#24405;&#38899;&#20197;&#21450;&#27599;&#20010;&#38899;&#31526;&#30340;&#36215;&#22987;&#12289;&#32467;&#26463;&#12289;&#38899;&#39640;&#21644;IPT&#27880;&#37322;&#12290;&#22240;&#20026;&#19981;&#21516;&#30340;IPT&#22312;&#20854;&#38271;&#24230;&#26041;&#38754;&#26377;&#24456;&#22823;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#23610;&#24230;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#22810;&#23610;&#24230;&#32593;&#32476;&#20174;&#19981;&#21516;&#30340;&#23610;&#24230;&#25552;&#21462;&#29305;&#24449;&#65292;&#32780;&#22312;&#26368;&#31895;&#31961;&#30340;&#23610;&#24230;&#19978;&#24212;&#29992;&#20110;&#29305;&#24449;&#26144;&#23556;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#38271;&#31243;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;IPT&#26816;&#27979;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#34920;&#26126;&#20102;&#20854;&#22312;&#22810;&#22768;&#37096;&#29420;&#22863;&#38899;&#20048;&#30340;IPT&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instrument playing technique (IPT) is a key element of musical presentation. However, most of the existing works for IPT detection only concern monophonic music signals, yet little has been done to detect IPTs in polyphonic instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we formulate it as a frame-level multi-label classification problem and apply it to Guzheng, a Chinese plucked string instrument. We create a new dataset, Guzheng\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT annotations of each note. Because different IPTs vary a lot in their lengths, we propose a new method to solve this problem using multi-scale network and self-attention. The multi-scale network extracts features from different scales, and the self-attention mechanism applied to the feature maps at the coarsest scale further enhances the long-range feature extraction. Our approach outperforms existing works by a large margin, indicating its effectiveness in IPT de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LipRF&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#21033;&#26222;&#24076;&#33576;&#26144;&#23556;&#23558;&#39044;&#35757;&#32451;&#30340;NeRF&#30340;&#22806;&#35266;&#34920;&#31034;&#36716;&#25442;&#20026;&#20855;&#26377;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#39118;&#26684;&#21270;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.13232</link><description>&lt;p&gt;
&#21033;&#26222;&#24076;&#33576;&#32593;&#32476;&#36716;&#25442;&#36752;&#23556;&#22330;&#65292;&#23454;&#29616;&#36924;&#30495;&#30340;&#19977;&#32500;&#22330;&#26223;&#39118;&#26684;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transforming Radiance Field with Lipschitz Network for Photorealistic 3D Scene Stylization. (arXiv:2303.13232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LipRF&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#21033;&#26222;&#24076;&#33576;&#26144;&#23556;&#23558;&#39044;&#35757;&#32451;&#30340;NeRF&#30340;&#22806;&#35266;&#34920;&#31034;&#36716;&#25442;&#20026;&#20855;&#26377;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#39118;&#26684;&#21270;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#22312;3D&#22330;&#26223;&#24314;&#27169;&#21644;&#26032;&#35270;&#35282;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;NeRF&#36827;&#34892;&#36924;&#30495;&#30340;&#19977;&#32500;&#22330;&#26223;&#39118;&#26684;&#21270;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#26032;&#35270;&#35282;&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#24230;&#30340;&#39118;&#26684;&#21270;&#22330;&#26223;&#12290;&#23558;NeRF&#21644;&#36924;&#30495;&#30340;&#39118;&#26684;&#36801;&#31227;&#65288;PST&#65289;&#30456;&#32467;&#21512;&#65292;&#20250;&#23548;&#33268;&#36328;&#35270;&#35282;&#19981;&#19968;&#33268;&#21644;&#39118;&#26684;&#21270;&#35270;&#35282;&#21512;&#25104;&#30340;&#38477;&#36136;&#12290;&#32463;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#38750;&#24120;&#35268;&#20219;&#21153;&#21487;&#20197;&#20197;&#26032;&#30340;&#26041;&#27861;&#31616;&#21270;&#22788;&#29702;&#65306;&#20351;&#29992;&#21033;&#26222;&#24076;&#33576;&#26144;&#23556;&#36716;&#25442;&#39044;&#35757;&#32451;&#30340;NeRF&#30340;&#22806;&#35266;&#34920;&#31034;&#65292;&#21017;&#20250;&#26080;&#32541;&#22320;&#23558;&#28304;&#35270;&#35282;&#30340;&#19968;&#33268;&#24615;&#21644;&#36924;&#30495;&#24615;&#32534;&#30721;&#21040;&#21512;&#25104;&#22270;&#20687;&#20013;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#27905;&#12289;&#28789;&#27963;&#30340;&#23398;&#20064;&#26694;&#26550;LipRF&#65292;&#36890;&#36807;&#21033;&#26222;&#24076;&#33576;&#26144;&#23556;&#21319;&#32423;&#20219;&#20309;&#36866;&#29992;&#20110;&#19977;&#32500;&#22330;&#26223;&#30340;&#20108;&#32500;PST&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in 3D scene representation and novel view synthesis have witnessed the rise of Neural Radiance Fields (NeRFs). Nevertheless, it is not trivial to exploit NeRF for the photorealistic 3D scene stylization task, which aims to generate visually consistent and photorealistic stylized scenes from novel views. Simply coupling NeRF with photorealistic style transfer (PST) will result in cross-view inconsistency and degradation of stylized view syntheses. Through a thorough analysis, we demonstrate that this non-trivial task can be simplified in a new light: When transforming the appearance representation of a pre-trained NeRF with Lipschitz mapping, the consistency and photorealism across source views will be seamlessly encoded into the syntheses. That motivates us to build a concise and flexible learning framework namely LipRF, which upgrades arbitrary 2D PST methods with Lipschitz mapping tailored for the 3D scene. Technically, LipRF first pre-trains a radiance field to recon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.13228</link><description>&lt;p&gt;
&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20016;&#23500;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#30340;&#36829;&#35268;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26159;&#29992;&#20110;&#36817;&#20284;&#38750;&#32447;&#24615;&#20851;&#31995;&#65288;&#20363;&#22914;AC-OPF&#65289;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#37096;&#32626;&#26102;&#23454;&#29616;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#36890;&#24120;&#22312;&#30005;&#21147;&#31995;&#32479;&#25991;&#29486;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26159;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#29983;&#25104;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#20854;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20016;&#23500;&#20851;&#38190;&#25968;&#25454;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#20943;&#23569;&#26368;&#22351;&#24773;&#20917;&#36829;&#35268;&#65292;&#25552;&#20379;&#20855;&#26377;&#25913;&#36827;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27979;&#35797;&#30005;&#21147;&#31995;&#32479;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#33539;&#22260;&#20174;39&#20010;&#24635;&#32447;&#21040;162&#20010;&#24635;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#31283;&#23450;&#30340;AI&#24037;&#31243;&#23454;&#36341;&#26469;&#30830;&#20445;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#25913;&#21892;&#24320;&#21457;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.13216</link><description>&lt;p&gt;
AI&#24037;&#31243;&#23454;&#36341;&#26696;&#20363;&#30740;&#31350;&#65306;&#24320;&#21457;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Case Study on AI Engineering Practices: Developing an Autonomous Stock Trading System. (arXiv:2303.13216v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#31283;&#23450;&#30340;AI&#24037;&#31243;&#23454;&#36341;&#26469;&#30830;&#20445;&#31995;&#32479;&#30340;&#36136;&#37327;&#21644;&#25913;&#21892;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31995;&#32479;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#28982;&#32780;&#65292;&#24320;&#21457;&#21487;&#25237;&#20837;&#29983;&#20135;&#30340;AI&#31995;&#32479;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#36981;&#24490;&#31283;&#23450;&#30340;AI&#24037;&#31243;&#23454;&#36341;&#20197;&#30830;&#20445;&#32467;&#26524;&#31995;&#32479;&#30340;&#36136;&#37327;&#24182;&#25913;&#21892;&#24320;&#21457;&#36807;&#31243;&#12290;&#26412;&#25991;&#25910;&#38598;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#23454;&#36341;&#32463;&#39564;&#65292;&#21363;&#24320;&#21457;&#19968;&#20010;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21151;&#33021;&#25237;&#36164;&#32929;&#31080;&#30340;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, many systems use artificial intelligence (AI) to solve complex problems. While this often increases system effectiveness, developing a production-ready AI-based system is a difficult task. Thus, solid AI engineering practices are required to ensure the quality of the resulting system and to improve the development process. While several practices have already been proposed for the development of AI-based systems, detailed practical experiences of applying these practices are rare.  In this paper, we aim to address this gap by collecting such experiences during a case study, namely the development of an autonomous stock trading system that uses machine learning functionality to invest in stocks. We selected 10 AI engineering practices from the literature and systematically applied them during development, with the goal to collect evidence about their applicability and effectiveness. Using structured field notes, we documented our experiences. Furthermore, we also used field notes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22810;&#27169;&#29305;&#24449;&#30340;&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#32858;&#21512;&#20102;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#21644;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;AU-ROC&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.13194</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#22810;&#27169;&#29305;&#24449;&#30340;&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Complementary Pseudo Multimodal Feature for Point Cloud Anomaly Detection. (arXiv:2303.13194v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22810;&#27169;&#29305;&#24449;&#30340;&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#32858;&#21512;&#20102;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#21644;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;AU-ROC&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#19981;&#26029;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#25163;&#24037;&#21046;&#20316;&#30340;&#28857;&#20113;&#25551;&#36848;&#31526;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;2D&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#26469;&#25552;&#39640;&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22810;&#27169;&#29305;&#24449; (CPMF) &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#28857;&#20113;&#25551;&#36848;&#31526;&#23558;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#32435;&#20837;3D&#27169;&#24577;&#20013;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#30340;&#20266;2D&#27169;&#24577;&#20013;&#25552;&#21462;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#12290;&#23545;&#20110;&#20840;&#23616;&#35821;&#20041;&#25552;&#21462;&#65292;CPMF&#23558;&#21407;&#22987;&#28857;&#20113;&#25237;&#24433;&#21040;&#21253;&#21547;&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#20266;2D&#27169;&#24577;&#20013;&#12290;&#36825;&#20123;&#22270;&#20687;&#34987;&#20256;&#36882;&#32473;&#39044;&#35757;&#32451;&#30340;2D&#31070;&#32463;&#32593;&#32476;&#20197;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;2D&#27169;&#24577;&#29305;&#24449;&#12290;3D&#21644;2D&#27169;&#24577;&#29305;&#24449;&#34987;&#32858;&#21512;&#20197;&#33719;&#24471;&#29992;&#20110;&#28857;&#20113;&#24322;&#24120;&#26816;&#27979;&#30340;CPMF&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;2D&#21644;3D&#27169;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#20114;&#34917;&#33021;&#21147;&#20197;&#21450;CPMF&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;95.15%&#30340;&#22270;&#20687;&#32423;AU-ROC&#21644;92.67%&#30340;&#28857;&#32423;AU-ROC&#12290;
&lt;/p&gt;
&lt;p&gt;
Point cloud (PCD) anomaly detection steadily emerges as a promising research area. This study aims to improve PCD anomaly detection performance by combining handcrafted PCD descriptions with powerful pre-trained 2D neural networks. To this end, this study proposes Complementary Pseudo Multimodal Feature (CPMF) that incorporates local geometrical information in 3D modality using handcrafted PCD descriptors and global semantic information in the generated pseudo 2D modality using pre-trained 2D neural networks. For global semantics extraction, CPMF projects the origin PCD into a pseudo 2D modality containing multi-view images. These images are delivered to pre-trained 2D neural networks for informative 2D modality feature extraction. The 3D and 2D modality features are aggregated to obtain the CPMF for PCD anomaly detection. Extensive experiments demonstrate the complementary capacity between 2D and 3D modality features and the effectiveness of CPMF, with 95.15% image-level AU-ROC and 92
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#23637;&#31034;&#20215;&#26684;&#30340;&#24191;&#21578;&#31454;&#20215;&#26426;&#21046;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#28608;&#21169;&#20860;&#23481;&#30340;&#23637;&#31034;&#20215;&#26684;&#31454;&#20215;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#22312;&#23637;&#31034;&#20215;&#26684;&#22806;&#29983;&#30830;&#23450;&#21644;&#24191;&#21578;&#21830;&#21046;&#23450;&#23637;&#31034;&#20215;&#26684;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#25293;&#21334;&#12290;</title><link>http://arxiv.org/abs/2303.13192</link><description>&lt;p&gt;
&#24102;&#23637;&#31034;&#20215;&#26684;&#30340;&#24191;&#21578;&#31454;&#20215;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Mechanism Design for Ad Auctions with Display Prices. (arXiv:2303.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#23637;&#31034;&#20215;&#26684;&#30340;&#24191;&#21578;&#31454;&#20215;&#26426;&#21046;&#35774;&#35745;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#28608;&#21169;&#20860;&#23481;&#30340;&#23637;&#31034;&#20215;&#26684;&#31454;&#20215;&#30340;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#22312;&#23637;&#31034;&#20215;&#26684;&#22806;&#29983;&#30830;&#23450;&#21644;&#24191;&#21578;&#21830;&#21046;&#23450;&#23637;&#31034;&#20215;&#26684;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#25293;&#21334;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#24191;&#21578;&#19982;&#20215;&#26684;&#19968;&#36215;&#23637;&#31034;&#65292;&#20197;&#20415;&#25552;&#20379;&#31867;&#20284;&#20135;&#21697;&#25110;&#26381;&#21153;&#30340;&#30452;&#25509;&#27604;&#36739;&#12290;&#20215;&#26684;&#23637;&#31034;&#21151;&#33021;&#19981;&#20165;&#24433;&#21709;&#28040;&#36153;&#32773;&#30340;&#20915;&#31574;&#65292;&#36824;&#24433;&#21709;&#24191;&#21578;&#21830;&#30340;&#31454;&#26631;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#26426;&#21046;&#35774;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#24102;&#26377;&#23637;&#31034;&#20215;&#26684;&#30340;&#24191;&#21578;&#31454;&#20215;&#65292;&#35201;&#27714;&#24191;&#21578;&#21830;&#25552;&#20132;&#20854;&#20135;&#21697;&#30340;&#25104;&#26412;&#21644;&#20215;&#26684;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#28608;&#21169;&#20860;&#23481;&#30340;&#23637;&#31034;&#20215;&#26684;&#31454;&#20215;&#30340;&#29305;&#24449;&#65292;&#24182;&#29992;&#23427;&#26469;&#35774;&#35745;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#25293;&#21334;&#12290;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#20551;&#35774;&#23637;&#31034;&#20215;&#26684;&#26159;&#22806;&#29983;&#30830;&#23450;&#30340;&#12290;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20219;&#20309;&#20215;&#26684;&#37197;&#32622;&#30340;&#26368;&#22823;&#31119;&#21033;&#21644;&#26368;&#22823;&#25910;&#20837;&#30340;&#25293;&#21334;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#24191;&#21578;&#21830;&#34987;&#20801;&#35768;&#20026;&#20182;&#20204;&#30340;&#21033;&#30410;&#21046;&#23450;&#23637;&#31034;&#20215;&#26684;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22330;&#26223;&#20869;&#20004;&#20010;&#26063;&#32676;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#24182;&#30830;&#23450;&#20102;&#22343;&#34913;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, ads are displayed together with the prices, so as to provide a direct comparison among similar products or services. The price-displaying feature not only influences the consumers' decisions, but also affects the advertisers' bidding behaviors. In this paper, we study ad auctions with display prices from the perspective of mechanism design, in which advertisers are asked to submit both the costs and prices of their products. We provide a characterization for all incentive compatible auctions with display prices, and use it to design auctions under two scenarios. In the former scenario, the display prices are assumed to be exogenously determined. For this setting, we derive the welfare-maximizing and revenue-maximizing auctions for any realization of the price profile. In the latter, advertisers are allowed to strategize display prices in their own interests. We investigate two families of allocation policies within the scenario and identify the equilibrium prices 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#25193;&#23637;&#39640;&#25928;&#29992;&#27169;&#24335;&#25366;&#25496;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#31181;&#26032;&#22411;&#25928;&#29992;&#20934;&#21017;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.13191</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#29992;&#27169;&#24335;&#25366;&#25496;&#65306;&#22522;&#20110;&#31572;&#26696;&#38598;&#32534;&#31243;&#26694;&#26550;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Extended High Utility Pattern Mining: An Answer Set Programming Based Framework and Applications. (arXiv:2303.13191v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#25193;&#23637;&#39640;&#25928;&#29992;&#27169;&#24335;&#25366;&#25496;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#31181;&#26032;&#22411;&#25928;&#29992;&#20934;&#21017;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#65292;&#26816;&#27979;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30456;&#20851;&#27169;&#24335;&#30340;&#38598;&#21512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#27169;&#24335;&#30340;&#30456;&#20851;&#24615;&#65288;&#20063;&#31216;&#20026;&#25928;&#29992;&#65289;&#26159;&#19968;&#20010;&#20027;&#35266;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#22522;&#20110;&#35268;&#21017;&#30340;&#35821;&#35328;&#65288;&#22914;&#31572;&#26696;&#38598;&#32534;&#31243;&#65289;&#20284;&#20046;&#24456;&#36866;&#21512;&#20197;&#32422;&#26463;&#24418;&#24335;&#35268;&#23450;&#29992;&#25143;&#25552;&#20379;&#30340;&#20934;&#21017;&#26469;&#35780;&#20272;&#27169;&#24335;&#25928;&#29992;&#65307;&#27492;&#22806;&#65292;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#22768;&#26126;&#24615;&#20351;&#24471;&#21487;&#20197;&#38750;&#24120;&#23481;&#26131;&#22320;&#22312;&#19981;&#21516;&#35282;&#24230;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25193;&#23637;&#39640;&#25928;&#29992;&#27169;&#24335;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24341;&#20837;&#20102;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#20197;&#21069;&#25991;&#29486;&#20013;&#26410;&#32771;&#34385;&#30340;&#26032;&#22411;&#25928;&#29992;&#20934;&#21017;&#31867;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#24102;&#26377;&#22806;&#37096;&#20989;&#25968;&#30340;&#31572;&#26696;&#38598;&#32534;&#31243;&#25193;&#23637;&#26469;&#25903;&#25345;&#19968;&#31181;&#24555;&#36895;&#26377;&#25928;&#30340;&#32534;&#30721;&#21644;&#27979;&#35797;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;HUPM&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#31034;&#20363;&#65292;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#27169;&#24335;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting sets of relevant patterns from a given dataset is an important challenge in data mining. The relevance of a pattern, also called utility in the literature, is a subjective measure and can be actually assessed from very different points of view. Rule-based languages like Answer Set Programming (ASP) seem well suited for specifying user-provided criteria to assess pattern utility in a form of constraints; moreover, declarativity of ASP allows for a very easy switch between several criteria in order to analyze the dataset from different points of view. In this paper, we make steps toward extending the notion of High Utility Pattern Mining (HUPM); in particular we introduce a new framework that allows for new classes of utility criteria not considered in the previous literature. We also show how recent extensions of ASP with external functions can support a fast and effective encoding and testing of the new framework. To demonstrate the potential of the proposed framework, we exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CMG-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#25509;&#35302;&#28857;&#30340;&#22810;&#25351;&#25235;&#21462;&#32593;&#32476;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#25235;&#21462;&#23039;&#21183;&#21644;&#25163;&#22411;&#37197;&#32622;&#65292;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#26410;&#30693;&#29289;&#20307;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.13182</link><description>&lt;p&gt;
CMG-Net: &#19968;&#31181;&#22522;&#20110;&#25509;&#35302;&#30340;&#22810;&#25351;&#28789;&#24039;&#25235;&#21462;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CMG-Net: An End-to-End Contact-Based Multi-Finger Dexterous Grasping Network. (arXiv:2303.13182v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CMG-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#25509;&#35302;&#28857;&#30340;&#22810;&#25351;&#25235;&#21462;&#32593;&#32476;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#25235;&#21462;&#23039;&#21183;&#21644;&#25163;&#22411;&#37197;&#32622;&#65292;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#26410;&#30693;&#29289;&#20307;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25235;&#21462;&#34920;&#36848;&#26041;&#24335;&#65292;&#20351;&#29992;&#26426;&#22120;&#25163;&#25351;&#19982;&#24453;&#25805;&#20316;&#29289;&#20307;&#20043;&#38388;&#30340;&#25509;&#35302;&#28857;&#12290;&#35813;&#34920;&#36848;&#26041;&#24335;&#26174;&#33879;&#20943;&#23569;&#20102;&#39044;&#27979;&#32500;&#24230;&#24182;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;CMG-Net&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#28857;&#20113;&#20013;&#39640;&#25928;&#22320;&#39044;&#27979;&#22810;&#25351;&#25235;&#21462;&#23039;&#21183;&#21644;&#25163;&#22411;&#37197;&#32622;&#65292;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25235;&#21462;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#21253;&#21547;&#20116;&#21315;&#20010;&#26434;&#20081;&#22330;&#26223;&#12289;80&#20010;&#29289;&#20307;&#31867;&#21035;&#21644;2000&#19975;&#27880;&#37322;&#30340;&#21512;&#25104;&#25235;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25235;&#21462;&#34920;&#36848;&#26041;&#24335;&#21644;CMG-Net&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#19977;&#25351;&#26426;&#22120;&#25163;&#30340;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20339;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel representation for grasping using contacts between multi-finger robotic hands and objects to be manipulated. This representation significantly reduces the prediction dimensions and accelerates the learning process. We present an effective end-to-end network, CMG-Net, for grasping unknown objects in a cluttered environment by efficiently predicting multi-finger grasp poses and hand configurations from a single-shot point cloud. Moreover, we create a synthetic grasp dataset that consists of five thousand cluttered scenes, 80 object categories, and 20 million annotations. We perform a comprehensive empirical study and demonstrate the effectiveness of our grasping representation and CMG-Net. Our work significantly outperforms the state-of-the-art for three-finger robotic hands. We also demonstrate that the model trained using synthetic data performs very well for real robots.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#21644;&#31995;&#32479;&#24615;&#33021;&#12290;&#20174;51&#31687;&#23398;&#26415;&#35770;&#25991;&#20013;&#36873;&#25321;&#20102;43&#20010;&#35774;&#35745;&#27169;&#24335;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;&#38024;&#23545;AI-based systems&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#20026;&#25968;&#25454;&#22788;&#29702;&#12289;&#24314;&#27169;&#12289;&#20915;&#31574;&#21644;&#23454;&#29616;&#22235;&#20010;&#31867;&#21035;&#12290;&#35813;&#30740;&#31350;&#20026;&#30740;&#31350;&#32773;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#35774;&#35745;&#27169;&#24335;&#24211;&#12290;</title><link>http://arxiv.org/abs/2303.13173</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#27169;&#24335;&#65306;&#22810;&#26041;&#20301;&#25991;&#29486;&#32508;&#36848;&#19982;&#27169;&#24335;&#23384;&#20648;&#24211;
&lt;/p&gt;
&lt;p&gt;
Design Patterns for AI-based Systems: A Multivocal Literature Review and Pattern Repository. (arXiv:2303.13173v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27010;&#36848;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#27169;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#21644;&#31995;&#32479;&#24615;&#33021;&#12290;&#20174;51&#31687;&#23398;&#26415;&#35770;&#25991;&#20013;&#36873;&#25321;&#20102;43&#20010;&#35774;&#35745;&#27169;&#24335;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;&#38024;&#23545;AI-based systems&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#20026;&#25968;&#25454;&#22788;&#29702;&#12289;&#24314;&#27169;&#12289;&#20915;&#31574;&#21644;&#23454;&#29616;&#22235;&#20010;&#31867;&#21035;&#12290;&#35813;&#30740;&#31350;&#20026;&#30740;&#31350;&#32773;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#35774;&#35745;&#27169;&#24335;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#32452;&#20214;&#26500;&#25104;&#30340;&#31995;&#32479;&#65292;&#21363;&#25152;&#35859;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#32452;&#32455;&#22312;&#23454;&#29616;&#36825;&#26679;&#30340;&#31995;&#32479;&#30340;&#29983;&#20135;&#20934;&#22791;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20316;&#20026;&#25552;&#39640;&#26576;&#20123;&#36719;&#20214;&#36136;&#37327;&#23646;&#24615;&#21644;&#35299;&#20915;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#30340;&#25163;&#27573;&#65292;&#35774;&#35745;&#27169;&#24335;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#34013;&#22270;&#12290;&#22312;AI-based&#31995;&#32479;&#20986;&#29616;&#26032;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#29616;&#26377;&#27169;&#24335;&#20063;&#24050;&#32463;&#34987;&#36866;&#24212;&#21040;&#20102;&#36825;&#20010;&#26032;&#30340;&#19978;&#19979;&#25991;&#29615;&#22659;&#20013;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#35774;&#35745;&#27169;&#24335;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#26032;&#27169;&#24335;&#21644;&#36866;&#24212;&#27169;&#24335;&#12290;&#25105;&#20204;&#24076;&#26395;&#25910;&#38598;&#21644;&#20998;&#31867;&#27169;&#24335;&#65292;&#24182;&#20351;&#20854;&#23545;&#30740;&#31350;&#32773;&#21644;&#20174;&#19994;&#32773;&#21487;&#35775;&#38382;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#22810;&#26041;&#20301;&#25991;&#29486;&#32508;&#36848;&#65288;MLR&#65289;&#20197;&#25910;&#38598;&#19982;AI-based&#31995;&#32479;&#19968;&#36215;&#20351;&#29992;&#30340;&#35774;&#35745;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#21019;&#24314;&#30340;&#27169;&#24335;&#38598;&#25104;&#21040;&#19968;&#20010;&#22522;&#20110;web&#30340;&#27169;&#24335;&#23384;&#20648;&#24211;&#20013;&#65292;&#20351;&#24471;&#36825;&#20123;&#27169;&#24335;&#26131;&#20110;&#27983;&#35272;&#21644;&#26597;&#25214;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;51&#31687;&#23398;&#26415;&#35770;&#25991;&#20013;&#30340;43&#20010;&#35774;&#35745;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#20998;&#20026;&#25968;&#25454;&#22788;&#29702;&#12289;&#24314;&#27169;&#12289;&#20915;&#31574;&#21644;&#23454;&#29616;&#22235;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#25552;&#20379;&#20102;&#38024;&#23545;AI-based systems &#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#21644;&#31995;&#32479;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20026;&#30740;&#31350;&#32773;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#12289;&#26131;&#20110;&#35775;&#38382;&#30340;&#35774;&#35745;&#27169;&#24335;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems with artificial intelligence components, so-called AI-based systems, have gained considerable attention recently. However, many organizations have issues with achieving production readiness with such systems. As a means to improve certain software quality attributes and to address frequently occurring problems, design patterns represent proven solution blueprints. While new patterns for AI-based systems are emerging, existing patterns have also been adapted to this new context.  The goal of this study is to provide an overview of design patterns for AI-based systems, both new and adapted ones. We want to collect and categorize patterns, and make them accessible for researchers and practitioners. To this end, we first performed a multivocal literature review (MLR) to collect design patterns used with AI-based systems. We then integrated the created pattern collection into a web-based pattern repository to make the patterns browsable and easy to find.  As a result, we selected 51
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20449;&#24565;&#20989;&#25968;&#36923;&#36753;&#65292;&#36890;&#36807;&#22312;MEL&#39030;&#37096;&#28155;&#21152;{\L}ukasiewicz&#36923;&#36753;&#26469;&#23454;&#29616;&#65292;&#20801;&#35768;&#26356;&#33258;&#28982;&#30340;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.13168</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#30340;&#20449;&#24565;&#20989;&#25968;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
An elementary belief function logic. (arXiv:2303.13168v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20449;&#24565;&#20989;&#25968;&#36923;&#36753;&#65292;&#36890;&#36807;&#22312;MEL&#39030;&#37096;&#28155;&#21152;{\L}ukasiewicz&#36923;&#36753;&#26469;&#23454;&#29616;&#65292;&#20801;&#35768;&#26356;&#33258;&#28982;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21487;&#21152;&#19981;&#30830;&#23450;&#24615;&#29702;&#35770;&#65292;&#23588;&#20854;&#26159;&#21487;&#33021;&#24615;&#29702;&#35770;&#12289;&#20449;&#24565;&#20989;&#25968;&#21644;&#19981;&#31934;&#30830;&#27010;&#29575;&#19982;&#27169;&#24577;&#36923;&#36753;&#20849;&#20139;&#19968;&#20010;&#20849;&#21516;&#29305;&#28857;&#65292;&#21363;&#21487;&#33021;&#24615;&#21644;&#24517;&#28982;&#24615;&#24230;&#37327;&#12289;&#32622;&#20449;&#24230;&#21644;&#21487;&#33021;&#24230;&#20989;&#25968;&#20197;&#21450;&#19978;&#19979;&#27010;&#29575;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#36136;&#23558;&#21487;&#33021;&#24615;&#21644;&#24517;&#28982;&#24615;&#27169;&#24577;&#30340;&#23545;&#20598;&#24615;&#25193;&#23637;&#21040;&#20998;&#32423;&#29615;&#22659;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20449;&#24565;&#20989;&#25968;&#36923;&#36753;&#65292;&#24182;&#36890;&#36807;&#22312;MEL&#39030;&#37096;&#28155;&#21152;{\L}ukasiewicz&#36923;&#36753;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#33258;&#28982;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-additive uncertainty theories, typically possibility theory, belief functions and imprecise probabilities share a common feature with modal logic: the duality properties between possibility and necessity measures, belief and plausibility functions as well as between upper and lower probabilities extend the duality between possibility and necessity modalities to the graded environment. It has been shown that the all-or-nothing version of possibility theory can be exactly captured by a minimal epistemic logic (MEL) that uses a very small fragment of the KD modal logic, without resorting to relational semantics. Besides, the case of belief functions has been studied independently, and a belief function logic has been obtained by extending the modal logic S5 to graded modalities using {\L}ukasiewicz logic, albeit using relational semantics. This paper shows that a simpler belief function logic can be devised by adding {\L}ukasiewicz logic on top of MEL. It allows for a more natural sem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32477;&#28909;&#37325;&#25918;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#37325;&#25918;&#19982;&#26032;&#25968;&#25454;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.13157</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#32477;&#28909;&#37325;&#25918;
&lt;/p&gt;
&lt;p&gt;
Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32477;&#28909;&#37325;&#25918;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#37325;&#25918;&#19982;&#26032;&#25968;&#25454;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22312;&#27599;&#20010;&#26032;&#25968;&#25454;&#30340;&#23398;&#20064;&#38454;&#27573;&#37325;&#25918;&#20195;&#34920;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#25152;&#26377;&#30693;&#35782;&#30340;&#26679;&#26412;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#30001;&#20110;&#22312;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#37327;&#38543;&#26102;&#38388;&#22686;&#38271;&#65292;&#29983;&#25104;&#24335;&#37325;&#25918;&#20250;&#33457;&#36153;&#36234;&#26469;&#36234;&#22810;&#30340;&#26102;&#38388;&#26469;&#37325;&#26032;&#23398;&#20064;&#24050;&#30693;&#20869;&#23481;&#12290;&#22312;&#36825;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#32477;&#28909;&#37325;&#25918;&#65288;AR&#65289;&#30340;&#37325;&#25918;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#25928;&#29575;&#26469;&#33258;&#20110;&#65288;&#21512;&#29702;&#30340;&#65289;&#20551;&#35774;&#27599;&#20010;&#26032;&#30340;&#23398;&#20064;&#38454;&#27573;&#37117;&#26159;&#32477;&#28909;&#30340;&#65292;&#21363;&#20165;&#20195;&#34920;&#29616;&#26377;&#30693;&#35782;&#30340;&#23567;&#24133;&#22686;&#21152;&#12290;&#27599;&#20010;&#26032;&#30340;&#23398;&#20064;&#38454;&#27573;&#20250;&#35302;&#21457;&#19968;&#20010;&#36873;&#25321;&#24615;&#37325;&#25918;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#20174;&#29616;&#26377;&#30693;&#35782;&#24211;&#20013;&#36873;&#25321;&#30456;&#20284;&#20110;&#26032;&#25968;&#25454;&#30340;&#26679;&#26412;&#36827;&#34892;&#37325;&#25918;&#65292;&#32780;&#19981;&#26159;&#20840;&#37096;&#37325;&#25918;&#12290;&#23436;&#20840;&#37325;&#25918;&#19981;&#26159;&#24517;&#39035;&#30340;&#65292;&#22240;&#20026;AR&#36890;&#36807;GMMs&#34920;&#31034;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#20123;&#20998;&#24067;&#33021;&#22815;&#26377;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#23427;&#20204;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;ISO25000&#36136;&#37327;&#27169;&#22411;&#24212;&#29992;&#20110;&#37326;&#33457;&#30417;&#27979;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#36136;&#37327;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.13151</link><description>&lt;p&gt;
&#20026;&#21487;&#20449;AI&#37326;&#33457;&#30417;&#27979;&#24179;&#21488;&#23450;&#20041;&#36136;&#37327;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Defining Quality Requirements for a Trustworthy AI Wildflower Monitoring Platform. (arXiv:2303.13151v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;ISO25000&#36136;&#37327;&#27169;&#22411;&#24212;&#29992;&#20110;&#37326;&#33457;&#30417;&#27979;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#36136;&#37327;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#23558;AI&#35299;&#20915;&#26041;&#26696;&#20174;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#28436;&#21464;&#20026;&#29983;&#20135;&#23601;&#32490;&#30340;AI&#31995;&#32479;&#65292;&#38656;&#35201;&#32771;&#34385;&#27604;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#26356;&#22810;&#30340;&#22240;&#32032;&#12290;&#29983;&#20135;&#23601;&#32490;&#30340;AI&#31995;&#32479;&#38656;&#35201;&#20855;&#22791;&#21487;&#20449;&#24615;&#65292;&#21363;&#39640;&#36136;&#37327;&#12290;&#26412;&#25991;&#20351;&#29992;ISO25000&#36136;&#37327;&#27169;&#22411;&#20043;&#19968;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;:&#30417;&#27979;&#37326;&#33457;&#30340;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19977;&#31181;&#29616;&#23454;&#22330;&#26223;&#65292;&#20998;&#21035;&#27010;&#36848;&#20102;&#37319;&#29992;&#12289;&#25193;&#23637;&#21644;&#36880;&#27493;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#29992;&#20110;&#37326;&#33457;&#35782;&#21035;&#21644;&#35745;&#25968;&#30340;&#21547;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36136;&#37327;&#27169;&#22411;&#20316;&#20026;&#32467;&#26500;&#21270;&#35789;&#20856;&#65292;&#20026;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#36719;&#20214;&#23450;&#20041;&#36136;&#37327;&#35201;&#27714;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
For an AI solution to evolve from a trained machine learning model into a production-ready AI system, many more things need to be considered than just the performance of the machine learning model. A production-ready AI system needs to be trustworthy, i.e. of high quality. But how to determine this in practice? For traditional software, ISO25000 and its predecessors have since long time been used to define and measure quality characteristics. Recently, quality models for AI systems, based on ISO25000, have been introduced. This paper applies one such quality model to a real-life case study: a deep learning platform for monitoring wildflowers. The paper presents three realistic scenarios sketching what it means to respectively use, extend and incrementally improve the deep learning platform for wildflower identification and counting. Next, it is shown how the quality model can be used as a structured dictionary to define quality requirements for data, model and software. Future work rem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.13126</link><description>&lt;p&gt;
MagicFusion&#65306;&#36890;&#36807;&#34701;&#21512;&#25193;&#25955;&#27169;&#22411;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SNB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#20004;&#20010;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;AI&#31038;&#21306;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#24378;&#22823;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Saliency-aware Noise Blending (SNB)&#8221; &#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#34701;&#21512;&#30340;&#25991;&#26412;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26356;&#21487;&#25511;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20998;&#31867;&#22120;&#33258;&#30001;&#25351;&#23548;&#30340;&#21709;&#24212;&#19982;&#29983;&#25104;&#22270;&#20687;&#30340;&#26174;&#30528;&#24615;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26174;&#30528;&#24615;&#24863;&#30693;&#30340;&#24773;&#20917;&#19979;&#28151;&#21512;&#20004;&#20010;&#25193;&#25955;&#27169;&#22411;&#30340;&#39044;&#27979;&#22122;&#22768;&#26469;&#20449;&#20219;&#20854;&#19987;&#19994;&#39046;&#22495;&#30340;&#19981;&#21516;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;SNB&#26159;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23436;&#25104;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;DDIM&#37319;&#26679;&#36807;&#31243;&#20013;&#33258;&#21160;&#23545;&#40784;&#20004;&#20010;&#22122;&#22768;&#31354;&#38388;&#30340;&#35821;&#20041;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#20363;&#22914;&#25513;&#27169;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;SNB&#30340;&#26174;&#30528;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The advent of open-source AI communities has produced a cornucopia of powerful text-guided diffusion models that are trained on various datasets. While few explorations have been conducted on ensembling such models to combine their strengths. In this work, we propose a simple yet effective method called Saliency-aware Noise Blending (SNB) that can empower the fused text-guided diffusion models to achieve more controllable generation. Specifically, we experimentally find that the responses of classifier-free guidance are highly related to the saliency of generated images. Thus we propose to trust different models in their areas of expertise by blending the predicted noises of two diffusion models in a saliency-aware manner. SNB is training-free and can be completed within a DDIM sampling process. Additionally, it can automatically align the semantics of two noise spaces without requiring additional annotations such as masks. Extensive experiments show the impressive effectiveness of SNB
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.13112</link><description>&lt;p&gt;
&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#35299;&#37322;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
A Simple Explanation for the Phase Transition in Large Language Models with List Decoding. (arXiv:2303.13112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30456;&#21464;&#29616;&#35937;&#30340;&#31616;&#21333;&#35299;&#37322;&#65292;&#21033;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#24314;&#27169;&#65292;&#23427;&#33021;&#22815;&#20445;&#35777;&#22312;LLM&#20302;&#20110;&#20020;&#30028;&#38408;&#20540;&#26102;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#30340;&#26399;&#26395;&#20445;&#25345;&#26377;&#30028;&#65292;&#32780;&#22312;&#39640;&#20110;&#35813;&#38408;&#20540;&#26102;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21576;&#29616;&#20986;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#24403;&#27169;&#22411;&#36798;&#21040;&#19968;&#23450;&#30340;&#35268;&#27169;&#20851;&#38190;&#28857;&#26102;&#65292;&#31995;&#32479;&#24615;&#33021;&#24471;&#21040;&#20102;&#26497;&#22823;&#22320;&#25552;&#39640;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#37322;&#65292;&#24182;&#23558;LLM&#24314;&#27169;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38543;&#26426;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#21015;&#34920;&#35793;&#30721;&#22120;&#20195;&#26367;&#27599;&#20010;&#27493;&#39588;&#30340;&#21363;&#26102;&#29983;&#25104;&#65292;&#35813;&#35793;&#30721;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20445;&#30041;&#19968;&#20010;&#20505;&#36873;&#24207;&#21015;&#21015;&#34920;&#65292;&#24182;&#22312;&#32467;&#26463;&#26102;&#25512;&#36831;&#36755;&#20986;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20010;&#20020;&#30028;&#38408;&#20540;&#65292;&#24403;LLM&#20302;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#30340;&#38169;&#35823;&#20505;&#36873;&#24207;&#21015;&#25968;&#20445;&#25345;&#26377;&#30028;&#65292;&#24403;LLM&#39640;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#26399;&#26395;&#38169;&#35823;&#24207;&#21015;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#26679;&#30340;&#38408;&#20540;&#19982;&#20256;&#26579;&#30149;&#30340;&#22522;&#26412;&#32321;&#27542;&#25968;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various recent experimental results show that large language models (LLM) exhibit emergent abilities that are not present in small models. System performance is greatly improved after passing a certain critical threshold of scale. In this letter, we provide a simple explanation for such a phase transition phenomenon. For this, we model an LLM as a sequence-to-sequence random function. Instead of using instant generation at each step, we use a list decoder that keeps a list of candidate sequences at each step and defers the generation of the output sequence at the end. We show that there is a critical threshold such that the expected number of erroneous candidate sequences remains bounded when an LLM is below the threshold, and it grows exponentially when an LLM is above the threshold. Such a threshold is related to the basic reproduction number in a contagious disease.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.13099</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#30340;&#38646;&#26679;&#26412;&#24320;&#25918;&#24847;&#22270;&#24402;&#32435;&#65306;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-View Zero-Shot Open Intent Induction from Dialogues: Multi Domain Batch and Proxy Gradient Transfer. (arXiv:2303.13099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#21644;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#30340;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24847;&#22270;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#24847;&#22270;&#30340;&#38382;&#39064;&#65292;&#22312;Open Intent Induction&#20013;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26816;&#27979;&#21644;&#35825;&#23548;&#26032;&#30340;&#24847;&#22270;&#26159;&#23558;&#35813;&#31995;&#32479;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#22810;&#35270;&#35282;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38590;&#39064;&#65306;&#65288;1&#65289;&#29992;&#20110;&#19968;&#33324;&#23884;&#20837;&#30340;SBERT&#65288;2&#65289;&#22810;&#39046;&#22495;&#25209;&#22788;&#29702;&#65288;MDB&#65289;&#29992;&#20110;&#23545;&#35805;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#21450;&#65288;3&#65289;&#29992;&#20110;&#38598;&#32676;&#19987;&#19994;&#35821;&#20041;&#30340;&#20195;&#29702;&#26799;&#24230;&#36716;&#31227;&#65288;PGT&#65289;&#12290; MDB&#19968;&#27425;&#21521;&#27169;&#22411;&#25552;&#20379;&#22810;&#31181;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23398;&#20064;&#22810;&#39046;&#22495;&#30693;&#35782;&#26469;&#35299;&#20915;&#22810;&#39046;&#22495;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;PGT&#65292;&#23427;&#37319;&#29992;Siamese&#32593;&#32476;&#30452;&#25509;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;PGT&#32858;&#31867;&#23545;&#35805;&#35821;&#21477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#27169;&#22411;&#19982;MDB&#21644;PGT&#26174;&#30528;&#25552;&#39640;&#20102;Open Intent Induction&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Task Oriented Dialogue (TOD) system, detecting and inducing new intents are two main challenges to apply the system in the real world. In this paper, we suggest the semantic multi-view model to resolve these two challenges: (1) SBERT for General Embedding (GE), (2) Multi Domain Batch (MDB) for dialogue domain knowledge, and (3) Proxy Gradient Transfer (PGT) for cluster-specialized semantic. MDB feeds diverse dialogue datasets to the model at once to tackle the multi-domain problem by learning the multiple domain knowledge. We introduce a novel method PGT, which employs the Siamese network to fine-tune the model with a clustering method directly.Our model can learn how to cluster dialogue utterances by using PGT. Experimental results demonstrate that our multi-view model with MDB and PGT significantly improves the Open Intent Induction performance compared to baseline systems.
&lt;/p&gt;</description></item><item><title>CP$^3$&#26159;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#32593;&#32476;&#30340;&#36890;&#36947;&#21098;&#26525;&#25554;&#20214;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#21644;PNN&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22352;&#26631;&#22686;&#24378;&#30340;&#36890;&#36947;&#37325;&#35201;&#24230;&#25351;&#26631;&#20197;&#23454;&#29616;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.13097</link><description>&lt;p&gt;
CP$^3$: &#22522;&#20110;&#28857;&#20113;&#32593;&#32476;&#30340;&#36890;&#36947;&#21098;&#26525;&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
CP$^3$: Channel Pruning Plug-in for Point-based Networks. (arXiv:2303.13097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13097
&lt;/p&gt;
&lt;p&gt;
CP$^3$&#26159;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#32593;&#32476;&#30340;&#36890;&#36947;&#21098;&#26525;&#25554;&#20214;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#21644;PNN&#30340;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#22352;&#26631;&#22686;&#24378;&#30340;&#36890;&#36947;&#37325;&#35201;&#24230;&#25351;&#26631;&#20197;&#23454;&#29616;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#21098;&#26525;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21407;&#22987;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#31934;&#24230;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;2D&#22522;&#20110;&#22270;&#20687;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24456;&#23569;&#23558;&#36890;&#36947;&#20462;&#21098;&#26041;&#27861;&#25193;&#23637;&#21040;3D&#22522;&#20110;&#28857;&#20113;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#12290;&#30452;&#25509;&#23558;2D CNN&#36890;&#36947;&#20462;&#21098;&#26041;&#27861;&#23454;&#29616;&#21040;PNN&#20250;&#30772;&#22351;PNN&#24615;&#33021;&#65292;&#22240;&#20026;2D&#22270;&#20687;&#21644;3D&#28857;&#20113;&#30340;&#19981;&#21516;&#34920;&#31034;&#20197;&#21450;&#32593;&#32476;&#32467;&#26500;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CP$^3$&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#32593;&#32476;&#30340;&#36890;&#36947;&#20462;&#21098;&#25554;&#20214;&#12290; CP$^3$&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#21033;&#29992;&#28857;&#20113;&#21644;PNN&#30340;&#29305;&#24615;&#65292;&#20197;&#20351;2D&#36890;&#36947;&#21098;&#26525;&#26041;&#27861;&#36866;&#29992;&#20110;PNN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#22352;&#26631;&#22686;&#24378;&#30340;&#36890;&#36947;&#37325;&#35201;&#24230;&#25351;&#26631;&#65292;&#20197;&#21453;&#26144;&#23610;&#23544;&#20449;&#24687;&#21644;&#20010;&#20307;&#36890;&#36947;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#36890;&#36947;&#21098;&#26525;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Channel pruning can effectively reduce both computational cost and memory footprint of the original network while keeping a comparable accuracy performance. Though great success has been achieved in channel pruning for 2D image-based convolutional networks (CNNs), existing works seldom extend the channel pruning methods to 3D point-based neural networks (PNNs). Directly implementing the 2D CNN channel pruning methods to PNNs undermine the performance of PNNs because of the different representations of 2D images and 3D point clouds as well as the network architecture disparity. In this paper, we proposed CP$^3$, which is a Channel Pruning Plug-in for Point-based network. CP$^3$ is elaborately designed to leverage the characteristics of point clouds and PNNs in order to enable 2D channel pruning methods for PNNs. Specifically, it presents a coordinate-enhanced channel importance metric to reflect the correlation between dimensional information and individual channel features, and it recy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CORA&#26694;&#26550;&#65292;&#36890;&#36807;Region Prompting&#21644;Anchor Pre-Matching&#35299;&#20915;&#20351;&#29992;CLIP&#36827;&#34892;OVD&#35757;&#32451;&#26102;&#36935;&#21040;&#30340;&#20998;&#24067;&#24046;&#24322;&#21644;&#30446;&#26631;&#23450;&#20301;&#31561;&#38590;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.13076</link><description>&lt;p&gt;
CORA&#65306;&#22522;&#20110; Region Prompting &#21644; Anchor Pre-Matching &#30340; CLIP &#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#27169;&#22411;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching. (arXiv:2303.13076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CORA&#26694;&#26550;&#65292;&#36890;&#36807;Region Prompting&#21644;Anchor Pre-Matching&#35299;&#20915;&#20351;&#29992;CLIP&#36827;&#34892;OVD&#35757;&#32451;&#26102;&#36935;&#21040;&#30340;&#20998;&#24067;&#24046;&#24322;&#21644;&#30446;&#26631;&#23450;&#20301;&#31561;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#35789;&#27719;&#26816;&#27979;(OVD)&#26159;&#19968;&#31181;&#26088;&#22312;&#26816;&#27979;&#22522;&#20110;&#23545;&#35937;&#23450;&#20301;&#22120;&#30340;&#26032;&#31867;&#21035;&#30340;&#30446;&#26631;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#30340;OVD&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#26032;&#22411;&#29289;&#20307;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#26816;&#27979;&#22120;&#35757;&#32451;&#26102;&#38656;&#35201;&#20811;&#26381;&#20004;&#20010;&#26680;&#24515;&#38590;&#28857;&#65306;1&#65289;&#24212;&#29992;VL&#27169;&#22411;&#35757;&#32451;&#20840;&#22270;&#20687;&#36827;&#34892;&#21306;&#22495;&#35782;&#21035;&#26102;&#30340;&#20998;&#24067;&#20559;&#24046;&#65307;2&#65289;&#23450;&#20301;&#38750;&#22522;&#30784;&#31867;&#21035;&#29289;&#20307;&#30340;&#22256;&#38590;&#12290;&#38024;&#23545;&#36825;&#20123;&#38590;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;CORA&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;Region prompting&#21644;Anchor pre-matching&#25216;&#26415;&#23558;CLIP&#25913;&#36827;&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;OVD&#30340;DETR&#39118;&#26684;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories beyond the base categories on which the detector is trained. Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detector training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adapts CLIP for Open-vocabulary detection by Region prompting and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the region features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localization by a class-aware matching mechanism. We evaluate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#26631;&#27880;&#26631;&#31614;&#65288;PCL&#65289;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20013;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#23545;&#35937;&#23454;&#20363;&#30340;&#23383;&#24149;&#26631;&#31614;&#65292;&#20197;&#25552;&#21462;&#20851;&#20110;&#26032;&#39062;&#23545;&#35937;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.13040</link><description>&lt;p&gt;
&#21033;&#29992;&#20266;&#26631;&#27880;&#26631;&#31614;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Object Detection using Pseudo Caption Labels. (arXiv:2303.13040v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20266;&#26631;&#27880;&#26631;&#31614;&#65288;PCL&#65289;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20013;&#20351;&#29992;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#23545;&#35937;&#23454;&#20363;&#30340;&#23383;&#24149;&#26631;&#31614;&#65292;&#20197;&#25552;&#21462;&#20851;&#20110;&#26032;&#39062;&#23545;&#35937;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20174;&#35757;&#32451;&#22312;&#22823;&#37327;&#22270;&#20687;&#25991;&#26412;&#23545;&#19978;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#33976;&#39311;&#30693;&#35782;&#26469;&#26816;&#27979;&#26032;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#23545;&#35937;&#31867;&#30340;&#22823;&#35789;&#27719;&#25968;&#25454;&#38598;&#65292;&#20197;&#20551;&#35774;&#36825;&#26679;&#30340;&#25968;&#25454;&#23558;&#20351;&#27169;&#22411;&#25552;&#21462;&#20851;&#20110;&#21508;&#31181;&#23545;&#35937;&#20043;&#38388;&#20851;&#31995;&#30340;&#32508;&#21512;&#30693;&#35782;&#65292;&#24182;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#26631;&#31614;&#26469;&#25552;&#21462;&#20016;&#23500;&#30340;&#26377;&#20851;&#26032;&#23545;&#35937;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23545;&#35937;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23427;&#20204;&#30340;&#21517;&#31216;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20266;&#26631;&#27880;&#26631;&#31614;&#65288;PCL&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#29983;&#25104;&#25551;&#36848;&#26469;&#33258;&#19981;&#21516;&#35282;&#24230;&#30340;&#23545;&#35937;&#23454;&#20363;&#30340;&#23383;&#24149;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20266;&#23383;&#24149;&#26631;&#31614;&#20026;&#30693;&#35782;&#25552;&#20379;&#20102;&#23494;&#38598;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs. To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen object classes. In this study, we argue that more fine-grained labels are necessary to extract richer knowledge about novel objects, including object attributes and relationships, in addition to their names. To address this challenge, we propose a simple and effective method named Pseudo Caption Labeling (PCL), which utilizes an image captioning model to generate captions that describe object instances from diverse perspectives. The resulting pseudo caption labels offer dense samples for knowledge di
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13034</link><description>&lt;p&gt;
&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13034
&lt;/p&gt;
&lt;p&gt;
PAC-MOO&#26159;&#19968;&#20010;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22312;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#19979;&#65292;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#37096;&#20998;&#36755;&#20837;&#31354;&#38388;&#26159;&#19981;&#21487;&#34892;&#65288;&#21363;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#65289;&#26102;&#65292;&#22522;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#21644;&#20174;&#19994;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#20559;&#22909;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#35768;&#22810;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#37117;&#23384;&#22312;&#65292;&#21253;&#25324;&#27169;&#25311;&#30005;&#36335;&#21644;&#30005;&#21147;&#31995;&#32479;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#24635;&#20307;&#30446;&#26631;&#26159;&#22312;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#19978;&#36817;&#20284;&#26368;&#20248;Pareto&#38598;&#21512;&#12290;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#35774;&#35745;&#31354;&#38388;&#30340;&#24040;&#22823;&#22823;&#23567;&#12289;&#22810;&#20010;&#30446;&#26631;&#21644;&#22823;&#37327;&#30340;&#32422;&#26463;&#26465;&#20214;&#20197;&#21450;&#21482;&#33021;&#22312;&#36827;&#34892;&#26114;&#36149;&#30340;&#20223;&#30495;&#21518;&#25165;&#33021;&#30830;&#35748;&#30340;&#21487;&#34892;&#30340;&#36755;&#20837;&#35774;&#35745;&#30340;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#20559;&#22909;&#24863;&#30693;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65288;PAC-MOO&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23398;&#20064;&#36755;&#20986;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20174;&#19994;&#32773;&#39044;&#27979;&#30340;&#20559;&#22909;&#36873;&#25321;&#35780;&#20272;&#30446;&#26631;&#30340;&#20505;&#36873;&#36755;&#20837;&#12290;PAC-MOO&#26681;&#25454;&#39044;&#27979;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#32852;&#21512;&#20559;&#22909;&#36845;&#20195;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#35201;&#27169;&#25311;&#30340;&#36755;&#20837;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#26032;&#33719;&#24471;&#30340;&#25968;&#25454;&#26356;&#26032;&#20195;&#29702;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tol-FL&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#21464;&#24322;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13015</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#23481;&#24525;&#25925;&#38556;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks. (arXiv:2303.13015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Tol-FL&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#30340;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#30340;&#21464;&#24322;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20998;&#24067;&#24335;&#25216;&#26415;&#30340;&#20998;&#26512;&#37117;&#26159;&#19987;&#27880;&#20110;&#23427;&#20204;&#30340;&#25928;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65288;&#25110;&#32570;&#20047;&#40065;&#26834;&#24615;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32771;&#34385;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#24403;&#35774;&#22791;&#25110;&#20013;&#22830;&#26381;&#21153;&#22120;&#20986;&#29616;&#25925;&#38556;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#30251;&#30186;&#20998;&#24067;&#24335;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#25153;&#24179;&#21644;&#26143;&#22411;&#25299;&#25169;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20123;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20004;&#32773;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;Tol-FL&#8221;&#65292;&#22240;&#20026;&#19982;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#23427;&#30340;&#25925;&#38556;&#23481;&#38169;&#33021;&#21147;&#25552;&#39640;&#20102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#25925;&#38556;&#30340;&#21508;&#31181;&#36924;&#30495;&#24773;&#20917;&#19979;&#65292;&#22312;&#24322;&#24120;&#26816;&#27979;AUROC&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#39640;&#36798;8&#65285;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35774;&#22791;&#25925;&#38556;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of distributed techniques is often focused upon their efficiency, without considering their robustness (or lack thereof). Such a consideration is particularly important when devices or central servers can fail, which can potentially cripple distributed systems. When such failures arise in wireless communications networks, important services that they use/provide (like anomaly detection) can be left inoperable and can result in a cascade of security problems. In this paper, we present a novel method to address these risks by combining both flat- and star-topologies, combining the performance and reliability benefits of both. We refer to this method as "Tol-FL", due to its increased failure-tolerance as compared to the technique of Federated Learning. Our approach both limits device failure risks while outperforming prior methods by up to 8% in terms of anomaly detection AUROC in a range of realistic settings that consider client as well as server failure, all while reducing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13010</link><description>&lt;p&gt;
&#29992;&#20110;&#35270;&#35273;&#27169;&#22411;&#35786;&#26029;&#30340;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Semantic Image Attack for Visual Model Diagnosis. (arXiv:2303.13010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#21487;&#20197;&#25552;&#20379;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#29305;&#23450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#36827;&#34892;&#24230;&#37327;&#20998;&#26512;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#25110;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#37096;&#20998;&#21407;&#22240;&#26159;&#65292;&#33719;&#24471;&#24179;&#34913;&#12289;&#22810;&#26679;&#21644;&#26631;&#35760;&#23436;&#32654;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#12289;&#32791;&#26102;&#21644;&#26131;&#20986;&#38169;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#8212;&#8212;&#35821;&#20041;&#22270;&#20687;&#25915;&#20987;&#65288;SIA&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#35821;&#20041;&#23545;&#25239;&#22270;&#20687;&#65292;&#20197;&#20415;&#36827;&#34892;&#27169;&#22411;&#35786;&#26029;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#26041;&#27861;&#19981;&#32467;&#21512;&#20004;&#20010;&#26041;&#38754;&#65292;&#26080;&#27861;&#35299;&#37322;&#21644;&#20998;&#26512;&#27169;&#22411;&#30340;&#32570;&#38519;&#65306;&#35821;&#20041;&#21487;&#36861;&#28335;&#24615;&#21644;&#24863;&#35273;&#36136;&#37327;&#12290;SIA&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#35821;&#20041;&#23646;&#24615;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#19978;&#30340;&#36845;&#20195;&#26799;&#24230;&#19978;&#21319;&#32467;&#21512;&#20102;&#20004;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SIA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone. Rather than relying on a carefully designed test set to assess ML models' failures, fairness, or robustness, this paper proposes Semantic Image Attack (SIA), a method based on the adversarial attack that provides semantic adversarial images to allow model diagnosis, interpretability, and robustness. Traditional adversarial training is a popular methodology for robustifying ML models against attacks. However, existing adversarial methods do not combine the two aspects that enable the interpretation and analysis of the model's flaws: semantic traceability and perceptual quality. SIA combines the two features via iterative gradient ascent on a predefined semantic attribute space and the image space. We illustrate the validi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#65292;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2303.12992</link><description>&lt;p&gt;
&#21382;&#21490;&#23398;&#20064;&#32508;&#36848;: &#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Survey of Historical Learning: Learning Models with Learning History. (arXiv:2303.12992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#65292;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#30340;&#30693;&#35782;&#28304;&#20110;&#26087;&#30340;&#30693;&#35782;&#12290;&#22312;&#35757;&#32451;&#21382;&#21490;&#35760;&#24405;&#20013;&#23384;&#20648;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#20803;&#32032;&#23545;&#20110;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#8220;&#21382;&#21490;&#23398;&#20064;&#65306;&#24102;&#26377;&#23398;&#20064;&#21382;&#21490;&#30340;&#23398;&#20064;&#27169;&#22411;&#8221;&#36825;&#20010;&#20027;&#39064;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20174;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#20013;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#21382;&#21490;&#31867;&#22411;&#12289;&#21151;&#33021;&#37096;&#20998;&#21644;&#23384;&#20648;&#24418;&#24335;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#39318;&#20010;&#31995;&#32479;&#30740;&#31350;&#21033;&#29992;&#21508;&#31181;&#21382;&#21490;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32508;&#36848;&#35770;&#25991;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#19982;&#27492;&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#22914;&#24490;&#29615;/&#35760;&#24518;&#32593;&#32476;&#12289;&#38598;&#25104;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25351;&#20986;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#26410;&#26469;&#25361;&#25112;&#65292;&#24182;&#40723;&#21169;&#23398;&#26415;&#30028;&#22312;&#35774;&#35745;&#31639;&#27861;&#26102;&#35748;&#30495;&#24605;&#32771;&#21382;&#21490;&#23398;&#20064;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
New knowledge originates from the old. The various types of elements, deposited in the training history, are a large amount of wealth for improving learning deep models. In this survey, we comprehensively review and summarize the topic--``Historical Learning: Learning Models with Learning History'', which learns better neural models with the help of their learning history during its optimization, from three detailed aspects: Historical Type (what), Functional Part (where) and Storage Form (how). To our best knowledge, it is the first survey that systematically studies the methodologies which make use of various historical statistics when training deep neural networks. The discussions with related topics like recurrent/memory networks, ensemble learning, and reinforcement learning are demonstrated. We also expose future challenges of this topic and encourage the community to pay attention to the think of historical learning principles when designing algorithms. The paper list related to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.12973</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#21453;&#20107;&#23454;&#20542;&#21521;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Calibration for Counterfactual Propensity Estimation in Recommendation. (arXiv:2303.12973v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#26657;&#20934;&#21518;&#30340;IPS&#20272;&#35745;&#22120;&#22312;Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#36873;&#25321;&#20559;&#24046;&#65292;&#35768;&#22810;&#35780;&#20998;&#20449;&#24687;&#37117;&#20002;&#22833;&#20102;&#65292;&#36825;&#34987;&#31216;&#20026;&#38750;&#38543;&#26426;&#32570;&#22833;&#12290;&#21453;&#20107;&#23454;&#36870;&#20542;&#21521;&#35780;&#20998;&#65288;IPS&#65289;&#34987;&#29992;&#20110;&#34913;&#37327;&#27599;&#20010;&#35266;&#23519;&#21040;&#30340;&#35780;&#20998;&#30340;&#22635;&#20805;&#38169;&#35823;&#12290;&#34429;&#28982;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;IPS&#20272;&#35745;&#30340;&#24615;&#33021;&#21463;&#21040;&#20542;&#21521;&#24615;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#20195;&#34920;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#25216;&#26415;&#65292;&#20197;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#20013;&#20542;&#21521;&#24615;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#36890;&#36807;&#23545;&#20559;&#35823;&#21644;&#25512;&#24191;&#30028;&#38480;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#20248;&#20110;&#26410;&#26657;&#20934;&#30340;IPS&#20272;&#35745;&#22120;&#12290; Coat&#21644;yahoo&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#24471;&#21040;&#25913;&#36827;&#65292;&#20174;&#32780;&#20351;&#25512;&#33616;&#32467;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems, a large portion of the ratings are missing due to the selection biases, which is known as Missing Not At Random. The counterfactual inverse propensity scoring (IPS) was used to weight the imputation error of every observed rating. Although effective in multiple scenarios, we argue that the performance of IPS estimation is limited due to the uncertainty miscalibration of propensity estimation. In this paper, we propose the uncertainty calibration for the propensity estimation in recommendation systems with multiple representative uncertainty calibration techniques. Theoretical analysis on the bias and generalization bound shows the superiority of the calibrated IPS estimator over the uncalibrated one. Experimental results on the coat and yahoo datasets shows that the uncertainty calibration is improved and hence brings the better recommendation results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12964</link><description>&lt;p&gt;
&#36830;&#32493;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CIPNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#23548;&#20986;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#35299;&#26512;&#35299;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;CIPAE&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CIPNN&#65288;Continuous Indeterminate Probability Neural Network&#65289;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;IPNN&#65292;&#29992;&#20110;&#31163;&#25955;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#12290;&#30446;&#21069;&#65292;&#36830;&#32493;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;IPNN&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#22235;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#36830;&#32493;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#21518;&#39564;&#35745;&#31639;&#30340;&#35299;&#26512;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27169;&#22411;&#65288;CIPNN&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#32534;&#30721;&#22120;&#8212;&#8212;CIPAE&#65288;Continuous Indeterminate Probability Auto-Encoder&#65289;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#37096;&#20998;&#19981;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26159;&#31532;&#19968;&#27425;&#20351;&#29992;&#20840;&#27010;&#29575;&#25512;&#29702;&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#28508;&#22312;&#38543;&#26426;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;N&#32500;&#28508;&#22312;&#21464;&#37327;&#20043;&#19968;&#20316;&#20026;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#36755;&#20837;&#22270;&#20687;&#65292;&#21363;&#20351;&#26159;&#20998;&#31867;&#20219;&#21153;&#20063;&#33021;&#36798;&#21040;&#25928;&#26524;&#65292;&#36825;&#26679;&#65292;&#25105;&#20204;&#21487;&#20197;&#30475;&#21040;&#27599;&#20010;&#28508;&#22312;&#21464;&#37327;&#20195;&#34920;&#20160;&#20040;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#36890;&#36807;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2303.12961</link><description>&lt;p&gt;
&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#30340;&#19981;&#31283;&#23450;&#22522;&#30784;&#65306;&#38024;&#23545; EMR &#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22823;&#22810;&#33539;&#22260;&#26377;&#38480;&#12289;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#19988;&#35780;&#20272;&#25351;&#26631;&#26410;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#36129;&#29486;&#25552;&#20379;&#26377;&#24847;&#20041;&#35265;&#35299;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284; ChatGPT &#21644; AlphaFold &#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#26500;&#24314;&#31867;&#20284;&#27169;&#22411;&#20197;&#25913;&#21892; EMR&#65288;&#30005;&#23376;&#30149;&#21382;&#65289;&#20197;&#25552;&#39640;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#38498;&#36816;&#33829;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#28818;&#20316;&#25513;&#30422;&#20102;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#20851;&#38190;&#32570;&#22833;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#36807;80&#20010;&#22312;&#38750;&#25104;&#20687; EMR &#25968;&#25454;&#65288;&#21363;&#20020;&#24202;&#25991;&#26412;&#21644;/&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#35828;&#26126;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#28508;&#22312;&#29992;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#26159;&#22312;&#23567;&#22411;&#12289;&#33539;&#22260;&#26377;&#38480;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;MIMIC-III&#65289;&#25110;&#24191;&#27867;&#30340;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#65288;&#20363;&#22914;PubMed&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#25552;&#20379;&#23545;&#20854;&#23545;&#21307;&#30103;&#31995;&#32479;&#26377;&#29992;&#22788;&#30340;&#26377;&#24847;&#20041;&#35265;&#35299;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#25509;&#36817;&#20110;&#21307;&#30103;&#20445;&#20581;&#37325;&#35201;&#25351;&#26631;&#30340;&#21307;&#30103;&#22522;&#30784;&#27169;&#22411;&#25928;&#30410;&#30340;&#25913;&#36827;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12959</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21435;&#32416;&#32544;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#24179;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#21435;&#32416;&#32544;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#26435;&#34913;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20043;&#21069;&#20165;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#36880;&#27493;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25193;&#23637;&#20102;&#20449;&#24687;&#29942;&#39048;&#65292;&#20197;&#20174;&#21435;&#32416;&#32544;&#21040;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#29942;&#39048;&#20250;&#22833;&#21435;&#21435;&#32416;&#32544;&#30340;&#32422;&#26463;&#65292;&#23548;&#33268;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#27493;&#20943;&#23569;&#20449;&#24687;&#29942;&#39048;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#21435;&#32416;&#32544;&#19981;&#21464;&#21464;&#25442;&#26469;&#20248;&#21270;&#19981;&#21516;&#23618;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#31216;&#20026;DeVAE&#12290;&#36890;&#36807;&#36880;&#28176;&#20943;&#23567;&#19981;&#21516;&#28508;&#22312;&#31354;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#65292;DeVAE &#24179;&#34913;&#20102;&#21435;&#32416;&#32544;&#21644;&#37325;&#26500;&#20445;&#30495;&#24230;&#12290;&#30001;&#20110;&#20855;&#26377;&#22810;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;DeVAE &#20801;&#35768;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#20197;&#22312;&#20445;&#25345;&#21435;&#32416;&#32544;&#32422;&#26463;&#30340;&#21516;&#26102;&#20248;&#21270;&#37325;&#26500;&#65292;&#36991;&#20813;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12957</link><description>&lt;p&gt;
&#20855;&#26377;&#22806;&#37096;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12957
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;MDP&#20998;&#35299;&#20026;&#22806;&#29983;&#21644;&#20869;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65292;&#22312;&#29366;&#24577;&#31354;&#38388;&#30340;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#27809;&#26377;&#20107;&#20808;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#27491;&#30830;&#30340;&#31639;&#27861;&#36827;&#34892;&#33258;&#21160;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#20250;&#36890;&#36807;&#21521;&#22870;&#21169;&#20449;&#21495;&#27880;&#20837;&#19981;&#21487;&#25511;&#30340;&#21464;&#21270;&#32780;&#20943;&#24930;&#24378;&#21270;&#23398;&#20064;&#30340;&#36895;&#24230;&#12290;&#26412;&#25991;&#23545;&#22806;&#37096;&#29366;&#24577;&#21464;&#37327;&#21644;&#22870;&#21169;&#36827;&#34892;&#20102;&#27491;&#24335;&#21270;&#65292;&#24182;&#34920;&#26126;&#22914;&#26524;&#22870;&#21169;&#20989;&#25968;&#21152;&#27861;&#20998;&#35299;&#25104;&#20869;&#29983;&#21644;&#22806;&#29983;&#20004;&#20010;&#37096;&#20998;&#65292;MDP&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#22806;&#29983;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#36807;&#31243;&#65288;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#65289;&#21644;&#19968;&#20010;&#20869;&#29983;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#20248;&#21270;&#20869;&#29983;&#22870;&#21169;&#65289;&#12290;&#20869;&#29983;MDP&#30340;&#20219;&#20309;&#26368;&#20248;&#31574;&#30053;&#20063;&#26159;&#21407;&#22987;MDP&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;&#20869;&#29983;&#22870;&#21169;&#36890;&#24120;&#20855;&#26377;&#38477;&#20302;&#30340;&#26041;&#24046;&#65292;&#22240;&#27492;&#20869;&#29983;MDP&#26356;&#23481;&#26131;&#27714;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#20026;&#20869;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#32780;&#36825;&#31181;&#29366;&#24577;&#31354;&#38388;&#20998;&#35299;&#24182;&#27809;&#26377;&#32473;&#20986;&#65292;&#32780;&#26159;&#24517;&#39035;&#21457;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#32452;&#21512;&#19979;&#21457;&#29616;&#20869;&#29983;&#21644;&#22806;&#29983;&#29366;&#24577;&#31354;&#38388;&#30340;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#25552;&#39640;&#37329;&#34701;&#26381;&#21153;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#23454;&#29616;&#36825;&#20123;&#25216;&#26415;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12944</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#20445;&#38556;&#37329;&#34701;&#26381;&#21153;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Use of Federated Learning and Blockchain towards Securing Financial Services. (arXiv:2303.12944v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#25552;&#39640;&#37329;&#34701;&#26381;&#21153;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#23454;&#29616;&#36825;&#20123;&#25216;&#26415;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29616;&#26377;&#21644;&#26032;&#30340;&#32593;&#32476;&#25915;&#20987;&#30340;&#22823;&#37327;&#20986;&#29616;&#23545;&#37329;&#34701;&#26381;&#21153;&#30340;&#31283;&#23450;&#24615;&#26500;&#25104;&#20102;&#26681;&#26412;&#24615;&#30340;&#23041;&#32961;&#12290;&#24456;&#38590;&#39044;&#27979;&#33021;&#22815;&#24341;&#21457;&#20005;&#37325;&#37329;&#34701;&#21361;&#26426;&#30340;&#25915;&#20987;&#30340;&#24615;&#36136;&#12290;&#37329;&#34701;&#26381;&#21153;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#25968;&#23383;&#21270;&#36716;&#22411;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#24471;&#21040;&#21152;&#36895;&#65292;&#24182;&#20173;&#22312;&#25345;&#32493;&#36827;&#34892;&#20013;&#12290;&#25915;&#20987;&#32773;&#27491;&#22312;&#21033;&#29992;&#36825;&#19968;&#36716;&#22411;&#65292;&#23545;&#37329;&#34701;&#31283;&#23450;&#24615;&#21644;&#23436;&#25972;&#24615;&#26500;&#25104;&#26032;&#30340;&#20840;&#29699;&#23041;&#32961;&#12290;&#35768;&#22810;&#22823;&#22411;&#26426;&#26500;&#27491;&#22312;&#20174;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;CeFi&#65289;&#36716;&#21521;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#65292;&#22240;&#20026;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#20855;&#26377;&#35768;&#22810;&#20248;&#21183;&#12290;&#21306;&#22359;&#38142;&#21487;&#20197;&#23545;&#37329;&#34701;&#19994;&#30340;&#21487;&#20449;&#24230;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35775;&#38382;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#24320;&#25918;&#24615;&#24102;&#26469;&#37325;&#22823;&#21644;&#28145;&#36828;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#21644;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#22312;&#37329;&#34701;&#26381;&#21153;&#20013;&#20351;&#29992;&#12290;&#23427;&#20174;&#26368;&#36817;&#30340;&#20004;&#20010;&#24212;&#29992;&#26696;&#20363;&#30340;&#27010;&#36848;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#22914;&#20309;&#25552;&#39640;&#37329;&#34701;&#26381;&#21153;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#38416;&#36848;&#20102;&#22312;&#23454;&#26045;&#36825;&#20123;&#25216;&#26415;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#24182;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent days, the proliferation of several existing and new cyber-attacks pose an axiomatic threat to the stability of financial services. It is hard to predict the nature of attacks that can trigger a serious financial crisis. The unprecedented digital transformation to financial services has been accelerated during the COVID-19 pandemic and it is still ongoing. Attackers are taking advantage of this transformation and pose a new global threat to financial stability and integrity. Many large organizations are switching from centralized finance (CeFi) to decentralized finance (DeFi) because decentralized finance has many advantages. Blockchain can bring big and far-reaching effects on the trustworthiness, safety, accessibility, cost-effectiveness, and openness of the financial sector. The present paper gives an in-depth look at how blockchain and federated learning (FL) are used in financial services. It starts with an overview of recent developments in both use cases. This paper exp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#30340;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.12942</link><description>&lt;p&gt;
&#35770;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Artificial Intelligence for Network Cybersecurity. (arXiv:2303.12942v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12942
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#30340;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#19968;&#30452;&#26159;&#29992;&#20110;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#30340;&#35768;&#22810;&#20851;&#27880;&#28857;&#30340;&#26469;&#28304;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#20026;&#20854;&#20915;&#31574;&#21644;&#34892;&#21160;&#25552;&#20379;&#28165;&#26224;&#21487;&#35299;&#37322;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;XAI&#20855;&#26377;&#36890;&#36807;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#23041;&#32961;&#30340;&#34892;&#20026;&#24182;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#38450;&#24481;&#26469;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#22788;&#29702;&#32593;&#32476;&#23433;&#20840;&#30340;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;XAI&#26041;&#27861;&#22312;&#32593;&#32476;&#23433;&#20840;&#32972;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#21246;&#30011;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of network cybersecurity, XAI has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of network-driven cybersecurity threats and issues. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#22312;&#31038;&#21306;&#23398;&#38498;&#30340;&#23454;&#38469;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#23454;&#26102;&#39640;&#20934;&#30830;&#24230;&#30340;&#35270;&#39057;&#20998;&#26512;&#22788;&#29702;&#12289;&#20113;&#31995;&#32479;&#22522;&#30784;&#26550;&#26500;&#21644;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#21457;&#12289;&#20197;&#21450;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.12934</link><description>&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#22522;&#20110;&#31038;&#21306;&#30340;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511; &#8212;&#8212; &#19968;&#25152;&#31038;&#21306;&#23398;&#38498;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Real-World Community-in-the-Loop Smart Video Surveillance -- A Case Study at a Community College. (arXiv:2303.12934v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#31038;&#21306;&#30340;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#22312;&#31038;&#21306;&#23398;&#38498;&#30340;&#23454;&#38469;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#35774;&#35745;&#21644;&#37096;&#32626;&#65292;&#30528;&#37325;&#35299;&#20915;&#20102;&#23454;&#26102;&#39640;&#20934;&#30830;&#24230;&#30340;&#35270;&#39057;&#20998;&#26512;&#22788;&#29702;&#12289;&#20113;&#31995;&#32479;&#22522;&#30784;&#26550;&#26500;&#21644;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#21457;&#12289;&#20197;&#21450;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#36817;&#26469;&#22312;&#20445;&#38556;&#20844;&#20849;&#23433;&#20840;&#21644;&#27835;&#23433;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#32467;&#21512;&#20302;&#24310;&#36831;&#30340;&#36890;&#30693;&#21644;&#25253;&#35686;&#24050;&#32463;&#20351;&#24471;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#21464;&#24471;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#31038;&#21306;&#23398;&#38498;&#30340;&#23454;&#38469;&#27979;&#35797;&#24179;&#21488;&#19978;&#35774;&#35745;&#21644;&#37096;&#32626;&#26234;&#33021;&#35270;&#39057;&#30417;&#25511;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#26234;&#33021;&#25668;&#20687;&#22836;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#21487;&#30097;/&#24322;&#24120;&#27963;&#21160;&#24182;&#31435;&#21363;&#36890;&#30693;&#30456;&#20851;&#20154;&#21592;&#21644;&#23621;&#27665;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#21644;&#35299;&#20915;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#35774;&#35745;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#27979;&#35797;&#24179;&#21488;&#20013;&#23454;&#26102;&#39640;&#20934;&#30830;&#24230;&#30340;&#35270;&#39057;&#20998;&#26512;&#22788;&#29702;&#12290;&#23427;&#36824;&#20171;&#32461;&#20102;&#20113;&#31995;&#32479;&#22522;&#30784;&#26550;&#26500;&#21644;&#31227;&#21160;&#24212;&#29992;&#20197;&#36827;&#34892;&#23454;&#26102;&#36890;&#30693;&#65292;&#20197;&#20445;&#25345;&#23398;&#29983;&#12289;&#25945;&#32844;&#21592;&#24037;&#21644;&#36127;&#36131;&#27835;&#23433;&#20154;&#21592;&#30340;&#21442;&#19982;&#12290;&#21516;&#26102;&#65292;&#23427;&#28085;&#30422;&#20102;&#31995;&#32479;&#35774;&#35745;&#26041;&#38754;&#30340;&#32771;&#34385;&#65292;&#20363;&#22914;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart Video surveillance systems have become important recently for ensuring public safety and security, especially in smart cities. However, applying real-time artificial intelligence technologies combined with low-latency notification and alarming has made deploying these systems quite challenging. This paper presents a case study for designing and deploying smart video surveillance systems based on a real-world testbed at a community college. We primarily focus on a smart camera-based system that can identify suspicious/abnormal activities and alert the stakeholders and residents immediately. The paper highlights and addresses different algorithmic and system design challenges to guarantee real-time high-accuracy video analytics processing in the testbed. It also presents an example of cloud system infrastructure and a mobile application for real-time notification to keep students, faculty/staff, and responsible security personnel in the loop. At the same time, it covers the design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12922</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#22312;&#38750;&#20984;&#26465;&#20214;&#19979;&#20351;&#29992;&#28145;&#23618;&#27169;&#22411;&#21644;&#26356;&#22797;&#26434;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26377;&#24456;&#22810;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#39564;&#35777;&#36825;&#20123;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#25110;&#21487;&#20449;&#24230;&#12290;&#26368;&#36817;&#65292;&#24433;&#21709;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#20010;&#26679;&#26412;&#19978;&#30340;&#28789;&#25935;&#24230;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#24433;&#21709;&#20989;&#25968;&#26131;&#21463;&#22122;&#22768;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#23545;&#31216;&#24615;&#24433;&#21709;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#24433;&#21709;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25506;&#31350;&#24433;&#21709;&#20989;&#25968;&#32972;&#21518;&#30340;&#26426;&#29702;&#65292;&#20174;&#32780;&#20026;&#22686;&#24378;&#24433;&#21709;&#20989;&#25968;&#30340;&#40065;&#26834;&#24615;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#24182;&#35777;&#26126;&#19968;&#20123;&#31995;&#32479;&#20855;&#22791;&#36275;&#22815;&#39640;&#25928;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#26469;&#23436;&#25104;&#31435;&#20307;&#30456;&#26426;&#22810;&#35270;&#39057;&#21516;&#27493;&#20219;&#21153;&#12290;&#36825;&#20010;&#25216;&#26415;&#23558;&#26377;&#26395;&#38477;&#20302;&#25972;&#20010;&#31995;&#32479;&#30340;&#25104;&#26412;&#12289;&#37325;&#37327;&#21644;&#22823;&#23567;&#65292;&#24182;&#20801;&#35768;&#22312;&#24314;&#31435;&#36825;&#26679;&#30340;&#31995;&#32479;&#26102;&#26356;&#21152;&#28789;&#27963;&#12290;</title><link>http://arxiv.org/abs/2303.12916</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31435;&#20307;&#30456;&#26426;&#22810;&#35270;&#39057;&#21516;&#27493;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based stereo camera multi-video synchronization. (arXiv:2303.12916v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#24182;&#35777;&#26126;&#19968;&#20123;&#31995;&#32479;&#20855;&#22791;&#36275;&#22815;&#39640;&#25928;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#26469;&#23436;&#25104;&#31435;&#20307;&#30456;&#26426;&#22810;&#35270;&#39057;&#21516;&#27493;&#20219;&#21153;&#12290;&#36825;&#20010;&#25216;&#26415;&#23558;&#26377;&#26395;&#38477;&#20302;&#25972;&#20010;&#31995;&#32479;&#30340;&#25104;&#26412;&#12289;&#37325;&#37327;&#21644;&#22823;&#23567;&#65292;&#24182;&#20801;&#35768;&#22312;&#24314;&#31435;&#36825;&#26679;&#30340;&#31995;&#32479;&#26102;&#26356;&#21152;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#20307;&#35270;&#35273;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#20004;&#20010;&#25668;&#20687;&#22836;&#20256;&#26469;&#30340;&#35270;&#27969;&#21516;&#27493;&#20027;&#35201;&#26159;&#20351;&#29992;&#30828;&#20214;&#26469;&#23436;&#25104;&#30340;&#12290;&#22522;&#20110;&#36719;&#20214;&#30340;&#21516;&#27493;&#26041;&#27861;&#23558;&#20250;&#38477;&#20302;&#25972;&#20010;&#31995;&#32479;&#30340;&#25104;&#26412;&#12289;&#37325;&#37327;&#21644;&#22823;&#23567;&#65292;&#24182;&#20801;&#35768;&#22312;&#24314;&#31435;&#36825;&#26679;&#30340;&#31995;&#32479;&#26102;&#26356;&#21152;&#28789;&#27963;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#31995;&#32479;&#36275;&#22815;&#39640;&#25928;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20026;&#29983;&#20135;&#20934;&#22791;&#23601;&#32490;&#30340;&#22522;&#20110;&#36719;&#20214;&#30340;&#35270;&#39057;&#21516;&#27493;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereo vision is essential for many applications. Currently, the synchronization of the streams coming from two cameras is done using mostly hardware. A software-based synchronization method would reduce the cost, weight and size of the entire system and allow for more flexibility when building such systems. With this goal in mind, we present here a comparison of different deep learning-based systems and prove that some are efficient and generalizable enough for such a task. This study paves the way to a production ready software-based video synchronization system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#19977;&#31181;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#26159;&#26368;&#20026;&#39640;&#25928;&#30340;&#65292;&#20165;&#29992;&#26368;&#20339;&#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#27169;&#22411;55%&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12891</link><description>&lt;p&gt;
&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems. (arXiv:2303.12891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#19977;&#31181;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#26159;&#26368;&#20026;&#39640;&#25928;&#30340;&#65292;&#20165;&#29992;&#26368;&#20339;&#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#27169;&#22411;55%&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26681;&#25454;&#25910;&#38598;&#21040;&#30340;&#35745;&#31639;&#26426;&#21644;&#32593;&#32476;&#25968;&#25454;&#26816;&#27979;&#21644;&#38450;&#27490;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;IDS&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26500;&#24314;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#29305;&#24449;&#36873;&#25321;&#65288;FS&#65289;&#21487;&#29992;&#20110;&#26500;&#24314;&#26356;&#24555;&#65292;&#26356;&#21487;&#35299;&#37322;&#21644;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;FS&#25216;&#26415;&#65307; &#38543;&#26426;&#26862;&#26519;&#20449;&#24687;&#22686;&#30410;&#65288;RF-IG&#65289;&#65292;&#20351;&#29992;&#34649;&#34656;&#31639;&#27861;&#30340;&#30456;&#20851;&#29305;&#24449;&#36873;&#25321;&#65288;CFS-BA&#65289;&#21644;&#20351;&#29992;&#38463;&#22522;&#25289;&#20248;&#21270;&#22120;&#30340;CFS&#65288;CFS-AO&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;CFS-BA&#26159;&#26368;&#26377;&#25928;&#30340;FS&#26041;&#27861;&#65292;&#20165;&#29992;&#26368;&#20339;RF-IG&#27169;&#22411;55&#65285;&#30340;&#26102;&#38388;&#26500;&#24314;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;99.99&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#21152;&#24378;&#20102;&#20808;&#21069;&#23545;CFS-BA&#20934;&#30830;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#26368;&#32456;&#32467;&#26524;&#20013;&#24314;&#31435;&#20102;&#23376;&#38598;&#22823;&#23567;&#65292;CFS&#24471;&#20998;&#21644;RF-IG&#24471;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of cybersecurity, intrusion detection systems (IDS) detect and prevent attacks based on collected computer and network data. In recent research, IDS models have been constructed using machine learning (ML) and deep learning (DL) methods such as Random Forest (RF) and deep neural networks (DNN). Feature selection (FS) can be used to construct faster, more interpretable, and more accurate models. We look at three different FS techniques; RF information gain (RF-IG), correlation feature selection using the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our results show CFS-BA to be the most efficient of the FS methods, building in 55% of the time of the best RF-IG model while achieving 99.99% of its accuracy. This reinforces prior contributions attesting to CFS-BA's accuracy while building upon the relationship between subset size, CFS score, and RF-IG score in final results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.12888</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#24577;&#39118;&#38505;&#35780;&#20998;&#25552;&#21069;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;
&lt;/p&gt;
&lt;p&gt;
A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;CShock&#65292;&#26088;&#22312;&#38024;&#23545;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#39044;&#27979;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#32908;&#26775;&#27515;&#21644;&#24515;&#21147;&#34928;&#31469;&#26159;&#20027;&#35201;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#32654;&#22269;&#25968;&#30334;&#19975;&#20154;&#30340;&#20581;&#24247;&#12290;&#21457;&#23637;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#24739;&#32773;&#20013;&#65292;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#23454;&#26045;&#27835;&#30103;&#25514;&#26045;&#21487;&#20197;&#38450;&#27490;&#32570;&#34880;&#12289;&#20302;&#34880;&#21387;&#20197;&#21450;&#30001;&#20110;&#24515;&#28304;&#24615;&#20241;&#20811;&#23548;&#33268;&#24515;&#36755;&#20986;&#37327;&#38477;&#20302;&#30340;&#26377;&#23475;&#24490;&#29615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#20013;&#28023;&#37327;&#25968;&#25454;&#30340;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#19982;&#32570;&#20047;&#26377;&#25928;&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#23545;&#24515;&#28304;&#24615;&#20241;&#20811;&#30340;&#26089;&#26399;&#35782;&#21035;&#19968;&#30452;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#31216;&#20026;CShock&#30340;&#39118;&#38505;&#20998;&#23618;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#20837;&#20303;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#30340;&#24613;&#24615;&#22833;&#20195;&#20607;&#24615;&#24515;&#21147;&#34928;&#31469;&#21644;/&#25110;&#24515;&#32908;&#26775;&#27515;&#24739;&#32773;&#30340;&#24515;&#28304;&#24615;&#20241;&#20811;&#21457;&#20316;&#12290;&#20026;&#20102;&#24320;&#21457;&#21644;&#39564;&#35777;CShock&#65292;&#25105;&#20204;&#20351;&#29992;&#30001;&#21307;&#24072;&#35009;&#23450;&#30340;&#32467;&#26524;&#27880;&#37322;&#20102;&#24515;&#33039;&#30417;&#25252;&#30149;&#25151;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#23545;&#27010;&#24565;&#39537;&#21160;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12872</link><description>&lt;p&gt;
&#27010;&#24565;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#20013;&#30340;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#23545;&#27010;&#24565;&#39537;&#21160;AI&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#24178;&#25200;&#22240;&#32032;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#37096;&#32626;AI&#31995;&#32479;&#65288;&#22914;&#21307;&#30103;AI&#31995;&#32479;&#19982;&#20020;&#24202;&#21307;&#29983;&#19968;&#36215;&#24037;&#20316;&#65289;&#26102;&#65292;&#23558;&#20154;&#31867;&#25918;&#20837;&#20854;&#20013;&#21487;&#33021;&#20250;&#20943;&#36731;&#19968;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#32531;&#35299;&#20154;&#38388;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#22240;&#32032;&#22312;&#27492;&#31867;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#20013;&#24341;&#36215;&#30340;&#39118;&#38505;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#19988;&#26410;&#34987;&#30740;&#31350;&#20805;&#20998;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27010;&#24565;&#39537;&#21160;&#27169;&#22411;&#20013;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31867;&#22312;AI&#31995;&#32479;&#20013;&#21551;&#29992;&#27010;&#24565;&#24178;&#39044;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#35813;&#21151;&#33021;&#26159;&#25351;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#27010;&#24565;&#19978;&#65292;&#19987;&#23478;&#23545;&#20854;&#36827;&#34892;&#24178;&#39044;&#20197;&#33719;&#24471;&#20154;&#31867;&#21453;&#39304;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23545;&#27492;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#26159;&#39044;&#35328;&#23478;&#65292;&#24635;&#26159;&#30830;&#23450;&#21644;&#27491;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20013;&#20154;&#31867;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#20063;&#20250;&#20986;&#29616;&#20598;&#23572;&#30340;&#38169;&#35823;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#22411;&#25968;&#25454;&#38598;&#65288;UMNIST&#21644;CUB-S&#65289;&#25506;&#35752;&#20102;&#29616;&#26377;&#30340;&#27010;&#24565;&#39537;&#21160;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#19981;&#30830;&#23450;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;Temporal Span Masking&#20013;&#38388;&#35757;&#32451;&#24182;&#19982;Salient Span Masking&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#24615;&#33021;&#21450;&#34920;&#31034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12860</link><description>&lt;p&gt;
&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;
&lt;/p&gt;
&lt;p&gt;
Salient Span Masking for Temporal Understanding. (arXiv:2303.12860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#29702;&#35299;&#30340;&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109;&#25216;&#26415;&#65292;&#36890;&#36807;&#24341;&#20837;Temporal Span Masking&#20013;&#38388;&#35757;&#32451;&#24182;&#19982;Salient Span Masking&#32467;&#21512;&#20351;&#29992;&#65292;&#26377;&#25928;&#25552;&#39640;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#24615;&#33021;&#21450;&#34920;&#31034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#33879;&#24615;&#36328;&#24230;&#25513;&#34109; (SSM) &#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#23553;&#38381;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290; SSM&#36890;&#36807;&#21019;&#24314;&#39069;&#22806;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#21477;&#23376;&#23545;&#26222;&#36890;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#36825;&#20123;&#21477;&#23376;&#23631;&#34109;&#20102;&#19968;&#20010;&#23454;&#20307;&#25110;&#26085;&#26399;&#36328;&#24230;&#65292;&#20174;&#32780;&#36807;&#24230;&#21462;&#26679;&#20102;&#20107;&#23454;&#20449;&#24687;&#12290; &#23613;&#31649;&#36825;&#31181;&#33539;&#24335;&#24456;&#25104;&#21151;&#65292;&#20294;&#36328;&#24230;&#31867;&#22411;&#21644;&#37319;&#26679;&#31574;&#30053;&#30456;&#23545;&#20219;&#24847;&#65292;&#24182;&#19988;&#19981;&#34987;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#20174;&#26102;&#38388;&#20219;&#21153;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;SSM&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23398;&#20064;&#21508;&#31181;&#26102;&#38388;&#34920;&#36798;&#30340;&#33391;&#22909;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#38388;&#22521;&#35757;Temporal Span Masking (TSM)&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20351;&#29992;SSM&#23601;&#21487;&#20197;&#24179;&#22343;&#25913;&#21892;&#19977;&#20010;&#26102;&#38388;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;5.8&#20010;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;TSM&#20219;&#21153;&#33021;&#22815;&#23454;&#29616;&#39069;&#22806;&#30340;&#25913;&#36827;&#65288;&#24179;&#22343;+0.29&#20010;&#28857;&#65289;&#12290;&#36825;&#20123;&#26159;&#30446;&#26631;&#20219;&#21153;&#25253;&#21578;&#30340;&#26032;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;SSM&#21644;TSM&#31574;&#30053;&#30340;&#25928;&#26524;&#23545;&#20110;&#22810;&#20010;&#26102;&#38388;&#20219;&#21153;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Salient Span Masking (SSM) has shown itself to be an effective strategy to improve closed-book question answering performance. SSM extends general masked language model pretraining by creating additional unsupervised training sentences that mask a single entity or date span, thus oversampling factual information. Despite the success of this paradigm, the span types and sampling strategies are relatively arbitrary and not widely studied for other tasks. Thus, we investigate SSM from the perspective of temporal tasks, where learning a good representation of various temporal expressions is important. To that end, we introduce Temporal Span Masking (TSM) intermediate training. First, we find that SSM alone improves the downstream performance on three temporal tasks by an avg. +5.8 points. Further, we are able to achieve additional improvements (avg. +0.29 points) by adding the TSM task. These comprise the new best reported results on the targeted tasks. Our analysis suggests that the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#32447;&#24615;&#23384;&#22312;&#35268;&#21017;&#30340;&#21322;&#26080;&#35760;&#24518;&#36861;&#36214;&#31639;&#27861;&#21450;&#20854;&#32456;&#27490;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;LERT&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#24182;&#24471;&#21040;&#22810;&#20010;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#21457;&#25496;&#20102;&#26032;&#30340;&#26377;&#36259;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.12851</link><description>&lt;p&gt;
&#32447;&#24615;&#23384;&#22312;&#35268;&#21017;&#30340;&#21322;&#26080;&#35760;&#24518;&#36861;&#36214;&#32456;&#27490;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semi-Oblivious Chase Termination for Linear Existential Rules: An Experimental Study. (arXiv:2303.12851v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32447;&#24615;&#23384;&#22312;&#35268;&#21017;&#30340;&#21322;&#26080;&#35760;&#24518;&#36861;&#36214;&#31639;&#27861;&#21450;&#20854;&#32456;&#27490;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;LERT&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#24182;&#24471;&#21040;&#22810;&#20010;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#21457;&#25496;&#20102;&#26032;&#30340;&#26377;&#36259;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36861;&#36214;&#36807;&#31243;&#26159;&#25968;&#25454;&#24211;&#20013;&#29992;&#20110;&#22788;&#29702;&#32422;&#26463;(&#20363;&#22914;&#23384;&#22312;&#35268;&#21017;)&#24182;&#19988;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#30340;&#22522;&#26412;&#31639;&#27861;&#24037;&#20855;&#12290;&#23427;&#25509;&#21463;&#25968;&#25454;&#24211;&#21644;&#32422;&#26463;&#38598;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#26681;&#25454;&#32422;&#26463;&#36845;&#20195;&#22320;&#23436;&#25104;&#25968;&#25454;&#24211;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#23427;&#21487;&#33021;&#19981;&#20250;&#32456;&#27490;&#65292;&#36825;&#23548;&#33268;&#38656;&#35201;&#26816;&#26597;&#32473;&#23450;&#25968;&#25454;&#24211;&#21644;&#19968;&#32452;&#32422;&#26463;&#26159;&#21542;&#32456;&#27490;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#26080;&#35760;&#24518;&#36861;&#36214;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#23427;&#38750;&#24120;&#36866;&#21512;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#38024;&#23545;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#20013;&#24515;&#32422;&#26463;&#31867;&#21035;&#65292;&#21363;&#32447;&#24615;&#23384;&#22312;&#35268;&#21017;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#24050;&#26377;&#36739;&#20026;&#25104;&#29087;&#30340;&#29702;&#35770;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#21487;&#36861;&#36214;&#32456;&#27490;&#30340;&#21477;&#27861;&#34920;&#24449;&#12289;&#26816;&#26597;&#36861;&#36214;&#32456;&#27490;&#30340;&#31639;&#27861;&#12289;&#31934;&#30830;&#30340;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#26377;&#38480;&#24773;&#20917;&#19979;&#36861;&#36214;&#32467;&#26524;&#22823;&#23567;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#36793;&#30028;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23454;&#39564;&#30740;&#31350;&#21322;&#26080;&#35760;&#24518;&#36861;&#36214;&#31639;&#27861;&#23545;&#20110;&#32447;&#24615;&#23384;&#22312;&#35268;&#21017;&#30340;&#32456;&#27490;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The chase procedure is a fundamental algorithmic tool in databases that allows us to reason with constraints, such as existential rules, with a plethora of applications. It takes as input a database and a set of constraints, and iteratively completes the database as dictated by the constraints. A key challenge, though, is the fact that it may not terminate, which leads to the problem of checking whether it terminates given a database and a set of constraints. In this work, we focus on the semi-oblivious version of the chase, which is well-suited for practical implementations, and linear existential rules, a central class of constraints with several applications. In this setting, there is a mature body of theoretical work that provides syntactic characterizations of when the chase terminates, algorithms for checking chase termination, precise complexity results, and worst-case optimal bounds on the size of the result of the chase (whenever is finite). Our main objective is to experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12834</link><description>&lt;p&gt;
&#37327;&#23376;&#21160;&#21147;&#23398;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#27973;&#23618;&#27979;&#37327;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#34429;&#28982;&#35813;&#31867;&#26694;&#26550;&#20026;&#25105;&#20204;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#26159;&#30740;&#31350;&#37327;&#23376;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#20043;&#19968;&#12290;&#28982;&#32780;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#25918;&#22312;&#20102;&#33258;&#26059;&#30456;&#24178;&#21644;&#22120;&#20214;&#33258;&#32806;&#21512;&#30340;&#27874;&#21160;&#20989;&#25968;&#19978;&#65292;&#30740;&#31350;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#31995;&#32479;&#21644;&#30446;&#26631;&#19981;&#30452;&#25509;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#21487;&#20197;&#34987;&#23398;&#20064;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#19981;&#30456;&#24178;&#30340;&#26694;&#26550;&#23454;&#38469;&#19978;&#38750;&#24120;&#21560;&#24341;&#20154;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25361;&#25112;&#24615;&#30340;&#28151;&#21512;&#32416;&#32544;&#26041;&#26696;&#20013;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#22312;&#19981;&#21516;&#29289;&#29702;&#24179;&#21488;&#20043;&#38388;&#36716;&#31227;&#37327;&#23376;&#36807;&#31243;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#38656;&#35201;&#20223;&#30495;&#30340;&#26126;&#30830;&#30340;&#30456;&#24178;&#23398;&#20064;&#31574;&#30053;&#30340;&#27979;&#37327;&#27425;&#25968;&#65292;&#25552;&#20379;&#20102;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#19979;&#23398;&#20064;&#24186;&#27491;&#36807;&#31243;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#20801;&#35768;&#20219;&#24847;&#27979;&#37327;&#65292;&#21017;&#20219;&#20309;&#26377;&#25928;&#34920;&#31034;&#30340;&#24186;&#27491;&#30697;&#38453;&#37117;&#21487;&#20197;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20869;&#34987;&#26377;&#25928;&#22320;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#20165;&#38480;&#20110;&#27973;&#23618;&#27979;&#37327;&#65292;&#21017;&#21482;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#20302;&#32416;&#32544;&#38376;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#22312;&#19981;&#30456;&#24178;&#26694;&#26550;&#20013;&#30340;&#33021;&#21147;&#19982;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22797;&#21512;&#25915;&#20987;&#30340;&#38750;&#32447;&#24615;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#20174;&#23646;&#19968;&#33268;&#24615;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#29983;&#23618;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#24377;&#24615;&#20998;&#24067;&#24335;&#36319;&#36394;&#25511;&#21046;&#20004;&#37096;&#20998;&#20219;&#21153;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12823</link><description>&lt;p&gt;
&#38754;&#21521;&#22797;&#21512;&#25915;&#20987;&#30340;&#38750;&#32447;&#24615;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#30340;&#20174;&#23646;&#19968;&#33268;&#24615;&#25511;&#21046;&#65306;&#21452;&#29983;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Leader-following Consensus for Nonlinear Multi-Agent Systems against Composite Attacks: A Twins Layer Approach. (arXiv:2303.12823v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22797;&#21512;&#25915;&#20987;&#30340;&#38750;&#32447;&#24615;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#20174;&#23646;&#19968;&#33268;&#24615;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#29983;&#23618;&#26041;&#27861;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#24377;&#24615;&#20998;&#24067;&#24335;&#36319;&#36394;&#25511;&#21046;&#20004;&#37096;&#20998;&#20219;&#21153;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#23545;&#22797;&#21512;&#25915;&#20987;&#65288;&#21253;&#25324;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#21644;&#25191;&#34892;&#25915;&#20987;&#65289;&#30340;&#19981;&#30830;&#23450;&#21644;&#38750;&#32447;&#24615;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#39046;&#23548;&#32773;&#20174;&#23646;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#25968;&#23383;&#21452;&#32990;&#32974;&#23618;&#65288;TL&#65292;&#21463;&#26368;&#36817;&#25968;&#23383;&#21452;&#32990;&#32974;&#25216;&#26415;&#30340;&#21551;&#21457;&#65289;&#26469;&#26500;&#24314;&#21452;&#23618;&#25511;&#21046;&#26694;&#26550;&#12290;&#22240;&#27492;&#65292;&#23545;&#25239;&#22797;&#21512;&#25915;&#20987;&#30340;&#24377;&#24615;&#25511;&#21046;&#20219;&#21153;&#34987;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#19968;&#37096;&#20998;&#26159;&#38024;&#23545;TL&#19978;&#30340;DoS&#25915;&#20987;&#30340;&#20998;&#24067;&#24335;&#20272;&#35745;&#65292;&#21478;&#19968;&#37096;&#20998;&#26159;&#23545;&#25239;CPL&#19978;&#30340;&#25191;&#34892;&#25915;&#20987;&#30340;&#24377;&#24615;&#20998;&#24067;&#24335;&#36319;&#36394;&#25511;&#21046;&#12290;&#25968;&#25454;&#39537;&#21160;&#26041;&#26696;&#29992;&#20110;&#22788;&#29702;&#27169;&#22411;&#38750;&#32447;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#25972;&#20010;&#25511;&#21046;&#36807;&#31243;&#21482;&#20351;&#29992;&#31995;&#32479;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#20999;&#25442;&#20272;&#35745;&#35268;&#24459;&#30340;&#20998;&#24067;&#24335;&#35266;&#27979;&#22120;&#34987;&#35774;&#35745;&#29992;&#20110;&#22312;TL&#19978;&#38024;&#23545;DoS&#25915;&#20987;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#21452;&#29983;&#23618;&#30340;&#20998;&#24067;&#24335;&#26080;&#27169;&#22411;&#33258;&#36866;&#24212;&#25511;&#21046;&#65288;DMFAC&#65289;&#21327;&#35758;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#22312;CPL&#19978;&#22788;&#29702;AAs&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the leader-following consensuses of uncertain and nonlinear multi-agent systems against composite attacks (CAs), including Denial of Service (DoS) attacks and actuation attacks (AAs). A double-layer control framework is formulated, where a digital twin layer (TL) is added beside the traditional cyber-physical layer (CPL), inspired by the recent Digital Twin technology. Consequently, the resilient control task against CAs can be divided into two parts: One is distributed estimation against DoS attacks on the TL and the other is resilient decentralized tracking control against actuation attacks on the CPL. %The data-driven scheme is used to deal with both model non-linearity and model uncertainty, in which only the input and output data of the system are employed throughout the whole control process. First, a distributed observer based on switching estimation law against DoS is designed on TL. Second, a distributed model free adaptive control (DMFAC) protocol based on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26426;&#21046;&#65292;&#20351;&#29992;&#31163;&#25955;&#30340;&#32534;&#30721;&#26041;&#24335;&#26469;&#35299;&#20915;&#21512;&#25104;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;VAE&#21644;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.12822</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25163;&#21183;&#20196;&#29260;&#23398;&#20064;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Co-Speech Gesture Synthesis using Discrete Gesture Token Learning. (arXiv:2303.12822v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12822
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26426;&#21046;&#65292;&#20351;&#29992;&#31163;&#25955;&#30340;&#32534;&#30721;&#26041;&#24335;&#26469;&#35299;&#20915;&#21512;&#25104;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#37319;&#29992;VAE&#21644;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#20316;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#29992;&#20110;&#39537;&#21160;&#20154;&#24418;&#26426;&#22120;&#20154;&#19982;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#21644;&#27807;&#36890;&#12290;&#36825;&#31181;&#33021;&#21147;&#23558;&#25913;&#21892;&#20154;&#31867;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#30340;&#21360;&#35937;&#65292;&#24182;&#22312;&#25945;&#32946;&#12289;&#22521;&#35757;&#21644;&#21307;&#30103;&#26381;&#21153;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;&#23398;&#20064;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#27169;&#22411;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#21516;&#19968;&#35821;&#38899;&#35805;&#35821;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#21512;&#29702;&#30340;&#25163;&#21183;&#36816;&#21160;&#12290;&#30830;&#23450;&#24615;&#22238;&#24402;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#20914;&#31361;&#26679;&#26412;&#65292;&#24182;&#21487;&#33021;&#20135;&#29983;&#36807;&#24230;&#24179;&#28369;&#25110;&#25233;&#21046;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25163;&#21183;&#29255;&#27573;&#24314;&#27169;&#20026;&#31163;&#25955;&#30340;&#28508;&#22312;&#32534;&#30721;&#26469;&#35299;&#20915;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;RQ-VAE&#22312;&#31532;&#19968;&#38454;&#27573;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#30001;&#25163;&#21183;&#20196;&#29260;&#32452;&#25104;&#30340;&#31163;&#25955;&#30721;&#26412;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20004;&#32423;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#27169;&#22411;&#23398;&#20064;&#27531;&#20313;&#30721;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#20197;&#21450;&#32473;&#20986;&#35821;&#38899;&#26102;&#25163;&#21183;&#20196;&#29260;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#20849;&#24615;&#35821;&#35328;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes cond
&lt;/p&gt;</description></item><item><title>DeepBlocks&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#20854;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.12821</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Towards A Visual Programming Tool to Create Deep Learning Models. (arXiv:2303.12821v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12821
&lt;/p&gt;
&lt;p&gt;
DeepBlocks&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;&#20854;&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65292;&#32467;&#26524;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24320;&#21457;&#20154;&#21592;&#26469;&#33258;&#19981;&#21516;&#30340;&#32972;&#26223;&#65292;&#20363;&#22914;&#21307;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#37329;&#34701;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#20026;&#20102;&#21019;&#24314;DL&#27169;&#22411;&#65292;&#20182;&#20204;&#24517;&#39035;&#23398;&#20064;&#21644;&#20351;&#29992;&#39640;&#32423;&#32534;&#31243;&#35821;&#35328;&#65288;&#20363;&#22914;Python&#65289;&#65292;&#22240;&#27492;&#38656;&#35201;&#22788;&#29702;&#30456;&#20851;&#35774;&#32622;&#21644;&#35299;&#20915;&#32534;&#31243;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepBlocks&#65292;&#36825;&#26159;&#19968;&#27454;&#21487;&#35270;&#21270;&#32534;&#31243;&#24037;&#20855;&#65292;&#20801;&#35768;DL&#24320;&#21457;&#20154;&#21592;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#29305;&#23450;&#30340;&#32534;&#31243;&#35821;&#35328;&#12290;DeepBlocks&#36890;&#36807;&#26500;&#24314;&#20856;&#22411;&#27169;&#22411;&#32467;&#26500;&#23454;&#29616;&#20854;&#24037;&#20316;&#21407;&#29702;&#65306;&#19968;&#31995;&#21015;&#21487;&#23398;&#20064;&#20989;&#25968;&#30340;&#39034;&#24207;&#25490;&#21015;&#23450;&#20041;&#20102;&#27169;&#22411;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;5&#20010;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#35775;&#35848;&#25512;&#23548;&#20986;&#20102;DeepBlocks&#30340;&#35774;&#35745;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#20856;&#22411;&#29992;&#20363;&#39564;&#35777;&#20102;&#35813;&#24037;&#20855;&#30340;&#31532;&#19968;&#20010;&#23454;&#29616;&#12290;&#32467;&#26524;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#34920;&#26126;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#35270;&#35273;&#19978;&#35774;&#35745;&#22797;&#26434;&#30340;DL&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) developers come from different backgrounds, e.g., medicine, genomics, finance, and computer science. To create a DL model, they must learn and use high-level programming languages (e.g., Python), thus needing to handle related setups and solve programming errors. This paper presents DeepBlocks, a visual programming tool that allows DL developers to design, train, and evaluate models without relying on specific programming languages. DeepBlocks works by building on the typical model structure: a sequence of learnable functions whose arrangement defines the specific characteristics of the model. We derived DeepBlocks' design goals from a 5-participants formative interview, and we validated the first implementation of the tool through a typical use case. Results are promising and show that developers could visually design complex DL architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12816</link><description>&lt;p&gt;
&#20174;&#23485;&#21040;&#28145;&#65306;&#32500;&#24230;&#25552;&#21319;&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22686;&#21152;&#28145;&#24230;&#20811;&#26381;&#22240;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#32780;&#23548;&#33268;&#30340;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#34920;&#31034;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;KGE&#26041;&#27861;&#38656;&#35201;&#30456;&#23545;&#39640;&#32500;&#30340;&#23454;&#20307;&#34920;&#31034;&#26469;&#20445;&#30041;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20294;&#20250;&#23548;&#33268;&#24222;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#20302;&#32500;&#23454;&#20307;&#34920;&#31034;&#26469;&#38477;&#20302;&#27169;&#22411;&#21442;&#25968;&#65292;&#21516;&#26102;&#24320;&#21457;&#25216;&#26415;&#65288;&#20363;&#22914;&#30693;&#35782;&#33976;&#39311;&#65289;&#26469;&#34917;&#20607;&#38477;&#32500;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25805;&#20316;&#20250;&#23548;&#33268;&#27169;&#22411;&#31934;&#24230;&#19979;&#38477;&#21644;&#27169;&#22411;&#21442;&#25968;&#20943;&#23569;&#26377;&#38480;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#23454;&#20307;&#34920;&#31034;&#30340;&#32423;&#32852;&#35270;&#20026;&#23884;&#20837;&#23618;&#65292;&#37027;&#20040;&#37319;&#29992;&#39640;&#32500;&#23454;&#20307;&#34920;&#31034;&#30340;&#20256;&#32479;KGE&#26041;&#27861;&#31561;&#21516;&#20110;&#25193;&#23637;&#23884;&#20837;&#23618;&#30340;&#23485;&#24230;&#20197;&#33719;&#24471;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#22312;&#19981;&#29306;&#29298;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#25928;&#29575;&#65292;&#25105;&#20204;&#30456;&#21453;&#22320;&#22686;&#21152;&#28145;&#24230;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26356;&#28145;&#30340;&#23454;&#20307;&#23884;&#20837;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
&lt;/p&gt;</description></item><item><title>SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#26080;&#32447;&#35774;&#22791;&#65292;&#19981;&#21463;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12811</link><description>&lt;p&gt;
SignCRF: &#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System. (arXiv:2303.12811v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12811
&lt;/p&gt;
&lt;p&gt;
SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26080;&#39057;&#36947;&#25968;&#25454;&#39537;&#21160;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35782;&#21035;&#26080;&#32447;&#35774;&#22791;&#65292;&#19981;&#21463;&#31227;&#21160;&#24615;&#24102;&#26469;&#30340;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#29575;&#25351;&#32441;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;(RFFDL)&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#32852;&#32593;&#36523;&#20221;&#35748;&#35777;&#25216;&#26415;&#65292;&#21033;&#29992;&#19982;&#29305;&#23450;&#35774;&#22791;&#30456;&#20851;&#30340;&#29420;&#29305;&#30828;&#20214;&#32423;&#21046;&#36896;&#32570;&#38519;&#26469;&#35782;&#21035;&#65288;&#25351;&#32441;&#65289;&#22522;&#20110;&#20256;&#36755;&#27874;&#24418;&#24341;&#20837;&#30340;&#21464;&#21270;&#30340;&#35774;&#22791;&#12290;&#25552;&#20986;&#30340;SignCRF&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#26080;&#39057;&#36947;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#23556;&#39057;&#35748;&#35777;&#24179;&#21488;&#65292;&#22312;&#35782;&#21035;&#22522;&#20110;&#20854;&#29420;&#29305;&#30340;&#21046;&#36896;&#32570;&#38519;&#30340;&#26080;&#32447;&#35774;&#22791;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#31934;&#24230;&#65292;&#24182;&#19988;&#29420;&#31435;&#20110;&#30001;&#31227;&#21160;&#24615;&#24341;&#36215;&#30340;&#21160;&#24577;&#20449;&#36947;&#19981;&#35268;&#21017;&#24615;&#12290;SignCRF&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;(i)&#22522;&#32447;&#20998;&#31867;&#22120;&#65292;&#32463;&#36807;&#31934;&#32454;&#35757;&#32451;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#25193;&#23637;&#35748;&#35777;&#35774;&#22791;;(ii)&#29615;&#22659;&#32763;&#35793;&#22120;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#33021;&#22815;&#20174;RF&#20449;&#21495;&#20013;&#21435;&#38500;&#21160;&#24577;&#20449;&#36947;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#25345;&#25910;&#21457;&#26426;&#20855;&#20307;&#30340;&#20449;&#21495;&#65307;(iii)&#26368;&#22823;&#35268;&#21017;&#27169;&#22359;&#36873;&#25321;&#22522;&#32447;&#20998;&#31867;&#22120;&#21644;&#29615;&#22659;&#32763;&#35793;&#22120;&#20043;&#38388;&#30340;&#26368;&#39640;&#31934;&#24230;&#35748;&#35777;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (fingerprint) the device based on variations introduced in the transmitted waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments and independent of the dynamic channel irregularities caused by mobility. SignCRF consists of (i) a baseline classifier finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific signature; (iii) a Max-Rule module that selects the highest precision authentication technique between the baseline classifier and the envir
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;PACO&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.12808</link><description>&lt;p&gt;
PACO: &#21253;&#21547;&#34892;&#21160;&#12289;&#25991;&#21270;&#21644;&#21387;&#36843;&#30340;&#25361;&#34885;
&lt;/p&gt;
&lt;p&gt;
PACO: Provocation Involving Action, Culture, and Oppression. (arXiv:2303.12808v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;PACO&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21360;&#24230;&#65292;&#20154;&#20204;&#26681;&#25454;&#26576;&#20123;&#23646;&#24615;&#65288;&#22914;&#23447;&#25945;&#65289;&#35748;&#21516;&#20110;&#29305;&#23450;&#32676;&#20307;&#65292;&#21516;&#19968;&#23447;&#25945;&#32676;&#20307;&#32463;&#24120;&#30456;&#20114;&#25361;&#34885;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25361;&#34885;&#22312;&#22686;&#21152;&#21360;&#24230;&#20004;&#20010;&#20027;&#35201;&#23447;&#25945;&#32676;&#20307;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#20986;&#29616;&#65292;&#36825;&#31181;&#25361;&#34885;&#20063;&#20986;&#29616;&#22312;WhatsApp&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30340;&#21360;&#24230;WhatsApp&#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#19977;&#31181;&#38024;&#23545;&#21360;&#24230;&#31302;&#26031;&#26519;&#30340;&#25361;&#34885;&#21477;&#23376;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#25361;&#34885;&#31867;&#21035;&#26631;&#35760;&#20102;7000&#20010;&#21477;&#23376;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;PACO&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;PACO&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#20174;WhatsApp&#24086;&#23376;&#20013;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#26159;&#31934;&#35843;RoBERTa&#65292;&#24182;&#22312;&#20116;&#20493;&#20132;&#21449;&#39564;&#35777;&#20013;&#23454;&#29616;&#20102;0.851&#30340;&#24179;&#22343;AUC&#20998;&#25968;&#12290;&#33258;&#21160;&#35782;&#21035;&#25361;&#34885;&#21477;&#23376;&#21487;&#20197;&#38459;&#27490;&#25361;&#34885;&#25991;&#26412;&#25193;&#25955;&#21040;&#32676;&#20247;&#20043;&#38388;&#65292;&#21487;&#20197;&#38450;&#27490;&#21487;&#33021;&#30340;&#27495;&#35270;&#25110;&#26292;&#21147;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In India, people identify with a particular group based on certain attributes such as religion. The same religious groups are often provoked against each other. Previous studies show the role of provocation in increasing tensions between India's two prominent religious groups: Hindus and Muslims. With the advent of the Internet, such provocation also surfaced on social media platforms such as WhatsApp.  By leveraging an existing dataset of Indian WhatsApp posts, we identified three categories of provoking sentences against Indian Muslims. Further, we labeled 7,000 sentences for three provocation categories and called this dataset PACO. We leveraged PACO to train a model that can identify provoking sentences from a WhatsApp post. Our best model is fine-tuned RoBERTa and achieved a 0.851 average AUC score over five-fold cross-validation. Automatically identifying provoking sentences could stop provoking text from reaching out to the masses, and can prevent possible discrimination or viol
&lt;/p&gt;</description></item><item><title>&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.12807</link><description>&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular-ball Optimization Algorithm. (arXiv:2303.12807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12807
&lt;/p&gt;
&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26234;&#33021;&#20248;&#21270;&#31639;&#27861;&#37117;&#26159;&#22522;&#20110;&#26368;&#23567;&#31890;&#24230;&#21363;&#28857;&#30340;&#35774;&#35745;&#65292;&#23548;&#33268;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#36739;&#24369;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#65292;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#23454;&#29616;&#12290;GBO&#20351;&#29992;&#22810;&#20010;&#31890;&#29699;&#26469;&#35206;&#30422;&#35299;&#31354;&#38388;&#65292;&#20351;&#29992;&#35768;&#22810;&#32454;&#23567;&#30340;&#32454;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#37325;&#35201;&#37096;&#20998;&#65292;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#31895;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#19981;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#31934;&#32454;&#30340;&#22810;&#31890;&#24230;&#25968;&#25454;&#25551;&#36848;&#33021;&#21147;&#25552;&#39640;&#20102;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#20108;&#21313;&#20010;&#22522;&#20934;&#20989;&#25968;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;GBO&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#26356;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#35774;&#35745;&#26356;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23581;&#35797;&#22312;&#36827;&#21270;&#35745;&#31639;&#19982;&#24378;&#21270;&#23398;&#20064;&#20013;&#30456;&#32467;&#21512;&#35299;&#20915;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;ME&#31639;&#27861;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#65292;&#20294;&#20063;&#20986;&#29616;&#20102;&#19968;&#20123;&#24120;&#35265;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.12803</link><description>&lt;p&gt;
&#22522;&#20110;MAP-Elites&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#32676;&#20307;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23581;&#35797;&#22312;&#36827;&#21270;&#35745;&#31639;&#19982;&#24378;&#21270;&#23398;&#20064;&#20013;&#30456;&#32467;&#21512;&#35299;&#20915;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;ME&#31639;&#27861;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#65292;&#20294;&#20063;&#20986;&#29616;&#20102;&#19968;&#20123;&#24120;&#35265;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21697;&#36136;&#22810;&#26679;&#24615;(QD)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#20248;&#21270;&#27169;&#24335;&#65292;&#26088;&#22312;&#29983;&#25104;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20195;&#34920;&#31639;&#27861;MAP-Elites(ME)&#36890;&#36807;&#21464;&#24322;&#21644;&#20132;&#21449;&#36827;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;ME&#23545;&#20110;&#26576;&#20123;&#38750;&#32467;&#26500;&#21270;&#38382;&#39064;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26089;&#26399;&#30340;ME&#23454;&#29616;&#20165;&#20381;&#36182;&#20110;&#38543;&#26426;&#25628;&#32034;&#26469;&#36827;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#65292;&#22240;&#27492;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#22914;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26102;&#24120;&#24120;&#26080;&#27861;&#26377;&#25928;&#22320;&#36827;&#34892;&#65292;&#25928;&#29575;&#26497;&#20302;&#12290;&#21518;&#32493;&#30340;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#36890;&#36807;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#25110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#25216;&#26415;&#26469;&#24341;&#23548;&#25628;&#32034;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23558;RL&#25216;&#24039;&#19982;ME&#32467;&#21512;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20063;&#22312;ME&#21464;&#20307;&#20013;&#24341;&#20837;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20849;&#21516;&#38480;&#21046;&#65292;&#20363;&#22914;&#23545;&#25506;&#32034;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a
&lt;/p&gt;</description></item><item><title>&#20869;&#37096;&#32593;&#32476;&#20013;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#21644;&#26410;&#30693;&#30340;IoT&#35774;&#22791;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12800</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;IoT&#35774;&#22791;&#32593;&#32476;&#36890;&#20449;&#20998;&#26512;&#19982;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
IoT Device Identification Based on Network Communication Analysis Using Deep Learning. (arXiv:2303.12800v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12800
&lt;/p&gt;
&lt;p&gt;
&#20869;&#37096;&#32593;&#32476;&#20013;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#21644;&#26410;&#30693;&#30340;IoT&#35774;&#22791;&#30340;&#35782;&#21035;&#21464;&#24471;&#36234;&#21457;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#19981;&#23433;&#20840;&#30340;IoT&#35774;&#22791;&#30340;&#20351;&#29992;&#65292;&#23545;&#20110;&#25915;&#20987;&#32773;&#26469;&#35828;&#65292;&#20837;&#20405;&#32452;&#32455;&#30340;&#25915;&#20987;&#26041;&#24335;&#36234;&#21457;&#22810;&#26679;&#12290;BYOD&#25919;&#31574;&#20351;&#24471;&#21592;&#24037;&#21487;&#20197;&#25658;&#24102;IoT&#35774;&#22791;&#36827;&#20837;&#32452;&#32455;&#24182;&#36830;&#25509;&#21040;&#32452;&#32455;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#32452;&#32455;&#32593;&#32476;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#21644;&#20445;&#25252;&#32593;&#32476;&#65292;&#32452;&#32455;&#36890;&#24120;&#23454;&#26045;&#23433;&#20840;&#31574;&#30053;&#65292;&#21482;&#20801;&#35768;&#21015;&#20837;&#30333;&#21517;&#21333;&#30340;IoT&#35774;&#22791;&#36830;&#25509;&#21040;&#32593;&#32476;&#12290;&#20026;&#20102;&#30417;&#27979;&#36825;&#31181;&#31574;&#30053;&#30340;&#21512;&#35268;&#24615;&#65292;&#35782;&#21035;&#32452;&#32455;&#32593;&#32476;&#20869;&#20801;&#35768;&#36830;&#25509;&#30340;IoT&#35774;&#22791;&#19982;&#19981;&#22312;&#30333;&#21517;&#21333;&#20013;&#65288;&#26410;&#30693;&#30340;&#65289;IoT&#35774;&#22791;&#20043;&#38388;&#30340;&#24046;&#24322;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#32593;&#32476;&#36890;&#20449;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#32593;&#32476;&#20869;IoT&#35774;&#22791;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;&#32593;&#32476;&#36890;&#20449;&#36827;&#34892;&#22797;&#26434;&#30340;&#29305;&#24449;&#24037;&#31243;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attack vectors for adversaries have increased in organizations because of the growing use of less secure IoT devices. The risk of attacks on an organization's network has also increased due to the bring your own device (BYOD) policy which permits employees to bring IoT devices onto the premises and attach them to the organization's network. To tackle this threat and protect their networks, organizations generally implement security policies in which only white listed IoT devices are allowed on the organization's network. To monitor compliance with such policies, it has become essential to distinguish IoT devices permitted within an organization's network from non white listed (unknown) IoT devices. In this research, deep learning is applied to network communication for the automated identification of IoT devices permitted on the network. In contrast to existing methods, the proposed approach does not require complex feature engineering of the network communication, because the 'communi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12799</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#22270;&#20687;&#65306;&#29992;&#35270;&#35273;transformer&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#36866;&#24212;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#21307;&#30103;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#19981;&#35268;&#21017;&#25277;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#39640;&#24230;&#23450;&#21046;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#19981;&#35268;&#21017;&#24615;&#38382;&#39064;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#22320;&#27169;&#25311;&#23427;&#20204;&#30340;&#22797;&#26434;&#21160;&#24577;&#21644;&#39640;&#31232;&#30095;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20840;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#32447;&#22270;&#20687;&#65292;&#24182;&#35843;&#25972;&#24378;&#22823;&#30340;&#35270;&#35273;transformer&#20197;&#25191;&#34892;&#19982;&#22270;&#20687;&#20998;&#31867;&#30456;&#21516;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20551;&#35774;&#20808;&#21069;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#28508;&#22312;&#22320;&#25193;&#23637;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#21307;&#30103;&#20445;&#20581;&#21644;&#20154;&#20307;&#27963;&#21160;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#19987;&#19994;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#20256;&#24863;&#22120;&#35774;&#32622;&#20013;&#65292;&#21363;&#22312;&#27979;&#35797;&#26399;&#38388;&#23631;&#34109;&#21464;&#37327;&#30340;&#23376;&#38598;&#20013;&#65292;&#24615;&#33021;&#27604;&#26368;&#20339;&#22522;&#20934;&#25552;&#39640;&#20102;&#39640;&#36798;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12797</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#23450;&#20041;&#20102;&#27604;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#25628;&#32034;&#31354;&#38388;&#26356;&#20026;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20801;&#35768;&#28151;&#21512;&#20351;&#29992;&#20256;&#32479;&#25805;&#20316;&#65292;&#22914;&#21367;&#31215;&#12289;&#24490;&#29615;&#21644;&#23494;&#38598;&#23618;&#65292;&#20197;&#21450;&#36739;&#20026;&#26032;&#39062;&#30340;&#25805;&#20316;&#65292;&#22914;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#35813;&#25628;&#32034;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37051;&#22495;&#25628;&#32034;&#31639;&#23376;&#21644;&#28436;&#21270;&#25628;&#32034;&#31639;&#23376;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#36825;&#20123;&#25628;&#32034;&#31639;&#23376;&#21487;&#19982;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......</title><link>http://arxiv.org/abs/2303.12796</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;......
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#29616;&#22312;&#20351;&#29992;&#20687;&#35895;&#27468;&#12289;&#38597;&#34382;&#21644;&#24517;&#24212;&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#26469;&#26597;&#25214;&#20114;&#32852;&#32593;&#19978;&#30340;&#20449;&#24687;&#12290;&#30001;&#20110;&#25968;&#25454;&#29190;&#28856;&#65292;&#22914;&#26524;&#20026;&#29992;&#25143;&#25552;&#20379;&#30456;&#20851;&#30340;&#25628;&#32034;&#32467;&#26524;&#25688;&#35201;&#32780;&#19981;&#20165;&#20165;&#26159;&#32593;&#39029;&#38142;&#25509;&#23558;&#20250;&#24456;&#26377;&#24110;&#21161;&#12290;&#25991;&#26412;&#25688;&#35201;&#24050;&#25104;&#20026;&#24110;&#21161;&#29992;&#25143;&#36805;&#36895;&#25484;&#25569;&#22823;&#37327;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#23545;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;google/pegasus-cnn-dailymail&#12289;T5-base&#12289;facebook/bart-large-cnn&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;CNN-dailymail&#12289;SAMSum&#21644;BillSum&#65292;&#20197;&#20174;&#19978;&#36848;&#19977;&#20010;&#27169;&#22411;&#20013;&#33719;&#21462;&#36755;&#20986;&#12290;&#36890;&#36807;ROUGH&#21644;BLEU&#25351;&#26631;&#65292;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#26377;2000&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12795</link><description>&lt;p&gt;
&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#30740;&#31350;&#20142;&#28857;&#33258;&#21160;&#29983;&#25104;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#65292;&#25506;&#31350;&#20854;&#26159;&#21542;&#33021;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31185;&#23398;&#35770;&#25991;&#25688;&#35201;&#29992;&#20110;&#24635;&#32467;&#35770;&#25991;&#20869;&#23481;&#12290;&#36817;&#26399;&#65292;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#25688;&#35201;&#30340;&#34917;&#20805;&#65292;&#32858;&#28966;&#20110;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#65292;&#20294;&#20351;&#29992;&#39057;&#29575;&#36824;&#19981;&#22914;&#25688;&#35201;&#26222;&#36941;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#35770;&#25991;&#19981;&#21516;&#37096;&#20998;&#30340;&#36755;&#20837;&#65292;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#12290;&#30740;&#31350;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#25506;&#31350;&#23427;&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#36136;&#37327;&#12290;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#31532;&#19968;&#20010;&#26159;&#25351;&#38024;-&#29983;&#25104;&#22120;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22312;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#22686;&#21152;&#20102;&#35206;&#30422;&#26426;&#21046;&#12290; &#28982;&#21518;&#23558;&#19978;&#36848;&#27599;&#20010;&#27169;&#22411;&#19982;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#29305;&#24449;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20026;&#32570;&#23569;&#20142;&#28857;&#30340;&#35770;&#25991;&#29983;&#25104;&#20142;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22686;&#21152;&#21629;&#21517;&#23454;&#20307;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30740;&#31350;&#20142;&#28857;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.12755</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;: &#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#24314;&#31569;&#31435;&#38754;&#35774;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text Semantics to Image Generation: A method of building facades design base on Stable Diffusion model. (arXiv:2303.12755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807; LoRA &#35757;&#32451;&#26041;&#27861;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#21644; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20026;&#21518;&#32493;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#20013;&#65292;&#20294;&#30446;&#21069;&#20173;&#26377;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#20869;&#23481;&#21487;&#25511;&#24615;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32593;&#32476;&#32467;&#21512;&#30340;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807; LoRA&#65288;&#20302;&#31209;&#33258;&#36866;&#24212;&#65289;&#26041;&#27861;&#22312; CMP Fa-cades &#25968;&#25454;&#38598;&#19978;&#23545;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#28982;&#21518;&#24212;&#29992; ControlNet &#27169;&#22411;&#36827;&#19968;&#27493;&#25511;&#21046;&#36755;&#20986;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#24314;&#31569;&#39118;&#26684;&#25991;&#26412;&#20869;&#23481;&#21644;&#25511;&#21046;&#31574;&#30053;&#19979;&#30340;&#31435;&#38754;&#29983;&#25104;&#32467;&#26524;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LoRA &#35757;&#32451;&#26041;&#27861;&#26174;&#30528;&#38477;&#20302;&#20102;&#24494;&#35843;&#31283;&#23450;&#25193;&#25955;&#22823;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780; ControlNet &#27169;&#22411;&#30340;&#28155;&#21152;&#22686;&#21152;&#20102;&#25991;&#26412;&#21040;&#24314;&#31569;&#31435;&#38754;&#22270;&#20687;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#20026;&#21518;&#32493;&#20851;&#20110;&#24314;&#31569;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion model has been extensively employed in the study of archi-tectural image generation, but there is still an opportunity to enhance in terms of the controllability of the generated image content. A multi-network combined text-to-building facade image generating method is proposed in this work. We first fine-tuned the Stable Diffusion model on the CMP Fa-cades dataset using the LoRA (Low-Rank Adaptation) approach, then we ap-ply the ControlNet model to further control the output. Finally, we contrast-ed the facade generating outcomes under various architectural style text con-tents and control strategies. The results demonstrate that the LoRA training approach significantly decreases the possibility of fine-tuning the Stable Dif-fusion large model, and the addition of the ControlNet model increases the controllability of the creation of text to building facade images. This pro-vides a foundation for subsequent studies on the generation of architectural images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2303.10365</link><description>&lt;p&gt;
CroSel: &#29992;&#20110;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#33258;&#20449;&#20266;&#26631;&#31614;&#30340;&#36328;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10365
&lt;/p&gt;
&lt;p&gt;
CroSel&#26159;&#19968;&#31181;&#22788;&#29702;&#20266;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#21644;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#20934;&#30830;&#35782;&#21035;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;(PLL)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#23427;&#20801;&#35768;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#26377;&#19968;&#20010;&#20505;&#36873;&#26631;&#31614;&#38598;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;ground-truth&#26631;&#31614;&#12290;&#24050;&#32463;&#24191;&#27867;&#25506;&#32034;&#20102;&#22522;&#20110;&#35782;&#21035;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;PLL&#20013;&#30340;&#26631;&#31614;&#27495;&#20041;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#23558;&#30495;&#23454;&#26631;&#31614;&#35270;&#20026;&#35201;&#35782;&#21035;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#21644;&#23436;&#25972;&#22320;&#35782;&#21035;&#30495;&#23454;&#26631;&#31614;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20250;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23548;&#33268;&#20266;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CroSel&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#30340;&#21382;&#21490;&#39044;&#27979;&#20449;&#24687;&#26469;&#35782;&#21035;&#22823;&#22810;&#25968;&#35757;&#32451;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#21449;&#36873;&#25321;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#28145;&#24230;&#27169;&#22411;&#21487;&#20197;&#30456;&#20114;&#36873;&#25321;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;co-mix&#65292;&#20197;&#36991;&#20813;&#22240;&#34394;&#20551;&#36873;&#25321;&#32780;&#24341;&#36215;&#30340;&#26679;&#26412;&#28010;&#36153;&#21644;&#24494;&#23567;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CroSel&#33021;&#22815;&#25361;&#36873;&#20986;&#22823;&#22810;&#25968;&#31034;&#20363;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
&lt;/p&gt;</description></item><item><title>CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.05725</link><description>&lt;p&gt;
CVT-SLR&#65306;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#19982;&#21464;&#20998;&#23545;&#40784;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment. (arXiv:2303.05725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05725
&lt;/p&gt;
&lt;p&gt;
CVT-SLR&#26159;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#21644;&#21464;&#20998;&#23545;&#40784;&#30340;&#26041;&#27861;&#26469;&#20805;&#20998;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#65292;&#20026;&#35299;&#20915;&#25163;&#35821;&#35782;&#21035;&#20013;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#35782;&#21035; (SLR) &#26159;&#19968;&#39033;&#24369;&#30417;&#30563;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#25163;&#35821;&#35270;&#39057;&#27880;&#37322;&#20026;&#25991;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#21487;&#29992;&#30340;&#25163;&#35821;&#25968;&#25454;&#38598;&#32780;&#23548;&#33268;&#30340;&#19981;&#20805;&#20998;&#35757;&#32451;&#25104;&#20026; SLR &#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968; SLR &#30340;&#24037;&#20316;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22359;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#27604;&#35270;&#35273;-&#25991;&#26412;&#21464;&#25442;&#27169;&#22411; CVT-SLR&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language recognition (SLR) is a weakly supervised task that annotates sign videos as textual glosses. Recent studies show that insufficient training caused by the lack of large-scale available sign language datasets becomes the main bottleneck for SLR. The majority of SLR works thereby adopt pretrained visual modules and develop two mainstream solutions. The multi-stream architectures extend multi-cue visual features, yielding the current SOTA performances but requiring complex designs and might introduce potential noise. Alternatively, the advanced single-cue SLR frameworks using explicit cross-modal alignment between visual and textual modalities are simple and effective, potentially competitive with the multi-cue framework. In this work, we propose a novel contrastive visual-textual transformation for SLR, CVT-SLR, to fully explore the pretrained knowledge of both the visual and language modalities. Based on the single-cue cross-modal alignment framework, we propose a variation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.04238</link><description>&lt;p&gt;
&#21306;&#22495;&#38544;&#24418;&#34917;&#19969;&#65306;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors. (arXiv:2303.04238v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#26080;&#26799;&#24230;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#30340;&#23545;&#25239;&#34917;&#19969;&#65292;&#25915;&#20987;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#21363;&#25152;&#35859;&#30340;&#30333;&#30418;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#20869;&#37096;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#30456;&#23545;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#38656;&#20351;&#29992;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#23398;&#20064;&#22270;&#20687;&#27969;&#24418;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#29289;&#29702;&#23545;&#25239;&#34917;&#19969;&#65292;&#29992;&#20110;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#23618;&#38754;&#19978;&#22343;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks on deep-learning models have been receiving increased attention in recent years. Work in this area has mostly focused on gradient-based techniques, so-called white-box attacks, wherein the attacker has access to the targeted model's internal parameters; such an assumption is usually unrealistic in the real world. Some attacks additionally use the entire pixel space to fool a given model, which is neither practical nor physical (i.e., real-world). On the contrary, we propose herein a gradient-free method that uses the learned image manifold of a pretrained generative adversarial network (GAN) to generate naturalistic physical adversarial patches for object detectors. We show that our proposed method works both digitally and physically.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#23646;&#20998;&#25968;&#21644;&#22240;&#26524;&#21453;&#20107;&#23454;&#22312;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#20998;&#25968;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.02829</link><description>&lt;p&gt;
&#20316;&#20026;&#35299;&#37322;&#30340;&#22240;&#26524;&#21453;&#20107;&#23454;&#21644;&#24402;&#23646;&#20998;&#25968;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence. (arXiv:2303.02829v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24402;&#23646;&#20998;&#25968;&#21644;&#22240;&#26524;&#21453;&#20107;&#23454;&#22312;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#30340;&#36923;&#36753;&#25512;&#29702;&#21644;&#20998;&#25968;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#35299;&#37322;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#24433;&#21709;&#21644;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#36215;&#28304;&#21644;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#29992;&#31616;&#21333;&#30340;&#26415;&#35821;&#25551;&#36848;&#20102;&#22522;&#20110;&#24402;&#23646;&#20998;&#25968;&#30340;&#25968;&#25454;&#31649;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#37322;&#65292;&#20197;&#21450;&#22312;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#20013;&#21457;&#29616;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#22312;&#22788;&#29702;&#21453;&#20107;&#23454;&#26102;&#36923;&#36753;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#29992;&#20110;&#35745;&#31639;&#20998;&#25968;&#30340;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this expository article we highlight the relevance of explanations for artificial intelligence, in general, and for the newer developments in {\em explainable AI}, referring to origins and connections of and among different approaches. We describe in simple terms, explanations in data management and machine learning that are based on attribution-scores, and counterfactuals as found in the area of causality. We elaborate on the importance of logical reasoning when dealing with counterfactuals, and their use for score computation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01047</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#23454;&#26102;&#35780;&#20272;&#65306;&#19968;&#20010;&#26032;&#24076;&#26395;
&lt;/p&gt;
&lt;p&gt;
Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#26041;&#38754;&#27809;&#26377;&#38480;&#21046;&#12290;&#36825;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#19990;&#30028;&#30340;&#29615;&#22659;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27969;&#19981;&#31561;&#24453;&#27169;&#22411;&#23436;&#25104;&#35757;&#32451;&#21363;&#25581;&#31034;&#19979;&#19968;&#20010;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#25104;&#26412;&#30340;&#35282;&#24230;&#35780;&#20272;&#24403;&#21069;&#30340;CL&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35780;&#20272;&#19979;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;CL&#32452;&#20214;&#65292;&#21253;&#25324;&#35760;&#24518;&#37319;&#26679;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#37117;&#26080;&#27861;&#19982;&#25105;&#20204;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;Dependency Graph&#8221;&#65288;DepGraph&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20219;&#20309;&#32467;&#26500;&#21098;&#26525;&#65292;&#23427;&#26126;&#30830;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#32467;&#26500;&#38382;&#39064;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2301.12900</link><description>&lt;p&gt;
DepGraph: &#23454;&#29616;&#20219;&#20309;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
DepGraph: Towards Any Structural Pruning. (arXiv:2301.12900v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;Dependency Graph&#8221;&#65288;DepGraph&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20219;&#20309;&#32467;&#26500;&#21098;&#26525;&#65292;&#23427;&#26126;&#30830;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#32467;&#26500;&#38382;&#39064;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21098;&#26525;&#36890;&#36807;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#21024;&#38500;&#32467;&#26500;&#20998;&#32452;&#21442;&#25968;&#26469;&#21152;&#36895;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#65292;&#21442;&#25968;&#20998;&#32452;&#27169;&#24335;&#24046;&#24322;&#24456;&#22823;&#65292;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#20998;&#32452;&#26041;&#26696;&#30340;&#29305;&#23450;&#32467;&#26500;&#21098;&#26525;&#22120;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#20294;&#40092;&#26377;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#20219;&#20309;&#32467;&#26500;&#21098;&#26525;&#65292;&#20197;&#24212;&#23545;&#20219;&#24847;&#26550;&#26500;&#65288;&#22914;CNN&#12289;RNN&#12289;GNN&#21644;Transformer&#65289;&#30340;&#36890;&#29992;&#32467;&#26500;&#21098;&#26525;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26368;&#22823;&#38556;&#30861;&#22312;&#20110;&#32467;&#26500;&#32806;&#21512;&#65292;&#23427;&#19981;&#20165;&#24378;&#21046;&#21516;&#26102;&#21098;&#26525;&#19981;&#21516;&#23618;&#65292;&#32780;&#19988;&#36824;&#26399;&#26395;&#25152;&#26377;&#24050;&#21024;&#38500;&#21442;&#25968;&#37117;&#19968;&#33268;&#19981;&#37325;&#35201;&#65292;&#20174;&#32780;&#36991;&#20813;&#32467;&#26500;&#38382;&#39064;&#21644;&#21098;&#26525;&#21518;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#8220;&#20381;&#36182;&#22270;&#8221;&#65288;DepGraph&#65289;&#65292;&#26469;&#26126;&#30830;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural pruning enables model acceleration by removing structurally-grouped parameters from neural networks. However, the parameter-grouping patterns vary widely across different models, making architecture-specific pruners, which rely on manually-designed grouping schemes, non-generalizable to new architectures. In this work, we study a highly-challenging yet barely-explored task, any structural pruning, to tackle general structural pruning of arbitrary architecture like CNNs, RNNs, GNNs and Transformers. The most prominent obstacle towards this goal lies in the structural coupling, which not only forces different layers to be pruned simultaneously, but also expects all removed parameters to be consistently unimportant, thereby avoiding structural issues and significant performance degradation after pruning. To address this problem, we propose a general and {fully automatic} method, \emph{Dependency Graph} (DepGraph), to explicitly model the dependency between layers and comprehens
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#20248;&#20808;&#32423;&#26469;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#19968;&#26102;&#21051;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;MORL&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#32463;&#39564;&#12290;</title><link>http://arxiv.org/abs/2301.07784</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#31574;&#30053;&#20248;&#21270;&#20248;&#20808;&#32423;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07784
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#20248;&#20808;&#32423;&#26469;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20174;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#19968;&#26102;&#21051;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;MORL&#38382;&#39064;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#21382;&#21490;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064; (MORL) &#31639;&#27861;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#21487;&#33021;&#20914;&#31361;&#30340;&#22870;&#21169;&#20989;&#25968;&#19978;&#26377;&#19981;&#21516;&#20559;&#22909;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#23398;&#20064;&#19968;&#32452;&#31574;&#30053;&#65288;&#27599;&#20010;&#31574;&#30053;&#37117;&#26159;&#20026;&#19981;&#21516;&#20195;&#29702;&#20559;&#22909;&#32780;&#20248;&#21270;&#30340;&#65289;&#65292;&#36825;&#32452;&#31574;&#30053;&#21487;&#20197;&#21518;&#26469;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26032;&#20559;&#22909;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827; (GPI) &#23450;&#20041;&#20102;&#21407;&#21017;&#19978;&#24471;&#20986;&#30340;&#32452;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22266;&#23450;&#26679;&#26412;&#25968;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#65292;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#21487;&#20197;&#22312;&#27599;&#19968;&#26102;&#21051;&#30830;&#23450;&#26368;&#26377;&#21069;&#36884;&#30340;&#20559;&#22909;&#25110;&#30446;&#26631;&#65292;&#20197;&#26356;&#24555;&#22320;&#35299;&#20915;&#32473;&#23450;&#30340;MORL&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#20063;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;Dyna&#39118;&#26684;&#30340;MORL&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23398;&#20064;&#29305;&#23450;&#20195;&#29702;&#20559;&#22909;&#30340;&#31574;&#30053;&#26102;&#26368;&#30456;&#20851;&#30340;&#20197;&#24448;&#32463;&#39564;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#22987;&#32456;&#22312;&#26377;&#38480;&#30340;&#27493;&#25968;&#20869;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#65292;&#25110;&#25910;&#25947;&#21040;&#36317;&#31163;&#26368;&#20248;&#35299; $\epsilon$-o&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\epsilon$-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#24352;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#65292;&#35757;&#32451;&#26102;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.02232</link><description>&lt;p&gt;
&#21333;&#24352;&#22270;&#20687;&#26080;&#31867;&#21035;3D&#20851;&#33410;&#36716;&#31227;&#30340;CA$^2$T-Net&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image. (arXiv:2301.02232v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#23558;&#21333;&#24352;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#65292;&#35757;&#32451;&#26102;&#21482;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21333;&#24103;&#22270;&#20687;&#20013;&#20851;&#33410;&#29289;&#20307;&#30340;&#36816;&#21160;&#36716;&#31227;&#21040;&#26410;&#32463;&#35843;&#25972;&#30340;3D&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#23398;&#20064;&#39044;&#27979;&#29289;&#20307;&#30340;&#23039;&#24577;&#12289;&#37096;&#20998;&#20998;&#21106;&#21644;&#30456;&#24212;&#30340;&#36816;&#21160;&#21442;&#25968;&#65292;&#20197;&#37325;&#29616;&#36755;&#20837;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#20851;&#33410;&#36816;&#21160;&#12290;&#32593;&#32476;&#30001;&#19977;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#23427;&#20204;&#37319;&#29992;&#20849;&#20139;&#30340;&#32852;&#21512;&#22270;&#20687;&#24418;&#29366;&#23884;&#20837;&#65292;&#24182;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23545;&#35937;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#33258;&#21160;&#22320;&#20026;&#32593;&#26684;&#28155;&#21152;&#21160;&#30011;&#65292;&#20174;&#30495;&#23454;&#22270;&#20687;&#20013;&#25512;&#26029;&#36816;&#21160;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#38388;&#23558;&#36816;&#21160;&#36716;&#31227;&#21040;&#21151;&#33021;&#19978;&#30456;&#20284;&#20294;&#20960;&#20309;&#19978;&#19981;&#21516;&#30340;3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network approach to transfer the motion from a single image of an articulated object to a rest-state (i.e., unarticulated) 3D model. Our network learns to predict the object's pose, part segmentation, and corresponding motion parameters to reproduce the articulation shown in the input image. The network is composed of three distinct branches that take a shared joint image-shape embedding and is trained end-to-end. Unlike previous methods, our approach is independent of the topology of the object and can work with objects from arbitrary categories. Our method, trained with only synthetic data, can be used to automatically animate a mesh, infer motion from real images, and transfer articulation to functionally similar but geometrically distinct 3D models at test time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#30340;&#26032;&#22411;WSSS&#26694;&#26550;CLIP-ES&#65292;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#37319;&#29992;softmax&#20989;&#25968;&#12289;&#38160;&#24230;&#39537;&#21160;&#25552;&#31034;&#36873;&#25321;&#21644;&#21516;&#20041;&#35789;&#34701;&#21512;&#31561;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;WSSS&#30340;&#19977;&#20010;&#38454;&#27573;&#30340;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#21033;&#29992;&#20102;CLIP&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.09506</link><description>&lt;p&gt;
CLIP&#20063;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#20998;&#21106;&#22120;&#65306;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation. (arXiv:2212.09506v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#30340;&#26032;&#22411;WSSS&#26694;&#26550;CLIP-ES&#65292;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#37319;&#29992;softmax&#20989;&#25968;&#12289;&#38160;&#24230;&#39537;&#21160;&#25552;&#31034;&#36873;&#25321;&#21644;&#21516;&#20041;&#35789;&#34701;&#21512;&#31561;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;WSSS&#30340;&#19977;&#20010;&#38454;&#27573;&#30340;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#21033;&#29992;&#20102;CLIP&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#27604;&#24230;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;CLIP&#65289;&#22312;&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26412;&#22320;&#21270;&#19981;&#21516;&#31867;&#21035;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIP-ES&#30340;&#26032;&#22411;WSSS&#26694;&#26550;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware a
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#38544;&#24335;&#20989;&#25968;&#21644;&#26174;&#24335;&#36523;&#20307;&#27169;&#22411;&#65292;ECON&#26041;&#27861;&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#30528;&#35013;&#31435;&#20307;&#20154;&#20307;&#65292;&#24182;&#21487;&#20197;&#24674;&#22797;&#23485;&#26494;&#26381;&#35013;&#31561;&#33258;&#30001;&#24418;&#24335;&#34920;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.07422</link><description>&lt;p&gt;
ECON: &#36890;&#36807;&#27861;&#32447;&#31215;&#20998;&#36827;&#34892;&#20248;&#21270;&#30340;&#26174;&#24335;&#34915;&#30528;&#20154;&#29289;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
ECON: Explicit Clothed humans Optimized via Normal integration. (arXiv:2212.07422v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07422
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#38544;&#24335;&#20989;&#25968;&#21644;&#26174;&#24335;&#36523;&#20307;&#27169;&#22411;&#65292;ECON&#26041;&#27861;&#25104;&#21151;&#22320;&#37325;&#24314;&#20102;&#30528;&#35013;&#31435;&#20307;&#20154;&#20307;&#65292;&#24182;&#21487;&#20197;&#24674;&#22797;&#23485;&#26494;&#26381;&#35013;&#31561;&#33258;&#30001;&#24418;&#24335;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#33402;&#26415;&#23478;&#31574;&#21010;&#30340;&#25195;&#25551;&#21644;&#38544;&#24335;&#20989;&#25968;&#65288;IF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20013;&#21019;&#24314;&#35814;&#32454;&#30340;&#30528;&#35013;&#31435;&#20307;&#20154;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23578;&#19981;&#23436;&#32654;&#12290;&#22522;&#20110;IF&#30340;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#33258;&#30001;&#24418;&#24335;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20294;&#26159;&#23545;&#20110;&#26032;&#23039;&#21183;&#25110;&#26032;&#35013;&#30340;&#20154;&#29289;&#65292;&#20250;&#20135;&#29983;&#33073;&#31163;&#36523;&#20307;&#30340;&#22235;&#32930;&#25110;&#36864;&#21270;&#30340;&#24418;&#29366;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#20123;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#29616;&#26377;&#24037;&#20316;&#20351;&#29992;&#26174;&#24335;&#21442;&#25968;&#20154;&#20307;&#27169;&#22411;&#32422;&#26463;&#34920;&#38754;&#37325;&#24314;&#65292;&#20294;&#36825;&#38480;&#21046;&#20102;&#20174;&#36523;&#20307;&#20559;&#31163;&#30340;&#23485;&#26494;&#26381;&#35013;&#31561;&#33258;&#30001;&#24418;&#24335;&#34920;&#38754;&#30340;&#24674;&#22797;&#12290;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#38544;&#24335;&#34920;&#31034;&#21644;&#26174;&#24335;&#36523;&#20307;&#35268;&#33539;&#21270;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26377;&#20004;&#20010;&#37325;&#35201;&#30340;&#35266;&#23519;&#65306;&#65288;1&#65289;&#24403;&#21069;&#30340;&#32593;&#32476;&#26356;&#25797;&#38271;&#25512;&#26029;&#35814;&#32454;&#30340;2D&#36148;&#22270;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#19977;&#32500;&#34920;&#38754;&#65292;&#65288;2&#65289;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#30475;&#20316;&#26159;&#23680;&#20102;&#23616;&#37096;&#34920;&#38754;&#25340;&#25509;&#30340;&#8220;&#30011;&#24067;&#8221;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;ECON&#26377;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;
&lt;/p&gt;
&lt;p&gt;
The combination of deep learning, artist-curated scans, and Implicit Functions (IF), is enabling the creation of detailed, clothed, 3D humans from images. However, existing methods are far from perfect. IF-based methods recover free-form geometry, but produce disembodied limbs or degenerate shapes for novel poses or clothes. To increase robustness for these cases, existing work uses an explicit parametric body model to constrain surface reconstruction, but this limits the recovery of free-form surfaces such as loose clothing that deviates from the body. What we want is a method that combines the best properties of implicit representation and explicit body regularization. To this end, we make two key observations: (1) current networks are better at inferring detailed 2D maps than full-3D surfaces, and (2) a parametric model can be seen as a "canvas" for stitching together detailed surface patches. Based on these, our method, ECON, has three main steps: (1) It infers detailed 2D normal m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;32&#31687;&#30456;&#20851;&#30740;&#31350;&#65292;&#24635;&#32467;&#20986;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#24212;&#29992;&#36739;&#26032;&#65292;&#30740;&#31350;&#31038;&#21306;&#30456;&#23545;&#20998;&#25955;&#12290;</title><link>http://arxiv.org/abs/2211.11482</link><description>&lt;p&gt;
&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Applications of statistical causal inference in software engineering. (arXiv:2211.11482v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;32&#31687;&#30456;&#20851;&#30740;&#31350;&#65292;&#24635;&#32467;&#20986;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#24212;&#29992;&#36739;&#26032;&#65292;&#30740;&#31350;&#31038;&#21306;&#30456;&#23545;&#20998;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#24212;&#29992;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#20851;&#30740;&#31350;&#24037;&#20316;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;2010&#24180;&#33267;2022&#24180;&#38388;&#21457;&#34920;&#30340;32&#31687;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30340;&#24212;&#29992;&#30456;&#23545;&#36739;&#26032;&#65292;&#30456;&#24212;&#30340;&#30740;&#31350;&#31038;&#21306;&#20173;&#28982;&#30456;&#23545;&#20998;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews existing work in software engineering that applies statistical causal inference methods. These methods aim at estimating causal effects from observational data. The review covers 32 papers published between 2010 and 2022. Our results show that the application of statistical causal inference methods is relatively recent and that the corresponding research community remains relatively fragmented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#34920;&#31034;&#27169;&#22411;NAR-Former&#26469;&#20840;&#38754;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#26631;&#35760;&#22120;&#21644;&#22810;&#38454;&#27573;&#34701;&#21512;&#21464;&#21387;&#22120;&#26500;&#24314;&#32039;&#20945;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#20449;&#24687;&#27969;&#19968;&#33268;&#24615;&#22686;&#24378;&#21644;&#26550;&#26500;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20272;&#35745;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.08024</link><description>&lt;p&gt;
NAR-Former&#65306;&#38754;&#21521;&#20840;&#38754;&#23646;&#24615;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction. (arXiv:2211.08024v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#34920;&#31034;&#27169;&#22411;NAR-Former&#26469;&#20840;&#38754;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#26631;&#35760;&#22120;&#21644;&#22810;&#38454;&#27573;&#34701;&#21512;&#21464;&#21387;&#22120;&#26500;&#24314;&#32039;&#20945;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#20449;&#24687;&#27969;&#19968;&#33268;&#24615;&#22686;&#24378;&#21644;&#26550;&#26500;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20272;&#35745;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#21644;&#28145;&#20837;&#37319;&#29992;&#65292;&#36234;&#26469;&#36234;&#38656;&#35201;&#23545;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#65292;&#32780;&#19981;&#38656;&#35201;&#36816;&#34892;&#23454;&#38469;&#30340;&#35757;&#32451;&#25110;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20840;&#38754;&#20272;&#35745;&#36825;&#20123;&#23646;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26631;&#35760;&#22120;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#25805;&#20316;&#21644;&#25299;&#25169;&#20449;&#24687;&#32534;&#30721;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#34701;&#21512;&#21464;&#21387;&#22120;&#65292;&#20174;&#36716;&#25442;&#21518;&#30340;&#24207;&#21015;&#20013;&#26500;&#24314;&#32039;&#20945;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20026;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20449;&#24687;&#27969;&#19968;&#33268;&#24615;&#22686;&#24378;&#65292;&#24182;&#30456;&#24212;&#22320;&#35774;&#35745;&#20102;&#19968;&#20010;&#26550;&#26500;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#19982;&#39044;&#27979;&#30340;&#25439;&#22833;&#30456;&#27604;&#65292;&#26356;&#23569;&#30340;&#22686;&#24191;&#26679;&#26412;&#24102;&#26469;&#20102;&#26356;&#22810;&#30340;&#22909;&#22788;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340; NAR-Former &#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19981;&#21516;&#31070;&#32463;&#26550;&#26500;&#30340;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20272;&#35745;&#26041;&#38754;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide and deep adoption of deep learning models in real applications, there is an increasing need to model and learn the representations of the neural networks themselves. These models can be used to estimate attributes of different neural network architectures such as the accuracy and latency, without running the actual training or inference tasks. In this paper, we propose a neural architecture representation model that can be used to estimate these attributes holistically. Specifically, we first propose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fusion transformer to build a compact vector representation from the converted sequence. For efficient model training, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20026;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#21644;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#32463;&#30001;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#30340;&#26816;&#39564;&#65292;&#33021;&#22815;&#22823;&#24133;&#24230;&#36229;&#36234;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2210.16848</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#21644;&#22270;&#24418;&#35013;&#37197;&#25913;&#36827;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#20026;&#23558;&#26356;&#22810;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#21644;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#32463;&#30001;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#30340;&#26816;&#39564;&#65292;&#33021;&#22815;&#22823;&#24133;&#24230;&#36229;&#36234;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#20302;&#12289;&#37096;&#32626;&#20415;&#25463;&#12289;&#31283;&#23450;&#24615;&#39640;&#65292;&#20256;&#32479;&#30340;&#38745;&#24577;&#23884;&#20837;&#65288;&#20363;&#22914;Skip-gram&#12289;Word2Vec&#65289;&#20173;&#22312;&#20302;&#36164;&#28304;&#21644;&#36731;&#37327;&#32423;&#29615;&#22659;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20197;&#19979;&#26041;&#27861;&#25913;&#36827;&#35789;&#23884;&#20837;&#65306;1&#65289;&#23558;&#26356;&#22810;&#20174;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#32435;&#20837;Skip-gram&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Context-to-Vec&#65307;2&#65289;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#20808;&#39564;&#21516;&#20041;&#35789;&#30693;&#35782;&#21644;&#21152;&#26435;&#21521;&#37327;&#20998;&#24067;&#30340;&#38745;&#24577;&#23884;&#20837;&#21518;&#22788;&#29702;&#35013;&#37197;&#26041;&#27861;&#65292;&#29420;&#31435;&#20110;&#35757;&#32451;&#12290;&#36890;&#36807;&#22806;&#37096;&#21644;&#20869;&#37096;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#20197;&#22823;&#24133;&#36229;&#36234;&#22522;&#20934;&#32447;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;</title><link>http://arxiv.org/abs/2210.15387</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#25345;&#32493;&#27835;&#30103;&#21644;&#24247;&#22797;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#38750;&#20856;&#22411;&#21457;&#38899;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;Wav2vec 2.0 XLS-R&#36827;&#34892;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#21644;&#36741;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#23545;&#20110;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#22768;&#23398;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22914;SVM&#12289;MLP&#21644;XGBoost&#12290;&#22312;&#38889;&#22269;&#21457;&#38899;&#38556;&#30861;QoLT&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#25506;&#31350;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1-score&#30456;&#23545;&#25552;&#39640;1.25%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#27809;&#26377;ASR&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;10.61%&#30340;&#30456;&#23545;&#30334;&#20998;&#27604;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of dysarthric speech is essential for sustained treatments and rehabilitation. However, obtaining atypical speech is challenging, often leading to data scarcity issues. To tackle the problem, we propose a novel automatic severity assessment method for dysarthric speech, using the self-supervised model in conjunction with multi-task learning. Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity classification and auxiliary automatic speech recognition (ASR). For the baseline experiments, we employ hand-crafted acoustic features and machine learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean dysarthric speech QoLT database, our model outperforms the traditional baseline methods, with a relative percentage increase of 1.25% for F1-score. In addition, the proposed model surpasses the model trained without ASR head, achieving 10.61% relative percentage improvements. Furthermore, we present how multi-task learning affects the seve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25903;&#25745;&#20989;&#25968;&#23545;&#32039;&#33268;&#38598;&#21512;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#20998;&#21035;&#20026;&#20984;&#35268;&#21010;&#21644;&#38750;&#20984;&#35268;&#21010;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2210.01919</link><description>&lt;p&gt;
&#20855;&#26377;&#20984;&#21644;&#38750;&#20984;&#23376;&#32447;&#24615;&#22238;&#24402;&#30340;&#30740;&#31350;&#21450;&#20854;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#21040;&#36798;&#38598;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets. (arXiv:2210.01919v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#25903;&#25745;&#20989;&#25968;&#23545;&#32039;&#33268;&#38598;&#21512;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#20998;&#21035;&#20026;&#20984;&#35268;&#21010;&#21644;&#38750;&#20984;&#35268;&#21010;&#12290;&#26412;&#25991;&#22312;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#36880;&#28176;&#36924;&#36817;&#19968;&#33268;&#20989;&#25968;&#65288;support function&#65289;&#30340;&#23376;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#20272;&#35745;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#12290;&#25903;&#25745;&#20989;&#25968;&#22312;&#20984;&#23553;&#38381;&#36816;&#31639;&#30340;&#24847;&#20041;&#19979;&#33021;&#22815;&#21807;&#19968;&#22320;&#21051;&#30011;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#65292;&#32780;&#19988;&#26159;&#23376;&#32447;&#24615;&#65288;&#20984;&#20197;&#21450;&#19968;&#27425;&#27491;&#40784;&#27425;&#30340;&#65289;&#12290;&#30456;&#21453;&#65292;&#20219;&#20309;&#23376;&#32447;&#24615;&#20989;&#25968;&#37117;&#26159;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#30340;&#25903;&#25745;&#20989;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#24615;&#36136;&#65292;&#23558;&#23398;&#20064;&#19968;&#20010;&#32039;&#33268;&#38598;&#21512;&#30340;&#20219;&#21153;&#36716;&#21270;&#20026;&#23398;&#20064;&#23427;&#30340;&#25903;&#25745;&#20989;&#25968;&#12290;&#20026;&#20102;&#36827;&#34892;&#23376;&#32447;&#24615;&#22238;&#24402;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#20984;&#35268;&#21010;&#27714;&#35299;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#24471;&#21040;&#25903;&#25745;&#20989;&#25968;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#35757;&#32451;&#23376;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#38750;&#20984;&#35268;&#21010;&#24471;&#21040;&#25903;&#25745;&#20989;&#25968;&#12290;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20855;&#26377;&#38598;&#21512;&#20540;&#36755;&#20837;&#19981;&#30830;&#23450;&#24615;&#30340;&#21463;&#25511;&#21160;&#24577;&#21040;&#36798;&#38598;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider estimating a compact set from finite data by approximating the support function of that set via sublinear regression. Support functions uniquely characterize a compact set up to closure of convexification, and are sublinear (convex as well as positive homogeneous of degree one). Conversely, any sublinear function is the support function of a compact set. We leverage this property to transcribe the task of learning a compact set to that of learning its support function. We propose two algorithms to perform the sublinear regression, one via convex and another via nonconvex programming. The convex programming approach involves solving a quadratic program (QP). The nonconvex programming approach involves training a input sublinear neural network. We illustrate the proposed methods via numerical examples on learning the reach sets of controlled dynamics subject to set-valued input uncertainties from trajectory data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.01117</link><description>&lt;p&gt;
Omnigrok&#65306;&#29702;&#35299;&#36229;&#36234;&#31639;&#27861;&#25968;&#25454;&#30340;&#8220;Grokking&#8221;
&lt;/p&gt;
&lt;p&gt;
Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#21457;&#29616;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#65292;&#25552;&#20986;&#20102;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#21644;&#28040;&#38500;&#20102;&#20854;grokking&#29616;&#35937;&#12290;&#23427;&#20204;&#30340;dramatic grokking&#20381;&#36182;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#26159;&#19968;&#31181;&#19981;&#23547;&#24120;&#30340;&#29616;&#35937;&#65292;&#25351;&#31639;&#27861;&#25968;&#25454;&#38598;&#22312;&#36807;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#21518;&#38271;&#26102;&#38388;&#20173;&#28982;&#33021;&#36827;&#34892;&#27867;&#21270;&#65292;&#19968;&#30452;&#20197;&#26469;&#19968;&#30452;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#26223;&#35266;&#26469;&#29702;&#35299;grokking&#65292;&#24182;&#30830;&#23450;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#26159;grokking&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;LU&#26426;&#21046;&#8221;&#65292;&#22240;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#25439;&#22833;&#65288;&#23545;&#27169;&#22411;&#26435;&#37325;&#35268;&#33539;&#65289;&#36890;&#24120;&#20998;&#21035;&#31867;&#20284;&#20110;&#8220;L&#8221;&#21644;&#8220;U&#8221;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26426;&#21046;&#21487;&#20197;&#24456;&#22909;&#22320;&#35299;&#37322;grokking&#30340;&#35768;&#22810;&#26041;&#38754;&#65306;&#25968;&#25454;&#22823;&#23567;&#20381;&#36182;&#24615;&#12289;&#26435;&#37325;&#34928;&#20943;&#20381;&#36182;&#24615;&#12289;&#34920;&#31034;&#30340;&#20986;&#29616;&#31561;&#12290;&#22312;&#30452;&#35273;&#19978;&#32473;&#23450;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#28041;&#21450;&#22270;&#20687;&#12289;&#35821;&#35328;&#21644;&#20998;&#23376;&#30340;&#20219;&#21153;&#20013;&#35825;&#23548;grokking&#12290;&#21453;&#21521;&#26469;&#30475;&#65292;&#25105;&#20204;&#33021;&#22815;&#28040;&#38500;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;grokking&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25968;&#25454;&#38598;&#30340;dramatic grokking&#24402;&#22240;&#20110;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2209.12693</link><description>&lt;p&gt;
&#21033;&#29992;&#26032;&#25968;&#25454;&#22312;&#30005;&#21147;&#32593;&#36890;&#35759;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;FiN-1&#21644;FiN-2&#65292;&#20849;&#25910;&#38598;&#20102;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24212;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#65292;&#20197;&#35299;&#20915;&#30005;&#21147;&#32593;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#21644;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#32593;&#24050;&#25104;&#20026;&#26085;&#24120;&#29983;&#27963;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#23613;&#31649;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24448;&#24448;&#19981;&#20250;&#27880;&#24847;&#21040;&#12290;&#21482;&#26377;&#24403;&#30005;&#21147;&#32593;&#19981;&#20877;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#36890;&#24120;&#25165;&#20250;&#29305;&#21035;&#24847;&#35782;&#21040;&#36825;&#31181;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#37325;&#22823;&#21464;&#21270;&#65292;&#22914;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#65288;&#20809;&#20239;&#12289;&#39118;&#21147;&#28065;&#36718;&#26426;&#31561;&#65289;&#30340;&#36807;&#28193;&#20197;&#21450;&#22797;&#26434;&#36127;&#36733;&#37197;&#32622;&#65288;&#30005;&#21160;&#27773;&#36710;&#12289;&#23478;&#24237;&#30005;&#27744;&#31995;&#32479;&#31561;&#65289;&#30340;&#33021;&#28304;&#28040;&#36153;&#32773;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#32473;&#30005;&#21147;&#32593;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#23485;&#24102;&#30005;&#21147;&#32447;&#36890;&#20449;&#65288;PLC&#65289;&#22522;&#30784;&#35774;&#26045;&#27979;&#37327;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598; FiN-1 &#21644; FiN-2 &#22312;&#24503;&#22269;&#20302;&#21387;&#30005;&#32593;&#30340;&#19968;&#37096;&#20998;&#23454;&#38469;&#20351;&#29992;&#20013;&#25910;&#38598;&#65292;&#21521;&#22823;&#32422;440&#19975;&#20154;&#25552;&#20379;&#26381;&#21153;&#65292;&#24182;&#26174;&#31034;5100&#22810;&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#36229;&#36807;130&#20159;&#20010;&#25968;&#25454;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#29992;&#20363;&#65292;&#29992;&#20110;&#36164;&#20135;&#31649;&#29702;&#12289;&#30005;&#32593;&#29366;&#24577;&#21487;&#35270;&#21270;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.06203</link><description>&lt;p&gt;
&#38024;&#23545;&#24178;&#39044;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#36890;&#24120;&#36890;&#36807;&#28508;&#22312;&#32467;&#26524;&#30340;&#22343;&#20540;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65289;&#26469;&#35745;&#31639;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#28508;&#22312;&#32467;&#26524;&#20998;&#24067;&#30340;&#20840;&#37096;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32452;&#21512;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#27969;&#65292;&#21363;&#65288;i&#65289;&#29992;&#20110;&#20272;&#35745;&#24178;&#25200;&#21442;&#25968;&#30340;nuisance flow&#21644;&#65288;ii&#65289;&#29992;&#20110;&#21442;&#25968;&#21270;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;target flow&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22522;&#20110;&#21333;&#27493;&#20559;&#24046;&#26657;&#27491;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#26041;&#24335;&#20272;&#35745;&#30446;&#26631;&#27969;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#30830;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#20272;&#35745;&#22120;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04747</link><description>&lt;p&gt;
&#35270;&#35273;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04747
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#35270;&#35273;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#20854;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#20197;&#21450;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#30340;&#35752;&#35770;&#65292;&#24182;&#24635;&#32467;&#27604;&#36739;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#23637;&#29616;&#20102;&#22312;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#38750;&#20961;&#30340;&#32467;&#26524;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21069;&#21521;&#25193;&#25955;&#21644;&#21453;&#21521;&#25193;&#25955;&#12290;&#22312;&#21069;&#21521;&#25193;&#25955;&#38454;&#27573;&#65292;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#36880;&#28176;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#21453;&#21521;&#38454;&#27573;&#65292;&#27169;&#22411;&#34987;&#20219;&#21153;&#20026;&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#36870;&#36716;&#25193;&#25955;&#36807;&#31243;&#65292;&#36880;&#27493;&#24674;&#22797;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#36739;&#22823;&#65292;&#21363;&#30001;&#20110;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#25968;&#37327;&#36739;&#22810;&#23548;&#33268;&#30340;&#36895;&#24230;&#36739;&#24930;&#65292;&#20294;&#20854;&#25152;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20173;&#28982;&#21463;&#21040;&#24191;&#27867;&#27427;&#36175;&#12290;&#22312;&#26412;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#35270;&#35273;&#20013;&#24212;&#29992;&#30340;&#32508;&#21512;&#24615;&#35780;&#35770;&#65292;&#21253;&#25324;&#35813;&#39046;&#22495;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20171;&#32461;&#20102;&#19977;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#65306;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#28151;&#21512;&#25193;&#25955;&#27169;&#22411;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#21508;&#31181;&#31639;&#27861;&#21644;&#26550;&#26500;&#26041;&#38754;&#65292;&#22914;&#20351;&#29992; L&#233;vy &#36807;&#31243;&#12289;&#19981;&#21516;&#24418;&#24335;&#30340;&#22122;&#22768;&#12289;&#27169;&#22411;&#26465;&#20214;&#21644;&#27491;&#21017;&#21270;&#12289;&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#24182;&#34892;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#24182;&#27604;&#36739;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2209.00796</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.00796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#24635;&#32467;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#21644;&#24212;&#29992;&#30740;&#31350;&#65292;&#21253;&#25324;&#39640;&#25928;&#37319;&#26679;&#12289;&#20284;&#28982;&#20272;&#35745;&#19982;&#29305;&#27530;&#32467;&#26500;&#25968;&#25454;&#22788;&#29702;&#65292;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#21147;&#24182;&#22238;&#39038;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31867;&#20855;&#26377;&#35760;&#24405;&#24615;&#33021;&#30340;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#21644;&#20998;&#23376;&#35774;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#36825;&#20221;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#25193;&#23637;&#30740;&#31350;&#65292;&#23558;&#30740;&#31350;&#20998;&#31867;&#20026;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39640;&#25928;&#37319;&#26679;&#12289;&#25913;&#36827;&#30340;&#20284;&#28982;&#20272;&#35745;&#21644;&#22788;&#29702;&#20855;&#26377;&#29305;&#27530;&#32467;&#26500;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#20197;&#23454;&#29616;&#22686;&#24378;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12289;&#26102;&#38388;&#25968;&#25454;&#24314;&#27169;&#20197;&#21450;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#30340;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#20010;&#35843;&#30740;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#32972;&#26223;&#30340;&#28145;&#20837;&#20102;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20851;&#38190;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#25351;&#26126;&#21487;&#33021;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2207.11143</link><description>&lt;p&gt;
&#12298;&#37319;&#29992;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#23454;&#29616;&#21512;&#20316;MARL&#20840;&#23616;&#26368;&#20248;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework. (arXiv:2207.11143v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;MARL&#31639;&#27861;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#19979;&#30340;&#27425;&#26368;&#20248;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#36716;&#21270;&#20026;&#21333;&#26234;&#33021;&#20307;MDP&#20197;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20998;&#25955;&#25191;&#34892;&#26159;&#19968;&#39033;&#26680;&#24515;&#38656;&#27714;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#30340;MARL&#31639;&#27861;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#26469;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#32771;&#34385;&#21040;&#20248;&#21270;&#26041;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#26799;&#24230;&#19979;&#38477;&#34987;&#36873;&#20026;&#20248;&#21270;&#26041;&#27861;&#26102;&#65292;&#21508;&#31181;&#27969;&#34892;&#30340;&#20998;&#25955;&#31574;&#30053;MARL&#31639;&#27861;&#22312;&#29609;&#20855;&#20219;&#21153;&#20013;&#37117;&#26159;&#27425;&#26368;&#20248;&#30340;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#37319;&#29992;&#20998;&#25955;&#31574;&#30053;&#30340;&#31639;&#27861;&#8212;&#8212;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#21644;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26102;&#30340;&#27425;&#26368;&#20248;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36716;&#21270;&#19982;&#33976;&#39311;&#65288;TAD&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#22810;&#26234;&#33021;&#20307;MDP&#37325;&#26032;&#21046;&#23450;&#20026;&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#32467;&#26500;&#30340;&#29305;&#27530;&#21333;&#26234;&#33021;&#20307;MDP&#65292;&#24182;&#36890;&#36807;&#33976;&#39311;&#23454;&#29616;&#20998;&#25955;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#25216;&#26415;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.11488</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#35757;&#32451;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11488
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#25216;&#26415;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#25991;&#29486;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#22823;&#22810;&#25968;&#26159;&#20351;&#29992;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#12290;&#36825;&#24341;&#36215;&#20102;&#25105;&#20204;&#23545;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#25506;&#32034;FL&#39044;&#35757;&#32451;&#30340;&#20852;&#36259;&#12290;&#22312;&#22810;&#20010;&#35270;&#35273;&#35782;&#21035;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;FL&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21487;&#20197;&#32553;&#23567;&#23427;&#19982;&#20013;&#24515;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#20934;&#30830;&#24230;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#23458;&#25143;&#25968;&#25454;&#30340;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#36866;&#29992;&#20110;&#27809;&#26377;&#30452;&#25509;&#33719;&#24471;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25110;&#29978;&#33267;&#20351;&#29992;&#23458;&#25143;&#31471;&#25968;&#25454;&#36827;&#34892;&#20998;&#25955;&#24335;&#39044;&#35757;&#32451;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#24050;&#32463;&#26174;&#33879;&#25913;&#21892;&#20102;FL&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#30340;&#35768;&#22810;&#25216;&#26415;&#20114;&#34917;&#24615;&#24456;&#24378;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25193;&#23637;&#28145;&#24230;FL&#30340;&#20851;&#38190;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con
&lt;/p&gt;</description></item><item><title>FeatER&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#29305;&#24449;&#22270;&#30340; transformer &#22788;&#29702;&#26469;&#23454;&#29616;&#20154;&#29289;&#37325;&#24314;&#65292;&#24182;&#35299;&#20915;&#20102;&#26082;&#33021;&#22815;&#22788;&#29702;&#20301;&#32622;&#25935;&#24863;&#24615;&#29305;&#24449;&#22270;&#21448;&#33021;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.15448</link><description>&lt;p&gt;
FeatER: &#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#22270;&#30340;TransformER&#23454;&#29616;&#20154;&#29289;&#37325;&#24314;&#30340;&#39640;&#25928;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER. (arXiv:2205.15448v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15448
&lt;/p&gt;
&lt;p&gt;
FeatER&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#36890;&#36807;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#29305;&#24449;&#22270;&#30340; transformer &#22788;&#29702;&#26469;&#23454;&#29616;&#20154;&#29289;&#37325;&#24314;&#65292;&#24182;&#35299;&#20915;&#20102;&#26082;&#33021;&#22815;&#22788;&#29702;&#20301;&#32622;&#25935;&#24863;&#24615;&#29305;&#24449;&#22270;&#21448;&#33021;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;transformer&#22312;2D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12289;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#20154;&#20307;&#32593;&#26684;&#37325;&#24314;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#30340;&#29305;&#24449;&#26144;&#23556;&#34920;&#31034;&#36890;&#24120;&#39318;&#20808;&#30001; CNN&#65288;&#22914; HRNet&#65289;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#28982;&#21518;&#36890;&#36807; transformer &#36827;&#19968;&#27493;&#22788;&#29702;&#20197;&#39044;&#27979; HPE &#25110; HMR &#30340;&#28909;&#21147;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; transformer &#26550;&#26500;&#19981;&#33021;&#30452;&#25509;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#22270;&#36755;&#20837;&#65292;&#38656;&#35201;&#23545;&#20855;&#26377;&#20301;&#32622;&#25935;&#24863;&#24615;&#30340;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#19981;&#33258;&#28982;&#30340;&#23637;&#24179;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340; HPE &#21644; HMR &#26041;&#27861;&#20013;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#24615;&#33021;&#25913;&#36827;&#26159;&#20197;&#36234;&#26469;&#36234;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FeatER&#65292;&#19968;&#31181;&#26032;&#39062;&#30340; transformer &#35774;&#35745;&#65292;&#21487;&#20197;&#20445;&#25345;&#20154;&#20307;&#32467;&#26500;&#20449;&#24687;&#30340;&#20869;&#22312;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D human pose estimation (2D HPE), 3D human pose estimation (3D HPE), and human mesh reconstruction (HMR) tasks. In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps (encodes each joint's location into a feature map with a Gaussian distribution) for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, we propose FeatER, a novel transformer design that preserves the inherent structure of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#33258;&#20027;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.11947</link><description>&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#22478;&#24066;&#39550;&#39542;&#29615;&#22659;&#20013;&#35780;&#20272;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#20027;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21333;&#20010;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#33258;&#20027;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#29992;&#20110;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#35757;&#32451;&#33258;&#20027;&#36710;&#31574;&#30053;&#12290;&#30001;&#20110;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22823;&#37327;&#21487;&#29992;&#24615;&#65292;&#24182;&#32570;&#20047;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#31995;&#32479;&#27604;&#36739;&#65292;&#25105;&#20204;&#19981;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26356;&#26377;&#25928;&#22320;&#29992;&#20110;&#21333;&#36710;&#21644;&#22810;&#36710;&#39550;&#39542;&#29615;&#22659;&#20013;&#33258;&#20027;&#27773;&#36710;&#36719;&#20214;&#30340;&#35757;&#32451;&#12290;&#20026;&#35780;&#20272;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#19988;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;&#21644;&#27604;&#36739;&#20998;&#26512;&#21333;&#20010;&#21644;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#21033;&#29992;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#65292;&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20195;&#29702;&#39550;&#39542;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning is actively used for training autonomous car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training autonomous car software in single-agent as well as multi-agent driving environments. A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of discrete and continuous action space deep reinforcement learning algorithms. We also prop
&lt;/p&gt;</description></item><item><title>AMRA*&#26159;&#19968;&#31181;&#20219;&#24847;&#22810;&#20998;&#36776;&#29575;&#22810;&#21551;&#21457;&#24335;A*&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#36234;&#22810;&#20010;&#20998;&#36776;&#29575;&#25628;&#32034;&#26102;&#27178;&#36328;&#22823;&#33539;&#22260;&#30340;&#26080;&#38556;&#30861;&#21306;&#22495;&#21644;&#29421;&#31364;&#36890;&#36947;&#65292;&#21487;&#20197;&#23613;&#21487;&#33021;&#20351;&#29992;&#31895;&#20998;&#36776;&#29575;&#25214;&#21040;&#35299;&#27861;&#24182;&#36880;&#27493;&#32454;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2110.05328</link><description>&lt;p&gt;
AMRA*: &#20219;&#24847;&#22810;&#20998;&#36776;&#29575;&#22810;&#21551;&#21457;&#24335; A*&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
AMRA*: Anytime Multi-Resolution Multi-Heuristic A*. (arXiv:2110.05328v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05328
&lt;/p&gt;
&lt;p&gt;
AMRA*&#26159;&#19968;&#31181;&#20219;&#24847;&#22810;&#20998;&#36776;&#29575;&#22810;&#21551;&#21457;&#24335;A*&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#36234;&#22810;&#20010;&#20998;&#36776;&#29575;&#25628;&#32034;&#26102;&#27178;&#36328;&#22823;&#33539;&#22260;&#30340;&#26080;&#38556;&#30861;&#21306;&#22495;&#21644;&#29421;&#31364;&#36890;&#36947;&#65292;&#21487;&#20197;&#23613;&#21487;&#33021;&#20351;&#29992;&#31895;&#20998;&#36776;&#29575;&#25214;&#21040;&#35299;&#27861;&#24182;&#36880;&#27493;&#32454;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#24335;&#25628;&#32034;&#22522;&#20110;&#31163;&#25955;&#21270;&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26368;&#30701;&#36335;&#38382;&#39064;&#65292;&#24615;&#33021;&#19982;&#31163;&#25955;&#21270;&#31243;&#24230;&#23494;&#20999;&#30456;&#20851;&#12290;&#36807;&#32454;&#30340;&#31163;&#25955;&#21270;&#22797;&#26434;&#24230;&#39640;&#20294;&#21487;&#20197;&#26356;&#22909;&#22320;&#36924;&#36817;&#36830;&#32493;&#25628;&#32034;&#31354;&#38388;&#65292;&#32780;&#36807;&#31895;&#30340;&#31163;&#25955;&#21270;&#21487;&#33021;&#20250;&#29306;&#29298;&#35299;&#27861;&#36136;&#37327;&#20197;&#24555;&#36895;&#24471;&#21040;&#35299;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#29366;&#24577;&#31354;&#38388;&#65292;&#36328;&#36234;&#22810;&#20010;&#20998;&#36776;&#29575;&#23547;&#25214;&#35299;&#27861;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23450;&#20041;&#31163;&#25955;&#21270;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22810;&#20998;&#36776;&#29575;A*(MRA*)&#31639;&#27861;&#36328;&#36234;&#22810;&#20010;&#20998;&#36776;&#29575;&#25628;&#32034;&#65292;&#23427;&#22312;&#31895;&#30053;&#20998;&#36776;&#29575;&#19978;&#36941;&#21382;&#22823;&#33539;&#22260;&#30340;&#26080;&#38556;&#30861;&#21306;&#22495;&#24182;&#36867;&#33073;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#21516;&#26102;&#22312;&#36739;&#32454;&#30053;&#20998;&#36776;&#29575;&#19978;&#21487;&#20197;&#31359;&#36234;&#25152;&#35859;&#30340;&#29421;&#31364;&#36890;&#36947;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AMRA*&#65292;MRA*&#30340;&#23454;&#26102;&#29256;&#26412;&#12290;AMRA*&#23613;&#21487;&#33021;&#20351;&#29992;&#31895;&#20998;&#36776;&#29575;&#24555;&#36895;&#25214;&#21040;&#35299;&#27861;&#65292;&#22914;&#26524;&#38656;&#35201;&#36880;&#27493;&#32454;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#21551;&#21457;&#24335;&#26469;&#22312;&#27599;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#20248;&#20808;&#32771;&#34385;&#25628;&#32034;&#26041;&#21521;&#12290;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;AMRA*&#20248;&#20110;MRA*&#21644;&#20854;&#20182;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heuristic search-based motion planning algorithms typically discretise the search space in order to solve the shortest path problem. Their performance is closely related to this discretisation. A fine discretisation allows for better approximations of the continuous search space, but makes the search for a solution more computationally costly. A coarser resolution might allow the algorithms to find solutions quickly at the expense of quality. For large state spaces, it can be beneficial to search for solutions across multiple resolutions even though defining the discretisations is challenging. The recently proposed algorithm Multi-Resolution A* (MRA*) searches over multiple resolutions. It traverses large areas of obstacle-free space and escapes local minima at a coarse resolution. It can also navigate so-called narrow passageways at a finer resolution. In this work, we develop AMRA*, an anytime version of MRA*. AMRA* tries to find a solution quickly using the coarse resolution as much
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#34920;&#36848;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#26377;&#26395;&#25552;&#39640;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#25928;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2109.04587</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35299;&#30721;&#25216;&#26415;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#26367;&#20195;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#34920;&#36848;&#24182;&#24212;&#29992;&#20102;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#26377;&#26395;&#25552;&#39640;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#30340;&#25928;&#29575;&#21644;&#25968;&#25454;&#20351;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#35299;&#26512;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#23558;&#35299;&#26512;&#20316;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#24207;&#21015;&#35299;&#30721;&#22120;&#29983;&#25104;&#39044;&#27979;&#20540;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26367;&#20195;&#33539;&#24335;&#12290;&#25105;&#20204;&#23558;&#35821;&#20041;&#35299;&#26512;&#20316;&#20026;&#20381;&#36182;&#35299;&#26512;&#20219;&#21153;&#36827;&#34892;&#20102;&#34920;&#36848;&#65292;&#24212;&#29992;&#20102;&#38024;&#23545;&#21477;&#27861;&#35299;&#26512;&#24320;&#21457;&#30340;&#22522;&#20110;&#22270;&#30340;&#35299;&#30721;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312; TOP &#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#32473;&#23450;&#30456;&#21516;&#39044;&#20808;&#35757;&#32451;&#30340; Transformer &#32534;&#30721;&#22120;&#30340;&#21508;&#31181;&#35299;&#30721;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#25110;&#20165;&#21253;&#21547;&#37096;&#20998;&#27880;&#37322;&#31034;&#20363;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#35774;&#32622;&#19978;&#19982;&#24207;&#21015;&#35299;&#30721;&#22120;&#30456;&#24403;&#31454;&#20105;&#65292;&#24182;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for semantic parsing in recent years is to formulate parsing as a sequence-to-sequence task, generating predictions with auto-regressive sequence decoders. In this work, we explore an alternative paradigm. We formulate semantic parsing as a dependency parsing task, applying graph-based decoding techniques developed for syntactic parsing. We compare various decoding techniques given the same pre-trained Transformer encoder on the TOP dataset, including settings where training data is limited or contains only partially-annotated examples. We find that our graph-based approach is competitive with sequence decoders on the standard setting, and offers significant improvements in data efficiency and settings where partially-annotated data is available.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#33410;&#28857;&#23884;&#20837;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26469;&#28040;&#38500;&#20256;&#36882;&#24615;&#20551;&#35774;&#65292;&#20351;&#24471;&#25311;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#21487;&#20197;&#39640;&#25928;&#22320;&#21387;&#32553;&#32593;&#32476;&#65292;&#20801;&#35768;&#22810;&#31181;&#26368;&#36817;&#37051;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#20197;&#34987;&#25554;&#20837;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2106.09671</link><description>&lt;p&gt;
&#26080;&#21521;&#22270;&#30340;&#25311;&#27431;&#20960;&#37324;&#24471;&#21560;&#24341;&#25490;&#26021;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Euclidean Attract-Repel Embeddings for Undirected Graphs. (arXiv:2106.09671v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09671
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#33410;&#28857;&#23884;&#20837;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26469;&#28040;&#38500;&#20256;&#36882;&#24615;&#20551;&#35774;&#65292;&#20351;&#24471;&#25311;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#21487;&#20197;&#39640;&#25928;&#22320;&#21387;&#32553;&#32593;&#32476;&#65292;&#20801;&#35768;&#22810;&#31181;&#26368;&#36817;&#37051;&#30340;&#27010;&#24565;&#65292;&#24182;&#21487;&#20197;&#34987;&#25554;&#20837;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#31215;&#23884;&#20837;&#23558;&#22270;&#26500;&#36896;&#20026;&#33410;&#28857;&#30690;&#37327;&#65292;&#20351;&#24471;&#20004;&#20010;&#30690;&#37327;&#20043;&#38388;&#30340;&#28857;&#31215;&#32473;&#20986;&#36793;&#30340;&#24378;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#29983;&#25104;&#22270;&#24418;&#30340;&#37325;&#35201;&#21147;&#37327;&#23548;&#33268;&#20102;&#38750;&#20256;&#36882;&#20851;&#31995;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#33410;&#28857;&#23884;&#20837;&#25311;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#26469;&#28040;&#38500;&#20256;&#36882;&#24615;&#20551;&#35774;&#65292;&#32473;&#27599;&#20010;&#33410;&#28857;&#19968;&#20010;&#21560;&#24341;&#21644;&#19968;&#20010;&#25490;&#26021;&#30690;&#37327;&#65292;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#31215;&#36890;&#36807;&#22312;&#21560;&#24341;&#30690;&#37327;&#20013;&#36827;&#34892;&#28857;&#31215;&#24182;&#22312;&#25490;&#26021;&#30690;&#37327;&#20013;&#36827;&#34892;&#28857;&#31215;&#30340;&#32467;&#26524;&#30456;&#20943;&#26469;&#23450;&#20041;&#12290;&#25311;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#21487;&#20197;&#39640;&#25928;&#22320;&#21387;&#32553;&#32593;&#32476;&#65292;&#20801;&#35768;&#22810;&#31181;&#26368;&#36817;&#37051;&#30340;&#27010;&#24565;&#65292;&#27599;&#31181;&#27010;&#24565;&#37117;&#26377;&#33258;&#24049;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#8220;&#25554;&#20837;&#8221;&#21040;&#29616;&#26377;&#30340;&#27169;&#22411;&#20013;&#65292;&#27604;&#22914;&#25351;&#25968;&#26063;&#23884;&#20837;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dot product embeddings take a graph and construct vectors for nodes such that dot products between two vectors give the strength of the edge. Dot products make a strong transitivity assumption, however, many important forces generating graphs in the real world lead to non-transitive relationships. We remove the transitivity assumption by embedding nodes into a pseudo-Euclidean space - giving each node an attract and a repel vector. The inner product between two nodes is defined by taking the dot product in attract vectors and subtracting the dot product in repel vectors. Pseudo-Euclidean embeddings can compress networks efficiently, allow for multiple notions of nearest neighbors each with their own interpretation, and can be `slotted' into existing models such as exponential family embeddings or graph neural networks for better link prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#36890;&#36807;&#21482;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#30456;&#37051;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#65292;&#24182;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#25628;&#32034;&#30340;Q*&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#39764;&#26041;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2102.04518</link><description>&lt;p&gt;
&#19981;&#25193;&#23637;&#30340;A*&#25628;&#32034;&#65306;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks. (arXiv:2102.04518v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#65292;&#36890;&#36807;&#21482;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#30456;&#37051;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#65292;&#24182;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#25628;&#32034;&#30340;Q*&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#22823;&#24133;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#22312;&#39764;&#26041;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20351;&#29992; A* &#25628;&#32034;&#35299;&#20915;&#20855;&#26377;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#26159;&#22240;&#20026; A* &#25628;&#32034;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#38543;&#30528;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#24403; A* &#25628;&#32034;&#20351;&#29992;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#23398;&#20064;&#21551;&#21457;&#24335;&#20989;&#25968;&#26102;&#65292;&#36825;&#31181;&#36127;&#25285;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Q* &#25628;&#32034;&#65292;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230; Q &#32593;&#32476;&#24341;&#23548;&#25628;&#32034;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#21033;&#29992;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#22312;&#19981;&#26174;&#24335;&#29983;&#25104;&#36825;&#20123;&#23376;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#33410;&#28857;&#30340;&#23376;&#33410;&#28857;&#30340;&#36716;&#31227;&#25104;&#26412;&#21644;&#21551;&#21457;&#24335;&#20540;&#20043;&#21644;&#21487;&#20197;&#36890;&#36807;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#35745;&#31639;&#12290;&#36825;&#26174;&#30528;&#38477;&#20302;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#19988;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992; Q* &#25628;&#32034;&#26469;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#20204;&#34920;&#31034;&#20026;&#19968;&#20010;&#21253;&#21547; 1872 &#20010;&#20803;&#21160;&#20316;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2004.12908</link><description>&lt;p&gt;
&#20195;&#34920;&#24615;&#38598;&#25104;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#21327;&#21516;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.12908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RELL&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21327;&#21516;&#38598;&#25104;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#22312;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;RELL&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#30528;&#25913;&#21892;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#24403;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#20043;&#21069;&#21644;&#23578;&#26410;&#36935;&#21040;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21017;&#20174;&#31354;&#30333;&#29366;&#24577;&#24320;&#22987;&#65292;&#20165;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#20351;&#29992;&#25968;&#25454;&#12290;&#34429;&#28982;&#20256;&#32479;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#21518;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65288;&#31216;&#20026;&#36951;&#24536;&#65289;&#12290;&#36817;&#26399;&#38024;&#23545;&#36830;&#32493;&#25110;&#32456;&#36523;&#23398;&#20064;&#30340;&#35768;&#22810;&#26041;&#27861;&#37117;&#35797;&#22270;&#22312;&#32473;&#23450;&#26032;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#20165;&#21162;&#21147;&#36991;&#20813;&#24536;&#35760;&#23558;&#30446;&#26631;&#23450;&#24471;&#36807;&#20302;&#12290;&#32456;&#36523;&#23398;&#20064;&#30340;&#30446;&#26631;&#19981;&#20165;&#24212;&#35813;&#26159;&#25552;&#39640;&#26410;&#26469;&#20219;&#21153;&#65288;&#21069;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#24212;&#35813;&#26159;&#29992;&#20219;&#20309;&#26032;&#25968;&#25454;&#25552;&#39640;&#36807;&#21435;&#20219;&#21153;&#65288;&#21453;&#21521;&#20256;&#36882;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#21327;&#21516;&#38598;&#25104;&#20998;&#21035;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#29420;&#31435;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#20934;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#32456;&#36523;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#38598;&#25104;&#65288;RELL&#65289;&#8221;&#65292;&#23427;&#38598;&#25104;&#20102;&#30693;&#35782;&#33976;&#39311;&#21644;&#30693;&#35782;&#20445;&#25345;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#19981;&#21516;&#34920;&#31034;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RELL&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#26356;&#22909;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
&lt;/p&gt;</description></item></channel></rss>