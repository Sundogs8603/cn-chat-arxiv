<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Lipschitz&#36830;&#32493;&#30340;Transformer&#27169;&#22411;LipsFormer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#22810;&#39033;Lipschitz&#36830;&#32493;&#30340;&#32452;&#20214;&#26469;&#20445;&#35777;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35753;&#28145;&#24230;Transformer&#26550;&#26500;&#31283;&#23450;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#36807;&#22810;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.09856</link><description>&lt;p&gt;
LipsFormer&#65306;&#21521;Vision Transformer&#24341;&#20837;Lipschitz&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
LipsFormer: Introducing Lipschitz Continuity to Vision Transformers. (arXiv:2304.09856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Lipschitz&#36830;&#32493;&#30340;Transformer&#27169;&#22411;LipsFormer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#22810;&#39033;Lipschitz&#36830;&#32493;&#30340;&#32452;&#20214;&#26469;&#20445;&#35777;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#20197;&#35753;&#28145;&#24230;Transformer&#26550;&#26500;&#31283;&#23450;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#36807;&#22810;&#35843;&#25972;&#23398;&#20064;&#29575;&#31561;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LipsFormer&#30340;Lipschitz&#36830;&#32493;Transformer&#65292;&#26088;&#22312;&#20174;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#36861;&#27714;Transformer-based&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;&#30456;&#27604;&#20110;&#20197;&#21069;&#36890;&#36807;&#23398;&#20064;&#29575;warmup&#12289;&#23618;&#24402;&#19968;&#21270;&#12289;attention&#20844;&#24335;&#12289;&#26435;&#37325;&#21021;&#22987;&#21270;&#31561;&#26041;&#27861;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#23454;&#38469;&#25216;&#24039;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#36830;&#32493;&#24615;&#26159;&#30830;&#20445;&#35757;&#32451;&#31283;&#23450;&#24615;&#30340;&#26356;&#37325;&#35201;&#30340;&#23646;&#24615;&#12290;&#22312;LipsFormer&#20013;&#65292;&#25105;&#20204;&#29992;Lipschitz&#36830;&#32493;&#30340;&#20013;&#24515;&#24402;&#19968;&#21270;&#20195;&#26367;&#19981;&#31283;&#23450;&#30340;Transformer&#32452;&#20214;&#27169;&#22359;&#65306;&#20013;&#24515;&#24402;&#19968;&#21270;&#20195;&#26367;&#23618;&#24402;&#19968;&#21270;&#65292;&#35889;&#21021;&#22987;&#21270;&#20195;&#26367;Xavier&#21021;&#22987;&#21270;&#65292;&#32553;&#25918;&#20313;&#24358;&#30456;&#20284;&#24230;&#27880;&#24847;&#20195;&#26367;&#28857;&#31215;&#27880;&#24847;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#27531;&#24046;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24341;&#20837;&#30340;&#27169;&#22359;&#26159;Lipschitz&#36830;&#32493;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;LipsFormer&#30340;Lipschitz&#24120;&#25968;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LipsFormer&#20801;&#35768;&#28145;&#24230;Transformer&#26550;&#26500;&#30340;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#20180;&#32454;&#35843;&#25972;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts: CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32463;&#39564;&#36801;&#31227;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#35757;&#32451;&#22797;&#26434;&#26426;&#22120;&#20154;&#36816;&#21160;&#25216;&#33021;&#30340;&#26694;&#26550;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#39640;&#24230;&#27424;&#39537;&#21160;&#31995;&#32479;&#20013;&#36827;&#34892;&#25506;&#32034;&#30340;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.09834</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#36866;&#24212;&#25935;&#25463;&#36816;&#21160;&#25216;&#33021;&#30340;&#26041;&#27861;&#65306;&#32463;&#39564;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learning and Adapting Agile Locomotion Skills by Transferring Experience. (arXiv:2304.09834v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32463;&#39564;&#36801;&#31227;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#35757;&#32451;&#22797;&#26434;&#26426;&#22120;&#20154;&#36816;&#21160;&#25216;&#33021;&#30340;&#26694;&#26550;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#39640;&#24230;&#27424;&#39537;&#21160;&#31995;&#32479;&#20013;&#36827;&#34892;&#25506;&#32034;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22312;&#19981;&#24179;&#25972;&#22320;&#24418;&#20013;&#23548;&#33322;&#21040;&#39640;&#36895;&#22868;&#36305;&#65292;&#26377;&#33151;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#28508;&#21147;&#24040;&#22823;&#12290;&#28982;&#32780;&#65292;&#20026;&#39640;&#24230;&#25935;&#25463;&#30340;&#21160;&#24577;&#36816;&#21160;&#35774;&#35745;&#31283;&#20581;&#30340;&#25511;&#21046;&#22120;&#20173;&#28982;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20154;&#21592;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35757;&#32451;&#36825;&#31181;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#39640;&#32500;&#24230;&#12289;&#27424;&#39537;&#21160;&#31995;&#32479;&#20013;&#36827;&#34892;&#25506;&#32034;&#20173;&#28982;&#26159;&#20351;&#26377;&#33151;&#26426;&#22120;&#20154;&#23398;&#20064;&#34920;&#29616;&#33391;&#22909;&#12289;&#33258;&#28982;&#32780;&#22810;&#25165;&#22810;&#33402;&#30340;&#25935;&#25463;&#25216;&#33021;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29616;&#26377;&#30340;&#25511;&#21046;&#22120;&#20013;&#36716;&#31227;&#32463;&#39564;&#26469;&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25216;&#33021;&#65292;&#20174;&#32780;&#21551;&#21160;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25511;&#21046;&#22120;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#20351;&#20854;&#22312;&#28304;&#19978;&#20855;&#26377;&#28789;&#27963;&#24615;--&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#20123;&#25511;&#21046;&#22120;&#21487;&#33021;&#24050;&#38024;&#23545;&#19981;&#21516;&#30340;&#21160;&#24577;&#32780;&#38024;&#23545;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#25110;&#32773;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#30340;&#29615;&#22659;&#30693;&#35782;--&#22240;&#27492;&#21487;&#33021;&#38750;&#24120;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legged robots have enormous potential in their range of capabilities, from navigating unstructured terrains to high-speed running. However, designing robust controllers for highly agile dynamic motions remains a substantial challenge for roboticists. Reinforcement learning (RL) offers a promising data-driven approach for automatically training such controllers. However, exploration in these high-dimensional, underactuated systems remains a significant hurdle for enabling legged robots to learn performant, naturalistic, and versatile agility skills. We propose a framework for training complex robotic skills by transferring experience from existing controllers to jumpstart learning new tasks. To leverage controllers we can acquire in practice, we design this framework to be flexible in terms of their source -that is, the controllers may have been optimized for a different objective under different dynamics, or may require different knowledge of the surroundings -- and thus may be highl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09826</link><description>&lt;p&gt;
AI&#30340;&#20844;&#24179;&#24615;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fairness in AI and Its Long-Term Implications on Society. (arXiv:2304.09826v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#20250;&#21152;&#28145;&#20559;&#35265;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#65292;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#65292;&#22240;&#27492;&#38656;&#35201;&#23547;&#27714;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#37096;&#32626;&#24050;&#32463;&#20026;&#20010;&#20154;&#21644;&#31038;&#20250;&#24102;&#26469;&#20102;&#35768;&#22810;&#31215;&#26497;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#27979;&#30340;&#20559;&#35265;&#65292;AI&#31995;&#32479;&#20063;&#34987;&#35777;&#26126;&#23545;&#37096;&#20998;&#20154;&#21475;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#30528;&#30524;&#20110;AI&#30340;&#20844;&#24179;&#24615;&#65292;&#20998;&#26512;&#20102;&#32570;&#20047;AI&#20844;&#24179;&#24615;&#26102;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#38543;&#30528;&#26102;&#38388;&#30340;&#21152;&#28145;&#32780;&#25104;&#20026;&#31038;&#20250;&#21387;&#21147;&#22240;&#32032;&#12290;&#22914;&#26524;&#38382;&#39064;&#25345;&#32493;&#23384;&#22312;&#65292;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#19982;&#20854;&#20182;&#39118;&#38505;&#30340;&#20132;&#20114;&#26469;&#21152;&#24378;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25552;&#39640;AI&#20844;&#24179;&#24615;&#30340;&#24403;&#21069;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#30830;&#20445;&#25105;&#20204;&#22312;&#19981;&#25439;&#23475;&#31038;&#20250;&#37325;&#35201;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;AI&#30340;&#22909;&#22788;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful deployment of artificial intelligence (AI) in various settings has led to numerous positive outcomes for individuals and society. However, AI systems have also been shown to harm parts of the population due to biased predictions. We take a closer look at AI fairness and analyse how lack of AI fairness can lead to deepening of biases over time and act as a social stressor. If the issues persist, it could have undesirable long-term implications on society, reinforced by interactions with other risks. We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without harming significant parts of the society.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09825</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#21152;&#36895;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20195;&#29702;&#33021;&#22815;&#23558;&#20854;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#20132;&#20114;&#12290;&#21463;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#36712;&#36857;&#30340;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#65288;2&#65289;&#21516;&#26102;&#35757;&#32451;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#31163;&#32447;&#36712;&#36857;&#30340;&#36136;&#37327;&#65288;&#36712;&#36857;&#30340;&#26368;&#20339;&#24615;&#65289;&#21644;&#22810;&#26679;&#24615;&#65288;&#36712;&#36857;&#25968;&#37327;&#21644;&#35206;&#30422;&#32423;&#21035;&#65289;&#23545;&#20004;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;MiniGrid&#29615;&#22659;&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09823</link><description>&lt;p&gt;
ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Future of ChatGPT-enabled Labor Market: A Preliminary Study. (arXiv:2304.09823v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#21450;&#22522;&#20110;&#32844;&#19994;&#30693;&#35782;&#22270;&#35889;&#24320;&#21457;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#39044;&#27979;&#20102;ChatGPT&#23545;&#26410;&#26469;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30446;&#21069;&#32422;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#38750;&#20961;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;ChatGPT&#22312;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26080;&#19982;&#20262;&#27604;&#30340;&#25104;&#21151;&#24182;&#36234;&#26469;&#36234;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#21644;&#24037;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;ChatGPT&#26679;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#26159;&#21542;&#20250;&#21462;&#20195;&#20154;&#31867;&#24037;&#20316;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#32780;&#19981;&#26159;&#23545;&#31435;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;ChatGPT&#21551;&#29992;&#30340;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#26410;&#26469;&#30340;&#21021;&#27493;&#25968;&#25454;&#39537;&#21160;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#20013;&#22269;&#26368;&#22823;&#30340;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;BOSS&#30452;&#32856;&#20013;&#30340;&#22823;&#35268;&#27169;&#32844;&#20301;&#21457;&#24067;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#21171;&#21160;&#21147;&#24066;&#22330;&#32422;&#26377;28&#65285;&#30340;&#32844;&#19994;&#38656;&#35201;ChatGPT&#30456;&#20851;&#25216;&#33021;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#32844;&#19994;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#20041;&#20449;&#24687;&#22686;&#24378;&#30340;&#21327;&#21516;&#36807;&#28388;&#31639;&#27861;&#65292;&#20197;&#39044;&#27979;&#26410;&#26469;&#32844;&#19994;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a phenomenal large language model, ChatGPT has achieved unparalleled success in various real-world tasks and increasingly plays an important role in our daily lives and work. However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs. To this end, in this paper, we introduce a preliminary data-driven study on the future of ChatGPT-enabled labor market from the view of Human-AI Symbiosis instead of Human-AI Confrontation. To be specific, we first conduct an in-depth analysis of large-scale job posting data in BOSS Zhipin, the largest online recruitment platform in China. The results indicate that about 28% of occupations in the current labor market require ChatGPT-related skills. Furthermore, based on a large-scale occupation-centered knowledge graph, we develop a semantic information enhanced collaborative filtering algorithm to predict the future occupation-skill
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#33021;&#22815;&#32553;&#23567;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2304.09821</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Reinforcement Learning for Metacognitive Interventions across Intelligent Tutoring Systems. (arXiv:2304.09821v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#33021;&#22815;&#32553;&#23567;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#26469;&#25552;&#20379;&#20803;&#35748;&#30693;&#24178;&#39044;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#20013;&#23398;&#29983;&#26410;&#26469;&#23398;&#20064;&#20934;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#36830;&#32493;&#30340;&#23398;&#26399;&#20013;&#36827;&#34892;&#20102;&#20004;&#20010;&#35838;&#22530;&#23454;&#39564;&#65306;&#23454;&#39564;1&#20351;&#29992;&#32463;&#20856;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#23558;&#23398;&#29983;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#20803;&#35748;&#30693;&#32452;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#20998;&#31867;&#32452;&#25552;&#20379;&#38745;&#24577;&#30340;&#24178;&#39044;&#12290;&#22312;&#23454;&#39564;2&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#25552;&#20379;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#32771;&#34385;&#21040;&#23398;&#29983;&#20803;&#35748;&#30693;&#27700;&#24179;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;&#23398;&#29983;&#25509;&#21463;&#20102;&#36825;&#20123;&#24178;&#39044;&#65292;&#23398;&#20064;&#22914;&#20309;&#21644;&#20309;&#26102;&#22312;&#36923;&#36753;&#36741;&#23548;&#31243;&#24207;&#19978;&#20351;&#29992;&#21521;&#21518;&#38142;&#25509;&#65288;BC&#65289;&#31574;&#30053;&#65292;&#35813;&#31243;&#24207;&#25903;&#25345;&#40664;&#35748;&#30340;&#21521;&#21069;&#38142;&#25509;&#31574;&#30053;&#12290;&#20845;&#21608;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21482;&#25903;&#25345;BC&#32780;&#27809;&#26377;&#24178;&#39044;&#30340;&#27010;&#29575;&#36741;&#23548;&#31243;&#24207;&#19978;&#23545;&#23398;&#29983;&#36827;&#34892;&#20102;&#22521;&#35757;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#36866;&#24212;&#30340;&#22522;&#20110;DRL&#30340;&#24178;&#39044;&#32553;&#23567;&#20102;&#23398;&#29983;&#20043;&#38388;&#30340;&#20803;&#35748;&#30693;&#25216;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work compares two approaches to provide metacognitive interventions and their impact on preparing students for future learning across Intelligent Tutoring Systems (ITSs). In two consecutive semesters, we conducted two classroom experiments: Exp. 1 used a classic artificial intelligence approach to classify students into different metacognitive groups and provide static interventions based on their classified groups. In Exp. 2, we leveraged Deep Reinforcement Learning (DRL) to provide adaptive interventions that consider the dynamic changes in the student's metacognitive levels. In both experiments, students received these interventions that taught how and when to use a backward-chaining (BC) strategy on a logic tutor that supports a default forward-chaining strategy. Six weeks later, we trained students on a probability tutor that only supports BC without interventions. Our results show that adaptive DRL-based interventions closed the metacognitive skills gap between students. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09820</link><description>&lt;p&gt;
&#19968;&#31181;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#21452;&#38454;&#27573;&#26694;&#26550;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain Text Classification. (arXiv:2304.09820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#21644;&#26469;&#33258;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#23436;&#25104;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#65292;&#21462;&#24471;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#30340;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#32570;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#23427;&#21033;&#29992;&#25110;&#37325;&#29992;&#19981;&#21516;&#20294;&#30456;&#20851;&#28304;&#39046;&#22495;&#30340;&#20016;&#23500;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#19987;&#27880;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#35201;&#20040;&#24573;&#30053;&#21487;&#33021;&#23384;&#22312;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#24182;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#29992;&#30340;&#39046;&#22495;&#24863;&#30693;&#29305;&#24449;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#21644;&#26469;&#33258;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#65288;SSD&#65289;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20854;&#24615;&#33021;&#65292;&#24182;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#24615;&#19978;&#22343;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain text classification aims to adapt models to a target domain that lacks labeled data. It leverages or reuses rich labeled data from the different but related source domain(s) and unlabeled data from the target domain. To this end, previous work focuses on either extracting domain-invariant features or task-agnostic features, ignoring domain-aware features that may be present in the target domain and could be useful for the downstream task. In this paper, we propose a two-stage framework for cross-domain text classification. In the first stage, we finetune the model with mask language modeling (MLM) and labeled data from the source domain. In the second stage, we further fine-tune the model with self-supervised distillation (SSD) and unlabeled data from the target domain. We evaluate its performance on a public cross-domain text classification benchmark and the experiment results show that our method achieves new state-of-the-art results for both single-source domain adaptat
&lt;/p&gt;</description></item><item><title>&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#38656;&#35201;&#20934;&#30830;&#21453;&#26144;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#23454;&#29616;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#26102;&#30340;&#30693;&#35273;&#38590;&#24230;&#65292;&#30446;&#21069;&#30740;&#31350;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09803</link><description>&lt;p&gt;
&#20851;&#20110;&#38590;&#24230;&#30693;&#35273;: &#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On the Perception of Difficulty: Differences between Humans and AI. (arXiv:2304.09803v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09803
&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#38656;&#35201;&#20934;&#30830;&#21453;&#26144;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#23454;&#29616;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#26102;&#30340;&#30693;&#35273;&#38590;&#24230;&#65292;&#30446;&#21069;&#30740;&#31350;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#22312;&#24037;&#19994;&#21644;&#31038;&#20250;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#20272;&#35745;&#38024;&#23545;&#21333;&#20010;&#20219;&#21153;&#23454;&#20363;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#38590;&#24230;&#12290;&#36825;&#20123;&#20272;&#35745;&#23545;&#20110;&#35780;&#20272;&#27599;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#23454;&#29616;&#26377;&#25928;&#30340;&#21327;&#20316;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#30740;&#31350;&#29420;&#31435;&#22320;&#20272;&#35745;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35273;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21644;&#20154;&#31867;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#25928;&#20132;&#20114;&#21462;&#20915;&#20110;&#20934;&#30830;&#21453;&#26144;&#27599;&#20010;&#20195;&#29702;&#22312;&#23454;&#29616;&#26377;&#20215;&#20540;&#30340;&#32467;&#26524;&#26102;&#30340;&#30693;&#35273;&#38590;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#30740;&#31350;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30693;&#35273;&#38590;&#24230;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20851;&#20110;&#30693;&#35273;&#38590;&#24230;&#30340;&#26368;&#26032;&#30740;&#31350;&#20197;&#21450;&#23545;&#27599;&#20010;&#20195;&#29702;&#30340;&#38590;&#24230;&#20272;&#35745;&#30340;&#36129;&#29486;&#22240;&#32032;&#36827;&#34892;&#20102;&#19968;&#33268;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased adoption of artificial intelligence (AI) in industry and society, effective human-AI interaction systems are becoming increasingly important. A central challenge in the interaction of humans with AI is the estimation of difficulty for human and AI agents for single task instances.These estimations are crucial to evaluate each agent's capabilities and, thus, required to facilitate effective collaboration. So far, research in the field of human-AI interaction estimates the perceived difficulty of humans and AI independently from each other. However, the effective interaction of human and AI agents depends on metrics that accurately reflect each agent's perceived difficulty in achieving valuable outcomes. Research to date has not yet adequately examined the differences in the perceived difficulty of humans and AI. Thus, this work reviews recent research on the perceived difficulty in human-AI interaction and contributing factors to consistently compare each agent's perc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#20272;&#35745;POMDP&#20013;&#38544;&#34255;&#29366;&#24577;&#24182;&#21487;&#35270;&#21270;&#20026;&#29366;&#24577;&#36716;&#31227;&#22270;&#30340;RL&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#24182;&#19988;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.09769</link><description>&lt;p&gt;
POMDPs&#21644;&#21487;&#35299;&#37322;&#26234;&#33021;&#20307;&#30340;&#31471;&#21040;&#31471;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Policy Gradient Method for POMDPs and Explainable Agents. (arXiv:2304.09769v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#20272;&#35745;POMDP&#20013;&#38544;&#34255;&#29366;&#24577;&#24182;&#21487;&#35270;&#21270;&#20026;&#29366;&#24577;&#36716;&#31227;&#22270;&#30340;RL&#31639;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#31616;&#21333;&#38382;&#39064;&#24182;&#19988;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#20915;&#31574;&#38382;&#39064;&#36890;&#24120;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#65292;&#24182;&#19988;&#26377;&#35768;&#22810;&#21487;&#20197;&#34987;&#24402;&#32467;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#24403;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;POMDP&#26102;&#65292;&#21512;&#29702;&#30340;&#38544;&#34255;&#29366;&#24577;&#20272;&#35745;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#26159;&#39318;&#36873;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#31561;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#20272;&#35745;&#38544;&#34255;&#29366;&#24577;&#24182;&#23558;&#20272;&#35745;&#21487;&#35270;&#21270;&#20026;&#29366;&#24577;&#36716;&#31227;&#22270;&#30340;RL&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#31616;&#21333;&#30340;POMDP&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#35270;&#21270;&#20351;&#20195;&#29702;&#30340;&#34892;&#20026;&#23545;&#20154;&#31867;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world decision-making problems are often partially observable, and many can be formulated as a Partially Observable Markov Decision Process (POMDP). When we apply reinforcement learning (RL) algorithms to the POMDP, reasonable estimation of the hidden states can help solve the problems. Furthermore, explainable decision-making is preferable, considering their application to real-world tasks such as autonomous driving cars. We proposed an RL algorithm that estimates the hidden states by end-to-end training, and visualize the estimation as a state-transition graph. Experimental results demonstrated that the proposed algorithm can solve simple POMDP problems and that the visualization makes the agent's behavior interpretable to humans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25298;&#32477;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.09762</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Practical Differentially Private and Byzantine-resilient Federated Learning. (arXiv:2304.09762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#25298;&#32477;&#22810;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#26159;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#19981;&#21487;&#25110;&#32570;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#38544;&#31169;&#21644;&#25308;&#21344;&#24237;&#23433;&#20840;&#37117;&#26377;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#21516;&#26102;&#32771;&#34385;&#36825;&#20004;&#20010;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#24456;&#23569;&#12290;&#36825;&#26159;&#30001;&#20110;&#21327;&#35843;&#38544;&#31169;&#20445;&#25252;&#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#31639;&#27861;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#28982;&#21518;&#24212;&#29992;&#25105;&#20204;&#30340;&#25308;&#21344;&#24237;&#23481;&#38169;&#31639;&#27861;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#24037;&#20316;&#36981;&#24490;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#23545; DP &#21644;&#25308;&#21344;&#24237;&#23481;&#38169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#24050;&#34987;&#24573;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20943;&#23569; DP &#24341;&#20837;&#30340;&#38543;&#26426;&#22122;&#22768;&#23545; Byzantine &#32858;&#21512;&#30340;&#24433;&#21709;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#21162;&#21147;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#38543;&#26426;&#22122;&#22768;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#65292;&#26377;&#25928;&#22320;&#25298;&#32477;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#22312;&#20855;&#26377;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#24403;&#30340;&#38544;&#31169;&#20445;&#25252;&#27700;&#24179;&#26102;&#65292;&#25552;&#21319;&#20102;&#25972;&#20307; FL &#31995;&#32479;&#30340; Byzantine &#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy and Byzantine resilience are two indispensable requirements for a federated learning (FL) system. Although there have been extensive studies on privacy and Byzantine security in their own track, solutions that consider both remain sparse. This is due to difficulties in reconciling privacy-preserving and Byzantine-resilient algorithms.  In this work, we propose a solution to such a two-fold issue. We use our version of differentially private stochastic gradient descent (DP-SGD) algorithm to preserve privacy and then apply our Byzantine-resilient algorithms. We note that while existing works follow this general approach, an in-depth analysis on the interplay between DP and Byzantine resilience has been ignored, leading to unsatisfactory performance. Specifically, for the random noise introduced by DP, previous works strive to reduce its impact on the Byzantine aggregation. In contrast, we leverage the random noise to construct an aggregation that effectively rejects many existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;</title><link>http://arxiv.org/abs/2304.09761</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#26032;&#26041;&#27861;&#29992;&#20110;&#20934;&#30830;&#30340;&#20892;&#20135;&#21697;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An innovative Deep Learning Based Approach for Accurate Agricultural Crop Price Prediction. (arXiv:2304.09761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#20135;&#21697;&#20215;&#26684;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20892;&#19994;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#20892;&#27665;&#12289;&#28040;&#36153;&#32773;&#12289;&#38646;&#21806;&#21830;&#12289;&#25209;&#21457;&#21830;&#21644;&#25919;&#24220;&#65289;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20892;&#27665;&#30340;&#32463;&#27982;&#31119;&#31049;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#21382;&#21490;&#20215;&#26684;&#20449;&#24687;&#12289;&#27668;&#20505;&#26465;&#20214;&#12289;&#22303;&#22756;&#31867;&#22411;&#12289;&#22320;&#29702;&#20301;&#32622;&#21644;&#20854;&#20182;&#20851;&#38190;&#20915;&#31574;&#22240;&#32032;&#65292;&#31934;&#30830;&#39044;&#27979;&#20892;&#20135;&#21697;&#20215;&#26684;&#12290;&#36825;&#26159;&#19968;&#20010;&#25216;&#26415;&#25361;&#25112;&#65292;&#20197;&#21069;&#20063;&#26366;&#34987;&#23581;&#35797;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#19982;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20215;&#26684;&#30340;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#19988;&#34920;&#29616;&#33267;&#23569;&#27604;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#22909;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of agricultural crop prices is a crucial input for decision-making by various stakeholders in agriculture: farmers, consumers, retailers, wholesalers, and the Government. These decisions have significant implications including, most importantly, the economic well-being of the farmers. In this paper, our objective is to accurately predict crop prices using historical price information, climate conditions, soil type, location, and other key determinants of crop prices. This is a technically challenging problem, which has been attempted before. In this paper, we propose an innovative deep learning based approach to achieve increased accuracy in price prediction. The proposed approach uses graph neural networks (GNNs) in conjunction with a standard convolutional neural network (CNN) model to exploit geospatial dependencies in prices. Our approach works well with noisy legacy data and produces a performance that is at least 20% better than the results available in the li
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;&#65292;&#20854;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09751</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#30340;&#34892;&#20026;&#20998;&#26512;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Skeleton-based action analysis for ADHD diagnosis. (arXiv:2304.09751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#29992;&#20110;&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861;&#30340;&#35786;&#26029;&#65292;&#20854;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#32570;&#38519;&#22810;&#21160;&#38556;&#30861; (ADHD)&#26159;&#19990;&#30028;&#33539;&#22260;&#20869;&#24120;&#35265;&#30340;&#31070;&#32463;&#34892;&#20026;&#38556;&#30861;&#12290;&#23613;&#31649;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ADHD&#35786;&#26029;&#19978;&#30340;&#30740;&#31350;&#38750;&#24120;&#24191;&#27867;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#39640;&#25104;&#26412;&#35774;&#22791;&#65292;&#20363;&#22914;MRI&#26426;&#22120;&#21644;EEG&#36148;&#29255;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;ADHD&#34892;&#20026;&#29305;&#24449;&#30340;&#20302;&#25104;&#26412;&#35786;&#26029;&#26041;&#27861;&#22791;&#21463;&#26399;&#24453;&#12290;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#30001;&#20110;&#20854;&#32858;&#28966;&#20110;&#21160;&#20316;&#21644;&#40065;&#26834;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ADHD&#35786;&#26029;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#21033;&#29992;&#30495;&#23454;&#30340;&#22810;&#27169;&#24577;ADHD&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#25104;&#26412;&#25928;&#30410;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#36827;&#34892;&#24191;&#27867;&#30340;&#21021;&#22987;ADHD&#35786;&#26029;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;AUC&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#35786;&#26029;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention Deficit Hyperactivity Disorder (ADHD) is a common neurobehavioral disorder worldwide. While extensive research has focused on machine learning methods for ADHD diagnosis, most research relies on high-cost equipment, e.g., MRI machine and EEG patch. Therefore, low-cost diagnostic methods based on the action characteristics of ADHD are desired. Skeleton-based action recognition has gained attention due to the action-focused nature and robustness. In this work, we propose a novel ADHD diagnosis system with a skeleton-based action recognition framework, utilizing a real multi-modal ADHD dataset and state-of-the-art detection algorithms. Compared to conventional methods, the proposed method shows cost-efficiency and significant performance improvement, making it more accessible for a broad range of initial ADHD diagnoses. Through the experiment results, the proposed method outperforms the conventional methods in accuracy and AUC. Meanwhile, our method is widely applicable for mass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32032;&#25551;&#36827;&#34892;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25152;&#38656;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#23545;&#22270;&#20687;&#30340;&#23376;&#37096;&#20998;&#36827;&#34892;&#32534;&#36753;&#25110;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23436;&#25104;&#32570;&#22833;&#30340;&#21306;&#22495;&#24182;&#32500;&#25345;&#32032;&#25551;&#30340;&#24341;&#23548;&#65292;&#20026;&#22270;&#20687;&#25805;&#20316;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2304.09748</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#21033;&#29992;&#32032;&#25551;&#36827;&#34892;&#21442;&#32771;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Reference-based Image Composition with Sketch via Structure-aware Diffusion Model. (arXiv:2304.09748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09748
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32032;&#25551;&#36827;&#34892;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25152;&#38656;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#23545;&#22270;&#20687;&#30340;&#23376;&#37096;&#20998;&#36827;&#34892;&#32534;&#36753;&#25110;&#23436;&#25104;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23436;&#25104;&#32570;&#22833;&#30340;&#21306;&#22495;&#24182;&#32500;&#25345;&#32032;&#25551;&#30340;&#24341;&#23548;&#65292;&#20026;&#22270;&#20687;&#25805;&#20316;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#19982;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26041;&#38754;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21487;&#32534;&#36753;&#24615;&#24182;&#20351;&#32454;&#31890;&#24230;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#36755;&#20837;&#26465;&#20214;&#30340;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#32032;&#25551;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#65292;&#20197;&#21450;&#21442;&#32771;&#22270;&#20687;&#12290;&#30001;&#20110;&#20351;&#29992;&#32032;&#25551;&#23545;&#36793;&#30028;&#36827;&#34892;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#25152;&#38656;&#30340;&#32467;&#26500;&#65288;&#21363;&#32032;&#25551;&#65289;&#21644;&#20869;&#23481;&#65288;&#21363;&#21442;&#32771;&#22270;&#20687;&#65289;&#26469;&#32534;&#36753;&#25110;&#23436;&#25104;&#22270;&#20687;&#30340;&#23376;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20351;&#29992;&#21442;&#32771;&#22270;&#20687;&#26469;&#23436;&#25104;&#32570;&#22833;&#30340;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32032;&#25551;&#24341;&#23548;&#12290;&#34429;&#28982;&#31616;&#21333;&#65292;&#20294;&#36825;&#23558;&#24102;&#26469;&#24191;&#27867;&#30340;&#26426;&#20250;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#38656;&#35201;&#33719;&#21462;&#25152;&#38656;&#22270;&#20687;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#22270;&#20687;&#25805;&#20316;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#29992;&#20363;&#65292;&#20351;&#29992;&#25143;&#39537;&#21160;&#20219;&#24847;&#22330;&#26223;&#30340;&#20462;&#25913;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., sketch) and content (i.e., reference image). Our framework fine-tunes a pre-trained diffusion model to complete missing regions using the reference image while maintaining sketch guidance. Albeit simple, this leads to wide opportunities to fulfill user needs for obtaining the in-demand images. Through extensive experiments, we demonstrate that our proposed method offers unique use cases for image manipulation, enabling user-driven modifications of arbitrary scenes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39592;&#26550;&#36523;&#20307;&#20851;&#33410;&#28857;&#23454;&#29616;&#24739;&#32773;&#24247;&#22797;&#36816;&#21160;&#37325;&#22797;&#20998;&#21106;&#21644;&#35745;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20811;&#26381;&#20256;&#24863;&#22120;&#38590;&#20197;&#20351;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2304.09735</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#26550;&#36523;&#20307;&#20851;&#33410;&#28857;&#30340;&#24247;&#22797;&#36816;&#21160;&#37325;&#22797;&#20998;&#21106;&#21644;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Repetition Segmentation and Counting using Skeletal Body Joints. (arXiv:2304.09735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39592;&#26550;&#36523;&#20307;&#20851;&#33410;&#28857;&#23454;&#29616;&#24739;&#32773;&#24247;&#22797;&#36816;&#21160;&#37325;&#22797;&#20998;&#21106;&#21644;&#35745;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20811;&#26381;&#20256;&#24863;&#22120;&#38590;&#20197;&#20351;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#32946;&#38203;&#28860;&#26159;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#30340;&#24247;&#22797;&#35745;&#21010;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;AI&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#35745;&#21010;&#20013;&#65292;&#24739;&#32773;&#29420;&#31435;&#22312;&#23478;&#23436;&#25104;&#36816;&#21160;&#65292;&#32780;AI&#31639;&#27861;&#20998;&#26512;&#36816;&#21160;&#25968;&#25454;&#65292;&#21521;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#25253;&#21578;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#20998;&#26512;&#36816;&#21160;&#25968;&#25454;&#30340;&#31532;&#19968;&#27493;&#26159;&#23558;&#20854;&#20998;&#21106;&#25104;&#36830;&#32493;&#30340;&#37325;&#22797;&#21160;&#20316;&#12290;&#20043;&#21069;&#24050;&#32463;&#26377;&#22823;&#37327;&#20851;&#20110;&#20351;&#29992;&#21407;&#22987;&#35270;&#39057;&#25968;&#25454;&#23545;&#20581;&#24247;&#20154;&#36827;&#34892;&#37325;&#22797;&#27963;&#21160;&#20998;&#21106;&#21644;&#35745;&#25968;&#30340;&#30740;&#31350;&#65292;&#36825;&#24341;&#21457;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#19988;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#12290;&#20197;&#21069;&#20851;&#20110;&#24739;&#32773;&#30340;&#24247;&#22797;&#36816;&#21160;&#20998;&#21106;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#22810;&#20010;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#24247;&#22797;&#24739;&#32773;&#23478;&#20013;&#20351;&#29992;&#36215;&#26469;&#24456;&#22256;&#38590;&#12290;&#19982;&#20581;&#24247;&#20154;&#30456;&#27604;&#65292;&#22312;&#24739;&#32773;&#20013;&#20998;&#21106;&#21644;&#35745;&#25968;&#36816;&#21160;&#37325;&#22797;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#22240;&#20026;&#24247;&#22797;&#36807;&#31243;&#20013;&#28041;&#21450;&#21040;&#30142;&#30149;&#21644;&#36523;&#20307;&#25439;&#20260;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical exercise is an essential component of rehabilitation programs that improve quality of life and reduce mortality and re-hospitalization rates. In AI-driven virtual rehabilitation programs, patients complete their exercises independently at home, while AI algorithms analyze the exercise data to provide feedback to patients and report their progress to clinicians. To analyze exercise data, the first step is to segment it into consecutive repetitions. There has been a significant amount of research performed on segmenting and counting the repetitive activities of healthy individuals using raw video data, which raises concerns regarding privacy and is computationally intensive. Previous research on patients' rehabilitation exercise segmentation relied on data collected by multiple wearable sensors, which are difficult to use at home by rehabilitation patients. Compared to healthy individuals, segmenting and counting exercise repetitions in patients is more challenging because of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09718</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#25928;&#29575;&#30340;&#27169;&#22411;&#39537;&#21160;&#37327;&#23376;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Model-based Reinforcement Learning for Quantum Control. (arXiv:2304.09718v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21463;&#21040;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20010;&#26041;&#27861;&#37319;&#29992;&#33258;&#21160;&#24494;&#20998;&#30340;ODE&#34920;&#36798;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#29615;&#22659;&#65292;&#22312;&#38376;&#25511;&#21046;&#21644;&#27721;&#23494;&#23572;&#39039;&#21442;&#25968;&#30340;&#23398;&#20064;&#20013;&#36890;&#36807;&#31995;&#32479;&#20132;&#20114;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#27604;&#26631;&#20934;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36866;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22122;&#22768;&#26102;&#21464;&#38376;&#20248;&#21270;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20248;&#20110;&#22522;&#20110;&#27169;&#22411;&#33258;&#30001;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#25511;&#21046;&#22120;&#19982;&#29289;&#29702;&#31995;&#32479;&#20132;&#20114;&#30340;&#27425;&#25968;&#12290;&#20511;&#21161;&#19968;&#20010;&#24402;&#32435;&#20559;&#32622;&#65292;&#21463;&#26368;&#36817;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#24494;&#30340;ODE&#65292;&#20854;&#30001;&#21487;&#23398;&#20064;&#30340;&#27721;&#23494;&#23572;&#39039;&#23433;&#25490;&#21442;&#25968;&#21270;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#36817;&#20284;&#29615;&#22659;&#65292;&#20854;&#26102;&#21464;&#37096;&#20998;&#65288;&#21253;&#25324;&#25511;&#21046;&#65289;&#23436;&#20840;&#24050;&#30693;&#12290;&#25511;&#21046;&#22120;&#21644;&#36830;&#32493;&#26102;&#22495;&#29420;&#31435;&#21442;&#25968;&#30340;&#27721;&#23494;&#23572;&#39039;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#30340;&#12290;&#22312;&#30495;&#23454;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#22312;&#20934;&#22791;&#19968;&#20123;&#26631;&#20934;&#21333;&#37327;&#23376;&#38376;&#30340;&#38381;&#21512;&#21644;&#24320;&#25918;&#31995;&#32479;&#21160;&#24577;&#26102;&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#19982;&#26631;&#20934;&#27169;&#22411;&#33258;&#30001;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#20855;&#26377;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#65292;&#36825;&#21253;&#25324;&#21333;&#27425;&#27979;&#37327;&#12289;&#20219;&#24847;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25130;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model-based reinforcement learning (RL) approach for noisy time-dependent gate optimization with improved sample complexity over model-free RL. Sample complexity is the number of controller interactions with the physical system. Leveraging an inductive bias, inspired by recent advances in neural ordinary differential equations (ODEs), we use an auto-differentiable ODE parametrised by a learnable Hamiltonian ansatz to represent the model approximating the environment whose time-dependent part, including the control, is fully known. Control alongside Hamiltonian learning of continuous time-independent parameters is addressed through interactions with the system. We demonstrate an order of magnitude advantage in the sample complexity of our method over standard model-free RL in preparing some standard unitary gates with closed and open system dynamics, in realistic numerical experiments incorporating single shot measurements, arbitrary Hilbert space truncations and uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#27454;&#33021;&#22815;&#22312;&#36229;&#35270;&#36317;&#31354;&#25112;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20195;&#29702;&#20154;,&#36890;&#36807;&#35745;&#31639;&#22870;&#21169;&#65292;&#19981;&#26029;&#23398;&#20064;&#21644;&#25552;&#39640;&#33258;&#36523;&#22312;&#31354;&#25112;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#26399;&#26395;&#29983;&#25104;&#20197;&#21069;&#20174;&#26410;&#20986;&#29616;&#36807;&#30340;&#31354;&#25112;&#31574;&#30053;&#65292;&#26368;&#32456;&#27979;&#35797;&#30495;&#27491;&#30340;&#39134;&#34892;&#21592;&#19982;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#30456;&#21516;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#20026;&#31354;&#20013;&#38450;&#24481;&#20219;&#21153;&#25552;&#39640;&#39134;&#34892;&#21592;&#30340;&#34920;&#29616;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2304.09669</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36229;&#35270;&#36317;&#31354;&#25112;&#33258;&#20027;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Autonomous Agent for Beyond Visual Range Air Combat: A Deep Reinforcement Learning Approach. (arXiv:2304.09669v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#27454;&#33021;&#22815;&#22312;&#36229;&#35270;&#36317;&#31354;&#25112;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20195;&#29702;&#20154;,&#36890;&#36807;&#35745;&#31639;&#22870;&#21169;&#65292;&#19981;&#26029;&#23398;&#20064;&#21644;&#25552;&#39640;&#33258;&#36523;&#22312;&#31354;&#25112;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#26399;&#26395;&#29983;&#25104;&#20197;&#21069;&#20174;&#26410;&#20986;&#29616;&#36807;&#30340;&#31354;&#25112;&#31574;&#30053;&#65292;&#26368;&#32456;&#27979;&#35797;&#30495;&#27491;&#30340;&#39134;&#34892;&#21592;&#19982;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#30456;&#21516;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#20026;&#31354;&#20013;&#38450;&#24481;&#20219;&#21153;&#25552;&#39640;&#39134;&#34892;&#21592;&#30340;&#34920;&#29616;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#20154;&#65292;&#33021;&#22815;&#22312;&#36229;&#35270;&#36317;&#31354;&#25112;&#27169;&#25311;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#20195;&#34920;&#39640;&#24615;&#33021;&#25112;&#26007;&#39134;&#26426;&#30340;&#20195;&#29702;&#20154;&#65292;&#24182;&#26681;&#25454;&#25805;&#20316;&#25351;&#26631;&#35745;&#31639;&#22870;&#21169;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#24182;&#25552;&#39640;&#20854;&#22312;&#36229;&#35270;&#36317;&#31354;&#25112;&#20013;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#24328;&#23454;&#39564;&#65292;&#26399;&#26395;&#29983;&#25104;&#20197;&#21069;&#20174;&#26410;&#20986;&#29616;&#36807;&#30340;&#31354;&#25112;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#27979;&#35797;&#30495;&#27491;&#30340;&#39134;&#34892;&#21592;&#20351;&#29992;&#34394;&#25311;&#27169;&#25311;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#20195;&#29702;&#20154;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#27604;&#36739;&#20182;&#20204;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#19982;&#30495;&#27491;&#30340;&#39134;&#34892;&#21592;&#20132;&#20114;&#30340;&#20195;&#29702;&#20154;&#65292;&#22312;&#31354;&#20013;&#38450;&#24481;&#20219;&#21153;&#20013;&#25552;&#39640;&#39134;&#34892;&#21592;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work contributes to developing an agent based on deep reinforcement learning capable of acting in a beyond visual range (BVR) air combat simulation environment. The paper presents an overview of building an agent representing a high-performance fighter aircraft that can learn and improve its role in BVR combat over time based on rewards calculated using operational metrics. Also, through self-play experiments, it expects to generate new air combat tactics never seen before. Finally, we hope to examine a real pilot's ability, using virtual simulation, to interact in the same environment with the trained agent and compare their performances. This research will contribute to the air combat training context by developing agents that can interact with real pilots to improve their performances in air defense missions.
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2304.09653</link><description>&lt;p&gt;
ReelFramer&#65306;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
ReelFramer: Co-creating News Reels on Social Media with Generative AI. (arXiv:2304.09653v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09653
&lt;/p&gt;
&lt;p&gt;
ReelFramer&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#31038;&#20132;&#23186;&#20307;&#20849;&#21516;&#21019;&#20316;&#26032;&#38395;&#29255;&#27573;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#24182;&#29983;&#25104;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#30701;&#35270;&#39057;&#26159;&#35768;&#22810;&#24180;&#36731;&#20154;&#21457;&#29616;&#21644;&#28040;&#36153;&#20869;&#23481;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#26032;&#38395;&#26426;&#26500;&#24076;&#26395;&#36890;&#36807;&#26032;&#38395;&#29255;&#27573;&#25509;&#35302;&#21463;&#20247;&#65292;&#20294;&#30446;&#21069;&#38590;&#20197;&#23558;&#20256;&#32479;&#30340;&#26032;&#38395;&#25253;&#36947;&#26684;&#24335;&#36716;&#21270;&#20026;&#19982;&#24179;&#21488;&#39118;&#26684;&#30456;&#21305;&#37197;&#30340;&#30701;&#23567;&#26377;&#36259;&#35270;&#39057;&#12290;&#26377;&#22810;&#31181;&#26041;&#27861;&#21487;&#20197;&#22260;&#32469;&#26032;&#38395;&#20107;&#20214;&#26500;&#24314;&#29255;&#27573;&#24335;&#21465;&#20107;&#65292;&#32780;&#36873;&#23450;&#20854;&#20013;&#19968;&#31181;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#19981;&#21516;&#30340;&#26032;&#38395;&#25925;&#20107;&#38656;&#35201;&#19981;&#21516;&#30340;&#21465;&#36848;&#26694;&#26550;&#65292;&#24182;&#38656;&#35201;&#22312;&#23089;&#20048;&#24615;&#21644;&#20449;&#24687;&#37327;&#20043;&#38388;&#36798;&#21040;&#19981;&#21516;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReelFramer&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#26469;&#24110;&#21161;&#35760;&#32773;&#25506;&#32034;&#19968;&#20010;&#25925;&#20107;&#30340;&#22810;&#31181;&#21465;&#20107;&#26694;&#26550;&#65292;&#28982;&#21518;&#29983;&#25104;&#20182;&#20204;&#21487;&#20197;&#32534;&#36753;&#21644;&#36845;&#20195;&#30340;&#33050;&#26412;&#12289;&#35282;&#33394;&#26495;&#21644;&#25925;&#20107;&#26495;&#12290;&#22312;&#19968;&#39033;&#30001;&#20116;&#21517;&#26032;&#38395;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#29983;&#21442;&#19982;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31995;&#32479;&#22823;&#22823;&#20943;&#36731;&#20102;&#23558;&#19968;&#31687;&#20070;&#38754;&#25253;&#36947;&#36716;&#21270;&#20026;&#26032;&#38395;&#29255;&#27573;&#30340;&#36127;&#25285;&#65292;&#24182;&#25506;&#32034;&#21465;&#20107;&#26694;&#26550;&#20197;&#25214;&#21040;&#27491;&#30830;&#30340;&#26694;&#26550;&#36807;&#31243;&#26159;&#38750;&#24120;&#26377;&#24847;&#20041;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short videos on social media are a prime way many young people find and consume content. News outlets would like to reach audiences through news reels, but currently struggle to translate traditional journalistic formats into the short, entertaining videos that match the style of the platform. There are many ways to frame a reel-style narrative around a news story, and selecting one is a challenge. Different news stories call for different framings, and require a different trade-off between entertainment and information. We present a system called ReelFramer that uses text and image generation to help journalists explore multiple narrative framings for a story, then generate scripts, character boards and storyboards they can edit and iterate on. A user study of five graduate students in journalism-related fields found the system greatly eased the burden of transforming a written story into a reel, and that exploring framings to find the right one was a rewarding process.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09648</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#30340;&#37327;&#23376;&#28145;&#24230; Q &#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum deep Q learning with distributed prioritized experience replay. (arXiv:2304.09648v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#23427;&#23558;&#20998;&#24067;&#24335;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; QDQN-DPER &#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064; (QRL) &#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#21644;&#24322;&#27493;&#35757;&#32451;&#32435;&#20837;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#20943;&#23569;&#39640;&#37319;&#26679;&#22797;&#26434;&#24615;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;QDQN-DPER &#22312;&#20855;&#26377;&#30456;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#20998;&#24067;&#24335;&#37327;&#23376; Q &#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#20445;&#25345;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#36866;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the QDQN-DPER framework to enhance the efficiency of quantum reinforcement learning (QRL) in solving sequential decision tasks. The framework incorporates prioritized experience replay and asynchronous training into the training algorithm to reduce the high sampling complexities. Numerical simulations demonstrate that QDQN-DPER outperforms the baseline distributed quantum Q learning with the same model architecture. The proposed framework holds potential for more complex tasks while maintaining training efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2304.09639</link><description>&lt;p&gt;
Krohn-Rhodes&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
The Krohn-Rhodes Logics. (arXiv:2304.09639v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#36807;&#21435;&#30340;&#27169;&#24577;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#33258;&#21160;&#26426;&#32423;&#32852;&#29702;&#35770;&#65292;&#22522;&#20110;Past LTL&#25193;&#23637;&#19968;&#32452;&#20016;&#23500;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#32780;&#33719;&#24471;&#12290;&#35813;&#29702;&#35770;&#25351;&#20986;&#65292;&#27599;&#20010;&#33258;&#21160;&#26426;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20123;&#31216;&#20026;prime automata&#30340;&#22522;&#26412;&#33258;&#21160;&#26426;&#30340;&#32423;&#32852;&#12290;&#20182;&#20204;&#26159;&#25152;&#26377;&#33258;&#21160;&#26426;&#30340;&#26500;&#24314;&#22359;&#65292;&#31867;&#20284;&#20110;&#36136;&#25968;&#26159;&#25152;&#26377;&#33258;&#28982;&#25968;&#30340;&#26500;&#24314;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#21435;&#30340;LTL&#23545;&#24212;&#20110;&#31216;&#20026;flip-flops&#30340;&#19968;&#31181;prime automata&#30340;&#32423;&#32852;&#12290;&#29305;&#21035;&#22320;&#65292;Past LTL&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#30001;flip-flops&#25429;&#33719;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#33021;&#25429;&#33719;&#20219;&#20309;&#20854;&#20182;prime automata&#65292;&#23558;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#22312;&#26143;&#21495;&#33258;&#30001;&#27491;&#21017;&#35821;&#35328;&#20869;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;Past LTL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36825;&#20123;&#36816;&#31639;&#31526;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#20135;&#29983;&#20102;&#26080;&#38480;&#25968;&#37327;&#30340;&#36923;&#36753;&#65292;&#25429;&#33719;&#20102;&#27491;&#21017;&#35821;&#35328;&#30340;&#26080;&#38480;&#25968;&#37327;&#30340;&#19981;&#21516;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new family of modal temporal logics of the past, obtained by extending Past LTL with a rich set of temporal operators based on the theory by Krohn and Rhodes for automata cascades. The theory says that every automaton can be expressed as a cascade of some basic automata called prime automata. They are the building blocks of all automata, analogously to prime numbers being the building blocks of all natural numbers. We show that Past LTL corresponds to cascades of one kind of prime automata called flip-flops. In particular, the temporal operators of Past LTL are captured by flip-flops, and they cannot capture any other prime automaton, confining the expressivity within the star-free regular languages. We propose novel temporal operators that can capture other prime automata, and hence extend the expressivity of Past LTL. Such operators are infinitely-many, and they yield an infinite number of logics capturing an infinite number of distinct fragments of the regular languages
&lt;/p&gt;</description></item><item><title>StyleDEM&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22320;&#24418;&#21019;&#20316;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#36890;&#36807;&#20132;&#20114;&#24335;&#31508;&#21047;&#36827;&#34892;&#21019;&#20316;&#21644;&#22686;&#24378;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#23089;&#20048;&#34892;&#19994;&#30340;&#35774;&#35745;&#24072;&#12290;</title><link>http://arxiv.org/abs/2304.09626</link><description>&lt;p&gt;
StyleDEM&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#22320;&#24418;&#21019;&#20316;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StyleDEM: a Versatile Model for Authoring Terrains. (arXiv:2304.09626v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09626
&lt;/p&gt;
&lt;p&gt;
StyleDEM&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22320;&#24418;&#21019;&#20316;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#36890;&#36807;&#20132;&#20114;&#24335;&#31508;&#21047;&#36827;&#34892;&#21019;&#20316;&#21644;&#22686;&#24378;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#23089;&#20048;&#34892;&#19994;&#30340;&#35774;&#35745;&#24072;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22320;&#24418;&#24314;&#27169;&#26041;&#27861;&#65292;&#25552;&#20379;&#39640;&#25928;&#19988;&#36890;&#24120;&#26159;&#20132;&#20114;&#24335;&#30340;&#21019;&#20316;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#21253;&#25324;&#20219;&#20309;&#39118;&#26684;&#30340;&#27010;&#24565;&#65292;&#32780;&#36825;&#23545;&#20110;&#23089;&#20048;&#34892;&#19994;&#30340;&#35774;&#35745;&#24072;&#26469;&#35828;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;StyleDEM&#30340;&#26032;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22320;&#24418;&#21512;&#25104;&#21644;&#21019;&#20316;&#65292;&#24182;&#20855;&#26377;&#39118;&#26684;&#30340;&#22810;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#36755;&#20837;&#30340;&#33609;&#22270;&#25110;&#29616;&#26377;&#22320;&#24418;&#24320;&#22987;&#12290;&#23427;&#36755;&#20986;&#30340;&#22320;&#24418;&#20855;&#26377;&#36890;&#36807;&#20132;&#20114;&#24335;&#31508;&#21047;&#36827;&#34892;&#21019;&#20316;&#24182;&#36890;&#36807;&#20854;&#20182;&#24037;&#20855;&#36827;&#34892;&#22686;&#24378;&#30340;&#29305;&#24449;&#65292;&#22914;&#39118;&#26684;&#25805;&#20316;&#25110;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#22312;&#20110;&#24037;&#20855;&#31665;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many terrain modelling methods have been proposed for the past decades, providing efficient and often interactive authoring tools. However, they generally do not include any notion of style, which is a critical aspect for designers in the entertainment industry. We introduce StyleDEM, a new generative adversarial network method for terrain synthesis and authoring, with a versatile toolbox of authoring methods with style. This method starts from an input sketch or an existing terrain. It outputs a terrain with features that can be authored using interactive brushes and enhanced with additional tools such as style manipulation or super-resolution. The strength of our approach resides in the versatility and interoperability of the toolbox.
&lt;/p&gt;</description></item><item><title>MMDR&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#21333;&#27169;&#24577;&#28304;&#30340;&#32467;&#26524;&#29305;&#24449;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#28145;&#23618;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;MMDR&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;&#28304;&#30340;&#27973;&#23618;&#21644;&#28145;&#23618;&#29305;&#24449;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.09609</link><description>&lt;p&gt;
MMDR&#65306;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#32467;&#26524;&#29305;&#24449;&#34701;&#21512;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MMDR: A Result Feature Fusion Object Detection Approach for Autonomous System. (arXiv:2304.09609v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09609
&lt;/p&gt;
&lt;p&gt;
MMDR&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#21333;&#27169;&#24577;&#28304;&#30340;&#32467;&#26524;&#29305;&#24449;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#34920;&#31034;&#28145;&#23618;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;MMDR&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;&#28304;&#30340;&#27973;&#23618;&#21644;&#28145;&#23618;&#29305;&#24449;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29289;&#20307;&#26816;&#27979;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;2D&#21644;3D&#29289;&#20307;&#26816;&#27979;&#12290;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22810;&#27169;&#24577;&#26041;&#27861;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26524;&#29305;&#24449;&#32423;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20174;&#21333;&#27169;&#24577;&#28304;&#20135;&#29983;&#30340;&#32467;&#26524;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#22522;&#20110;&#27492;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29289;&#20307;&#26816;&#27979;&#65292;&#21033;&#29992;&#21333;&#27169;&#24577;&#32467;&#26524;&#20316;&#20026;&#29305;&#24449;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22522;&#20110;&#32467;&#26524;&#29305;&#24449;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#22120;(MMDR)&#65292;&#26088;&#22312;&#24212;&#29992;&#20110;2D&#21644;3D&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#12290;&#19982;&#20197;&#24448;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36739;&#26202;&#30340;&#38454;&#27573;&#25191;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#34920;&#31034;&#21333;&#27169;&#24577;&#28304;&#30340;&#28145;&#23618;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;MMDR&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;&#28304;&#30340;&#27973;&#23618;&#21644;&#28145;&#23618;&#29305;&#24449;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection has been extensively utilized in autonomous systems in recent years, encompassing both 2D and 3D object detection. Recent research in this field has primarily centered around multimodal approaches for addressing this issue.In this paper, a multimodal fusion approach based on result feature-level fusion is proposed. This method utilizes the outcome features generated from single modality sources, and fuses them for downstream tasks.Based on this method, a new post-fusing network is proposed for multimodal object detection, which leverages the single modality outcomes as features. The proposed approach, called Multi-Modal Detector based on Result features (MMDR), is designed to work for both 2D and 3D object detection tasks. Compared to previous multimodal models, the proposed approach in this paper performs feature fusion at a later stage, enabling better representation of the deep-level features of single modality sources. Additionally, the MMDR model incorporates shal
&lt;/p&gt;</description></item><item><title>LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09599</link><description>&lt;p&gt;
LEA: &#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36229;&#36234;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy. (arXiv:2304.09599v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09599
&lt;/p&gt;
&lt;p&gt;
LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#25104;&#20026;&#26114;&#36149;&#40657;&#30418;&#20248;&#21270;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#40657;&#30418;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#20851;&#38190;&#30340;&#38556;&#30861;&#26159;&#25214;&#20986;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#34920;&#24449;&#19981;&#36275;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20302;&#25928;&#20132;&#20114;&#32780;&#26174;&#24471;&#34180;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65288;LEA&#65289;&#65292;&#20197;&#23454;&#29616;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#21040;&#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36716;&#25442;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#21442;&#25968;&#21644;&#26356;&#26032;&#35268;&#21017;&#12290;&#19982;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#19981;&#21516;&#65292;LEA&#23545;&#30446;&#26631;&#20219;&#21153;&#20855;&#26377;&#39640;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LEA&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have emerged as a powerful framework for expensive black-box optimization. Obtaining better solutions with less computational cost is essential and challenging for black-box optimization. The most critical obstacle is figuring out how to effectively use the target task information to form an efficient optimization strategy. However, current methods are weak due to the poor representation of the optimization strategy and the inefficient interaction between the optimization strategy and the target task. To overcome the above limitations, we design a learned EA (LEA) to realize the move from hand-designed optimization strategies to learned optimization strategies, including not only hyperparameters but also update rules. Unlike traditional EAs, LEA has high adaptability to the target task and can obtain better solutions with less computational cost. LEA is also able to effectively utilize the low-fidelity information of the target task to form an efficient op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2304.09572</link><description>&lt;p&gt;
&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65306;&#35843;&#26597;&#19982;&#30740;&#31350;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
An Ecosystem for Personal Knowledge Graphs: A Survey and Research Roadmap. (arXiv:2304.09572v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;PKG&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#35201;&#35299;&#38145;PKG&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;PKG&#30340;&#32508;&#21512;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#20154;&#30693;&#35782;&#22270;&#35889;&#65288;PKG&#65289;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#36890;&#24120;&#23450;&#20041;&#20026;&#26377;&#20851;&#20010;&#20154;&#30456;&#20851;&#23454;&#20307;&#12289;&#20854;&#23646;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#36164;&#28304;&#12290;PKG&#26159;&#23433;&#20840;&#12289;&#31934;&#23494;&#30340;&#20010;&#20154;&#25968;&#25454;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20851;&#38190;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;PKG&#33021;&#22815;&#24191;&#27867;&#24212;&#29992;&#20043;&#21069;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#26159;&#20851;&#20110;PKG&#30340;&#23450;&#20041;&#65292;&#22240;&#20026;&#26415;&#35821;&#26377;&#22810;&#31181;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;PKG&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#65288;1&#65289;&#21333;&#20010;&#20010;&#20307;&#25317;&#26377;&#25968;&#25454;&#21644;&#65288;2&#65289;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20316;&#20026;&#20027;&#35201;&#30446;&#30340;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;PKG&#35270;&#22270;&#65292;&#38656;&#35201;&#35299;&#38145;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;PKG&#26159;&#26356;&#22823;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#19982;&#25968;&#25454;&#26381;&#21153;&#21644;&#25968;&#25454;&#28304;&#30340;&#25509;&#21475;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#23545;&#24403;&#21069;PKG&#30740;&#31350;&#30340;&#20840;&#38754;&#35843;&#26597;&#21644;&#30740;&#31350;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an ecosystem for personal knowledge graphs (PKG), commonly defined as resources of structured information about entities related to an individual, their attributes, and the relations between them. PKGs are a key enabler of secure and sophisticated personal data management and personalized services. However, there are challenges that need to be addressed before PKGs can achieve widespread adoption. One of the fundamental challenges is the very definition of what constitutes a PKG, as there are multiple interpretations of the term. We propose our own definition of a PKG, emphasizing the aspects of (1) data ownership by a single individual and (2) the delivery of personalized services as the primary purpose. We further argue that a holistic view of PKGs is needed to unlock their full potential, and propose a unified framework for PKGs, where the PKG is a part of a larger ecosystem with clear interfaces towards data services and data sources. A comprehensive survey and 
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>SelfAct&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.09530</link><description>&lt;p&gt;
SelfAct: &#22522;&#20110;&#33258;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SelfAct: Personalized Activity Recognition based on Self-Supervised and Active Learning. (arXiv:2304.09530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09530
&lt;/p&gt;
&lt;p&gt;
SelfAct&#26159;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#24050;&#32463;&#25104;&#20026;&#20102;&#31359;&#25140;&#35774;&#22791;&#21644;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#24191;&#27867;&#30340;&#26041;&#27861;&#65292;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#30446;&#21069;&#39046;&#20808;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#38598;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#26159;&#32791;&#26102;&#12289;&#26114;&#36149;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#27963;&#21160;&#25191;&#34892;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#21487;&#21464;&#24615;&#65292;&#27963;&#21160;&#27169;&#22411;&#24212;&#35813;&#20026;&#27599;&#20010;&#29992;&#25143;&#20010;&#24615;&#21270;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SelfAct&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#33258;&#30417;&#30563;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;SelfAct&#21033;&#29992;&#20174;&#35768;&#22810;&#29992;&#25143;&#25910;&#38598;&#30340;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26377;&#24847;&#20041;&#12289;&#39640;&#25928;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#30001;&#27492;&#24471;&#20986;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#34987;&#26032;&#29992;&#25143;&#24403;&#22320;&#20351;&#29992;&#65292;&#20182;&#20204;&#23558;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#24494;&#35843;&#36825;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SelfAct&#22312;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#25928;&#29575;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised Deep Learning (DL) models are currently the leading approach for sensor-based Human Activity Recognition (HAR) on wearable and mobile devices. However, training them requires large amounts of labeled data whose collection is often time-consuming, expensive, and error-prone. At the same time, due to the intra- and inter-variability of activity execution, activity models should be personalized for each user. In this work, we propose SelfAct: a novel framework for HAR combining self-supervised and active learning to mitigate these problems. SelfAct leverages a large pool of unlabeled data collected from many users to pre-train through self-supervision a DL model, with the goal of learning a meaningful and efficient latent representation of sensor data. The resulting pre-trained model can be locally used by new users, which will fine-tune it thanks to a novel unsupervised active learning strategy. Our experiments on two publicly available HAR datasets demonstrate that SelfAct ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09513</link><description>&lt;p&gt;
NetGPT&#65306;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#20026;&#36755;&#20837;&#27969;&#37327;&#29983;&#25104;&#21487;&#21306;&#20998;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#27969;&#37327;&#20998;&#31867;&#12289;&#25915;&#20987;&#26816;&#27979;&#12289;&#36164;&#28304;&#35843;&#24230;&#12289;&#21327;&#35758;&#20998;&#26512;&#21644;&#27969;&#37327;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NetGPT&#65292;&#26088;&#22312;&#20026;&#32593;&#32476;&#27969;&#37327;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#35299;&#20915;&#22810;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#32467;&#26500;&#26500;&#24314;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#26356;&#21512;&#29702;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#36830;&#25509;&#21098;&#26525;&#26041;&#27861;&#65292;&#21160;&#24577;&#20248;&#21270;&#31361;&#35302;&#36830;&#25509;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.09500</link><description>&lt;p&gt;
&#21453;&#21521;&#30693;&#35782;&#33976;&#39311;&#32467;&#21512;&#20223;&#29983;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Biologically inspired structure learning with reverse knowledge distillation for spiking neural networks. (arXiv:2304.09500v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#32467;&#26500;&#26500;&#24314;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#26356;&#21512;&#29702;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#33976;&#39311;&#21644;&#36830;&#25509;&#21098;&#26525;&#26041;&#27861;&#65292;&#21160;&#24577;&#20248;&#21270;&#31361;&#35302;&#36830;&#25509;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#29289;&#23398;&#26412;&#28304;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#22312;&#24863;&#30693;&#20449;&#24687;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#24403;&#21069;&#22522;&#20110;&#33033;&#20914;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#36825;&#24847;&#21619;&#30528;&#20840;&#36830;&#25509;&#25110;&#36807;&#28145;&#30340;&#32467;&#26500;&#20250;&#24102;&#26469;&#36807;&#22810;&#30340;&#20887;&#20313;&#65292;&#36825;&#26159;&#38459;&#30861;SNNs&#23454;&#38469;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#32467;&#26500;&#26500;&#24314;&#26041;&#27861;&#26469;&#26500;&#24314;&#26356;&#21512;&#29702;&#30340;SNNs&#12290;&#36890;&#36807;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21644;&#36830;&#25509;&#21098;&#26525;&#26041;&#27861;&#65292;SNNs&#30340;&#31361;&#35302;&#36830;&#25509;&#21487;&#20197;&#21160;&#24577;&#20248;&#21270;&#20197;&#36798;&#21040;&#26368;&#20339;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;SNNs&#30340;&#32467;&#26500;&#19981;&#20165;&#21487;&#20197;&#21560;&#25910;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#36845;&#20195;&#25628;&#32034;&#20197;&#20248;&#21270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have superb characteristics in sensory information recognition tasks due to their biological plausibility. However, the performance of some current spiking-based models is limited by their structures which means either fully connected or too-deep structures bring too much redundancy. This redundancy from both connection and neurons is one of the key factors hindering the practical application of SNNs. Although Some pruning methods were proposed to tackle this problem, they normally ignored the fact the neural topology in the human brain could be adjusted dynamically. Inspired by this, this paper proposed an evolutionary-based structure construction method for constructing more reasonable SNNs. By integrating the knowledge distillation and connection pruning method, the synaptic connections in SNNs can be optimized dynamically to reach an optimal state. As a result, the structure of SNNs could not only absorb knowledge from the teacher model but also searc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20840;&#38754;&#32508;&#36848;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#20351;&#29992;&#24773;&#24863;&#20449;&#24687;&#36827;&#34892;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.09493</link><description>&lt;p&gt;
&#24773;&#24863;&#34701;&#21512;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24212;&#29992;&#65306;&#29992;&#20110;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emotion fusion for mental illness detection from social media: A survey. (arXiv:2304.09493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20840;&#38754;&#32508;&#36848;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#20351;&#29992;&#24773;&#24863;&#20449;&#24687;&#36827;&#34892;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#65292;&#24182;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#30142;&#30149;&#26159;&#20840;&#29699;&#26368;&#26222;&#36941;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#20043;&#19968;&#65292;&#23545;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#31038;&#20250;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#31456;&#26469;&#26089;&#26399;&#21457;&#29616;&#24515;&#29702;&#30142;&#30149;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26681;&#25454;&#24773;&#32490;&#21644;&#24515;&#29702;&#30142;&#30149;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#21644;&#34701;&#21512;&#24773;&#24863;&#20449;&#24687;&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24773;&#24863;&#34701;&#21512;&#30340;&#26041;&#27861;&#29992;&#20110;&#24515;&#29702;&#30142;&#30149;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#20851;&#25968;&#25454;&#38598;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental illnesses are one of the most prevalent public health problems worldwide, which negatively influence people's lives and society's health. With the increasing popularity of social media, there has been a growing research interest in the early detection of mental illness by analysing user-generated posts on social media. According to the correlation between emotions and mental illness, leveraging and fusing emotion information has developed into a valuable research topic. In this article, we provide a comprehensive survey of approaches to mental illness detection in social media that incorporate emotion fusion. We begin by reviewing different fusion strategies, along with their advantages and disadvantages. Subsequently, we discuss the major challenges faced by researchers working in this area, including issues surrounding the availability and quality of datasets, the performance of algorithms and interpretability. We additionally suggest some potential directions for future resea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.09444</link><description>&lt;p&gt;
&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#39640;&#32500;&#26114;&#36149;&#38382;&#39064;&#30340;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#36827;&#21270;&#31639;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#32780;&#35745;&#31639;&#20195;&#20215;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#36741;&#20197;&#20195;&#29702;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#20250;&#24613;&#21095;&#24694;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#21644;&#23616;&#37096;&#27169;&#22411;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861; (CLMEA)&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#26114;&#36149;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#31867;&#22120;&#36741;&#21161;&#30340;&#25490;&#21517;&#23398;&#20064;&#12289;&#36229;&#20307;&#31215;&#38750;&#25903;&#37197;&#25628;&#32034;&#21644;&#30456;&#23545;&#31232;&#30095;&#30446;&#26631;&#31354;&#38388;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20998;&#31867;&#22120;&#65292;&#23558;&#21518;&#20195;&#21010;&#20998;&#20026;&#20960;&#20010;&#31561;&#32423;&#12290;&#19981;&#21516;&#31561;&#32423;&#30340;&#21518;&#20195;&#20351;&#29992;&#25490;&#21517;&#23398;&#20064;&#31574;&#30053;&#29983;&#25104;&#26356;&#20855;&#26377;&#21069;&#26223;&#24615;&#21644;&#20449;&#24687;&#24615;&#30340;&#20505;&#36873;&#35299;&#29992;&#20110;&#23454;&#38469;&#20248;&#21270;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate-assisted evolutionary algorithms have been widely developed to solve complex and computationally expensive multi-objective optimization problems in recent years. However, when dealing with high-dimensional optimization problems, the performance of these surrogate-assisted multi-objective evolutionary algorithms deteriorate drastically. In this work, a novel Classifier-assisted rank-based learning and Local Model based multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional expensive multi-objective optimization problems. The proposed algorithm consists of three parts: classifier-assisted rank-based learning, hypervolume-based non-dominated search, and local search in the relatively sparse objective space. Specifically, a probabilistic neural network is built as classifier to divide the offspring into a number of ranks. The offspring in different ranks uses rank-based learning strategy to generate more promising and informative candidates for real funct
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#65292;&#19981;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;GPU&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09439</link><description>&lt;p&gt;
&#29992;&#20110;GPU&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#30340;&#26412;&#22320;&#29289;&#20307;&#35009;&#21098;&#30896;&#25758;&#32593;&#32476;&#30340;&#39640;&#25928;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators. (arXiv:2304.09439v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09439
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#65292;&#19981;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;GPU&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;GPU&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#30340;&#39640;&#25928;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#12290;&#30446;&#21069;GPU&#27169;&#25311;&#22120;&#22312;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#26102;&#38656;&#35201;&#22312;&#36895;&#24230;&#12289;&#36890;&#29992;&#24615;&#21644;&#31934;&#24230;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#65292;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#29616;&#26377;&#30340;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#31934;&#24230;&#20165;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#32780;&#19981;&#26159;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym and Brax must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert-Johnson-Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra). Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312; benchmark &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09431</link><description>&lt;p&gt;
&#38789;&#21518;&#39564;&#31070;&#32463;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Martingale Posterior Neural Processes. (arXiv:2304.09431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312; benchmark &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;(NP)&#21487;&#29992;&#20110;&#20272;&#35745;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38544;&#24335;&#23450;&#20041;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#39044;&#20808;&#35268;&#23450;&#24050;&#30693;&#20808;&#39564;&#30340;&#36807;&#31243;&#65292;&#20363;&#22914;&#39640;&#26031;&#36807;&#31243;&#12290;&#29702;&#24819;&#30340;NP&#23558;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20999;&#32780;&#27809;&#26377;&#20219;&#20309;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#24120;&#24120;&#20026;&#20102;&#26041;&#20415;&#20272;&#35745;&#32780;&#38480;&#21046;&#20102;&#38543;&#26426;&#36807;&#31243;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38789;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#31181;&#39044;&#27979;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#23567;&#20102;&#29983;&#25104;&#20989;&#25968;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;MPNP&#26694;&#26550;&#65292;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;NP&#26041;&#27861;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more "data-driven" source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09424</link><description>&lt;p&gt;
&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26657;&#20934;&#21487;&#26368;&#23567;&#21270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Loss minimization yields multicalibration for large neural networks. (arXiv:2304.09424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#25439;&#22833;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#65292;&#20197;&#25552;&#20379;&#20844;&#24179;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26657;&#20934;&#26159;&#19968;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#36328;&#22823;&#37327;&#22242;&#20307;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#20989;&#25968;&#65292;&#22810;&#26657;&#20934;&#20063;&#34987;&#35748;&#20026;&#26159;&#19982;&#26368;&#23567;&#21270;&#25439;&#22833;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#65288;&#20960;&#20046;&#25152;&#26377;&#30340;&#65289;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#65292;&#26368;&#20248;&#22320;&#26368;&#23567;&#21270;&#24179;&#26041;&#35823;&#24046;&#20250;&#23548;&#33268;&#22810;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#24449;&#26041;&#38754;&#65292;&#32780;&#19981;&#26159;&#20851;&#20110;&#31639;&#27861;&#25110;&#26679;&#26412;&#22797;&#26434;&#24615;&#32771;&#34385;&#12290;&#20197;&#21069;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#20165;&#36866;&#29992;&#20110;&#20960;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#39044;&#27979;&#22120;&#65292;&#22240;&#27492;&#26159;&#34920;&#24449;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#22914; SGD&#65292;&#24182;&#19988;&#19981;&#24212;&#35299;&#37322;&#20026;&#8220;&#20844;&#24179;&#24615;&#20174;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33719;&#24471;&#20813;&#36153;&#30340;&#22909;&#22788;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multicalibration is a notion of fairness that aims to provide accurate predictions across a large set of groups. Multicalibration is known to be a different goal than loss minimization, even for simple predictors such as linear functions. In this note, we show that for (almost all) large neural network sizes, optimally minimizing squared error leads to multicalibration. Our results are about representational aspects of neural networks, and not about algorithmic or sample complexity considerations. Previous such results were known only for predictors that were nearly Bayes-optimal and were therefore representation independent. We emphasize that our results do not apply to specific algorithms for optimizing neural networks, such as SGD, and they should not be interpreted as "fairness comes for free from optimizing neural networks".
&lt;/p&gt;</description></item><item><title>Pointerformer&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;DRL&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#25351;&#38024;Transformer&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#23427;&#20351;&#29992;&#21487;&#36870;&#27531;&#24046;&#32593;&#32476;&#21644;&#22810;&#25351;&#38024;&#32593;&#32476;&#26469;&#25511;&#21046;&#20869;&#23384;&#28040;&#32791;&#65292;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#26469;&#25351;&#23548;&#35299;&#30721;&#22120;&#65292;&#26159;&#30446;&#21069;&#22312;&#22823;&#35268;&#27169;TSP&#23454;&#20363;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09407</link><description>&lt;p&gt;
Pointerformer: &#22810;&#25351;&#38024;&#28145;&#24230;&#24378;&#21270;Transformer&#31639;&#27861;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Pointerformer: Deep Reinforced Multi-Pointer Transformer for the Traveling Salesman Problem. (arXiv:2304.09407v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09407
&lt;/p&gt;
&lt;p&gt;
Pointerformer&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;DRL&#31639;&#27861;&#65292;&#37319;&#29992;&#22810;&#25351;&#38024;Transformer&#31639;&#27861;&#26469;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#23427;&#20351;&#29992;&#21487;&#36870;&#27531;&#24046;&#32593;&#32476;&#21644;&#22810;&#25351;&#38024;&#32593;&#32476;&#26469;&#25511;&#21046;&#20869;&#23384;&#28040;&#32791;&#65292;&#20351;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#26469;&#25351;&#23548;&#35299;&#30721;&#22120;&#65292;&#26159;&#30446;&#21069;&#22312;&#22823;&#35268;&#27169;TSP&#23454;&#20363;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#25928;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#21830;&#38382;&#39064;(TSP)&#26368;&#21021;&#36215;&#28304;&#20110;&#20132;&#36890;&#21644;&#29289;&#27969;&#34892;&#19994;&#30340;&#32463;&#20856;&#36335;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#29616;&#24050;&#25104;&#20026;&#21046;&#36896;&#21644;&#29983;&#29289;&#31561;&#26356;&#24191;&#27867;&#39046;&#22495;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22240;&#20854;&#39640;&#25512;&#29702;&#25928;&#29575;&#32780;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#26469;&#35299;&#20915;TSP&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;DRL&#31639;&#27861;&#20165;&#22312;&#23567;&#35268;&#27169;TSP&#23454;&#20363;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20869;&#23384;&#28040;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#24613;&#21095;&#22686;&#21152;&#65292;&#24456;&#38590;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;DRL&#26041;&#27861;&#65292;&#31216;&#20026;Pointerformer&#65292;&#22522;&#20110;&#22810;&#25351;&#38024;Transformer&#12290;&#29305;&#21035;&#22320;&#65292;Pointerformer&#22312;&#32534;&#30721;&#22120;&#20013;&#37319;&#29992;&#20102;&#21487;&#36870;&#27531;&#24046;&#32593;&#32476;&#65292;&#22312;&#35299;&#30721;&#22120;&#20013;&#37319;&#29992;&#20102;&#22810;&#25351;&#38024;&#32593;&#32476;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;TSP&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;Pointerformer&#36824;&#20351;&#29992;&#20102;&#28145;&#24230;Q&#32593;&#32476;(DQN)&#26469;&#25351;&#23548;&#35299;&#30721;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Pointerformer&#22312;&#22823;&#35268;&#27169;TSP&#23454;&#20363;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Salesman Problem (TSP), as a classic routing optimization problem originally arising in the domain of transportation and logistics, has become a critical task in broader domains, such as manufacturing and biology. Recently, Deep Reinforcement Learning (DRL) has been increasingly employed to solve TSP due to its high inference efficiency. Nevertheless, most of existing end-to-end DRL algorithms only perform well on small TSP instances and can hardly generalize to large scale because of the drastically soaring memory consumption and computation time along with the enlarging problem scale. In this paper, we propose a novel end-to-end DRL approach, referred to as Pointerformer, based on multi-pointer Transformer. Particularly, Pointerformer adopts both reversible residual network in the encoder and multi-pointer network in the decoder to effectively contain memory consumption of the encoder-decoder architecture. To further improve the performance of TSP solutions, Pointerformer e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;H-TSP&#30340;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19978;&#19979;&#23618;&#31574;&#30053;&#65292;&#30452;&#25509;&#29983;&#25104;&#32473;&#23450;TSP&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#20219;&#20309;&#32791;&#26102;&#30340;&#25628;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#29983;&#25104;&#30340;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;H-TSP&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65288;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#38388;&#30340;&#38388;&#38553;&#20026;3.42% vs. 7.3%&#65289;&#65292;&#32780;&#22312;&#20855;&#26377;2000&#20010;&#20197;&#19978;&#33410;&#28857;&#30340;&#23454;&#20363;&#19978;&#21017;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09395</link><description>&lt;p&gt;
H-TSP&#65306;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#26053;&#34892;&#21830;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
H-TSP: Hierarchically Solving the Large-Scale Travelling Salesman Problem. (arXiv:2304.09395v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09395
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;H-TSP&#30340;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#19978;&#19979;&#23618;&#31574;&#30053;&#65292;&#30452;&#25509;&#29983;&#25104;&#32473;&#23450;TSP&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#20219;&#20309;&#32791;&#26102;&#30340;&#25628;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#29983;&#25104;&#30340;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;H-TSP&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65288;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#38388;&#30340;&#38388;&#38553;&#20026;3.42% vs. 7.3%&#65289;&#65292;&#32780;&#22312;&#20855;&#26377;2000&#20010;&#20197;&#19978;&#33410;&#28857;&#30340;&#23454;&#20363;&#19978;&#21017;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;H-TSP&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#12290;H-TSP&#20174;&#22836;&#24320;&#22987;&#26500;&#24314;TSP&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20381;&#36182;&#20110;&#20004;&#20010;&#32452;&#20214;&#65306;&#19978;&#23618;&#31574;&#30053;&#20174;&#35201;&#36941;&#21382;&#30340;&#25152;&#26377;&#33410;&#28857;&#20013;&#36873;&#25321;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#26368;&#22810;&#20026;200&#20010;&#65289;&#65292;&#32780;&#19979;&#23618;&#31574;&#30053;&#23558;&#25152;&#36873;&#33410;&#28857;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#36830;&#25509;&#23427;&#20204;&#19982;&#29616;&#26377;&#37096;&#20998;&#36335;&#24452;&#65288;&#26368;&#21021;&#20165;&#21253;&#21547;&#36710;&#31449;&#65289;&#30456;&#36830;&#30340;&#36335;&#24452;&#12290;&#22312;&#32852;&#21512;&#35757;&#32451;&#19978;&#19979;&#23618;&#31574;&#30053;&#20043;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#32473;&#23450;TSP&#23454;&#20363;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#20110;&#20219;&#20309;&#32791;&#26102;&#30340;&#25628;&#32034;&#36807;&#31243;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#33410;&#28857;&#25968;&#30340;&#38543;&#26426;&#29983;&#25104;TSP&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;1000&#20010;&#33410;&#28857;&#30340;&#23454;&#20363;&#19978;&#65292;H-TSP&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65288;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#38388;&#30340;&#38388;&#38553;&#20026;3.42% vs. 7.3%&#65289;&#65292;&#32780;&#22312;&#20855;&#26377;2000&#20010;&#20197;&#19978;&#33410;&#28857;&#30340;&#23454;&#20363;&#19978;&#21017;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end learning framework based on hierarchical reinforcement learning, called H-TSP, for addressing the large-scale Travelling Salesman Problem (TSP). The proposed H-TSP constructs a solution of a TSP instance starting from the scratch relying on two components: the upper-level policy chooses a small subset of nodes (up to 200 in our experiment) from all nodes that are to be traversed, while the lower-level policy takes the chosen nodes as input and outputs a tour connecting them to the existing partial route (initially only containing the depot). After jointly training the upper-level and lower-level policies, our approach can directly generate solutions for the given TSP instances without relying on any time-consuming search procedures. To demonstrate effectiveness of the proposed approach, we have conducted extensive experiments on randomly generated TSP instances with different numbers of nodes. We show that H-TSP can achieve comparable results (gap 3.42% vs. 7.3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#23610;&#24230;&#25968;&#25454;&#38598;&#25104;&#35782;&#21035;C&#22411;&#24314;&#31569;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35782;&#21035;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09391</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#23610;&#24230;&#25968;&#25454;&#38598;&#25104;&#25512;&#26029;&#39640;&#23618;&#22320;&#29702;&#27010;&#24565;&#65306;C&#22411;&#24314;&#31569;&#27169;&#24335;&#35782;&#21035;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring High-level Geographical Concepts via Knowledge Graph and Multi-scale Data Integration: A Case Study of C-shaped Building Pattern Recognition. (arXiv:2304.09391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#23610;&#24230;&#25968;&#25454;&#38598;&#25104;&#35782;&#21035;C&#22411;&#24314;&#31569;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35782;&#21035;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24314;&#31569;&#27169;&#24335;&#35782;&#21035;&#23545;&#20110;&#20102;&#35299;&#22478;&#24066;&#24418;&#24577;&#12289;&#33258;&#21160;&#21270;&#22320;&#22270;&#27010;&#25324;&#21644;&#21487;&#35270;&#21270;3D&#22478;&#24066;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#35268;&#21017;&#21644;&#37051;&#36817;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#20110;&#23545;&#35937;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#35270;&#35273;&#26159;&#19968;&#20010;&#22522;&#20110;&#37096;&#20214;&#30340;&#31995;&#32479;&#65292;&#27169;&#24335;&#35782;&#21035;&#21487;&#33021;&#38656;&#35201;&#23558;&#24418;&#29366;&#20998;&#35299;&#20026;&#37096;&#20214;&#25110;&#23558;&#20854;&#20998;&#32452;&#20026;&#31751;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#25152;&#26377;&#35270;&#35273;&#24863;&#30693;&#27169;&#24335;&#65292;&#24182;&#19988;&#37051;&#36817;&#22270;&#27169;&#22411;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#22810;&#23610;&#24230;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;C&#22411;&#24314;&#31569;&#27169;&#24335;&#35782;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23646;&#24615;&#22270;&#34920;&#31034;&#21442;&#19982;C&#22411;&#24314;&#31569;&#27169;&#24335;&#35782;&#21035;&#30340;&#19981;&#21516;&#23610;&#24230;&#20869;&#21644;&#36328;&#23610;&#24230;&#20043;&#38388;&#30340;&#24314;&#31569;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#30693;&#35782;&#22270;&#35889;&#23384;&#20648;&#22312;&#22270;&#24418;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#23558;C&#22411;&#27169;&#24335;&#35782;&#21035;&#21644;&#20016;&#23500;&#30340;&#35268;&#21017;&#36716;&#25442;&#20026;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective building pattern recognition is critical for understanding urban form, automating map generalization, and visualizing 3D city models. Most existing studies use object-independent methods based on visual perception rules and proximity graph models to extract patterns. However, because human vision is a part-based system, pattern recognition may require decomposing shapes into parts or grouping them into clusters. Existing methods may not recognize all visually aware patterns, and the proximity graph model can be inefficient. To improve efficiency and effectiveness, we integrate multi-scale data using a knowledge graph, focusing on the recognition of C-shaped building patterns. First, we use a property graph to represent the relationships between buildings within and across different scales involved in C-shaped building pattern recognition. Next, we store this knowledge graph in a graph database and convert the rules for C-shaped pattern recognition and enrichment into query co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#25928;&#26524;&#65292;&#24182;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#23637;&#31034;&#20102;&#33976;&#39311;&#26041;&#27861;&#23545;&#27169;&#22411;&#22823;&#23567;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09388</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models. (arXiv:2304.09388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21387;&#32553;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#35777;&#25928;&#26524;&#65292;&#24182;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#23637;&#31034;&#20102;&#33976;&#39311;&#26041;&#27861;&#23545;&#27169;&#22411;&#22823;&#23567;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#21387;&#32553;&#31070;&#32463;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;MNMT&#65288;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#26222;&#21450;&#21644;&#20248;&#36234;&#24615;&#65292;&#20294;&#20174;&#22823;&#22411;MNMT&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#30740;&#31350;&#23454;&#38469;&#19978;&#24182;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21387;&#32553;MNMT&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#35821;&#35328;&#26080;&#20851;&#21644;&#35821;&#35328;&#24863;&#30693;&#30340;&#33976;&#39311;&#26041;&#27861;&#21487;&#20197;&#20351;&#27169;&#22411;&#21387;&#32553;4-5&#20493;&#65292;&#20294;&#24615;&#33021;&#19979;&#38477;&#22810;&#36798;3.5 BLEU&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#35774;&#35745;&#19978;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#28145;&#23618;&#27169;&#22411;&#21644;&#27973;&#23618;&#27169;&#22411;&#12289;&#21442;&#25968;&#20849;&#20139;&#12289;&#22810;&#38454;&#27573;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;&#31561;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#28145;&#23618;&#32039;&#20945;&#27169;&#22411;&#24448;&#24448;&#19982;&#27973;&#23618;&#38750;&#32039;&#20945;&#27169;&#22411;&#19968;&#26679;&#22909;&#65292;&#23558;&#33976;&#39311;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#23376;&#38598;&#19978;&#24494;&#35843;&#21487;&#20197;&#31245;&#24494;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multi-stage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones, and that fine-tuning a distilled model on a High-Quality subset slightly boosts translation quality. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.09358</link><description>&lt;p&gt;
&#25506;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#19977;&#32500;&#27867;&#21270;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating the Nature of 3D Generalization in Deep Neural Networks. (arXiv:2304.09358v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#35937;&#35782;&#21035;&#31995;&#32479;&#38656;&#35201;&#20174;&#19968;&#32452;&#20108;&#32500;&#35757;&#32451;&#35270;&#22270;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#12290;&#22914;&#20309;&#20351;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#30340;&#38382;&#39064;&#24050;&#32463;&#22312;&#24515;&#29702;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#31070;&#32463;&#31185;&#23398;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#24314;&#27169;&#12290;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#29992;&#20110;&#23545;&#35937;&#35782;&#21035;&#23545;&#26032;&#35270;&#22270;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#24120;&#35265;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#23545;&#26032;&#35270;&#22270;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#20854;&#21046;&#23450;&#20026;&#19968;&#20010;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#26631;&#31614;&#23545;&#24212;&#20110;&#21807;&#19968;&#30340;&#19977;&#32500;&#29289;&#20307;&#65292;&#31034;&#20363;&#23545;&#24212;&#20110;&#29289;&#20307;&#22312;&#19981;&#21516;&#19977;&#32500;&#26041;&#21521;&#19978;&#30340;&#20108;&#32500;&#35270;&#22270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#25512;&#24191;&#21040;&#26032;&#35270;&#22270;&#30340;&#27169;&#22411;&#65306;(i)&#23436;&#20840;&#30340;&#19977;&#32500;&#27867;&#21270;&#65292;(ii)&#32431;&#20108;&#32500;&#21305;&#37197;&#65292;(iii)&#22522;&#20110;&#35270;&#22270;&#30340;&#32447;&#24615;&#32452;&#21512;&#21305;&#37197;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#28145;&#24230;&#27169;&#22411;&#20855;&#26377;&#24456;&#22909;&#30340;&#25512;&#24191;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#26041;&#24335;&#19982;&#25152;&#26377;&#36825;&#20123;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#12290;&#22806;&#25512;&#21040;vi
&lt;/p&gt;
&lt;p&gt;
Visual object recognition systems need to generalize from a set of 2D training views to novel views. The question of how the human visual system can generalize to novel views has been studied and modeled in psychology, computer vision, and neuroscience. Modern deep learning architectures for object recognition generalize well to novel views, but the mechanisms are not well understood. In this paper, we characterize the ability of common deep learning architectures to generalize to novel views. We formulate this as a supervised classification task where labels correspond to unique 3D objects and examples correspond to 2D views of the objects at different 3D orientations. We consider three common models of generalization to novel views: (i) full 3D generalization, (ii) pure 2D matching, and (iii) matching based on a linear combination of views. We find that deep models generalize well to novel views, but they do so in a way that differs from all these existing models. Extrapolation to vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#26469;&#20248;&#21270;&#27880;&#20837;&#22120;&#21644;&#30417;&#27979;&#20117;&#20301;&#32622;&#65292;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;CO2&#20648;&#23384;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#22788;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.09352</link><description>&lt;p&gt;
&#38271;&#26399;&#23433;&#20840;&#20248;&#21270;&#30899;&#20648;&#23384;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Optimizing Carbon Storage Operations for Long-Term Safety. (arXiv:2304.09352v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#26469;&#20248;&#21270;&#27880;&#20837;&#22120;&#21644;&#30417;&#27979;&#20117;&#20301;&#32622;&#65292;&#22312;&#30830;&#20445;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;CO2&#20648;&#23384;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26469;&#22788;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#21464;&#26262;&#21644;&#20943;&#32531;&#19982;&#27668;&#20505;&#21464;&#21270;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#30899;&#25429;&#38598;&#21644;&#23553;&#23384;&#25216;&#26415;&#65288;CCS&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23558;CO2&#23433;&#20840;&#22320;&#23553;&#23384;&#22312;&#22320;&#36136;&#23618;&#20013;&#38271;&#26399;&#20648;&#23384;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#30899;&#20648;&#23384;&#25805;&#20316;&#30340;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#24182;&#20351;&#29992;&#20449;&#24565;&#29366;&#24577;&#35268;&#21010;&#26469;&#20248;&#21270;&#27880;&#20837;&#22120;&#21644;&#30417;&#27979;&#20117;&#30340;&#20301;&#32622;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20648;&#23384;CO2&#65292;&#21516;&#26102;&#20445;&#25345;&#23433;&#20840;&#12290;&#20223;&#30495;&#23454;&#39564;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#30830;&#20445;&#20102;&#23433;&#20840;&#30340;&#38271;&#26399;&#30899;&#20648;&#23384;&#25805;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#31181;&#19981;&#21516;&#30340;&#30417;&#27979;&#31574;&#30053;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#23545;&#20915;&#31574;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;POMDP&#20915;&#31574;&#36807;&#31243;&#22797;&#26434;&#21160;&#24577;&#30340;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To combat global warming and mitigate the risks associated with climate change, carbon capture and storage (CCS) has emerged as a crucial technology. However, safely sequestering CO2 in geological formations for long-term storage presents several challenges. In this study, we address these issues by modeling the decision-making process for carbon storage operations as a partially observable Markov decision process (POMDP). We solve the POMDP using belief state planning to optimize injector and monitoring well locations, with the goal of maximizing stored CO2 while maintaining safety. Empirical results in simulation demonstrate that our approach is effective in ensuring safe long-term carbon storage operations. We showcase the flexibility of our approach by introducing three different monitoring strategies and examining their impact on decision quality. Additionally, we introduce a neural network surrogate model for the POMDP decision-making process to handle the complex dynamics of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>Promptify&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#21160;&#31995;&#32479;&#65292;&#29992;&#20110;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25552;&#31034;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#24314;&#35758;&#24341;&#25806;&#24555;&#36895;&#25506;&#32034;&#21644;&#21019;&#20316;&#26679;&#24335;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#32780;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#20248;&#21270;&#20182;&#20204;&#30340;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#25152;&#26399;&#26395;&#30340;&#29305;&#24449;&#24182;&#36991;&#20813;&#19981;&#38656;&#35201;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.09337</link><description>&lt;p&gt;
Promptify: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#21160;&#25552;&#31034;&#25506;&#32034;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models. (arXiv:2304.09337v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09337
&lt;/p&gt;
&lt;p&gt;
Promptify&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#21160;&#31995;&#32479;&#65292;&#29992;&#20110;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25552;&#31034;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#24314;&#35758;&#24341;&#25806;&#24555;&#36895;&#25506;&#32034;&#21644;&#21019;&#20316;&#26679;&#24335;&#22810;&#26679;&#30340;&#25552;&#31034;&#65292;&#32780;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#20248;&#21270;&#20182;&#20204;&#30340;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#25152;&#26399;&#26395;&#30340;&#29305;&#24449;&#24182;&#36991;&#20813;&#19981;&#38656;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#24050;&#32463;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#31934;&#30830;&#25429;&#25417;&#29992;&#25143;&#21019;&#24847;&#24847;&#22270;&#30340;&#25552;&#31034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#35797;&#38169;&#24615;&#30340;&#25805;&#20316;&#65292;&#30830;&#20445;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#25552;&#31034;&#21644;&#29992;&#25143;&#24847;&#22270;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Promptify&#65292;&#19968;&#31181;&#25903;&#25345;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25552;&#31034;&#25506;&#32034;&#21644;&#20248;&#21270;&#30340;&#20114;&#21160;&#31995;&#32479;&#12290;Promptify &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#24341;&#25806;&#65292;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#25506;&#32034;&#21644;&#21019;&#20316;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#30028;&#38754;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#22320;&#32452;&#32455;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#20559;&#22909;&#65292;Promptify &#25552;&#20379;&#23545;&#21407;&#22987;&#25552;&#31034;&#30340;&#24314;&#35758;&#26356;&#25913;&#12290;&#36825;&#31181;&#21453;&#39304;&#24490;&#29615;&#20351;&#29992;&#25143;&#33021;&#22815;&#36845;&#20195;&#22320;&#20248;&#21270;&#20182;&#20204;&#30340;&#25552;&#31034;&#65292;&#24182;&#22686;&#24378;&#25152;&#26399;&#26395;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#38656;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;Promptify &#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user's creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention. To address the challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20004;&#20010;&#22270;&#23398;&#20064;&#23618;&#20998;&#21035;&#27169;&#22411;&#21270;&#20102;SST&#25968;&#25454;&#30340;&#22266;&#23450;&#32593;&#32476;&#21644;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#23618;&#20197;&#31934;&#30830;&#39044;&#27979;&#26102;&#31354;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09290</link><description>&lt;p&gt;
&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Spatio-temporal Sea Surface Temperature Forecasting via Static and Dynamic Learnable Personalized Graph Convolution Network. (arXiv:2304.09290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26102;&#31354;&#28023;&#34920;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20004;&#20010;&#22270;&#23398;&#20064;&#23618;&#20998;&#21035;&#27169;&#22411;&#21270;&#20102;SST&#25968;&#25454;&#30340;&#22266;&#23450;&#32593;&#32476;&#21644;&#21160;&#24577;&#32593;&#32476;&#65292;&#24182;&#35774;&#35745;&#20102;&#20010;&#24615;&#21270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#23618;&#20197;&#31934;&#30830;&#39044;&#27979;&#26102;&#31354;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#34920;&#28201;&#24230;&#23545;&#20110;&#22320;&#29699;&#22823;&#27668;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20854;&#21160;&#21147;&#23398;&#23545;&#20110;&#22609;&#36896;&#26412;&#22320;&#21644;&#20840;&#29699;&#27668;&#20505;&#26377;&#24456;&#22823;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#28145;&#21051;&#22320;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#24577;&#31995;&#32479;&#12290;&#20934;&#30830;&#39044;&#27979;&#28023;&#34920;&#28201;&#24230;&#33021;&#22815;&#24102;&#26469;&#37325;&#22823;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#24433;&#21709;&#65292;&#20363;&#22914;&#25552;&#21069;&#25968;&#26376;&#26356;&#22909;&#22320;&#20934;&#22791;&#26497;&#31471;&#22825;&#27668;&#65292;&#22914;&#20005;&#37325;&#24178;&#26097;&#25110;&#28909;&#24102;&#27668;&#26059;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28023;&#27915;&#31995;&#32479;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#21487;&#23398;&#20064;&#20010;&#24615;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476; (SD-LPGC)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SD-LPGC&#26041;&#27861;&#22312;&#39044;&#27979;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sea surface temperature (SST) is uniquely important to the Earth's atmosphere since its dynamics are a major force in shaping local and global climate and profoundly affect our ecosystems. Accurate forecasting of SST brings significant economic and social implications, for example, better preparation for extreme weather such as severe droughts or tropical cyclones months ahead. However, such a task faces unique challenges due to the intrinsic complexity and uncertainty of ocean systems. Recently, deep learning techniques, such as graphical neural networks (GNN), have been applied to address this task. Even though these methods have some success, they frequently have serious drawbacks when it comes to investigating dynamic spatiotemporal dependencies between signals. To solve this problem, this paper proposes a novel static and dynamic learnable personalized graph convolution network (SD-LPGC). Specifically, two graph learning layers are first constructed to respectively model the stabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22522;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#30340;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#20013;&#23436;&#25972;&#24615;&#21644;&#22403;&#22334;&#24615;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#32034;&#24341;&#22788;&#29702;&#21644;&#38024;&#23545;&#24615;&#29992;&#25143;&#20998;&#32452;&#22788;&#29702;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09287</link><description>&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#30340;&#23436;&#25972;&#24615;&#21644;&#22403;&#22334;&#22788;&#29702;&#26041;&#27861;&#65306;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrity and Junkiness Failure Handling for Embedding-based Retrieval: A Case Study in Social Network Search. (arXiv:2304.09287v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#22522;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#30340;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#20013;&#23436;&#25972;&#24615;&#21644;&#22403;&#22334;&#24615;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#32034;&#24341;&#22788;&#29702;&#21644;&#38024;&#23545;&#24615;&#29992;&#25143;&#20998;&#32452;&#22788;&#29702;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#31561;&#21508;&#31181;&#25628;&#32034;&#24212;&#29992;&#20013;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#21305;&#37197;&#21644;&#19978;&#19979;&#25991;&#25628;&#32034;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#23427;&#20173;&#21463;&#21040;&#26080;&#27861;&#25511;&#21046;&#30340;&#20851;&#32852;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#22312;&#23545;&#26089;&#26399;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#30340;&#23884;&#20837;&#24335;&#26816;&#32034;&#36827;&#34892;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#23450;&#20041;&#20102;&#24341;&#20837;&#30340;&#20004;&#31181;&#20027;&#35201;&#30340;&#25925;&#38556;&#31867;&#22411;&#65292;&#23436;&#25972;&#24615;&#21644;&#22403;&#22334;&#24615;&#12290;&#21069;&#32773;&#26159;&#25351;&#21487;&#20197;&#20005;&#37325;&#21361;&#23475;&#29992;&#25143;&#20307;&#39564;&#30340;&#20167;&#24680;&#35328;&#35770;&#21644;&#25915;&#20987;&#24615;&#20869;&#23481;&#31561;&#38382;&#39064;&#65292;&#32780;&#21518;&#32773;&#21253;&#25324;&#31867;&#20284;&#27169;&#31946;&#25991;&#26412;&#21305;&#37197;&#25110;&#35821;&#35328;&#19981;&#21305;&#37197;&#30340;&#26080;&#20851;&#32467;&#26524;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#27169;&#22411;&#25512;&#26029;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#32034;&#24341;&#22788;&#29702;&#21644;&#38024;&#23545;&#24615;&#29992;&#25143;&#20998;&#32452;&#22788;&#29702;&#31561;&#12290;&#23613;&#31649;&#26041;&#27861;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#31163;&#32447;NDCG&#21644;&#22312;&#32447;A/B&#27979;&#35797;&#25351;&#26631;&#22686;&#30410;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#25925;&#38556;&#30340;&#21407;&#22240;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#22312;&#22522;&#20110;&#23884;&#20837;&#24335;&#26816;&#32034;&#30340;&#31038;&#20132;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#23436;&#25972;&#24615;&#21644;&#22403;&#22334;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based retrieval has seen its usage in a variety of search applications like e-commerce, social networking search etc. While the approach has demonstrated its efficacy in tasks like semantic matching and contextual search, it is plagued by the problem of uncontrollable relevance. In this paper, we conduct an analysis of embedding-based retrieval launched in early 2021 on our social network search engine, and define two main categories of failures introduced by it, integrity and junkiness. The former refers to issues such as hate speech and offensive content that can severely harm user experience, while the latter includes irrelevant results like fuzzy text matching or language mismatches. Efficient methods during model inference are further proposed to resolve the issue, including indexing treatments and targeted user cohort treatments, etc. Though being simple, we show the methods have good offline NDCG and online A/B tests metrics gain in practice. We analyze the reasons for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pelphix&#30340;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#27169;&#25311;&#36807;&#31243;&#24182;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09285</link><description>&lt;p&gt;
Pelphix&#65306;&#32463;&#30382;&#30406;&#39592;&#22266;&#23450;&#26415;&#20013;&#22522;&#20110;X&#20809;&#22270;&#20687;&#30340;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pelphix: Surgical Phase Recognition from X-ray Images in Percutaneous Pelvic Fixation. (arXiv:2304.09285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pelphix&#30340;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#27169;&#25311;&#36807;&#31243;&#24182;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;(SPR)&#26159;&#29616;&#20195;&#25163;&#26415;&#23460;&#25968;&#23383;&#21270;&#36716;&#22411;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20803;&#32032;&#12290;&#34429;&#28982;&#22522;&#20110;&#35270;&#39057;&#28304;&#30340;SPR&#24050;&#32463;&#24456;&#25104;&#29087;&#65292;&#20294;&#25554;&#31649;X&#20809;&#24207;&#21015;&#30340;&#25972;&#21512;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Pelphix&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;X&#20809;&#24341;&#23548;&#19979;&#30340;&#32463;&#30382;&#30406;&#39592;&#25240;&#20462;&#22797;&#30340;SPR&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#35813;&#36807;&#31243;&#24314;&#27169;&#20026;&#22235;&#20010;&#31890;&#24230;&#27700;&#24179;&#8212;&#8212;&#36208;&#24266;&#12289;&#27963;&#21160;&#12289;&#35270;&#22270;&#21644;&#24103;&#20540;&#8212;&#8212;&#23558;&#30406;&#39592;&#25240;&#20462;&#22797;&#24037;&#20316;&#27969;&#31243;&#27169;&#25311;&#20026;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#23436;&#20840;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290; &#20351;&#29992;&#20174;&#39592;&#36208;&#24266;&#12289;&#24037;&#20855;&#21644;&#35299;&#21078;&#23398;&#26816;&#27979;&#20013;&#28155;&#21152;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#22235;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#22238;&#24402;&#25163;&#26415;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20110;X&#20809;&#30340;SPR&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#27169;&#25311;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;93.8&#65285;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65292;&#22312;&#23608;&#20307;&#20013;&#23454;&#29616;&#20102;67.57&#65285;&#30340;&#25152;&#26377;&#31890;&#24230;&#32423;&#21035;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#36208;&#24266;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;88&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical phase recognition (SPR) is a crucial element in the digital transformation of the modern operating theater. While SPR based on video sources is well-established, incorporation of interventional X-ray sequences has not yet been explored. This paper presents Pelphix, a first approach to SPR for X-ray-guided percutaneous pelvic fracture fixation, which models the procedure at four levels of granularity -- corridor, activity, view, and frame value -- simulating the pelvic fracture fixation workflow as a Markov process to provide fully annotated training data. Using added supervision from detection of bony corridors, tools, and anatomy, we learn image representations that are fed into a transformer model to regress surgical phases at the four granularity levels. Our approach demonstrates the feasibility of X-ray-based SPR, achieving an average accuracy of 93.8% on simulated sequences and 67.57% in cadaver across all granularity levels, with up to 88% accuracy for the target corrido
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.09278</link><description>&lt;p&gt;
&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#20248;&#21270;&#22810;&#30446;&#26631;&#21046;&#36896;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A Data Driven Sequential Learning Framework to Accelerate and Optimize Multi-Objective Manufacturing Decisions. (arXiv:2304.09278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#25110;&#24615;&#36136;&#32452;&#21512;&#30340;&#20808;&#36827;&#26448;&#26009;&#21644;&#20135;&#21697;&#36890;&#24120;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25214;&#21040;&#33021;&#22815;&#29983;&#25104;&#36825;&#20123;&#24615;&#36136;&#29702;&#24819;&#32452;&#21512;&#30340;&#26368;&#20339;&#37197;&#26041;&#25110;&#22788;&#29702;&#26465;&#20214;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#26102;&#20505;&#65292;&#38656;&#35201;&#36827;&#34892;&#36275;&#22815;&#25968;&#37327;&#30340;&#23454;&#39564;&#25165;&#33021;&#29983;&#25104;Pareto&#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#21046;&#36896;&#23454;&#39564;&#36890;&#24120;&#24456;&#26114;&#36149;&#65292;&#29978;&#33267;&#36827;&#34892;&#19968;&#27425;&#23454;&#39564;&#20063;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#30830;&#23450;&#26368;&#20339;&#25968;&#25454;&#25910;&#38598;&#20301;&#32622;&#20197;&#33719;&#24471;&#23545;&#36807;&#31243;&#30340;&#26368;&#20840;&#38754;&#29702;&#35299;&#38750;&#24120;&#20851;&#38190;&#12290;&#24207;&#21015;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#36827;&#34892;&#20013;&#30340;&#23454;&#39564;&#20013;&#20027;&#21160;&#23398;&#20064;&#65292;&#36845;&#20195;&#26356;&#26032;&#22522;&#30784;&#20248;&#21270;&#20363;&#31243;&#65292;&#24182;&#38543;&#26102;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#24207;&#21015;&#23398;&#20064;&#26469;&#39640;&#25928;&#20248;&#21270;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30446;&#26631;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing advanced materials and products with a specific property or combination of properties is often warranted. To achieve that it is crucial to find out the optimum recipe or processing conditions that can generate the ideal combination of these properties. Most of the time, a sufficient number of experiments are needed to generate a Pareto front. However, manufacturing experiments are usually costly and even conducting a single experiment can be a time-consuming process. So, it's critical to determine the optimal location for data collection to gain the most comprehensive understanding of the process. Sequential learning is a promising approach to actively learn from the ongoing experiments, iteratively update the underlying optimization routine, and adapt the data collection process on the go. This paper presents a novel data-driven Bayesian optimization framework that utilizes sequential learning to efficiently optimize complex systems with multiple conflicting objectives. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.09276</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65306;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#36935;&#35265;&#35745;&#31639;&#21644;&#20989;&#25968;&#24335;&#32534;&#31243;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Neural Lambda Calculus: Neurosymbolic AI meets the foundations of computing and functional programming. (arXiv:2304.09276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#955;&#28436;&#31639;&#27861;&#65292;&#20351;&#29992;&#955;&#35821;&#35328;&#32534;&#31243;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25299;&#23637;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#25104;&#20026;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20027;&#23548;&#33539;&#24335;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#35748;&#20026;&#22312;&#31526;&#21495;&#23398;&#20064;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26159;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#12290;&#20026;&#20102;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#22312;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#23398;&#26500;&#36896;&#65288;&#22914;&#21152;&#27861;&#21644;&#20056;&#27861;&#65289;&#12289;&#36923;&#36753;&#25512;&#29702;&#65288;&#22914;&#23450;&#29702;&#35777;&#26126;&#22120;&#65289;&#29978;&#33267;&#25191;&#34892;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21518;&#32773;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#22826;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#24182;&#19981;&#24635;&#26159;&#25104;&#21151;&#30340;&#65292;&#24182;&#19988;&#24448;&#24448;&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#26377;&#20559;&#35265;&#30340;&#20803;&#32032;&#65292;&#20197;&#38480;&#21046;&#21487;&#33021;&#35201;&#25191;&#34892;&#30340;&#31243;&#24207;&#30340;&#33539;&#22260;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#25972;&#20010;&#31243;&#24207;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19981;&#20351;&#29992;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#65292;&#32780;&#26159;&#37319;&#29992;&#955;&#35821;&#35328;&#36827;&#34892;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#23431;&#23449;&#30340;&#27010;&#36848;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#21644;&#26041;&#21521;&#65292;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#23558;&#20250;&#32473;&#25216;&#26415;&#12289;&#28216;&#25103;&#12289;&#25945;&#32946;&#12289;&#33402;&#26415;&#21644;&#25991;&#21270;&#39046;&#22495;&#24102;&#26469;&#26356;&#22810;&#30340;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#27969;&#31243;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#20803;&#23431;&#23449;&#30340;&#20840;&#38754;&#12289;&#36328;&#23398;&#31185;&#12289;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.09240</link><description>&lt;p&gt;
&#12298;&#20803;&#23431;&#23449;&#65306;&#27010;&#36848;&#12289;&#36235;&#21183;&#12289;&#26032;&#22411;&#29983;&#24577;&#31995;&#32479;&#21644;&#26410;&#26469;&#26041;&#21521;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Metaverse: Survey, Trends, Novel Pipeline Ecosystem &amp; Future Directions. (arXiv:2304.09240v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#23431;&#23449;&#30340;&#27010;&#36848;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#25351;&#23548;&#21644;&#26041;&#21521;&#65292;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#23558;&#20250;&#32473;&#25216;&#26415;&#12289;&#28216;&#25103;&#12289;&#25945;&#32946;&#12289;&#33402;&#26415;&#21644;&#25991;&#21270;&#39046;&#22495;&#24102;&#26469;&#26356;&#22810;&#30340;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#27969;&#31243;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#20803;&#23431;&#23449;&#30340;&#20840;&#38754;&#12289;&#36328;&#23398;&#31185;&#12289;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454; (VR) &#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#20010;&#36229;&#29616;&#23454;&#30340;&#31532;&#20108;&#20010;&#19990;&#30028;&#65292;&#20854;&#20013;&#19981;&#23384;&#22312;&#20219;&#20309;&#36793;&#30028;&#65292;&#36890;&#36807;&#21442;&#19982;&#21644;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#25552;&#20379;&#20102;&#26080;&#38480;&#21487;&#33021;&#30340;&#21457;&#23637;&#12290;&#35768;&#22810;&#39046;&#22495;&#21487;&#20197;&#21463;&#30410;&#20110;&#20803;&#23431;&#23449;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#25216;&#26415;&#12289;&#28216;&#25103;&#12289;&#25945;&#32946;&#12289;&#33402;&#26415;&#21644;&#25991;&#21270;&#12290;&#28982;&#32780;&#65292;&#23558;&#20803;&#23431;&#23449;&#29615;&#22659;&#24320;&#21457;&#21040;&#20854;&#23436;&#25972;&#28508;&#21147;&#26159;&#19968;&#39033;&#27169;&#31946;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36866;&#24403;&#30340;&#25351;&#23548;&#21644;&#26041;&#21521;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#20803;&#23431;&#23449;&#30340;&#35843;&#26597;&#20165;&#20851;&#27880;&#20803;&#23431;&#23449;&#30340;&#29305;&#23450;&#26041;&#38754;&#21644;&#23398;&#31185;&#65292;&#24182;&#32570;&#20047;&#23545;&#25972;&#20010;&#36807;&#31243;&#30340;&#20840;&#38754;&#35270;&#35282;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#20840;&#38754;&#12289;&#36328;&#23398;&#31185;&#12289;&#28145;&#20837;&#21644;&#23398;&#26415;&#21644;&#34892;&#19994;&#23548;&#21521;&#30340;&#30740;&#31350;&#65292;&#20197;&#25552;&#20379;&#20803;&#23431;&#23449;&#24320;&#21457;&#27969;&#31243;&#30340;&#24443;&#24213;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23618;&#27425;&#27969;&#31243;&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#20803;&#23431;&#23449;&#35745;&#31639;&#12289;&#32593;&#32476;&#12289;&#36890;&#20449;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Metaverse offers a second world beyond reality, where boundaries are non-existent, and possibilities are endless through engagement and immersive experiences using the virtual reality (VR) technology. Many disciplines can benefit from the advancement of the Metaverse when accurately developed, including the fields of technology, gaming, education, art, and culture. Nevertheless, developing the Metaverse environment to its full potential is an ambiguous task that needs proper guidance and directions. Existing surveys on the Metaverse focus only on a specific aspect and discipline of the Metaverse and lack a holistic view of the entire process. To this end, a more holistic, multi-disciplinary, in-depth, and academic and industry-oriented review is required to provide a thorough study of the Metaverse development pipeline. To address these issues, we present in this survey a novel multi-layered pipeline ecosystem composed of (1) the Metaverse computing, networking, communications and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#38382;&#39064;&#65292;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#35813;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.09182</link><description>&lt;p&gt;
&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Framework for Traffic Data Imputation Considering Spatiotemporal Dependencies. (arXiv:2304.09182v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20132;&#36890;&#25968;&#25454;&#22635;&#34917;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#38382;&#39064;&#65292;&#20197;&#36827;&#19968;&#27493;&#24212;&#29992;&#35813;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#26102;&#31354;&#65288;ST&#65289;&#25968;&#25454;&#21487;&#20197;&#34920;&#31034;&#20026;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#36825;&#26159;&#25353;&#26102;&#38388;&#39034;&#24207;&#21015;&#20986;&#30340;&#25968;&#25454;&#28857;&#24207;&#21015;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#26377;&#29992;&#20449;&#24687;&#65292;&#20294;ST&#25968;&#25454;&#36890;&#24120;&#23384;&#22312;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36825;&#20063;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#25968;&#25454;&#22635;&#34917;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32463;&#24120;&#29992;&#20110;&#39044;&#22788;&#29702;&#25968;&#25454;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#20013;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21464;&#21270;&#30340;&#22797;&#26434;&#24615;&#65292;&#26102;&#31354;&#25968;&#25454;&#22635;&#34917;&#38750;&#24120;&#22256;&#38590;&#65292;&#26159;&#36827;&#19968;&#27493;&#24212;&#29992;&#30340;&#20851;&#38190;&#21069;&#25552;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#21482;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#25110;&#38745;&#24577;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#20182;&#20204;&#26080;&#27861;&#30452;&#25509;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal (ST) data collected by sensors can be represented as multi-variate time series, which is a sequence of data points listed in an order of time. Despite the vast amount of useful information, the ST data usually suffer from the issue of missing or incomplete data, which also limits its applications. Imputation is one viable solution and is often used to prepossess the data for further applications. However, in practice, n practice, spatiotemporal data imputation is quite difficult due to the complexity of spatiotemporal dependencies with dynamic changes in the traffic network and is a crucial prepossessing task for further applications. Existing approaches mostly only capture the temporal dependencies in time series or static spatial dependencies. They fail to directly model the spatiotemporal dependencies, and the representation ability of the models is relatively limited.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.09179</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#36741;&#21161;&#35270;&#35273;&#35745;&#21010;&#32773;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models as Visual Planners for Human Assistance. (arXiv:2304.09179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;AI&#21161;&#25163;&#25351;&#23548;&#29992;&#25143;&#23436;&#25104;&#22797;&#26434;&#22810;&#27493;&#39588;&#30446;&#26631;&#30340;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#36741;&#21161;&#35745;&#21010;&#65288;VPA&#65289;&#30340;&#20219;&#21153;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#31616;&#35201;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;&#8220;&#21046;&#20316;&#20070;&#26550;&#8221;&#65292;&#20197;&#21450;&#29992;&#25143;&#36804;&#20170;&#20026;&#27490;&#30340;&#35270;&#39057;&#36827;&#23637;&#65292;VPA&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#19968;&#20010;&#35745;&#21010;&#65292;&#21363;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#22914;&#8220;&#30722;&#20809;&#20070;&#26550;&#8221;&#12289;&#8220;&#28034;&#28422;&#20070;&#26550;&#8221;&#31561;&#65292;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#36825;&#38656;&#35201;&#35780;&#20272;&#29992;&#25143;&#22312;&#26410;&#32463;&#20462;&#21098;&#30340;&#35270;&#39057;&#20013;&#30340;&#36827;&#23637;&#65292;&#24182;&#19982;&#24213;&#23618;&#30446;&#26631;&#30340;&#35201;&#27714;&#30456;&#20851;&#32852;&#65292;&#21363;&#34892;&#21160;&#30340;&#30456;&#20851;&#24615;&#21644;&#20854;&#20013;&#30340;&#25490;&#24207;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#36825;&#38656;&#35201;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#35270;&#39057;&#21382;&#21490;&#35760;&#24405;&#21644;&#20219;&#24847;&#22797;&#26434;&#30340;&#34892;&#21160;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;VPA&#20998;&#35299;&#20026;&#35270;&#39057;&#34892;&#21160;&#20998;&#21106;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#27493;&#39588;&#20844;&#24335;&#21270;&#20026;&#22810;&#27169;&#24577;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#21010;&#32773;&#65288;VLaMP&#65289;&#65292;&#20854;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;Epic Kitchen&#21644;Charades-Ego&#65289;&#19978;&#23637;&#31034;&#20102;VLaMP&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;VLaMP&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make progress towards multi-modal AI assistants which can guide users to achieve complex multi-step goals, we propose the task of Visual Planning for Assistance (VPA). Given a goal briefly described in natural language, e.g., "make a shelf", and a video of the user's progress so far, the aim of VPA is to obtain a plan, i.e., a sequence of actions such as "sand shelf", "paint shelf", etc., to achieve the goal. This requires assessing the user's progress from the untrimmed video, and relating it to the requirements of underlying goal, i.e., relevance of actions and ordering dependencies amongst them. Consequently, this requires handling long video history, and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. We formulate the forecasting step as a multi-modal sequence modeling problem and present Visual Language Model based Planner (VLaMP), which leverages pre-trained LMs as the sequence model. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#30740;&#31350;&#28909;&#28857;&#65292;&#21487;&#29992;&#20110;&#26089;&#26399;&#30149;&#21464;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2304.09178</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Alzheimers Disease Diagnosis using Machine Learning: A Review. (arXiv:2304.09178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20998;&#26512;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26159;&#30740;&#31350;&#28909;&#28857;&#65292;&#21487;&#29992;&#20110;&#26089;&#26399;&#30149;&#21464;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20250;&#36880;&#28176;&#23548;&#33268;&#35760;&#24518;&#21147;&#20007;&#22833;&#12290;&#36825;&#31181;&#33268;&#21629;&#24615;&#33041;&#30149;&#20027;&#35201;&#24433;&#21709;&#32769;&#24180;&#20154;&#65292;&#23548;&#33268;&#35748;&#30693;&#21644;&#29983;&#29289;&#23398;&#21151;&#33021;&#34928;&#36864;&#24182;&#36880;&#28176;&#24341;&#36215;&#33041;&#33806;&#32553;&#12290;&#20026;&#20102;&#20934;&#30830;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#38656;&#35201;&#37319;&#29992;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#34892;&#19994;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#24212;&#29992;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21028;&#26029;&#19968;&#20010;&#20154;&#26159;&#21542;&#26377;&#26089;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;2008&#24180;&#33267;2023&#24180;&#38388;&#36890;&#36807;&#35895;&#27468;&#23398;&#26415;&#21457;&#29616;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimers Disease AD is an acute neuro disease that degenerates the brain cells and thus leads to memory loss progressively. It is a fatal brain disease that mostly affects the elderly. It steers the decline of cognitive and biological functions of the brain and shrinks the brain successively, which in turn is known as Atrophy. For an accurate diagnosis of Alzheimers disease, cutting edge methods like machine learning are essential. Recently, machine learning has gained a lot of attention and popularity in the medical industry. As the illness progresses, those with Alzheimers have a far more difficult time doing even the most basic tasks, and in the worst case, their brain completely stops functioning. A persons likelihood of having early-stage Alzheimers disease may be determined using the ML method. In this analysis, papers on Alzheimers disease diagnosis based on deep learning techniques and reinforcement learning between 2008 and 2023 found in google scholar were studied. Sixty re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#65288;PDAOM&#65289;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#21521;&#20854;&#25552;&#20379;&#22312;&#29420;&#31435;&#29992;&#25143;&#32452;&#20013;&#32039;&#23494;&#30456;&#20851;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20197;&#20419;&#36827;&#20998;&#31867;&#22120;&#20851;&#27880;&#19981;&#26131;&#21306;&#20998;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#36824;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.09176</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#21487;&#24494;&#20998;&#30340;&#32676;&#32452; AUC &#20248;&#21270;&#25552;&#21319;&#20010;&#24615;&#21270;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Enhancing Personalized Ranking With Differentiable Group AUC Optimization. (arXiv:2304.09176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;&#30340;AUC&#20248;&#21270;&#26041;&#27861;&#65288;PDAOM&#65289;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#21521;&#20854;&#25552;&#20379;&#22312;&#29420;&#31435;&#29992;&#25143;&#32452;&#20013;&#32039;&#23494;&#30456;&#20851;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20197;&#20419;&#36827;&#20998;&#31867;&#22120;&#20851;&#27880;&#19981;&#26131;&#21306;&#20998;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#36824;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AUC&#26159;&#35780;&#20272;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#31867;&#22120;&#26159;&#20351;&#29992;&#20132;&#21449;&#29109;&#35757;&#32451;&#30340;&#65292;&#23427;&#24182;&#19981;&#30452;&#25509;&#20248;&#21270;AUC&#25351;&#26631;&#65292;&#36825;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PDAOM&#25439;&#22833;&#65292;&#19968;&#31181;&#20855;&#26377;&#26368;&#22823;&#36829;&#35268;&#35268;&#23450;&#30340;&#20010;&#24615;&#21270;&#21644;&#21487;&#24494;&#20998;AUC&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#30452;&#25509;&#24212;&#29992;&#20110;&#35757;&#32451;&#20108;&#20803;&#20998;&#31867;&#22120;&#24182;&#29992;&#26799;&#24230;&#20248;&#21270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#25104;&#23545;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#29992;&#25143;ID&#20998;&#32452;&#30340;&#23376;&#25209;&#27425;&#20013;&#30340;&#38590;&#20998;&#36776;&#27491;&#36127;&#26679;&#26412;&#23545;&#25286;&#20998;&#20986;&#26469;&#65292;&#26088;&#22312;&#25351;&#23548;&#20998;&#31867;&#22120;&#20174;&#29420;&#31435;&#29992;&#25143;&#30340;&#35282;&#24230;&#20851;&#27880;&#30456;&#21453;&#26679;&#26412;&#20043;&#38388;&#30340;&#38590;&#20197;&#21306;&#20998;&#30340;&#20851;&#31995;&#12290;&#19982;&#25104;&#23545;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#22987;&#24418;&#24335;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;PDAOM&#25439;&#22833;&#20989;&#25968;&#19981;&#20165;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#25552;&#39640;&#20102;AUC&#21644;GAUC&#25351;&#26631;&#65292;&#32780;&#19988;&#20943;&#23569;&#20102;&#35757;&#32451;&#30446;&#26631;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
AUC is a common metric for evaluating the performance of a classifier. However, most classifiers are trained with cross entropy, and it does not optimize the AUC metric directly, which leaves a gap between the training and evaluation stage. In this paper, we propose the PDAOM loss, a Personalized and Differentiable AUC Optimization method with Maximum violation, which can be directly applied when training a binary classifier and optimized with gradient-based methods. Specifically, we construct the pairwise exponential loss with difficult pair of positive and negative samples within sub-batches grouped by user ID, aiming to guide the classifier to pay attention to the relation between hard-distinguished pairs of opposite samples from the perspective of independent users. Compared to the origin form of pairwise exponential loss, the proposed PDAOM loss not only improves the AUC and GAUC metrics in the offline evaluation, but also reduces the computation complexity of the training objecti
&lt;/p&gt;</description></item><item><title>Memento&#26159;&#19968;&#20010;Python&#21253;&#65292;&#26088;&#22312;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#22320;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#24182;&#21457;&#36816;&#34892;&#23454;&#39564;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09175</link><description>&lt;p&gt;
Memento: &#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#36731;&#26494;&#12289;&#39640;&#25928;&#21644;&#21487;&#38752;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Memento: Facilitating Effortless, Efficient, and Reliable ML Experiments. (arXiv:2304.09175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09175
&lt;/p&gt;
&lt;p&gt;
Memento&#26159;&#19968;&#20010;Python&#21253;&#65292;&#26088;&#22312;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#39640;&#25928;&#22320;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#12290;&#23427;&#25552;&#20379;&#20102;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#24182;&#21457;&#36816;&#34892;&#23454;&#39564;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#26694;&#26550;&#65292;&#36816;&#34892;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#38598;&#38750;&#24120;&#20855;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#12290;&#36825;&#36843;&#20351;&#30740;&#31350;&#20154;&#21592;&#33258;&#24049;&#33457;&#36153;&#26102;&#38388;&#23454;&#29616;&#24517;&#35201;&#30340;&#21151;&#33021;&#65292;&#22914;&#24182;&#34892;&#21270;&#12289;&#32531;&#23384;&#21644;&#26816;&#26597;&#28857;&#65292;&#32780;&#19981;&#26159;&#38598;&#20013;&#31934;&#21147;&#20110;&#20182;&#20204;&#30340;&#39033;&#30446;&#12290;&#20026;&#20102;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102; Memento&#65292;&#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#39640;&#25928;&#31649;&#29702;&#21644;&#25191;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#23454;&#39564;&#30340; Python &#21253;&#12290;Memento &#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#26126;&#20102;&#30340;&#37197;&#32622;&#30697;&#38453;&#21644;&#33021;&#22815;&#21516;&#26102;&#36816;&#34892;&#22810;&#20010;&#32447;&#31243;&#30340;&#23454;&#39564;&#26469;&#20248;&#21270;&#20219;&#20309;&#23454;&#39564;&#27969;&#31243;&#12290;Memento &#30340;&#28436;&#31034;&#21487;&#22312;&#20197;&#19979;&#32593;&#31449;&#26597;&#30475;&#65306;https://wickerlab.org/publication/memento&#12290;
&lt;/p&gt;
&lt;p&gt;
Running complex sets of machine learning experiments is challenging and time-consuming due to the lack of a unified framework. This leaves researchers forced to spend time implementing necessary features such as parallelization, caching, and checkpointing themselves instead of focussing on their project. To simplify the process, in this paper, we introduce Memento, a Python package that is designed to aid researchers and data scientists in the efficient management and execution of computationally intensive experiments. Memento has the capacity to streamline any experimental pipeline by providing a straightforward configuration matrix and the ability to concurrently run experiments across multiple threads. A demonstration of Memento is available at: https://wickerlab.org/publication/memento.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.08801</link><description>&lt;p&gt;
&#22810;&#26041;&#20250;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Speaker Profiling in Multiparty Conversations. (arXiv:2304.08801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPC&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#32773;&#29983;&#25104;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20219;&#21153;&#34987;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#20219;&#21153;&#23545;&#20110;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#20351;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#22238;&#24212;&#27599;&#20010;&#21457;&#35328;&#20154;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23637;&#29616;&#20986;&#29420;&#29305;&#30340;&#34892;&#20026;&#65292;&#20351;&#24471;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#20026;&#23545;&#35805;&#20195;&#29702;&#29983;&#25104;&#22238;&#24212;&#12290;&#34429;&#28982;&#36807;&#21435;&#30340;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#21457;&#35328;&#20154;&#20010;&#20154;&#20449;&#24687;&#21019;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#21069;&#25552;&#65292;&#21363;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#24050;&#32463;&#34987;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#38134;&#34892;&#12289;&#37202;&#24215;&#39044;&#35746;&#21644;&#33322;&#31354;&#39044;&#35746;&#31561;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#21457;&#35328;&#20154;&#20010;&#20154;&#29305;&#24449;&#20998;&#26512; (SPC)&#20219;&#21153;&#26469;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#12290;SPC&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#35805;&#20013;&#27599;&#20010;&#21457;&#35328;&#20154;&#20135;&#29983;&#20010;&#20154;&#29305;&#24449;&#25688;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#20010;&#20154;&#29305;&#24449;&#21457;&#29616;&#12289;&#20010;&#20154;&#29305;&#24449;&#31867;&#22411;&#35782;&#21035;&#21644;&#20010;&#20154;&#29305;&#24449;&#20215;&#20540;&#25552;&#21462;&#12290;&#22312;&#32473;&#23450;&#23545;&#35805;&#30340;&#24773;&#20917;&#19979;&#65292;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#21253;&#21547;&#20010;&#20154;&#20449;&#24687;&#30340;&#25152;&#26377;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational settings, individuals exhibit unique behaviors, rendering a one-size-fits-all approach insufficient for generating responses by dialogue agents. Although past studies have aimed to create personalized dialogue agents using speaker persona information, they have relied on the assumption that the speaker's persona is already provided. However, this assumption is not always valid, especially when it comes to chatbots utilized in industries like banking, hotel reservations, and airline bookings. This research paper aims to fill this gap by exploring the task of Speaker Profiling in Conversations (SPC). The primary objective of SPC is to produce a summary of persona characteristics for each individual speaker present in a dialogue. To accomplish this, we have divided the task into three subtasks: persona discovery, persona-type identification, and persona-value extraction. Given a dialogue, the first subtask aims to identify all utterances that contain persona information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.08799</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65306;&#22522;&#20110;&#39592;&#26550;&#20113;&#30528;&#33394;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization. (arXiv:2304.08799v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#30340;&#26080;&#30417;&#30563;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#19977;&#32500;&#39592;&#26550;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#36880;&#28176;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#26631;&#27880;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#19977;&#32500;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#39592;&#26550;&#20113;&#30528;&#33394;&#25216;&#26415;&#23545;&#26410;&#26631;&#27880;&#30340;&#39592;&#26550;&#25968;&#25454;&#36827;&#34892;&#31354;&#38388;&#21644;&#26102;&#38388;&#34920;&#31034;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D Skeleton-based human action recognition has attracted increasing attention in recent years. Most of the existing work focuses on supervised learning which requires a large number of labeled action sequences that are often expensive and time-consuming to annotate. In this paper, we address self-supervised 3D action representation learning for skeleton-based action recognition. We investigate self-supervised representation learning and design a novel skeleton cloud colorization technique that is capable of learning spatial and temporal skeleton representations from unlabeled skeleton sequence data. We represent a skeleton action sequence as a 3D skeleton cloud and colorize each point in the cloud according to its temporal and spatial orders in the original (unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud, we design an auto-encoder framework that can learn spatial-temporal features from the artificial color labels of skeleton joints effectively. Specifical
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Masked Language Model Based Textual Adversarial Example Detection. (arXiv:2304.08767v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08767
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#21363;&#29992;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#23545;&#20998;&#31867;&#20219;&#21153;&#12289;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#26080;&#20381;&#36182;&#30340;&#21069;&#25552;&#19979;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#23545;&#25239;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#23433;&#20840;&#24212;&#29992;&#20013;&#21487;&#38752;&#37096;&#32626;&#30340;&#20005;&#37325;&#23041;&#32961;&#65292;&#31245;&#24494;&#20462;&#25913;&#36755;&#20837;&#21363;&#21487;&#35823;&#23548;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#26679;&#26412;&#24448;&#24448;&#20559;&#31163;&#27491;&#24120;&#26679;&#26412;&#30340;&#22522;&#30784;&#25968;&#25454;&#27969;&#24418;&#65292;&#32780;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27491;&#24120;&#30340;NLP&#25968;&#25454;&#27969;&#24418;&#12290;&#20026;&#20102;&#25506;&#32034;&#22914;&#20309;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#25239;&#24615;&#26816;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#23545;&#25239;&#20363;&#23376;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#65288;MLMD&#65289;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#30340;&#27969;&#24418;&#21464;&#21270;&#65292;&#22312;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#20135;&#29983;&#26126;&#26174;&#21487;&#21306;&#20998;&#30340;&#20449;&#21495;&#12290;MLMD&#20855;&#26377;&#21363;&#25554;&#21363;&#29992;&#30340;&#20351;&#29992;&#26041;&#27861;&#65288;&#21363;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21463;&#23475;&#27169;&#22411;&#65289;&#29992;&#20110;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#32780;&#19988;&#19981;&#21463;&#20998;&#31867;&#20219;&#21153;&#12289;&#21463;&#23475;&#27169;&#22411;&#32467;&#26500;&#21644;&#24453;&#38450;&#24481;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks are a serious threat to the reliable deployment of machine learning models in safety-critical applications. They can misguide current models to predict incorrectly by slightly modifying the inputs. Recently, substantial work has shown that adversarial examples tend to deviate from the underlying data manifold of normal examples, whereas pre-trained masked language models can fit the manifold of normal NLP data. To explore how to use the masked language model in adversarial detection, we propose a novel textual adversarial example detection method, namely Masked Language Model-based Detection (MLMD), which can produce clearly distinguishable signals between normal examples and adversarial examples by exploring the changes in manifolds induced by the masked language model. MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended a
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07880</link><description>&lt;p&gt;
Sabi&#225;: &#33889;&#33796;&#29273;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a: Portuguese Large Language Models. (arXiv:2304.07880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07880
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#33021;&#22815;&#22312;&#19968;&#31995;&#21015;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#25552;&#39640;&#65292;&#8221;&#19968;&#20992;&#20999;&#8220;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#20027;&#27969;&#12290;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#20840;&#29699;&#20351;&#29992;&#30340;&#35821;&#35328;&#25968;&#37327;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#19988;&#20854;&#20013;&#24456;&#22810;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#20027;&#35201;&#30340;&#20570;&#27861;&#26159;&#23545;&#22810;&#31181;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#20570;&#27861;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#35821;&#35328;&#36827;&#34892;&#21333;&#35821;&#35328;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#21512;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#29992;3%&#25110;&#26356;&#23569;&#30340;&#21407;&#22987;&#39044;&#35757;&#32451;&#39044;&#31639;&#22312;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;GPT-J&#21644;LLaMA&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Poeta&#65288;&#19968;&#22871;&#30001;14&#20010;&#33889;&#33796;&#29273;&#35821;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#22871;&#20214;&#65289;&#19978;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#34920;&#29616;&#19978;&#36828;&#20248;&#20110;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#21644;&#22810;&#35821;&#35328;&#30340;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;Sabi&#225;-65B&#30340;&#34920;&#29616;&#19982;GPT-3.5-turbo&#25345;&#24179;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#24050;&#32463;&#35774;&#24819;&#20102;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#32463;&#36807;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of language models continue to advance, it is conceivable that "one-size-fits-all" model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabi\'a-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as transl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.07655</link><description>&lt;p&gt;
EEG SN&#65306;&#38754;&#21521; EEG &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#20302;&#24310;&#36831;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEGSN: Towards Efficient Low-latency Decoding of EEG with Graph Spiking Neural Networks. (arXiv:2304.07655v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EEGSN &#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26550;&#26500;&#65292;&#38754;&#21521;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#23398;&#20064;&#20998;&#24067;&#24335; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#20026;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#35757;&#32451;&#20381;&#36182;&#20110;&#24402;&#32435;&#20559;&#24046;&#65292;&#36825;&#24182;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#22810;&#20010;&#38656;&#35201;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290; &#22522;&#20110;&#30456;&#20851;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#25512;&#26029;&#22823;&#33041;&#34892;&#20026;&#23601;&#26159;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#23398;&#20064;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20250;&#20005;&#37325;&#24433;&#21709;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#30446;&#21069;&#65292;SNN&#20165;&#20165;&#20381;&#38752;&#19968;&#33324;&#24402;&#32435;&#20559;&#24046;&#26469;&#27169;&#25311;&#19981;&#21516;&#25968;&#25454;&#27969;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947; EEG &#20998;&#31867;&#30340;&#22270;&#24418;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;EEGSN&#65289;&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#20998;&#24067;&#22312; EEG &#20256;&#24863;&#22120;&#20013;&#30340;&#21160;&#24577;&#20851;&#31995;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25512;&#26029;&#35745;&#31639;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;20&#20493;&#65292;&#21516;&#26102;&#22312;&#36816;&#21160;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#35757;&#32451; EEG &#25968;&#25454;&#30340;&#22270;&#24418;SNN&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#21151;&#32791;&#25928;&#29575;&#30340;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast majority of spiking neural networks (SNNs) are trained based on inductive biases that are not necessarily a good fit for several critical tasks that require low-latency and power efficiency. Inferring brain behavior based on the associated electroenchephalography (EEG) signals is an example of how networks training and inference efficiency can be heavily impacted by learning spatio-temporal dependencies. Up to now, SNNs rely solely on general inductive biases to model the dynamic relations between different data streams. Here, we propose a graph spiking neural network architecture for multi-channel EEG classification (EEGSN) that learns the dynamic relational information present in the distributed EEG sensors. Our method reduced the inference computational complexity by $\times 20$ compared to the state-of-the-art SNNs, while achieved comparable accuracy on motor execution classification tasks. Overall, our work provides a framework for interpretable and efficient training of gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05055</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Graph Representation Learning. (arXiv:2304.05055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23558;&#39640;&#32500;&#31232;&#30095;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#26377;&#25928;&#22320;&#32534;&#30721;&#25104;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#20256;&#32479;&#22270;&#23884;&#20837;&#26041;&#27861;&#36981;&#24490;&#36825;&#26679;&#19968;&#31181;&#22522;&#26412;&#24605;&#24819;&#65292;&#21363;&#22270;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#30340;&#23884;&#20837;&#30690;&#37327;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#30456;&#23545;&#25509;&#36817;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20256;&#32479;&#26041;&#27861;&#30340;&#27169;&#22411;&#23481;&#37327;&#21463;&#38480;&#65292;&#38480;&#21046;&#20102;&#23398;&#20064;&#24615;&#33021;; &#65288;ii&#65289;&#29616;&#26377;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#65292;&#26080;&#27861;&#19982;&#26368;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#30456;&#32467;&#21512;&#65307;&#65288;iii&#65289;&#34920;&#31034;&#23398;&#20064;&#21644;&#19979;&#28216;&#20219;&#21153;&#30456;&#20114;&#20381;&#23384;&#65292;&#24212;&#20849;&#21516;&#21152;&#24378;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26174;&#30528;&#25104;&#21151;&#65292;&#28145;&#24230;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;MCMC&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#36890;&#36807;&#26377;&#25928;&#30340;&#37319;&#26679;&#21644;&#20840;&#23616;&#20248;&#21270;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#24182;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.04455</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#28608;&#27963;&#20989;&#25968;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization for sparse neural networks with trainable activation functions. (arXiv:2304.04455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;MCMC&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#36890;&#36807;&#26377;&#25928;&#30340;&#37319;&#26679;&#21644;&#20840;&#23616;&#20248;&#21270;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#24182;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#29486;&#20013;&#65292;&#20154;&#20204;&#23545;&#24320;&#21457;&#33021;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#28608;&#27963;&#20989;&#25968;&#38750;&#24120;&#24863;&#20852;&#36259;&#12290;&#26368;&#36817;&#65292;&#31185;&#23398;&#30028;&#25552;&#20986;&#20102;&#21487;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20284;&#20046;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#38656;&#35201;&#20272;&#35745;&#20854;&#21442;&#25968;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#33258;&#21160;&#20174;&#23398;&#20064;&#25968;&#25454;&#20013;&#20272;&#35745;&#20986;&#27169;&#22411;&#26435;&#37325;&#21644;&#28608;&#27963;&#20989;&#25968;&#21442;&#25968;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;MCMC&#30340;&#20248;&#21270;&#26041;&#26696;&#26469;&#26500;&#24314;&#25512;&#29702;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#37319;&#26679;&#26041;&#26696;&#26469;&#20445;&#35777;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#22823;&#20540;&#65292;&#20174;&#32780;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#25913;&#21892;&#25910;&#25947;&#26102;&#38388;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;CNN&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the literature on deep neural networks, there is considerable interest in developing activation functions that can enhance neural network performance. In recent years, there has been renewed scientific interest in proposing activation functions that can be trained throughout the learning process, as they appear to improve network performance, especially by reducing overfitting. In this paper, we propose a trainable activation function whose parameters need to be estimated. A fully Bayesian model is developed to automatically estimate from the learning data both the model weights and activation function parameters. An MCMC-based optimization scheme is developed to build the inference. The proposed method aims to solve the aforementioned problems and improve convergence time by using an efficient sampling scheme that guarantees convergence to the global maximum. The proposed scheme is tested on three datasets with three different CNNs. Promising results demonstrate the usefulness of o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.02819</link><description>&lt;p&gt;
GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT detectors are biased against non-native English writers. (arXiv:2304.02819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;GPT&#26816;&#27979;&#22120;&#23545;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#23384;&#22312;&#20559;&#35265;&#65292;&#23481;&#26131;&#23558;&#20854;&#20869;&#23481;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#21516;&#26102;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#20250;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#25512;&#24191;&#24102;&#26469;&#20102;&#25968;&#23383;&#36890;&#20449;&#26041;&#38754;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;AI&#29983;&#25104;&#20869;&#23481;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26041;&#27861;&#26469;&#21306;&#20998;AI&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20294;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#33521;&#35821;&#27597;&#35821;&#21644;&#38750;&#33521;&#35821;&#27597;&#35821;&#20316;&#32773;&#30340;&#20889;&#20316;&#26679;&#26412;&#35780;&#20272;&#20102;&#20960;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GPT&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#25345;&#32493;&#23558;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20889;&#20316;&#26679;&#26412;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#32780;&#21407;&#29983;&#20889;&#20316;&#26679;&#26412;&#21017;&#33021;&#22815;&#34987;&#20934;&#30830;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25552;&#31034;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;GPT&#26816;&#27979;&#22120;&#65292;&#36825;&#34920;&#26126;GPT&#26816;&#27979;&#22120;&#21487;&#33021;&#26080;&#24847;&#20013;&#24809;&#32602;&#20855;&#26377;&#21463;&#38480;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#30340;&#20316;&#32773;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21628;&#21505;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative language models has brought about substantial advancements in digital communication, while simultaneously raising concerns regarding the potential misuse of AI-generated content. Although numerous detection methods have been proposed to differentiate between AI and human-generated content, the fairness and robustness of these detectors remain underexplored. In this study, we evaluate the performance of several widely-used GPT detectors using writing samples from native and non-native English writers. Our findings reveal that these detectors consistently misclassify non-native English writing samples as AI-generated, whereas native writing samples are accurately identified. Furthermore, we demonstrate that simple prompting strategies can not only mitigate this bias but also effectively bypass GPT detectors, suggesting that GPT detectors may unintentionally penalize writers with constrained linguistic expressions. Our results call for a broader conversati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.00215</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#24402;&#32435;&#20851;&#31995;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inductive Relation Prediction from Relational Paths and Context with Hierarchical Transformers. (arXiv:2304.00215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;Transformer&#30340;&#26041;&#27861;&#65292;&#21363;REPORT&#65292;&#33021;&#22815;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#12290;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#23884;&#20837;&#24335;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#23548;&#35774;&#32622;&#65292;&#32570;&#20047;&#24402;&#32435;&#33021;&#21147;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#25512;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#20998;&#23618;Transformer&#26694;&#26550;&#65292;&#21363;REPORT&#65292;&#21516;&#26102;&#32858;&#21512;&#20851;&#31995;&#36335;&#24452;&#21644;&#19978;&#19979;&#25991;&#65292;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#20869;&#22312;&#29305;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#20851;&#31995;&#35821;&#20041;&#65292;&#24182;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#23436;&#20840;&#24402;&#32435;&#30340;&#35774;&#32622;&#20013;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;REPORT&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#20004;&#20010;&#23436;&#20840;&#24402;&#32435;&#30340;&#25968;&#25454;&#38598;&#30340;&#20843;&#20010;&#29256;&#26412;&#23376;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;REPORT&#33021;&#22815;&#23558;&#25512;&#29702;&#25512;&#24191;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#27809;&#26377;&#20844;&#20849;&#23454;&#20307;&#30340;&#26032;&#23454;&#20307;&#19978;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation prediction on knowledge graphs (KGs) is a key research topic. Dominant embedding-based methods mainly focus on the transductive setting and lack the inductive ability to generalize to new entities for inference. Existing methods for inductive reasoning mostly mine the connections between entities, i.e., relational paths, without considering the nature of head and tail entities contained in the relational context. This paper proposes a novel method that captures both connections between entities and the intrinsic nature of entities, by simultaneously aggregating RElational Paths and cOntext with a unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely on relation semantics and can naturally generalize to the fully-inductive setting, where KGs for training and inference have no common entities. In the experiments, REPORT performs consistently better than all baselines on almost all the eight version subsets of two fully-inductive datasets. Moreover. REPO
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.15916</link><description>&lt;p&gt;
&#20174;&#31169;&#26377;&#21040;&#20844;&#26377;&#65306;&#22312;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#19979;&#23545;GAN&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#23545;&#20004;&#31181;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20197;GSWGAN&#34920;&#29616;&#26368;&#20339;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#29983;&#25104;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#20844;&#20849;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#37117;&#24456;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#31169;&#20154;&#25968;&#25454;&#26102;&#65292;&#20960;&#20010;&#38480;&#21046;&#20351;&#24471;&#38590;&#20197;&#22312;&#36825;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#31169;&#23494;&#22320;&#29983;&#25104;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#22312;&#20998;&#31867;&#22120;&#20043;&#19978;&#30452;&#25509;&#24212;&#29992;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#20197;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#24335;&#20174;&#31169;&#26377;&#25968;&#25454;&#21019;&#24314;&#20844;&#20849;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38024;&#23545;&#31169;&#26377;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#24773;&#22659;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#38750;&#24120;&#31361;&#20986;&#30340;&#22522;&#20110;GAN&#30340;&#26550;&#26500;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#23616;&#38480;&#20110;&#22270;&#20687;&#39046;&#22495;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#30340;&#33539;&#22260;&#26159;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23588;&#20854;&#26159;GSWGAN&#22312;&#22810;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;DPWGAN&#12290;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;GSWGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#30340;&#24773;&#22659;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;TempT&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;&#20854;&#20165;&#21033;&#29992;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.10536</link><description>&lt;p&gt;
TempT&#65306;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TempT: Temporal consistency for Test-time adaptation. (arXiv:2303.10536v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;TempT&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;&#20854;&#20165;&#21033;&#29992;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#22312;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;TempT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#36830;&#32493;&#24103;&#20043;&#38388;&#30340;&#39044;&#27979;&#20855;&#26377;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#23454;&#29616;&#23545;&#35270;&#39057;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#12290;TempT&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#35270;&#39057;&#20013;&#30340;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#65288;FER&#65289;&#12290;&#25105;&#20204;&#23558;TempT&#22312;AffWild2&#25968;&#25454;&#38598;&#19978;&#20316;&#20026;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#27604;&#36187;&#65288;ABAW&#65289;&#31532;&#20116;&#23626;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#30340;&#34920;&#24773;&#20998;&#31867;&#25361;&#25112;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#30340;&#35270;&#35273;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#32780;&#19981;&#26159;&#36739;&#22823;&#30340;&#24207;&#21015;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TempT&#19982;&#24448;&#24180;&#25253;&#21578;&#30340;&#34920;&#29616;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20854;&#26377;&#25928;&#24615;&#20026;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#27010;&#24565;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical report, we introduce TempT, a novel method for test time adaptation on videos by ensuring temporal coherence of predictions across sequential frames. TempT is a powerful tool with broad applications in computer vision tasks, including facial expression recognition (FER) in videos. We evaluate TempT's performance on the AffWild2 dataset as part of the Expression Classification Challenge at the 5th Workshop and Competition on Affective Behavior Analysis in the wild (ABAW). Our approach focuses solely on the unimodal visual aspect of the data and utilizes a popular 2D CNN backbone, in contrast to larger sequential or attention based models. Our experimental results demonstrate that TempT has competitive performance in comparison to previous years reported performances, and its efficacy provides a compelling proof of concept for its use in various real world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2303.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#20869;&#37096;&#38543;&#26426;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20445;&#30041;&#21644;&#25805;&#20316;&#22270;&#20687;&#20869;&#23481;&#21644;&#36523;&#20221;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22914;&#8220;&#26790;&#22659;&#30456;&#26426;&#8221;&#21644;&#8220;&#25991;&#26412;&#21453;&#36716;&#8221;&#25552;&#20986;&#20102;&#20351;&#29992;&#27169;&#22411;&#25110;&#28508;&#22312;&#34920;&#31034;&#20010;&#24615;&#21270;&#26469;&#20445;&#25345;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#23545;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#21644;&#22797;&#26434;&#35757;&#32451;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#38750;&#24120;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#65288;HiPer&#65289;&#25991;&#26412;&#23884;&#20837;&#36890;&#36807;&#20998;&#35299;CLIP&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#20010;&#24615;&#21270;&#21644;&#20869;&#23481;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#22411;&#24494;&#35843;&#25110;&#35782;&#21035;&#31526;&#65292;&#20294;&#20173;&#21487;&#20197;&#20165;&#36890;&#36807;&#21333;&#20010;&#22270;&#20687;&#21644;&#30446;&#26631;&#25991;&#26412;&#26469;&#23454;&#29616;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#25991;&#26412;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#20135;&#29983;&#39640;&#24230;&#20010;&#24615;&#21270;&#21644;&#22797;&#26434;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02618</link><description>&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Ensemble Reinforcement Learning: A Survey. (arXiv:2303.02618v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02618
&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#26159;&#19968;&#31181;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30456;&#32467;&#21512;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#21508;&#31181;&#31185;&#23398;&#21644;&#24212;&#29992;&#38382;&#39064;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26576;&#20123;&#22797;&#26434;&#20219;&#21153;&#20173;&#38590;&#20197;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21644;&#31639;&#27861;&#35299;&#20915;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;RL&#21644;&#38598;&#25104;&#23398;&#20064;&#65288;EL&#65289;&#30340;&#20248;&#28857;&#65292;&#24050;&#32463;&#24191;&#27867;&#21463;&#21040;&#27426;&#36814;&#12290;ERL&#21033;&#29992;&#22810;&#20010;&#27169;&#22411;&#25110;&#22521;&#35757;&#31639;&#27861;&#20840;&#38754;&#25506;&#32034;&#38382;&#39064;&#31354;&#38388;&#65292;&#24182;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;ERL&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#20197;&#20415;&#20026;&#35835;&#32773;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;ERL&#30340;&#32972;&#26223;&#21644;&#21160;&#26426;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#25104;&#21151;&#24212;&#29992;&#20110;ERL&#20013;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#27169;&#22411;&#24179;&#22343;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#27169;&#22411;&#32452;&#21512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#24182;&#20998;&#26512;&#20102;&#25152;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a highly effective technique for addressing various scientific and applied problems. Despite its success, certain complex tasks remain challenging to be addressed solely with a single model and algorithm. In response, ensemble reinforcement learning (ERL), a promising approach that combines the benefits of both RL and ensemble learning (EL), has gained widespread popularity. ERL leverages multiple models or training algorithms to comprehensively explore the problem space and possesses strong generalization capabilities. In this study, we present a comprehensive survey on ERL to provide readers with an overview of recent advances and challenges in the field. First, we introduce the background and motivation for ERL. Second, we analyze in detail the strategies that have been successfully applied in ERL, including model averaging, model selection, and model combination. Subsequently, we summarize the datasets and analyze algorithms used in releva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax&#65292;&#36890;&#36807;&#20351;&#29992;&#21482;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#32452;&#25104;&#30340;In-Sample softmax&#26469;&#35299;&#20915;&#25805;&#20316;&#35206;&#30422;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#19988;In-Sample Actor-Critic&#19982;&#35813;&#26041;&#27861;&#30456;&#27604;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.14372</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax
&lt;/p&gt;
&lt;p&gt;
The In-Sample Softmax for Offline Reinforcement Learning. (arXiv:2302.14372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;In-Sample Softmax&#65292;&#36890;&#36807;&#20351;&#29992;&#21482;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#25805;&#20316;&#32452;&#25104;&#30340;In-Sample softmax&#26469;&#35299;&#20915;&#25805;&#20316;&#35206;&#30422;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#19988;In-Sample Actor-Critic&#19982;&#35813;&#26041;&#27861;&#30456;&#27604;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;RL&#20195;&#29702;&#21487;&#20197;&#21033;&#29992;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#25209;&#27425;&#26469;&#25552;&#21462;&#21512;&#29702;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#31163;&#32447;RL&#35774;&#32622;&#20013;&#65292;&#19968;&#20010;&#19981;&#26029;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#65292;&#35768;&#22810;&#26041;&#27861;&#19979;&#30340;bootstrapping&#26356;&#26032;&#21463;&#21040;&#34892;&#21160;&#35206;&#30422;&#19981;&#36275;&#30340;&#24433;&#21709;&#65306;&#26631;&#20934;max&#36816;&#31639;&#31526;&#21487;&#33021;&#20250;&#36873;&#25321;&#22312;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26368;&#22823;&#21160;&#20316;&#12290;&#20174;&#36825;&#20123;&#19981;&#20934;&#30830;&#30340;&#20540;&#36827;&#34892;bootstrapping&#26356;&#26032;&#20250;&#23548;&#33268;&#39640;&#20272;&#29978;&#33267;&#21457;&#25955;&#12290;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#23581;&#35797;&#36817;&#20284;&#19968;&#20010;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#28085;&#30422;&#33391;&#22909;&#30340;&#25805;&#20316;&#30340;in-sample max&#12290;&#26412;&#25991;&#24378;&#35843;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65306;&#20351;&#29992;&#20165;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;&#21160;&#20316;&#36817;&#20284;In-Sample softmax&#26356;&#21152;&#30452;&#35266;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;In-Sample softmax&#22522;&#30784;&#19978;&#30340;&#31574;&#30053;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#28201;&#24230;&#30340;&#19979;&#38477;&#65292;&#23427;&#20250;&#25509;&#36817;In-Sample max&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;In-Sample softmax&#25512;&#23548;&#20986;&#19968;&#20010;In-Sample Actor-Critic&#65288;AC&#65289;&#65292;&#24182;&#19988;&#35777;&#26126;&#20854;&#22312;&#31283;&#23450;&#24615;&#25110;&#24615;&#33021;&#19978;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an \emph{in-sample} max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample \emph{softmax} using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2301.05816</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#21160;&#24577;&#29702;&#35299;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Understanding the Spectral Bias of Coordinate Based MLPs Via Training Dynamics. (arXiv:2301.05816v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22352;&#26631;&#30340;MLPs&#30340;&#35889;&#20559;&#32622;&#23545;&#39640;&#39057;&#32452;&#20214;&#25910;&#25947;&#30340;&#38459;&#30861;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20559;&#32622;&#26159;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37325;&#35201;&#35266;&#23519;&#32467;&#26524;&#65292;&#23427;&#34920;&#31034;&#32593;&#32476;&#22312;&#25910;&#25947;&#21040;&#26356;&#39640;&#39057;&#29575;&#32452;&#20214;&#21069;&#65292;&#20250;&#23398;&#20064;&#30446;&#26631;&#20989;&#25968;&#30340;&#20302;&#39057;&#34920;&#31034;&#12290;&#36825;&#19968;&#23646;&#24615;&#19982;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#26377;&#20851;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#22330;&#26223;&#28210;&#26579;&#26102;&#65292;&#37319;&#29992;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;(MLPs)&#21033;&#29992;&#23494;&#38598;&#30340;&#20302;&#32500;&#22352;&#26631;&#36755;&#20837;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#35889;&#20559;&#24046;&#65292;&#23436;&#20840;&#38459;&#30861;&#20102;&#25910;&#25947;&#21040;&#39640;&#39057;&#32452;&#20214;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#21487;&#20197;&#20351;&#29992;&#39640;&#39057;&#27491;&#24358;&#27874;&#32534;&#30721;&#36755;&#20837;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#21644;&#20613;&#37324;&#21494;&#20998;&#26512;&#26469;&#35299;&#37322;&#22352;&#26631;&#31995;&#20013;&#30340;&#35889;&#20559;&#24046;&#21450;&#20854;&#20005;&#37325;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#22240;&#20026;NTK&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#27491;&#30340;&#32593;&#32476;&#21160;&#24577;&#65292;&#32780;&#20613;&#37324;&#21494;&#20998;&#26512;&#21482;&#33021;&#25552;&#20379;&#23545;&#39057;&#29575;&#32452;&#20214;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral bias is an important observation of neural network training, stating that the network will learn a low frequency representation of the target function before converging to higher frequency components. This property is interesting due to its link to good generalization in over-parameterized networks. However, in applications to scene rendering, where multi-layer perceptrons (MLPs) with ReLU activations utilize dense, low dimensional coordinate based inputs, a severe spectral bias occurs that obstructs convergence to high freqeuncy components entirely. In order to overcome this limitation, one can encode the inputs using high frequency sinusoids. Previous works attempted to explain both spectral bias and its severity in the coordinate based regime using Neural Tangent Kernel (NTK) and Fourier analysis. However, such methods come with various limitations, since NTK does not capture real network dynamics, and Fourier analysis only offers a global perspective on the frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23556;&#20987;&#65288;MLF&#65289;&#26041;&#27861;&#21644;&#23574;&#23792;&#27785;&#40664;&#31105;&#21046;&#27531;&#24046;&#32593;&#32476;&#65288;spiking DS-ResNet&#65289;&#65292;&#36890;&#36807; MLF &#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#22686;&#37327;&#34920;&#36798;&#33021;&#21147;&#65307;DS-ResNet &#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#31163;&#25955;&#23574;&#23792;&#30340;&#24658;&#31561;&#26144;&#23556;&#65292;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.06386</link><description>&lt;p&gt;
&#22810;&#23618;&#23556;&#20987;&#21644;&#27785;&#40664;&#31105;&#21046;&#27531;&#24046;&#32593;&#32476;&#65306;&#23454;&#29616;&#26356;&#22909;&#21644;&#26356;&#28145;&#30340;&#30452;&#25509;&#35757;&#32451;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks. (arXiv:2210.06386v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#23556;&#20987;&#65288;MLF&#65289;&#26041;&#27861;&#21644;&#23574;&#23792;&#27785;&#40664;&#31105;&#21046;&#27531;&#24046;&#32593;&#32476;&#65288;spiking DS-ResNet&#65289;&#65292;&#36890;&#36807; MLF &#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#22686;&#21152;&#31070;&#32463;&#20803;&#30340;&#22686;&#37327;&#34920;&#36798;&#33021;&#21147;&#65307;DS-ResNet &#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#31163;&#25955;&#23574;&#23792;&#30340;&#24658;&#31561;&#26144;&#23556;&#65292;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20855;&#26377;&#24322;&#27493;&#31163;&#25955;&#21644;&#31232;&#30095;&#29305;&#24615;&#65292;&#26159;&#19968;&#31181;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20204;&#22312;&#20302;&#33021;&#32791;&#26041;&#38754;&#36234;&#26469;&#36234;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#30452;&#25509;&#35757;&#32451;SNN&#12290;&#28982;&#32780;&#65292;&#23574;&#23792;&#27963;&#21160;&#30340;&#20108;&#36827;&#21046;&#21644;&#19981;&#21487;&#24494;&#30340;&#29305;&#24615;&#36843;&#20351;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#36973;&#21463;&#20005;&#37325;&#30340;&#26799;&#24230;&#28040;&#22833;&#21644;&#32593;&#32476;&#36864;&#21270;&#65292;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#30452;&#25509;&#35757;&#32451;&#30340;SNN&#30340;&#24615;&#33021;&#24182;&#38459;&#27490;&#23427;&#20204;&#21464;&#24471;&#26356;&#28145;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#26102;&#31354;&#21453;&#21521;&#20256;&#25773;&#65288;STBP&#65289;&#26041;&#27861;&#30340;&#22810;&#23618;&#23556;&#20987;&#65288;MLF&#65289;&#26041;&#27861;&#21644;&#23574;&#23792;&#27785;&#40664;&#31105;&#21046;&#27531;&#24046;&#32593;&#32476;&#65288;spiking DS-ResNet&#65289;&#12290;MLF&#20351;&#26799;&#24230;&#20256;&#25773;&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#22686;&#37327;&#34920;&#36798;&#31070;&#32463;&#20803;&#30340;&#33021;&#21147;&#12290;&#23574;&#23792;DS-ResNet&#21487;&#20197;&#26377;&#25928;&#22320;&#25191;&#34892;&#31163;&#25955;&#23574;&#23792;&#30340;&#24658;&#31561;&#26144;&#23556;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#21512;&#36866;&#30340;&#36830;&#25509;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) are bio-inspired neural networks with asynchronous discrete and sparse characteristics, which have increasingly manifested their superiority in low energy consumption. Recent research is devoted to utilizing spatio-temporal information to directly train SNNs by backpropagation. However, the binary and non-differentiable properties of spike activities force directly trained SNNs to suffer from serious gradient vanishing and network degradation, which greatly limits the performance of directly trained SNNs and prevents them from going deeper. In this paper, we propose a multi-level firing (MLF) method based on the existing spatio-temporal back propagation (STBP) method, and spiking dormant-suppressed residual network (spiking DS-ResNet). MLF enables more efficient gradient propagation and the incremental expression ability of the neurons. Spiking DS-ResNet can efficiently perform identity mapping of discrete spikes, as well as provide a more suitable connec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#35268;&#21010;&#31354;&#38388;&#26469;&#35774;&#35745;&#19968;&#20010;&#38024;&#23545;&#22810;&#26426;&#26800;&#25163;&#29289;&#20307;&#37325;&#25490;&#30340;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#36895;&#24230;&#21644;&#35268;&#21010;&#33539;&#22260;&#65292;&#21516;&#26102;&#20351;&#29992;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#25628;&#32034;&#26641;&#20013;&#35768;&#22810;&#26080;&#29992;&#30340;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2210.04333</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#19982;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-based Multi-Robot Task and Motion Planning. (arXiv:2210.04333v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;&#35268;&#21010;&#31354;&#38388;&#26469;&#35774;&#35745;&#19968;&#20010;&#38024;&#23545;&#22810;&#26426;&#26800;&#25163;&#29289;&#20307;&#37325;&#25490;&#30340;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#36895;&#24230;&#21644;&#35268;&#21010;&#33539;&#22260;&#65292;&#21516;&#26102;&#20351;&#29992;&#26032;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#25628;&#32034;&#26641;&#20013;&#35768;&#22810;&#26080;&#29992;&#30340;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#26426;&#26800;&#25163;&#30340;&#25490;&#21015;&#26469;&#24688;&#24403;&#22320;&#23433;&#25490;&#29289;&#20307;&#12290;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#26102;&#38388;&#19978;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#22320;&#35268;&#21010;&#20102;&#22810;&#36798;20&#20010;&#29289;&#20307;&#30340;&#38382;&#39064;&#65292;&#27604;&#21487;&#27604;&#26041;&#27861;&#22810;&#19977;&#20493;&#20197;&#19978;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35268;&#21010;&#31354;&#38388;&#20998;&#35299;&#20026;&#20165;&#32771;&#34385;&#26426;&#26800;&#25163;&#12289;&#29289;&#20307;&#21644;&#25658;&#24102;&#29289;&#20307;&#30340;&#26426;&#26800;&#25163;&#20043;&#38388;&#19977;&#20010;&#37096;&#20998;&#26469;&#23454;&#29616;&#36825;&#19968;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#36229;&#22270;&#34920;&#31034;&#36825;&#31181;&#20998;&#35299;&#65292;&#20854;&#20013;&#39030;&#28857;&#26159;&#35268;&#21010;&#31354;&#38388;&#30340;&#20998;&#35299;&#20803;&#32032;&#65292;&#36229;&#24359;&#26159;&#20803;&#32032;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#20854;&#20013;&#39030;&#28857;&#26159;&#23436;&#25972;&#30340;&#22797;&#21512;&#31354;&#38388;&#65292;&#36793;&#32536;&#26159;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#20351;&#29992;&#36229;&#22270;&#21487;&#20197;&#20943;&#23567;&#35268;&#21010;&#31354;&#38388;&#30340;&#34920;&#31034;&#22823;&#23567;&#65292;&#23545;&#20110;&#22810;&#26426;&#26800;&#25163;&#29289;&#20307;&#37325;&#25490;&#65292;&#36229;&#22270;&#39030;&#28857;&#30340;&#25968;&#37327;&#38543;&#26426;&#22120;&#20154;&#25110;&#29289;&#20307;&#25968;&#37327;&#32447;&#24615;&#22686;&#21152;&#65292;&#32780;&#36229;&#36793;&#30340;&#25968;&#37327;&#38543;&#26426;&#26800;&#25163;&#25968;&#37327;&#32447;&#24615;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21033;&#29992;&#20102;&#28304;&#33258;&#20110;&#23545;&#25152;&#38656;&#37325;&#26032;&#25235;&#21462;&#27425;&#25968;&#30340;&#20005;&#35880;&#29702;&#35770;&#19978;&#30028;&#30340;&#26032;&#39062;&#21551;&#21457;&#24335;&#12290;&#36825;&#20010;&#21098;&#26525;&#27493;&#39588;&#28040;&#38500;&#20102;&#25628;&#32034;&#26641;&#20013;&#35768;&#22810;&#26080;&#29992;&#30340;&#20998;&#25903;&#65292;&#21152;&#24555;&#20102;&#35268;&#21010;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25342;&#21462;&#21644;&#25918;&#32622;&#12289;&#37325;&#25490;&#21644;&#26500;&#36896;&#20219;&#21153;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#35268;&#21010;&#26102;&#38388;&#30340;&#21152;&#24555;&#21644;&#22788;&#29702;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22823;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multi-robot task and motion planning method that, when applied to the rearrangement of objects by manipulators, results in solution times up to three orders of magnitude faster than existing methods and successfully plans for problems with up to twenty objects, more than three times as many objects as comparable methods. We achieve this improvement by decomposing the planning space to consider manipulators alone, objects, and manipulators holding objects. We represent this decomposition with a hypergraph where vertices are decomposed elements of the planning spaces and hyperarcs are transitions between elements. Existing methods use graph-based representations where vertices are full composite spaces and edges are transitions between these. Using the hypergraph reduces the representation size of the planning space-for multi-manipulator object rearrangement, the number of hypergraph vertices scales linearly with the number of either robots or objects, while the number of hy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02897</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#23454;&#38469;&#34013;&#29273;&#23556;&#39057;&#25351;&#32441;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Embedding-Assisted Attentional Deep Learning for Real-World RF Fingerprinting of Bluetooth. (arXiv:2210.02897v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65288;Mbed-ATN&#65289;&#65292;&#36866;&#29992;&#20110;&#33719;&#21462;&#23454;&#38469;&#34013;&#29273;&#35774;&#22791;&#30340;&#25351;&#32441;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#28436;&#31034;&#20102;&#26679;&#26412;&#38271;&#24230;&#21644;&#25239;&#28151;&#21472;&#25277;&#21462;&#30340;&#25928;&#26524;&#12290;&#23884;&#20837;&#24335;&#27169;&#22359;&#20316;&#20026;&#38477;&#32500;&#21333;&#20803;&#65292;&#23558;&#39640;&#32500;&#30340;&#19977;&#32500;&#36755;&#20837;&#24352;&#37327;&#26144;&#23556;&#20026;&#19968;&#32500;&#29305;&#24449;&#21521;&#37327;&#65292;&#20379;ATN&#27169;&#22359;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#23494;&#20999;&#35780;&#20272;&#65292;&#24182;&#22312;&#35757;&#32451;&#21518;&#20351;&#29992;&#21478;&#19968;&#20010;&#22312;&#19981;&#21516;&#26102;&#27573;&#21644;&#23454;&#39564;&#35774;&#32622;&#19979;&#25910;&#38598;&#30340;&#30495;&#23454;&#34013;&#29273;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#20854;&#25351;&#32441;&#33021;&#21147;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20808;&#21069;&#30740;&#31350;&#25152;&#27809;&#26377;&#20570;&#36807;&#30340;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26679;&#26412;&#38271;&#24230;&#20026;100 kS&#26102;&#65292;&#19982;&#22522;&#20934;&#8212;&#8212;GRU&#21644;Oracle&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20869;&#23384;&#20351;&#29992;&#37327;&#20998;&#21035;&#38477;&#20302;&#20102;9.17&#20493;&#21644;65.2&#20493;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;Mbed-ATN&#22312;&#23460;&#20869;&#21644;&#23460;&#22806;&#29615;&#22659;&#20013;&#22343;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;99.4&#65285;&#21644;97.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scalable and computationally efficient framework is designed to fingerprint real-world Bluetooth devices. We propose an embedding-assisted attentional framework (Mbed-ATN) suitable for fingerprinting actual Bluetooth devices. Its generalization capability is analyzed in different settings and the effect of sample length and anti-aliasing decimation is demonstrated. The embedding module serves as a dimensionality reduction unit that maps the high dimensional 3D input tensor to a 1D feature vector for further processing by the ATN module. Furthermore, unlike the prior research in this field, we closely evaluate the complexity of the model and test its fingerprinting capability with real-world Bluetooth dataset collected under a different time frame and experimental setting while being trained on another. Our study reveals a 9.17x and 65.2x lesser memory usage at a sample length of 100 kS when compared to the benchmark - GRU and Oracle models respectively. Further, the proposed Mbed-ATN
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#21487;&#20197;&#35753;&#27169;&#22411;&#26356;&#21152;&#23433;&#20840;&#12289;&#21487;&#38752;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2208.14488</link><description>&lt;p&gt;
&#38480;&#21046;&#32593;&#32476;&#34920;&#31034;&#65292;&#35753;&#27169;&#22411;&#30693;&#36947;&#33258;&#24049;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;
Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14488
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#21487;&#20197;&#35753;&#27169;&#22411;&#26356;&#21152;&#23433;&#20840;&#12289;&#21487;&#38752;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#24050;&#30693;&#22833;&#36133;&#27169;&#24335;&#26159;&#23427;&#20204;&#21487;&#33021;&#33258;&#20449;&#22320;&#36820;&#22238;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#19981;&#23433;&#20840;&#30340;&#34892;&#20026;&#22312;&#20351;&#29992;&#26696;&#20363;&#30053;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#29305;&#21035;&#39057;&#32321;&#65292;&#25110;&#32773;&#22312;&#38754;&#23545;&#25932;&#25163;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20197;&#19968;&#31181;&#24191;&#27867;&#19988;&#19968;&#33324;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#23545;&#27169;&#22411;&#20869;&#37096;&#30340;&#28608;&#27963;&#27169;&#24335;&#26045;&#21152;&#31867;&#24863;&#30693;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#31867;&#20998;&#37197;&#19968;&#20010;&#29420;&#29305;&#30340;&#12289;&#22266;&#23450;&#30340;&#12289;&#38543;&#26426;&#29983;&#25104;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#65288;&#21518;&#25991;&#31216;&#20026;&#31867;&#32534;&#30721;&#65289;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36890;&#36807;&#20132;&#21449;&#28145;&#24230;&#30340;&#28608;&#27963;&#27169;&#24335;&#26681;&#25454;&#36755;&#20837;&#26679;&#26412;&#30340;&#31867;&#21035;&#39044;&#27979;&#30456;&#24212;&#30340;&#31867;&#32534;&#30721;&#12290;&#32467;&#26524;&#39044;&#27979;&#22120;&#34987;&#31216;&#20026;&#24635;&#28608;&#27963;&#20998;&#31867;&#22120;&#65288;TAC&#65289;&#65292;TAC&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20063;&#21487;&#20197;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#39030;&#37096;&#29992;&#26497;&#23567;&#30340;&#20195;&#20215;&#20316;&#20026;&#34180;&#23618;&#38468;&#21152;&#20351;&#29992;&#12290;TAC&#30340;&#28608;&#27963;&#27169;&#24335;&#19982;&#26368;&#25509;&#36817;&#30340;&#26377;&#25928;&#32534;&#30721;&#20043;&#38388;&#30340;&#36317;&#31163;&#20316;&#20026;&#39069;&#22806;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence scor
&lt;/p&gt;</description></item><item><title>EC-KitY&#26159;&#19968;&#27454;&#22522;&#20110;Python&#30340;&#20840;&#38754;&#36827;&#21270;&#35745;&#31639;&#24037;&#20855;&#21253;&#65292;&#37319;&#29992;BSD 3-Clause&#35768;&#21487;&#35777;&#65292;&#20860;&#23481;scikit-learn&#65292;&#25903;&#25345;&#22810;&#31181;&#27969;&#34892;&#30340;EC&#33539;&#20363;&#65292;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#31616;&#20415;&#30340;EC&#23454;&#39564;&#35774;&#32622;&#21644;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10367</link><description>&lt;p&gt;
EC-KitY&#65306;&#26080;&#32541;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#30340;Python&#36827;&#21270;&#35745;&#31639;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless Machine Learning Integration. (arXiv:2207.10367v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10367
&lt;/p&gt;
&lt;p&gt;
EC-KitY&#26159;&#19968;&#27454;&#22522;&#20110;Python&#30340;&#20840;&#38754;&#36827;&#21270;&#35745;&#31639;&#24037;&#20855;&#21253;&#65292;&#37319;&#29992;BSD 3-Clause&#35768;&#21487;&#35777;&#65292;&#20860;&#23481;scikit-learn&#65292;&#25903;&#25345;&#22810;&#31181;&#27969;&#34892;&#30340;EC&#33539;&#20363;&#65292;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#31616;&#20415;&#30340;EC&#23454;&#39564;&#35774;&#32622;&#21644;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
EC-KitY&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#36827;&#34892;&#36827;&#21270;&#35745;&#31639;(EC)&#65292;&#37319;&#29992;BSD 3-Clause&#35768;&#21487;&#35777;&#65292;&#24182;&#20860;&#23481;scikit-learn&#12290;&#35774;&#35745;&#26102;&#32771;&#34385;&#20102;&#29616;&#20195;&#36719;&#20214;&#24037;&#31243;&#21644;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#65292;EC-KitY&#21487;&#20197;&#25903;&#25345;&#25152;&#26377;&#27969;&#34892;&#30340;EC&#33539;&#20363;&#65292;&#21253;&#25324;&#36951;&#20256;&#31639;&#27861;&#65292;&#36951;&#20256;&#32534;&#31243;&#65292;&#21327;&#21516;&#36827;&#21270;&#65292;&#36827;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#36719;&#20214;&#21253;&#30340;&#27010;&#20917;&#65292;&#21253;&#25324;&#35774;&#32622;EC&#23454;&#39564;&#30340;&#31616;&#20415;&#24615;&#12289;&#26550;&#26500;&#12289;&#20027;&#35201;&#29305;&#28857;&#20197;&#21450;&#19982;&#20854;&#20182;&#24211;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
EC-KitY is a comprehensive Python library for doing evolutionary computation (EC), licensed under the BSD 3-Clause License, and compatible with scikit-learn. Designed with modern software engineering and machine learning integration in mind, EC-KitY can support all popular EC paradigms, including genetic algorithms, genetic programming, coevolution, evolutionary multi-objective optimization, and more. This paper provides an overview of the package, including the ease of setting up an EC experiment, the architecture, the main features, and a comparison with other libraries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#21644;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#12290;DAGNs&#21487;&#20197;&#26500;&#24314;&#36923;&#36753;&#22270;&#24182;&#36890;&#36807;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.01450</link><description>&lt;p&gt;
&#38754;&#21521;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#30340;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Discourse-Aware Graph Networks for Textual Logical Reasoning. (arXiv:2207.01450v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#21644;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#12290;DAGNs&#21487;&#20197;&#26500;&#24314;&#36923;&#36753;&#22270;&#24182;&#36890;&#36807;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#21040;&#36923;&#36753;&#25512;&#29702;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#38656;&#35201;&#24847;&#35782;&#21040;&#29305;&#23450;&#30340;&#36923;&#36753;&#32467;&#26500;&#12290;&#27573;&#33853;&#32423;&#36923;&#36753;&#20851;&#31995;&#20195;&#34920;&#20102;&#21629;&#39064;&#21333;&#20803;&#20043;&#38388;&#30340;&#34164;&#28085;&#25110;&#30683;&#30462;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#32467;&#35770;&#21477;&#65289;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#30340;&#38382;&#31572;&#31995;&#32479;&#20391;&#37325;&#20110;&#22522;&#20110;&#23454;&#20307;&#30340;&#20851;&#31995;&#65292;&#36825;&#31181;&#32467;&#26500;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36923;&#36753;&#32467;&#26500;&#32422;&#26463;&#24314;&#27169;&#26469;&#35299;&#20915;&#36923;&#36753;&#25512;&#29702;&#38382;&#31572;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#35805;&#35821;&#24863;&#30693;&#22270;&#32593;&#32476;&#65288;DAGNs&#65289;&#12290;&#35813;&#32593;&#32476;&#39318;&#20808;&#21033;&#29992;&#34892;&#38388;&#35805;&#35821;&#36830;&#25509;&#35789;&#21644;&#36890;&#29992;&#36923;&#36753;&#29702;&#35770;&#26500;&#24314;&#36923;&#36753;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#36793;&#32536;&#25512;&#29702;&#26426;&#21046;&#21644;&#22270;&#29305;&#24449;&#26356;&#26032;&#26469;&#23398;&#20064;&#36923;&#36753;&#34920;&#31034;&#12290;&#36825;&#20010;&#27969;&#31243;&#24212;&#29992;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#65292;&#20854;&#22522;&#26412;&#29305;&#24449;&#19982;&#39640;&#23618;&#36923;&#36753;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#31572;&#26696;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#25991;&#26412;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual logical reasoning, especially question-answering (QA) tasks with logical reasoning, requires awareness of particular logical structures. The passage-level logical relations represent entailment or contradiction between propositional units (e.g., a concluding sentence). However, such structures are unexplored as current QA systems focus on entity-based relations. In this work, we propose logic structural-constraint modeling to solve the logical reasoning QA and introduce discourse-aware graph networks (DAGNs). The networks first construct logic graphs leveraging in-line discourse connectives and generic logic theories, then learn logic representations by end-to-end evolving the logic relations with an edge-reasoning mechanism and updating the graph features. This pipeline is applied to a general encoder, whose fundamental features are joined with the high-level logic features for answer prediction. Experiments on three textual logical reasoning datasets demonstrate the reasonabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BATFormer&#30340;&#26041;&#27861;&#65292;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#23454;&#29616;&#36328;&#23610;&#24230;&#20840;&#23616;&#20132;&#20114;&#65292;&#21363;&#24314;&#31435;&#38271;&#36317;&#31163;&#20381;&#36182;&#12290;&#21516;&#26102;&#65292;&#25351;&#23548;&#19979;&#28789;&#27963;&#22320;&#29983;&#25104;&#31383;&#21475;&#65292;&#22686;&#24378;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24418;&#29366;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.14409</link><description>&lt;p&gt;
BATFormer: &#29992;&#20110;&#39640;&#25928;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36793;&#30028;&#24863;&#30693;&#36731;&#37327;&#32423;Transformer
&lt;/p&gt;
&lt;p&gt;
BATFormer: Towards Boundary-Aware Lightweight Transformer for Efficient Medical Image Segmentation. (arXiv:2206.14409v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BATFormer&#30340;&#26041;&#27861;&#65292;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#23454;&#29616;&#36328;&#23610;&#24230;&#20840;&#23616;&#20132;&#20114;&#65292;&#21363;&#24314;&#31435;&#38271;&#36317;&#31163;&#20381;&#36182;&#12290;&#21516;&#26102;&#65292;&#25351;&#23548;&#19979;&#28789;&#27963;&#22320;&#29983;&#25104;&#31383;&#21475;&#65292;&#22686;&#24378;&#20102;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24418;&#29366;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;Transformer&#36817;&#26469;&#22240;&#20854;&#24357;&#34917;&#20102;CNN&#19981;&#36275;&#30340;&#24863;&#21463;&#37326;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20294;&#20840;&#23616;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20197;&#21450;&#21018;&#24615;&#30340;&#31383;&#21475;&#20998;&#21106;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;Transformer&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36793;&#30028;&#24863;&#30693;&#36731;&#37327;&#32423;Transformer&#65288;BATFormer&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24314;&#31435;&#36328;&#23610;&#24230;&#20840;&#23616;&#20132;&#20114;&#65292;&#24182;&#22312;&#29109;&#30340;&#25351;&#23548;&#19979;&#28789;&#27963;&#22320;&#29983;&#25104;&#31383;&#21475;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;Transformer&#22312;&#24314;&#31435;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#23610;&#24230;&#20840;&#23616;Transformer&#65288;CGT&#65289;&#27169;&#22359;&#65292;&#20197;&#32852;&#21512;&#21033;&#29992;&#22810;&#20010;&#23567;&#23610;&#24230;&#29305;&#24449;&#22270;&#65292;&#29983;&#25104;&#26356;&#20016;&#23500;&#30340;&#20840;&#23616;&#29305;&#24449;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#12290;&#37492;&#20110;&#24418;&#29366;&#24314;&#27169;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#36793;&#30028;&#24863;&#30693;&#26412;&#22320;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Transformers, born to remedy the inadequate receptive fields of CNNs, have drawn explosive attention recently. However, the daunting computational complexity of global representation learning, together with rigid window partitioning, hinders their deployment in medical image segmentation. This work aims to address the above two issues in transformers for better medical image segmentation. Methods: We propose a boundary-aware lightweight transformer (BATFormer) that can build cross-scale global interaction with lower computational complexity and generate windows flexibly under the guidance of entropy. Specifically, to fully explore the benefits of transformers in long-range dependency establishment, a cross-scale global transformer (CGT) module is introduced to jointly utilize multiple small-scale feature maps for richer global features with lower computational complexity. Given the importance of shape modeling in medical image segmentation, a boundary-aware local transformer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;POMG&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#32452;&#32455;&#36861;&#25429;&#65288;SOP&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#27169;&#31946;&#33258;&#32452;&#32455;&#21327;&#20316;&#20849;&#21516;&#28436;&#21270;&#65288;FSC2&#65289;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;SOP&#20013;&#30340;&#19977;&#22823;&#25361;&#25112;&#65306;&#20998;&#24067;&#24335;&#33258;&#32452;&#32455;&#25628;&#32034;&#65288;SOS&#65289;&#12289;&#20998;&#24067;&#24335;&#20219;&#21153;&#20998;&#37197;&#21644;&#20998;&#24067;&#24335;&#21333;&#30446;&#26631;&#36861;&#36394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FSC2&#31639;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;SOP&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.12330</link><description>&lt;p&gt;
&#38754;&#21521;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#22810;&#30446;&#26631;&#33258;&#20027;&#32452;&#32455;&#36861;&#25429;
&lt;/p&gt;
&lt;p&gt;
Toward multi-target self-organizing pursuit in a partially observable Markov game. (arXiv:2206.12330v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;POMG&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#32452;&#32455;&#36861;&#25429;&#65288;SOP&#65289;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#27169;&#31946;&#33258;&#32452;&#32455;&#21327;&#20316;&#20849;&#21516;&#28436;&#21270;&#65288;FSC2&#65289;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;SOP&#20013;&#30340;&#19977;&#22823;&#25361;&#25112;&#65306;&#20998;&#24067;&#24335;&#33258;&#32452;&#32455;&#25628;&#32034;&#65288;SOS&#65289;&#12289;&#20998;&#24067;&#24335;&#20219;&#21153;&#20998;&#37197;&#21644;&#20998;&#24067;&#24335;&#21333;&#30446;&#26631;&#36861;&#36394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FSC2&#31639;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;SOP&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#33258;&#20027;&#32452;&#32455;&#36861;&#25429;&#65288;SOP&#65289;&#38382;&#39064;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#20998;&#24067;&#24335;&#31995;&#32479;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#32452;&#32455;&#28216;&#25103;&#65292;&#22312;&#35813;&#28216;&#25103;&#20013;&#65292;&#26234;&#33021;&#20195;&#29702;&#21327;&#20316;&#22320;&#36861;&#36394;&#22810;&#20010;&#21160;&#24577;&#30446;&#26631;&#65292;&#20854;&#20013;&#21448;&#23384;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#25955;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;&#25628;&#32034;&#21644;&#36861;&#36880;&#20013;&#30340;&#38544;&#24335;&#21327;&#35843;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#33258;&#32452;&#32455;&#31995;&#32479;&#24314;&#27169;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;POMG&#65289;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#22823;&#35268;&#27169;&#24615;&#12289;&#21435;&#20013;&#24515;&#21270;&#12289;&#37096;&#20998;&#21487;&#35266;&#23519;&#21644;&#38750;&#36890;&#20449;&#31561;&#29305;&#28857;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;&#27169;&#31946;&#33258;&#32452;&#32455;&#21327;&#20316;&#20849;&#21516;&#28436;&#21270;&#65288;FSC2&#65289;&#20998;&#24067;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;SOP&#20013;&#30340;&#19977;&#22823;&#25361;&#25112;&#65306;&#20998;&#24067;&#24335;&#33258;&#32452;&#32455;&#25628;&#32034;&#65288;SOS&#65289;&#12289;&#20998;&#24067;&#24335;&#20219;&#21153;&#20998;&#37197;&#21644;&#20998;&#24067;&#24335;&#21333;&#30446;&#26631;&#36861;&#36394;&#12290;FSC2&#21253;&#25324;&#19968;&#31181;&#21327;&#35843;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20351;&#21516;&#36136;&#26234;&#33021;&#20307;&#23398;&#20064;&#33258;&#28982;&#30340;SOS&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#25512;&#29702;&#30340;&#20998;&#25955;&#24335;&#20219;&#21153;&#20998;&#37197;&#26426;&#21046;&#65292;&#20197;&#23558;&#25628;&#32034;&#20219;&#21153;&#20998;&#37197;&#32473;&#20195;&#29702;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#21333;&#30446;&#26631;&#36861;&#36394;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;SOS&#21644;&#20219;&#21153;&#20998;&#37197;&#30340;&#32467;&#26524;&#65292;&#20351;&#24471;&#36861;&#36394;&#30446;&#26631;&#26356;&#21152;&#26377;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;FSC2&#31639;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;SOP&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multiple-target self-organizing pursuit (SOP) problem has wide applications and has been considered a challenging self-organization game for distributed systems, in which intelligent agents cooperatively pursue multiple dynamic targets with partial observations. This work proposes a framework for decentralized multi-agent systems to improve the implicit coordination capabilities in search and pursuit. We model a self-organizing system as a partially observable Markov game (POMG) featured by large-scale, decentralization, partial observation, and noncommunication. The proposed distributed algorithm: fuzzy self-organizing cooperative coevolution (FSC2) is then leveraged to resolve the three challenges in multi-target SOP: distributed self-organizing search (SOS), distributed task allocation, and distributed single-target pursuit. FSC2 includes a coordinated multi-agent deep reinforcement learning (MARL) method that enables homogeneous agents to learn natural SOS patterns. Additionall
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiLo&#27880;&#24847;&#21147;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#22320;&#36816;&#34892;&#35270;&#35273;Transformer&#65292;&#24182;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.13213</link><description>&lt;p&gt;
&#39640;&#20302;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Fast Vision Transformers with HiLo Attention. (arXiv:2205.13213v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13213
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiLo&#27880;&#24847;&#21147;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#21487;&#20197;&#26356;&#21152;&#39640;&#25928;&#22320;&#36816;&#34892;&#35270;&#35273;Transformer&#65292;&#24182;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer (ViT) &#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#23427;&#20204;&#30340;&#39640;&#25928;&#35774;&#35745;&#22823;&#22810;&#30001;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#38388;&#25509;&#25351;&#26631;&#65288;&#21363; FLOP&#65289;&#25351;&#23548;&#65292;&#32780;&#19982;&#30452;&#25509;&#25351;&#26631;&#65288;&#22914;&#21534;&#21520;&#37327;&#65289;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30446;&#26631;&#24179;&#21488;&#19978;&#30340;&#30452;&#25509;&#36895;&#24230;&#35780;&#20272;&#20316;&#20026;&#39640;&#25928;ViTs&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LITv2&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;ViT&#65292;&#23427;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#33539;&#22260;&#20869;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#65292;&#36895;&#24230;&#26356;&#24555;&#12290;LITv2&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#39640;&#20302;&#27880;&#24847;&#21147;&#8221;&#12290;&#39640;&#20302;&#27880;&#24847;&#21147;&#30340;&#28789;&#24863;&#26469;&#33258;&#20110;&#22270;&#20687;&#20013;&#30340;&#39640;&#39057;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20302;&#39057;&#19987;&#27880;&#20110;&#20840;&#23616;&#32467;&#26500;&#65292;&#32780;&#22810;&#22836;&#33258;&#27880;&#24847;&#23618;&#24573;&#30053;&#20102;&#19981;&#21516;&#39057;&#29575;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#22312;&#27880;&#24847;&#21147;&#36716;&#25442;&#22120;&#20013;&#20998;&#35299;&#39640;/&#20302;&#39057;&#27169;&#24335;&#65292;&#20854;&#20013;&#33258;&#27880;&#24847;&#23618;&#20998;&#20026;&#20004;&#20010;&#20998;&#25903;&#65292;&#27599;&#20010;&#20998;&#25903;&#19987;&#38376;&#25429;&#25417;&#23616;&#37096;&#25110;&#20840;&#23616;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have triggered the most recent and significant breakthroughs in computer vision. Their efficient designs are mostly guided by the indirect metric of computational complexity, i.e., FLOPs, which however has a clear gap with the direct metric such as throughput. Thus, we propose to use the direct speed evaluation on the target platform as the design principle for efficient ViTs. Particularly, we introduce LITv2, a simple and effective ViT which performs favourably against the existing state-of-the-art methods across a spectrum of different model sizes with faster speed. At the core of LITv2 is a novel self-attention mechanism, which we dub HiLo. HiLo is inspired by the insight that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies. Therefore, we propose to disentangle the high/low frequency patterns in an attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#20102;&#35821;&#38899;&#35782;&#21035;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35745;&#31639;&#12289;&#20248;&#21270;&#30340;&#35266;&#23519;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#26426;&#22120;&#22312;&#24102;&#26377;&#22122;&#22768;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#30340;&#26102;&#38388;&#33539;&#22260;&#19978;&#34920;&#29616;&#20986;&#26469;&#30340;&#40065;&#26834;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2204.03740</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#20851;&#38190;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Successes and critical failures of neural networks in capturing human-like speech recognition. (arXiv:2204.03740v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#20102;&#35821;&#38899;&#35782;&#21035;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35745;&#31639;&#12289;&#20248;&#21270;&#30340;&#35266;&#23519;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#26426;&#22120;&#22312;&#24102;&#26377;&#22122;&#22768;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#30340;&#26102;&#38388;&#33539;&#22260;&#19978;&#34920;&#29616;&#20986;&#26469;&#30340;&#40065;&#26834;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#19982;&#20154;&#24037;&#21548;&#35273;&#21407;&#21017;&#19978;&#21487;&#20197;&#33719;&#24471;&#19981;&#21516;&#30340;&#32473;&#23450;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#30340;&#32422;&#26463;&#21487;&#20197;&#20419;&#20351;&#21548;&#35273;&#35748;&#30693;&#31185;&#23398;&#21644;&#24037;&#31243;&#23450;&#24615;&#25910;&#25947;&#65292;&#34920;&#26126;&#26356;&#32039;&#23494;&#30340;&#30456;&#20114;&#23457;&#26597;&#26377;&#21487;&#33021;&#20016;&#23500;&#20154;&#24037;&#21548;&#35273;&#31995;&#32479;&#21644;&#22823;&#33041;&#30340;&#22788;&#29702;&#27169;&#22411;&#12290;&#35821;&#38899;&#35782;&#21035;&#26159;&#19968;&#20010;&#20540;&#24471;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#23427;&#22312;&#20154;&#31867;&#20013;&#22825;&#29983;&#23545;&#20110;&#21508;&#31181;&#20809;&#35889;&#26102;&#38388;&#23610;&#24230;&#30340;&#36716;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#39640;&#24615;&#33021;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#26159;&#21542;&#23436;&#20840;&#32771;&#34385;&#20102;&#36825;&#20123;&#40065;&#26834;&#24615;&#29305;&#24449;&#20173;&#26377;&#24453;&#30830;&#23450;&#12290;&#25105;&#20204;&#23558;&#35821;&#38899;&#35782;&#21035;&#23454;&#39564;&#38598;&#25104;&#21040;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#20013;&#65292;&#20197;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21487;&#35745;&#31639;&#12289;&#20248;&#21270;&#30340;&#35266;&#23519;&#22120;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#65288;1&#65289;&#28548;&#28165;&#20102;&#25991;&#29486;&#20013;&#26377;&#24433;&#21709;&#21147;&#30340;&#35821;&#38899;&#22788;&#29702;&#19982;&#33258;&#28982;&#35821;&#38899;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#65288;2&#65289;&#23637;&#31034;&#20102;&#26426;&#22120;&#34920;&#29616;&#20986;&#24102;&#26377;&#22122;&#22768;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#30340;&#26102;&#38388;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural and artificial audition can in principle acquire different solutions to a given problem. The constraints of the task, however, can nudge the cognitive science and engineering of audition to qualitatively converge, suggesting that a closer mutual examination would potentially enrich artificial hearing systems and process models of the mind and brain. Speech recognition an area ripe for such exploration - is inherently robust in humans to a number transformations at various spectrotemporal granularities. To what extent are these robustness profiles accounted for by high-performing neural network systems? We bring together experiments in speech recognition under a single synthesis framework to evaluate state-of-the-art neural networks as stimulus-computable, optimized observers. In a series of experiments, we (1) clarify how influential speech manipulations in the literature relate to each other and to natural speech, (2) show the granularities at which machines exhibit out-of-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19982;&#26368;&#26032;&#31639;&#27861;&#31361;&#30772;&#65292;&#31163;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2203.01387</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#22238;&#39038;&#21644;&#26410;&#35299;&#20915;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#19982;&#26368;&#26032;&#31639;&#27861;&#31361;&#30772;&#65292;&#31163;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#20197;&#24448;&#26080;&#27861;&#22788;&#29702;&#30340;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22914;&#20174;&#20687;&#32032;&#35266;&#23519;&#20013;&#29609;&#22797;&#26434;&#28216;&#25103;&#12289;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#20197;&#21450;&#25511;&#21046;&#26426;&#22120;&#20154;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#35768;&#22810;&#39046;&#22495;&#30001;&#20110;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#39640;&#25104;&#26412;&#21644;&#21361;&#38505;&#32780;&#26080;&#27861;&#29992;RL&#35299;&#20915;&#12290;&#31163;&#32447;RL&#26159;&#19968;&#31181;&#33539;&#24335;&#65292;&#23427;&#20165;&#20174;&#20197;&#21069;&#25910;&#38598;&#30340;&#20132;&#20114;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#22240;&#27492;&#21487;&#20197;&#20174;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#22521;&#35757;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#31574;&#30053;&#12290;&#26377;&#25928;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#27604;&#22312;&#32447;RL&#31639;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#31163;&#32447;RL&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#39046;&#22495;&#26368;&#26032;&#30340;&#31639;&#27861;&#31361;&#30772;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; CADRE&#65292;&#36890;&#36807; CoPM &#31163;&#32447;&#35757;&#32451;&#65292;&#37319;&#29992;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#20174;&#39044;&#25910;&#38598;&#30340;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#35270;&#35273;&#21644;&#25511;&#21046;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#37319;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#39640;&#25928;&#20998;&#24067;&#24335; PPO &#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#39550;&#39542;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2202.08557</link><description>&lt;p&gt;
CADRE:&#22522;&#20110;&#32423;&#32852;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#22478;&#24066;&#39550;&#39542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-based Autonomous Urban Driving. (arXiv:2202.08557v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; CADRE&#65292;&#36890;&#36807; CoPM &#31163;&#32447;&#35757;&#32451;&#65292;&#37319;&#29992;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#20174;&#39044;&#25910;&#38598;&#30340;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#35270;&#35273;&#21644;&#25511;&#21046;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#37319;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22870;&#21169;&#20989;&#25968;&#25351;&#23548;&#19979;&#65292;&#36890;&#36807;&#39640;&#25928;&#20998;&#24067;&#24335; PPO &#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#39550;&#39542;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#29615;&#22659;&#21644;&#39550;&#39542;&#34892;&#20026;&#21160;&#24577;&#30340;&#25361;&#25112;&#19979;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#22478;&#24066;&#39550;&#39542;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#24037;&#35268;&#21017;&#65292;&#35201;&#20040;&#23398;&#20064;&#26469;&#33258;&#20110;&#26377;&#38480;&#20154;&#31867;&#32463;&#39564;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25512;&#24191;&#21040;&#32597;&#35265;&#20294;&#20851;&#38190;&#30340;&#24773;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32423;&#32852;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; CADRE&#65292;&#20197;&#23454;&#29616;&#26080;&#27169;&#22411;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#22478;&#24066;&#39550;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-based autonomous urban driving in dense traffic is quite challenging due to the complicated urban environment and the dynamics of the driving behaviors. Widely-applied methods either heavily rely on hand-crafted rules or learn from limited human experience, which makes them hard to generalize to rare but critical scenarios. In this paper, we present a novel CAscade Deep REinforcement learning framework, CADRE, to achieve model-free vision-based autonomous urban driving. In CADRE, to derive representative latent features from raw observations, we first offline train a Co-attention Perception Module (CoPM) that leverages the co-attention mechanism to learn the inter-relationships between the visual and control information from a pre-collected driving dataset. Cascaded by the frozen CoPM, we then present an efficient distributed proximal policy optimization framework to online learn the driving policy under the guidance of particularly designed reward functions. We perform a compre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#37324;&#65292;&#23398;&#20064;&#32773;&#35201;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#21644;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20256;&#32479;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19981;&#21516;&#30340;&#29305;&#27530;&#29616;&#35937;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2107.05289</link><description>&lt;p&gt;
&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Time Bandits With Sampling Costs. (arXiv:2107.05289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.05289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#37319;&#26679;&#25104;&#26412;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#37324;&#65292;&#23398;&#20064;&#32773;&#35201;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#21644;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#20256;&#32479;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19981;&#21516;&#30340;&#29305;&#27530;&#29616;&#35937;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#19979;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;(Continuous Time Multi-arm Bandit Problem&#65292;CTMAB)&#12290;&#22312;&#32473;&#23450;&#26102;&#38388;&#27573;&#20869;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#23545;&#33218;&#36827;&#34892;&#20219;&#24847;&#27425;&#37319;&#26679;&#65292;&#27599;&#27425;&#37319;&#26679;&#37117;&#33021;&#33719;&#24471;&#38543;&#26426;&#22870;&#21169;&#65292;&#20294;&#37319;&#26679;&#39057;&#29575;&#30340;&#25552;&#39640;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#24809;&#32602;/&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#23384;&#22312;&#33719;&#24471;&#26356;&#39640;&#22870;&#21169;&#19982;&#25215;&#25285;&#37319;&#26679;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#36951;&#25022;&#65288;regret&#65292;&#23450;&#20041;&#20026;&#23398;&#20064;&#31639;&#27861;&#19982;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#25910;&#30410;&#20043;&#38388;&#30340;&#24046;&#20540;&#65289;&#36798;&#21040;&#26368;&#23567;&#12290;CTMAB&#19982;&#36890;&#24120;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;(Multi-armed Bandit Problem&#65292;MAB)&#26377;&#26681;&#26412;&#30340;&#21306;&#21035;&#65292;&#20363;&#22914;&#65292;&#22312;CTMAB&#20013;&#65292;&#21333;&#33218;&#24773;&#20917;&#37117;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#26368;&#20248;&#37319;&#26679;&#39057;&#29575;&#21462;&#20915;&#20110;&#33218;&#30340;&#22343;&#20540;&#65292;&#32780;&#35813;&#22343;&#20540;&#38656;&#35201;&#34987;&#20272;&#35745;&#12290;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#25152;&#26377;&#31639;&#27861;&#21487;&#36798;&#21040;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#36798;&#21040;&#19979;&#30028;&#30340;&#31639;&#27861;&#12290;&#23545;&#20110;&#21333;&#33218;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19979;&#38480;&#21644;&#19978;&#38480;&#22823;&#33268;&#31526;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#32463;&#20856;MAB&#38382;&#39064;&#20013;&#19981;&#23384;&#22312;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a continuous-time multi-arm bandit problem (CTMAB), where the learner can sample arms any number of times in a given interval and obtain a random reward from each sample, however, increasing the frequency of sampling incurs an additive penalty/cost. Thus, there is a tradeoff between obtaining large reward and incurring sampling cost as a function of the sampling frequency. The goal is to design a learning algorithm that minimizes regret, that is defined as the difference of the payoff of the oracle policy and that of the learning algorithm. CTMAB is fundamentally different than the usual multi-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial in CTMAB, since the optimal sampling frequency depends on the mean of the arm, which needs to be estimated. We first establish lower bounds on the regret achievable with any algorithm and then propose algorithms that achieve the lower bound up to logarithmic factors. For the single-arm case, we show that the lower
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.07217</link><description>&lt;p&gt;
&#22522;&#20110;&#36807;&#25311;&#21512;&#27169;&#22411;&#29305;&#24615;&#30340;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-Fit: Noisy-Label Detection based on the Overfitted Model Property. (arXiv:2106.07217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#23481;&#37327;&#29305;&#24615;&#65292;&#21363;&#20351;&#26159;&#22122;&#22768;&#26631;&#31614;&#65292;&#20063;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21547;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#26174;&#33879;&#25552;&#39640;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29305;&#24615;&#26469;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#24182;&#25913;&#21892;&#20915;&#31574;&#36793;&#30028;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20855;&#26377;&#24456;&#22909;&#30340;&#21327;&#21516;&#25928;&#26524;&#12290;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network can easily overfit to even noisy labels due to its high capacity, which degrades the generalization performance of a model. To overcome this issue, we propose a new approach for learning from noisy labels (LNL) via post-training, which can significantly improve the generalization performance of any pre-trained model on noisy label data. To this end, we rather exploit the overfitting property of a trained model to identify mislabeled samples. Specifically, our post-training approach gradually removes samples with high influence on the decision boundary and refines the decision boundary to improve generalization performance. Our post-training approach creates great synergies when combined with the existing LNL methods. Experimental results on various real-world and synthetic benchmark datasets demonstrate the validity of our approach in diverse realistic scenarios.
&lt;/p&gt;</description></item><item><title>&#32479;&#19968;&#19968;&#32500;&#20998;&#29255;&#26159;&#19968;&#31181;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#21487;&#23558;&#20108;&#20803;&#36923;&#36753;&#25193;&#23637;&#21040;&#20855;&#26377;&#20219;&#24847;&#20803;&#25968;&#20851;&#31995;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#20854;&#25551;&#36848;&#36923;&#36753;&#29256;&#26412;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#65292;&#19982;&#33021;&#22815;&#23481;&#32435;&#26356;&#39640;&#20803;&#25968;&#20851;&#31995;&#30340;&#25551;&#36848;&#36923;&#36753;&#32039;&#23494;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/1604.01673</link><description>&lt;p&gt;
&#20851;&#20110;&#32479;&#19968;&#19968;&#32500;&#20998;&#29255;
&lt;/p&gt;
&lt;p&gt;
On the uniform one-dimensional fragment. (arXiv:1604.01673v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1604.01673
&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#19968;&#32500;&#20998;&#29255;&#26159;&#19968;&#31181;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#21487;&#23558;&#20108;&#20803;&#36923;&#36753;&#25193;&#23637;&#21040;&#20855;&#26377;&#20219;&#24847;&#20803;&#25968;&#20851;&#31995;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#20854;&#25551;&#36848;&#36923;&#36753;&#29256;&#26412;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#65292;&#19982;&#33021;&#22815;&#23481;&#32435;&#26356;&#39640;&#20803;&#25968;&#20851;&#31995;&#30340;&#25551;&#36848;&#36923;&#36753;&#32039;&#23494;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#19968;&#32500;&#20998;&#29255;&#65288;U1&#65289;&#26159;&#19968;&#31181;&#24418;&#24335;&#21270;&#35821;&#35328;&#65292;&#20197;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#23558;&#20108;&#20803;&#36923;&#36753;&#25193;&#23637;&#21040;&#20855;&#26377;&#20219;&#24847;&#20803;&#25968;&#20851;&#31995;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;U1&#30340;&#23646;&#24615;&#24182;&#30740;&#31350;&#20102;&#23427;&#19982;&#26088;&#22312;&#23481;&#32435;&#26356;&#39640;&#20803;&#25968;&#20851;&#31995;&#30340;&#25551;&#36848;&#36923;&#36753;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#20851;&#27880;DLR_reg&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;U1&#30340;&#19968;&#20010;&#21464;&#20307;&#30340;&#25551;&#36848;&#36923;&#36753;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#20851;U1&#21450;&#20854;&#30456;&#20851;&#36923;&#36753;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#31995;&#21015;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The uniform one-dimensional fragment of first-order logic, U1, is a formalism that extends two-variable logic in a natural way to contexts with relations of all arities. We survey properties of U1 and investigate its relationship to description logics designed to accommodate higher arity relations, with particular attention given to DLR_reg. We also define a description logic version of a variant of U1 and prove a range of new results concerning the expressivity of U1 and related logics.
&lt;/p&gt;</description></item></channel></rss>