<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16081</link><description>&lt;p&gt;
&#25945;&#32946;&#20013;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Interplay of Learning, Analytics, and Artificial Intelligence in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31508;&#32773;&#25361;&#25112;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#20165;&#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#26222;&#36941;&#35266;&#24565;&#65292;&#20363;&#22914;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20027;&#24352;&#37325;&#35270;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26367;&#20195;&#27010;&#24565;&#12290;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#31867;&#26234;&#33021;&#19982;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;AI&#31639;&#27861;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;AI&#20063;&#21487;&#20197;&#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#20174;&#23558;AI&#35270;&#20026;&#20154;&#31867;&#26234;&#33021;&#30340;&#31867;&#27604;&#30340;&#26089;&#26399;&#23398;&#20064;&#31185;&#23398;&#21644;&#25945;&#32946;&#20013;&#30340;AI&#30740;&#31350;&#24050;&#32463;&#20559;&#31163;&#36825;&#19968;&#35266;&#28857;&#65292;&#20419;&#20351;&#26377;&#24517;&#35201;&#37325;&#26032;&#28857;&#29123;&#36825;&#31181;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#65306;&#20154;&#31867;&#35748;&#30693;&#30340;&#22806;&#37096;&#21270;&#12289;&#20869;&#21270;AI&#27169;&#22411;&#20197;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16081v1 Announce Type: cross  Abstract: This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought pr
&lt;/p&gt;</description></item><item><title>TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15879</link><description>&lt;p&gt;
TrustSQL: &#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15879
&lt;/p&gt;
&lt;p&gt;
TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;SQL&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#36825;&#20123;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#33021;&#21542;&#21487;&#38752;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#21333;&#19968;&#25968;&#25454;&#24211;&#21644;&#36328;&#25968;&#25454;&#24211;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#22522;&#20934;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#20004;&#31181;&#32467;&#26524;&#20043;&#19968;&#65306;1&#65289;SQL&#39044;&#27979;&#65307;&#25110;2&#65289;&#22312;&#29983;&#25104;&#30340;SQL&#20013;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#25110;&#38754;&#20020;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#26102;&#25918;&#24323;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19987;&#38376;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#21508;&#31181;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#21487;&#22238;&#31572;&#24615;&#20248;&#21270;&#21333;&#29420;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.15529</link><description>&lt;p&gt;
LimGen: &#25506;&#31350;&#29992;&#20110;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#23616;&#38480;&#26159;&#23398;&#26415;&#30740;&#31350;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#21487;&#33021;&#32570;&#20047;&#20915;&#23450;&#24615;&#25110;&#38656;&#35201;&#21152;&#24378;&#30340;&#26041;&#38754;&#12290;&#36825;&#26377;&#21161;&#20110;&#35835;&#32773;&#32771;&#34385;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#29983;&#25104;&#65288;SLG&#65289;&#30340;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LimGen&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;ACL&#25991;&#38598;&#30340;4068&#31687;&#30740;&#31350;&#35770;&#25991;&#21450;&#20854;&#30456;&#20851;&#23616;&#38480;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#24443;&#24213;&#30740;&#31350;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;LimGen&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/armbf/LimGen &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.18409</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22270;&#20687;&#25512;&#29702;&#21644;&#25551;&#36848;&#30340;&#35748;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21457;&#29616;LVLMs&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#21463;&#21040;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#27979;&#35797;&#12290;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#27979;&#35797;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#8220;&#20599;&#39292;&#24178;&#8221;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21033;&#29992;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#30340;&#22270;&#20687;&#35780;&#20272;LVLMs&#30340;&#39640;&#32423;&#35748;&#30693;&#33021;&#21147;&#12290;&#23427;&#23450;&#20041;&#20102;&#20843;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21253;&#25324;&#22270;&#20687;&#25551;&#36848;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;&#30693;&#21517;LVLMs&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;LVLMs&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#35748;&#30693;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18409v1 Announce Type: new  Abstract: Large Vision Language Models (LVLMs), despite their recent success, are hardly comprehensively tested for their cognitive abilities. Inspired by the prevalent use of the "Cookie Theft" task in human cognition test, we propose a novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs using images with rich semantics. It defines eight reasoning capabilities and consists of an image description task and a visual question answering task. Our evaluation on well-known LVLMs shows that there is still a large gap in cognitive ability between LVLMs and humans.
&lt;/p&gt;</description></item><item><title>INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.14492</link><description>&lt;p&gt;
INSTRAUG&#65306;&#29992;&#20110;&#22810;&#27169;&#25351;&#20196;&#24494;&#35843;&#30340;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14492
&lt;/p&gt;
&lt;p&gt;
INSTRAUG&#26159;&#19968;&#31181;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#27169;&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20219;&#21153;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#23545;&#26032;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26368;&#36817;&#20851;&#20110;&#39640;&#36136;&#37327;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#29983;&#25104;&#21644;&#36873;&#25321;&#30340;&#24037;&#20316;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#20197;&#20026;&#32473;&#23450;&#20219;&#21153;&#26500;&#24605;&#27169;&#22411;&#21487;&#29702;&#35299;&#30340;&#25351;&#20196;&#65292;&#24182;&#35880;&#24910;&#36807;&#28388;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;INSTRAUG&#30340;&#22810;&#27169;&#20219;&#21153;&#33258;&#21160;&#25351;&#20196;&#22686;&#24378;&#26041;&#27861;&#12290;&#23427;&#20174;&#19968;&#20123;&#22522;&#26412;&#21644;&#31616;&#21333;&#30340;&#20803;&#25351;&#20196;&#24320;&#22987;&#65292;&#20294;&#33021;&#23558;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#25193;&#22823;30&#20493;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22810;&#27169;&#25351;&#20196;&#36319;&#38543;&#22522;&#20934;&#27979;&#35797;&#38598;MULTIINSTRUCT&#21644;InstructBLIP&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;INSTRAUG&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#36328;12&#20010;&#22810;&#27169;&#20219;&#21153;&#30340;&#22810;&#27169;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#23545;&#40784;&#65292;&#29978;&#33267;&#30456;&#24403;&#20110;&#22686;&#21152;&#35757;&#32451;&#35268;&#27169;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14492v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named INSTRAUG in multimodal tasks. It starts from a handful of basic and straightforward meta instructions but can expand an instruction-following dataset by 30 times. Results on two popular multimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show that INSTRAUG can significantly improve the alignment of multimodal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10998</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Neural Network Controllers via Differential Dynamic Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20316;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;&#25511;&#21046;&#22120;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#39564;&#35777;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;NN&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23545;&#26080;&#30028;&#26102;&#38388;&#33539;&#22260;&#36827;&#34892;&#23433;&#20840;&#24615;&#39564;&#35777;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VerSAILLE&#65288;&#36890;&#36807;&#36923;&#36753;&#38142;&#25509;&#21253;&#39564;&#35777;&#30340;&#21487;&#39564;&#35777;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#65289;&#65306;&#36825;&#26159;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#65288;dL&#65289;&#21644;NN&#39564;&#35777;&#32452;&#21512;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;dL&#30340;&#20005;&#35880;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#20449;&#23553;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#65292;&#20197;&#35777;&#26126;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#20855;&#20307;NNCS&#30340;&#23433;&#20840;&#24615;&#12290;VerSAILLE&#23548;&#33268;&#30340;NN&#39564;&#35777;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#31639;&#26415;&#65292;&#32780;&#39640;&#25928;&#30340;NN&#39564;&#35777;&#24037;&#20855;&#20165;&#25903;&#25345;&#32447;&#24615;&#31639;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2401.17626</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative AI to Generate Test Data Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20551;&#25968;&#25454;&#26159;&#29616;&#20195;&#36719;&#20214;&#27979;&#35797;&#30340;&#37325;&#35201;&#32500;&#24230;&#20043;&#19968;&#65292;&#20247;&#22810;&#25968;&#25454;&#20266;&#36896;&#24211;&#30340;&#25968;&#37327;&#21644;&#37325;&#35201;&#24615;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#20266;&#36896;&#24211;&#30340;&#24320;&#21457;&#32773;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#21644;&#39046;&#22495;&#25152;&#38656;&#29983;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#33539;&#22260;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#25191;&#34892;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#65306;1&#65289;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#65292;2&#65289;&#21512;&#25104;&#29305;&#23450;&#35821;&#35328;&#30340;&#31243;&#24207;&#20197;&#29983;&#25104;&#26377;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;3&#65289;&#29983;&#25104;&#20351;&#29992;&#23574;&#31471;&#20266;&#36896;&#24211;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;LLMs&#20026;11&#20010;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#38598;&#25104;&#32423;&#21035;&#19978;&#65292;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;EX-FEVER&#65292;&#21253;&#21547;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#32463;&#36807;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#20855;&#26377;&#35814;&#32454;&#30340;&#35299;&#37322;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2310.09754</link><description>&lt;p&gt;
EX-FEVER&#65306;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EX-FEVER: A Dataset for Multi-hop Explainable Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;EX-FEVER&#65292;&#21253;&#21547;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#32463;&#36807;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#20855;&#26377;&#35814;&#32454;&#30340;&#35299;&#37322;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#26681;&#25454;&#22810;&#20010;&#35777;&#25454;&#33258;&#21160;&#25506;&#31350;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#29616;&#26377;&#24037;&#20316;&#22987;&#32456;&#33268;&#21147;&#20110;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#26356;&#19981;&#29992;&#35828;&#35299;&#37322;&#24615;&#20102;&#65292;&#36825;&#26159;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#33021;&#21147;&#12290;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#22810;&#36339;&#22330;&#26223;&#20013;&#35299;&#37322;&#30340;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#19968;&#30452;&#21463;&#21046;&#20110;&#32570;&#20047;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EX-FEVER&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#36339;&#21487;&#35299;&#37322;&#20107;&#23454;&#39564;&#35777;&#30340;&#24320;&#21019;&#24615;&#25968;&#25454;&#38598;&#12290;&#36229;&#36807;6&#19975;&#20010;&#22768;&#26126;&#28041;&#21450;2&#36339;&#21644;3&#36339;&#25512;&#29702;&#65292;&#27599;&#20010;&#22768;&#26126;&#37117;&#26159;&#36890;&#36807;&#24635;&#32467;&#21644;&#20462;&#25913;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#36229;&#38142;&#25509;&#25991;&#26723;&#30340;&#20449;&#24687;&#32780;&#21019;&#24314;&#30340;&#12290;&#27599;&#20010;&#23454;&#20363;&#37117;&#38468;&#24102;&#19968;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#21644;&#19968;&#20010;&#35828;&#26126;&#65292;&#27010;&#36848;&#25903;&#25345;&#30495;&#23454;&#24615;&#20998;&#31867;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09754v2 Announce Type: replace  Abstract: Fact verification aims to automatically probe the veracity of a claim based on several pieces of evidence. Existing works are always engaging in accuracy improvement, let alone explainability, a critical capability of fact verification systems. Constructing an explainable fact verification system in a complex multi-hop scenario is consistently impeded by the absence of a relevant, high-quality dataset. Previous datasets either suffer from excessive simplification or fail to incorporate essential considerations for explainability. To address this, we present EXFEVER, a pioneering dataset for multi-hop explainable fact verification. With over 60,000 claims involving 2-hop and 3-hop reasoning, each is created by summarizing and modifying information from hyperlinked Wikipedia documents. Each instance is accompanied by a veracity label and an explanation that outlines the reasoning path supporting the veracity classification. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09851</link><description>&lt;p&gt;
&#20223;&#30495;&#34892;&#20026;&#65306;&#25506;&#32034;&#31185;&#23398;&#30340;&#21487;&#33021;&#19979;&#19968;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#29983;&#29289;&#31181;&#32676;&#12290;&#23427;&#26159;&#22788;&#29702;&#22797;&#26434;&#31995;&#32479;&#38382;&#39064;&#30340;&#26368;&#20339;&#24037;&#20855;&#65292;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#26080;&#27861;&#20351;&#29992;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#19988;&#30446;&#26631;&#20998;&#24067;&#36807;&#20110;&#22797;&#26434;&#32780;&#26080;&#27861;&#23436;&#20840;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#26159;&#19968;&#33268;&#30340;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#35282;&#24230;&#24402;&#32435;&#20102;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#20223;&#30495;&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#19982;&#26032;&#33539;&#24335;&#30340;&#20986;&#29616;&#30456;&#36866;&#24212;&#65292;&#24182;&#21457;&#29616;&#20808;&#36827;&#30340;&#20223;&#30495;&#25216;&#26415;&#26159;&#33539;&#24335;&#25972;&#21512;&#30340;&#20856;&#22411;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#65288;BS&#65289;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#34892;&#20026;&#20223;&#30495;&#65288;SBS&#65289;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.08879</link><description>&lt;p&gt;
&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Contribution Functions for Quantitative Bipolar Argumentation Graphs: A Principle-based Analysis. (arXiv:2401.08879v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#20026;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#37327;&#21270;&#21452;&#26497;&#35770;&#35777;&#22270;&#30340;&#36129;&#29486;&#20989;&#25968;&#36827;&#34892;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#20998;&#26512;&#65292;&#35813;&#20989;&#25968;&#37327;&#21270;&#20102;&#19968;&#20010;&#35770;&#35777;&#23545;&#21478;&#19968;&#20010;&#35770;&#35777;&#30340;&#36129;&#29486;&#12290;&#24341;&#20837;&#30340;&#21407;&#21017;&#23558;&#19981;&#21516;&#30340;&#36129;&#29486;&#20989;&#25968;&#30340;&#30452;&#35273;&#24418;&#24335;&#21270;&#65292;&#24182;&#23545;&#36129;&#29486;&#20989;&#25968;&#30340;&#34892;&#20026;&#26377;&#20102;&#26399;&#26395;&#12290;&#30001;&#20110;&#27809;&#26377;&#19968;&#20010;&#35206;&#30422;&#30340;&#36129;&#29486;&#20989;&#25968;&#28385;&#36275;&#25152;&#26377;&#21407;&#21017;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#24037;&#20855;&#65292;&#26681;&#25454;&#32473;&#23450;&#29992;&#20363;&#30340;&#35201;&#27714;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a principle-based analysis of contribution functions for quantitative bipolar argumentation graphs that quantify the contribution of one argument to another. The introduced principles formalise the intuitions underlying different contribution functions as well as expectations one would have regarding the behaviour of contribution functions in general. As none of the covered contribution functions satisfies all principles, our analysis can serve as a tool that enables the selection of the most suitable function based on the requirements of a given use case.
&lt;/p&gt;</description></item><item><title>BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02982</link><description>&lt;p&gt;
BIBench: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#20998;&#26512;&#30693;&#35782;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BIBench: Benchmarking Data Analysis Knowledge of Large Language Models. (arXiv:2401.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02982
&lt;/p&gt;
&lt;p&gt;
BIBench&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#25968;&#25454;&#20998;&#26512;&#39046;&#22495;&#20013;&#33021;&#21147;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#36890;&#36807;&#27979;&#35797;&#27169;&#22411;&#22312;BI&#22522;&#30784;&#30693;&#35782;&#12289;&#24212;&#29992;&#30693;&#35782;&#21644;&#25216;&#26415;&#25216;&#33021;&#19977;&#20010;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#29087;&#32451;&#24230;&#21644;&#21487;&#38752;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#24605;&#32500;&#20026;&#37325;&#28857;&#30340;&#39046;&#22495;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#21830;&#19994;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#26512;&#33021;&#21147;&#12290;BIBench&#36890;&#36807;&#19977;&#20010;&#32500;&#24230;&#35780;&#20272;LLMs&#65306;1&#65289;BI&#22522;&#30784;&#30693;&#35782;&#65292;&#35780;&#20272;&#27169;&#22411;&#30340;&#25968;&#20540;&#25512;&#29702;&#33021;&#21147;&#21644;&#23545;&#37329;&#34701;&#27010;&#24565;&#30340;&#29087;&#24713;&#31243;&#24230;&#65307;2&#65289;BI&#30693;&#35782;&#24212;&#29992;&#65292;&#30830;&#23450;&#27169;&#22411;&#24555;&#36895;&#29702;&#35299;&#25991;&#26412;&#20449;&#24687;&#24182;&#20174;&#22810;&#20010;&#35270;&#35282;&#29983;&#25104;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65307;3&#65289;BI&#25216;&#26415;&#25216;&#33021;&#65292;&#26816;&#26597;&#27169;&#22411;&#20351;&#29992;&#25216;&#26415;&#30693;&#35782;&#35299;&#20915;&#29616;&#23454;&#25968;&#25454;&#20998;&#26512;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;BIBench&#21253;&#25324;11&#20010;&#23376;&#20219;&#21153;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;&#25552;&#21462;&#21644;&#29983;&#25104;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. However, their proficiency and reliability in the specialized domain of Data Analysis, particularly with a focus on data-driven thinking, remain uncertain. To bridge this gap, we introduce BIBench, a comprehensive benchmark designed to evaluate the data analysis capabilities of LLMs within the context of Business Intelligence (BI). BIBench assesses LLMs across three dimensions: 1) BI foundational knowledge, evaluating the models' numerical reasoning and familiarity with financial concepts; 2) BI knowledge application, determining the models' ability to quickly comprehend textual information and generate analysis questions from multiple views; and 3) BI technical skills, examining the models' use of technical knowledge to address real-world data analysis challenges. BIBench comprises 11 sub-tasks, spanning three categories of task types: classification, extraction, and generation. Additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12768</link><description>&lt;p&gt;
SemantIC: &#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#22312;6G&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#24178;&#25200;&#25216;&#26415;&#65292;&#21363;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;SemantIC&#21482;&#38656;&#35201;&#25509;&#25910;&#22120;&#23558;&#20449;&#36947;&#35299;&#30721;&#22120;&#19982;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#26500;&#24314;&#20102;&#19968;&#20010;&#36845;&#20195;&#24490;&#29615;&#65292;&#20132;&#26367;&#28040;&#38500;&#20449;&#21495;&#22495;&#21644;&#35821;&#20041;&#22495;&#20013;&#30340;&#22122;&#22768;&#12290;&#20174;&#32593;&#32476;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#35757;&#32451;&#23384;&#20648;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#22312;&#36845;&#20195;&#35299;&#30721;&#20013;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#65292;&#20316;&#20026;Wyner-Ziv&#23450;&#29702;&#30340;&#19968;&#31181;&#23454;&#29616;&#12290;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;SemantIC&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#26469;&#25913;&#21892;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36991;&#20813;&#22240;&#22806;&#35266;&#12289;&#20202;&#22120;&#32423;&#21035;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#21464;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06807</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#29992;&#20110;&#25913;&#36827;&#24687;&#35782;&#21035;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bayesian uncertainty-weighted loss for improved generalisability on polyp segmentation task. (arXiv:2309.06807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#25439;&#22833;&#26469;&#25913;&#21892;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36991;&#20813;&#22240;&#22806;&#35266;&#12289;&#20202;&#22120;&#32423;&#21035;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#21464;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#24687;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#22312;&#22810;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#30001;&#20110;&#19981;&#21516;&#20013;&#24515;&#24687;&#30340;&#22806;&#35266;&#21464;&#24322;&#12289;&#20869;&#31397;&#38236;&#20202;&#22120;&#31561;&#32423;&#30340;&#24046;&#24322;&#21644;&#37319;&#38598;&#36136;&#37327;&#30340;&#19981;&#21516;&#65292;&#23548;&#33268;&#36825;&#20123;&#26041;&#27861;&#22312;&#20869;&#37096;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#22806;&#37096;&#27979;&#35797;&#25110;&#20195;&#34920;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#23545;&#20020;&#24202;&#24212;&#29992;&#20855;&#26377;&#20005;&#37325;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#23545;&#20020;&#24202;&#24212;&#29992;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#38544;&#24335;&#20559;&#24046;&#20943;&#36731;&#26041;&#27861;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#40723;&#21169;&#27169;&#22411;&#19987;&#27880;&#20110;&#20195;&#34920;&#24615;&#26679;&#26412;&#21306;&#22495;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#20013;&#24515;&#21644;&#22270;&#20687;&#27169;&#24335;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20013;&#24515;&#24687;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;PolypGen&#65289;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several previous studies have devised methods for segmentation of polyps, most of these methods are not rigorously assessed on multi-center datasets. Variability due to appearance of polyps from one center to another, difference in endoscopic instrument grades, and acquisition quality result in methods with good performance on in-distribution test data, and poor performance on out-of-distribution or underrepresented samples. Unfair models have serious implications and pose a critical challenge to clinical applications. We adapt an implicit bias mitigation method which leverages Bayesian epistemic uncertainties during training to encourage the model to focus on underrepresented sample regions. We demonstrate the potential of this approach to improve generalisability without sacrificing state-of-the-art performance on a challenging multi-center polyp segmentation dataset (PolypGen) with different centers and image modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.08828</link><description>&lt;p&gt;
&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lifted Algorithms for Symmetric Weighted First-Order Model Sampling. (arXiv:2308.08828v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#31216;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#25277;&#26679;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#38382;&#39064;&#20063;&#23384;&#22312;&#21487;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#27169;&#22411;&#35745;&#25968;&#65288;WMC&#65289;&#26159;&#35745;&#31639;&#21629;&#39064;&#20844;&#24335;&#30340;&#25152;&#26377;&#28385;&#36275;&#35299;&#65288;&#27169;&#22411;&#65289;&#30340;&#26435;&#37325;&#20043;&#21644;&#30340;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#65288;WMS&#65289;&#26088;&#22312;&#20197;&#27010;&#29575;&#19982;&#23427;&#20204;&#30456;&#24212;&#30340;&#26435;&#37325;&#25104;&#27604;&#20363;&#22320;&#38543;&#26426;&#29983;&#25104;&#27169;&#22411;&#12290;WMC&#21644;WMS&#37117;&#38590;&#20197;&#31934;&#30830;&#27714;&#35299;&#65292;&#23646;&#20110;\#P-hard&#22797;&#26434;&#24230;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22914;&#26524;&#21629;&#39064;&#20844;&#24335;&#21487;&#20197;&#20197;&#32039;&#20945;&#30340;&#26041;&#24335;&#34920;&#31034;&#24182;&#29992;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#65292;&#26377;&#26102;&#35745;&#25968;&#38382;&#39064;&#21487;&#33021;&#26159;&#21487;&#20197;&#22788;&#29702;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#21487;&#20197;&#22312;&#22495;&#22823;&#23567;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#65292;&#24182;&#34987;&#31216;&#20026;&#8220;&#22495;&#21487;&#25552;&#21319;&#8221;&#12290;&#28982;&#21518;&#65292;&#23601;&#20250;&#20986;&#29616;&#20197;&#19979;&#38382;&#39064;&#65306;&#21152;&#26435;&#27169;&#22411;&#25277;&#26679;&#26159;&#21542;&#20063;&#26159;&#22914;&#27492;&#65311;&#26412;&#25991;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#8220;&#22495;&#21487;&#25552;&#21319;&#25277;&#26679;&#8221;&#30340;&#20869;&#23481;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted model counting (WMC) is the task of computing the weighted sum of all satisfying assignments (i.e., models) of a propositional formula. Similarly, weighted model sampling (WMS) aims to randomly generate models with probability proportional to their respective weights. Both WMC and WMS are hard to solve exactly, falling under the \#P-hard complexity class. However, it is known that the counting problem may sometimes be tractable, if the propositional formula can be compactly represented and expressed in first-order logic. In such cases, model counting problems can be solved in time polynomial in the domain size, and are known as \textit{domain-liftable}. The following question then arises: Is it also the case for weighted model sampling? This paper addresses this question and answers it affirmatively. Specifically, we prove the \textit{domain-liftability under sampling} for the two-variables fragment of first-order logic with counting quantifiers in this paper, by devising an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11898</link><description>&lt;p&gt;
&#26080;&#27861;&#35299;&#37322;&#30340;&#35299;&#37322;&#65306;&#35299;&#37322;tSNE&#21644;UMAP&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#35299;&#37322;&#20026;&#21560;&#24341;/&#25490;&#26021;&#38477;&#32500;&#65288;ARDR&#65289;&#26041;&#27861;&#65288;&#22914;tSNE&#21644;UMAP&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#36825;&#21462;&#20915;&#20110;2D&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#19982;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#19968;&#33268;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#32463;&#35777;&#26126;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;ARDR&#31639;&#27861;&#26159;&#21542;&#26377;&#20219;&#20309;&#25910;&#25947;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#21560;&#24341;&#21644;&#25490;&#26021;&#26469;&#23436;&#20840;&#24674;&#22797;PCA&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#31245;&#21152;&#20462;&#25913;&#65292;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE&#65289;&#21487;&#20197;&#22797;&#29616;ARDR&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#36825;&#20123;&#29468;&#24819;&#25104;&#31435;&#65292;&#23601;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>SALSA&#26159;&#19968;&#31181;&#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#24555;&#36895;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#21033;&#29992;&#26032;&#31574;&#30053;&#23558;&#31351;&#20030;&#25628;&#32034;&#21644;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#25214;&#21040;&#26356;&#20302;&#33021;&#37327;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.12931</link><description>&lt;p&gt;
SALSA: &#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#27169;&#25311;&#36864;&#28779;&#24490;&#29615;&#25490;&#24207;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
SALSA: Simulated Annealing based Loop-Ordering Scheduler for DNN Accelerators. (arXiv:2304.12931v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12931
&lt;/p&gt;
&lt;p&gt;
SALSA&#26159;&#19968;&#31181;&#29992;&#20110;DNN&#21152;&#36895;&#22120;&#30340;&#24555;&#36895;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#21033;&#29992;&#26032;&#31574;&#30053;&#23558;&#31351;&#20030;&#25628;&#32034;&#21644;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#25214;&#21040;&#26356;&#20302;&#33021;&#37327;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;DNN&#35745;&#31639;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#19987;&#29992;&#30340;&#30828;&#20214;&#20307;&#31995;&#32467;&#26500;&#12290;&#27599;&#20010;DNN&#23618;&#24212;&#35813;&#26144;&#23556;&#21040;&#26368;&#26377;&#25928;&#30340;&#30828;&#20214;&#19978;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#20248;&#21270;&#22120;&#24448;&#24448;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#20026;&#25152;&#26377;DNN-HW&#32452;&#21512;&#25552;&#20379;&#26368;&#20339;&#30340;&#25191;&#34892;&#35745;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SALSA&#65292;&#19968;&#31181;&#24555;&#36895;&#30340;&#21452;&#24341;&#25806;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#22343;&#21248;&#21644;&#38750;&#22343;&#21248;&#26144;&#23556;&#30340;&#26368;&#20248;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#23558;&#31351;&#20030;&#25628;&#32034;&#19982;&#27169;&#25311;&#36864;&#28779;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#23618;&#20043;&#38388;&#24490;&#29615;&#25490;&#24207;&#35774;&#35745;&#31354;&#38388;&#22823;&#23567;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;SALSA&#22312;5&#20010;&#19981;&#21516;&#30340;DNN&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;LOMA&#21644;Timeloop&#30456;&#27604;&#65292;SALSA&#22312;&#21152;&#36895;&#25628;&#32034;&#30340;&#21516;&#26102;&#33021;&#22815;&#25214;&#21040;11.9&#65285;&#21644;7.6&#65285;&#26356;&#20302;&#30340;&#33021;&#37327;&#35843;&#24230;&#65292;&#20998;&#21035;&#25552;&#39640;&#20102;1.7&#20493;&#21644;24&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
To meet the growing need for computational power for DNNs, multiple specialized hardware architectures have been proposed. Each DNN layer should be mapped onto the hardware with the most efficient schedule, however, SotA schedulers struggle to consistently provide optimum schedules in a reasonable time across all DNN-HW combinations.  This paper proposes SALSA, a fast dual-engine scheduler to generate optimal execution schedules for both even and uneven mapping. We introduce a new strategy, combining exhaustive search with simulated annealing to address the dynamic nature of the loop ordering design space size across layers. SALSA is extensively benchmarked against two SotA schedulers, LOMA and Timeloop on 5 different DNNs, on average SALSA finds schedules with 11.9% and 7.6% lower energy while speeding up the search by 1.7x and 24x compared to LOMA and Timeloop, respectively.
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item></channel></rss>