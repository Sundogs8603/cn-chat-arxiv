<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02877</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#20803;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-learning in healthcare: A survey. (arXiv:2308.02877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02877
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#19981;&#36275;&#21644;&#25968;&#25454;&#25910;&#38598;&#24046;&#24322;&#12290;&#20027;&#35201;&#21253;&#25324;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#32463;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#36866;&#24403;&#22320;&#35299;&#20915;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#26679;&#26412;&#25968;&#37327;&#19981;&#36275;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#29305;&#28857;&#20351;&#20803;&#23398;&#20064;&#25104;&#20026;&#22312;&#21508;&#31181;&#21307;&#30103;&#29615;&#22659;&#20013;&#24320;&#21457;&#26377;&#24433;&#21709;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21512;&#36866;&#36873;&#25321;&#65292;&#36825;&#20123;&#29615;&#22659;&#20013;&#21487;&#29992;&#25968;&#25454;&#36890;&#24120;&#19981;&#36275;&#65292;&#24182;&#19988;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20063;&#19981;&#21516;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20102;&#35299;&#23427;&#22914;&#20309;&#20197;&#21450;&#22312;&#21738;&#20123;&#26041;&#38754;&#21487;&#20197;&#35299;&#20915;&#20851;&#38190;&#30340;&#21307;&#30103;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;&#20803;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#22312;&#21307;&#30103;&#39046;&#22495;&#24212;&#29992;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#20998;&#20026;&#22810;/&#21333;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20004;&#22823;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#20013;&#20986;&#34880;&#21306;&#22495;&#30340;&#20998;&#21106;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#24179;&#22343;&#25945;&#24072;&#8221;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23398;&#29983; U-Net &#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;VCE&#22270;&#20687;&#20013;&#20986;&#34880;&#21306;&#22495;&#30340;&#20934;&#30830;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.02869</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#20013;&#20986;&#34880;&#21306;&#22495;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Learning for Segmentation of Bleeding Regions in Video Capsule Endoscopy. (arXiv:2308.02869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#20013;&#20986;&#34880;&#21306;&#22495;&#30340;&#20998;&#21106;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#24179;&#22343;&#25945;&#24072;&#8221;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23398;&#29983; U-Net &#27169;&#22411;&#21644;&#19968;&#20010;&#25945;&#24072;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;VCE&#22270;&#20687;&#20013;&#20986;&#34880;&#21306;&#22495;&#30340;&#20934;&#30830;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35786;&#26029;&#25216;&#26415;&#39046;&#22495;&#65292;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;(VCE)&#22240;&#20854;&#22312;&#35786;&#26029;&#21508;&#31181;&#32963;&#32928;&#36947;&#30142;&#30149;&#65292;&#21253;&#25324;&#38544;&#24615;&#20986;&#34880;&#31561;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#21644;&#38750;&#20405;&#20837;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#23545;&#20110;&#25104;&#21151;&#35786;&#26029;&#21644;&#27835;&#30103;&#36825;&#20123;&#30142;&#30149;&#65292;&#20934;&#30830;&#35782;&#21035;VCE&#22270;&#20687;&#20013;&#30340;&#20986;&#34880;&#21306;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;VCE&#22270;&#20687;&#33258;&#21160;&#20998;&#26512;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20855;&#26377;&#20840;&#38754;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#33719;&#21462;&#36825;&#20123;&#26631;&#35760;&#25968;&#25454;&#38598;&#24448;&#24448;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#24182;&#19988;&#38656;&#35201;&#20016;&#23500;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;(SSL)&#26041;&#27861;&#26469;&#23545;VCE&#20013;&#30340;&#20986;&#34880;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#12290;&#36890;&#36807;&#37319;&#29992;&#8220;&#24179;&#22343;&#25945;&#24072;&#8221;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#37197;&#22791;&#20102;scSE&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#23398;&#29983;U-Net&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#30456;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#26159;&#20132;&#26367;&#26356;&#26032;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of modern diagnostic technology, video capsule endoscopy (VCE) is a standout for its high efficacy and non-invasive nature in diagnosing various gastrointestinal (GI) conditions, including obscure bleeding. Importantly, for the successful diagnosis and treatment of these conditions, accurate recognition of bleeding regions in VCE images is crucial. While deep learning-based methods have emerged as powerful tools for the automated analysis of VCE images, they often demand large training datasets with comprehensive annotations. Acquiring these labeled datasets tends to be time-consuming, costly, and requires significant domain expertise. To mitigate this issue, we have embraced a semi-supervised learning (SSL) approach for the bleeding regions segmentation within VCE. By adopting the `Mean Teacher' method, we construct a student U-Net equipped with an scSE attention block, alongside a teacher model of the same architecture. These models' parameters are alternately updated th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#21517;&#20026;STARank&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#25490;&#21015;&#26469;&#26367;&#20195;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.02860</link><description>&lt;p&gt;
&#29992;&#23433;&#25490;&#21462;&#20195;&#35780;&#20998;&#65306;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#25490;&#24207;&#30340;&#19978;&#19979;&#25991;&#38598;&#21512;&#21040;&#25490;&#21015;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Replace Scoring with Arrangement: A Contextual Set-to-Arrangement Framework for Learning-to-Rank. (arXiv:2308.02860v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02860
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#25490;&#24207;&#26694;&#26550;&#65292;&#21517;&#20026;STARank&#65292;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#25490;&#21015;&#26469;&#26367;&#20195;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#26159;top-N&#25512;&#33616;&#20219;&#21153;&#20013;&#30340;&#26680;&#24515;&#25216;&#26415;&#65292;&#29702;&#24819;&#30340;&#25490;&#21517;&#22120;&#24212;&#35813;&#26159;&#19968;&#20010;&#20174;&#39033;&#30446;&#38598;&#21512;&#21040;&#25490;&#21015;&#65288;&#21363;&#25490;&#21015;&#65289;&#30340;&#26144;&#23556;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#23646;&#20110;&#27010;&#29575;&#25490;&#24207;&#21407;&#21017;&#65288;PRP&#65289;&#33539;&#24335;&#65292;&#21363;&#39318;&#20808;&#23545;&#20505;&#36873;&#38598;&#20013;&#30340;&#27599;&#20010;&#39033;&#30446;&#36827;&#34892;&#35780;&#20998;&#65292;&#28982;&#21518;&#25191;&#34892;&#25490;&#24207;&#25805;&#20316;&#20197;&#29983;&#25104;&#25490;&#21517;&#21015;&#34920;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#20010;&#20307;&#35780;&#20998;&#36807;&#31243;&#20013;&#20505;&#36873;&#39033;&#30446;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#25490;&#24207;&#25805;&#20316;&#26159;&#19981;&#21487;&#24494;&#20998;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STARank&#30340;&#38598;&#21512;&#21040;&#25490;&#21015;&#25490;&#24207;&#26694;&#26550;&#65292;&#23427;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21015;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#20010;&#21035;&#35780;&#20998;&#21644;&#25490;&#24207;&#25805;&#20316;&#65292;&#24182;&#19988;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#12290;&#22240;&#27492;&#65292;STARank&#21487;&#20197;&#22312;&#21482;&#26377;&#30495;&#23454;&#25490;&#21015;&#21487;&#35775;&#38382;&#20294;&#27809;&#26377;&#39033;&#30446;&#30340;&#30495;&#23454;&#30456;&#20851;&#24230;&#20998;&#25968;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;STARank&#39318;&#20808;&#38405;&#35835;&#20505;&#36873;&#39033;&#30446;...
&lt;/p&gt;
&lt;p&gt;
Learning-to-rank is a core technique in the top-N recommendation task, where an ideal ranker would be a mapping from an item set to an arrangement (a.k.a. permutation). Most existing solutions fall in the paradigm of probabilistic ranking principle (PRP), i.e., first score each item in the candidate set and then perform a sort operation to generate the top ranking list. However, these approaches neglect the contextual dependence among candidate items during individual scoring, and the sort operation is non-differentiable. To bypass the above issues, we propose Set-To-Arrangement Ranking (STARank), a new framework directly generates the permutations of the candidate items without the need for individually scoring and sort operations; and is end-to-end differentiable. As a result, STARank can operate when only the ground-truth permutations are accessible without requiring access to the ground-truth relevance scores for items. For this purpose, STARank first reads the candidate items in t
&lt;/p&gt;</description></item><item><title>feather&#26159;&#19968;&#20010;Python SDK&#65292;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#29992;&#24456;&#23569;&#30340;&#20195;&#30721;&#26500;&#24314;&#21487;&#20849;&#20139;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;URL&#21644;API&#31471;&#28857;&#20379;&#20854;&#20182;&#20154;&#35775;&#38382;&#21644;&#20351;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.02838</link><description>&lt;p&gt;
feather -- &#19968;&#20010;&#29992;&#20110;&#20998;&#20139;&#21644;&#37096;&#32626;&#27169;&#22411;&#30340;Python SDK
&lt;/p&gt;
&lt;p&gt;
feather -- a Python SDK to share and deploy models. (arXiv:2308.02838v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02838
&lt;/p&gt;
&lt;p&gt;
feather&#26159;&#19968;&#20010;Python SDK&#65292;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#29992;&#24456;&#23569;&#30340;&#20195;&#30721;&#26500;&#24314;&#21487;&#20849;&#20139;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#25552;&#20379;URL&#21644;API&#31471;&#28857;&#20379;&#20854;&#20182;&#20154;&#35775;&#38382;&#21644;&#20351;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20854;&#26680;&#24515;&#65292;feather&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#20801;&#35768;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#20165;&#29992;&#19981;&#21040;20&#34892;&#20195;&#30721;&#26500;&#24314;&#21487;&#20849;&#20139;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;&#20351;&#29992;Python SDK&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#25351;&#23450;&#29992;&#25143;&#23558;&#19982;&#20043;&#20132;&#20114;&#30340;&#35270;&#35273;&#32452;&#20214;&#65288;&#20363;&#22914;&#65292;FileUpload&#32452;&#20214;&#65292;&#20801;&#35768;&#29992;&#25143;&#19978;&#20256;&#25991;&#20214;&#65289;&#12290;&#25105;&#20204;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#20197;&#19979;&#21151;&#33021;&#65306;1&#65289;URL&#65292;&#20801;&#35768;&#20854;&#20182;&#20154;&#36890;&#36807;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#21487;&#35270;&#21270;&#35775;&#38382;&#21644;&#20351;&#29992;&#27169;&#22411;&#65307;2&#65289;API&#31471;&#28857;&#65292;&#20801;&#35768;&#23545;&#27169;&#22411;&#21457;&#20986;&#32534;&#31243;&#35831;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;feather&#30340;&#21160;&#26426;&#21644;&#25105;&#20204;&#25171;&#31639;&#20026;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#30340;&#20215;&#20540;&#12290;&#20363;&#22914;&#65292;&#35813;SDK&#25903;&#25345;&#22810;&#27493;&#39588;&#27169;&#22411;&#65292;&#24182;&#21487;&#25193;&#23637;&#20197;&#23545;&#20445;&#30041;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#23613;&#30340;&#25216;&#26415;&#21644;&#23454;&#29616;&#32454;&#33410;&#12290;&#27880;&#24847;&#65292;feather&#30446;&#21069;&#26159;&#19968;&#20010;&#20241;&#30496;&#39033;&#30446;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#24320;&#28304;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#65306;https://github.com/feather-ai/
&lt;/p&gt;
&lt;p&gt;
At its core, feather was a tool that allowed model developers to build shareable user interfaces for their models in under 20 lines of code. Using the Python SDK, developers specified visual components that users would interact with. (e.g. a FileUpload component to allow users to upload a file). Our service then provided 1) a URL that allowed others to access and use the model visually via a user interface; 2) an API endpoint to allow programmatic requests to a model. In this paper, we discuss feather's motivations and the value we intended to offer AI researchers and developers. For example, the SDK can support multi-step models and can be extended to run automatic evaluation against held out datasets. We additionally provide comprehensive technical and implementation details.  N.B. feather is presently a dormant project. We have open sourced our code for research purposes: https://github.com/feather-ai/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#24182;&#35780;&#20272;&#20854;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#28216;&#25103;&#20316;&#20026;&#31034;&#20363;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.02835</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Task Generation Through Causal Sequence of Physical Interactions. (arXiv:2308.02835v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20132;&#20114;&#22240;&#26524;&#24207;&#21015;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#24182;&#35780;&#20272;&#20854;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#28216;&#25103;&#20316;&#20026;&#31034;&#20363;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#20110;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26469;&#25191;&#34892;&#29289;&#29702;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#21364;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29289;&#29702;&#27169;&#25311;&#20219;&#21153;&#36890;&#24120;&#34987;&#29992;&#26469;&#20419;&#36827;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#20307;&#20043;&#38388;&#30340;&#29289;&#29702;&#20132;&#20114;&#30340;&#22240;&#26524;&#24207;&#21015;&#26469;&#23450;&#20041;&#29289;&#29702;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#23398;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35299;&#20915;&#22522;&#20110;&#29289;&#29702;&#30340;&#20219;&#21153;&#25152;&#38656;&#30340;&#24494;&#35266;&#21147;&#23398;&#65292;&#20174;&#32780;&#20026;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#25552;&#20379;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#30410;&#26234;&#28216;&#25103;&#8220;&#24868;&#24594;&#30340;&#23567;&#40479;&#8221;&#26469;&#28436;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#25351;&#26631;&#23545;&#29983;&#25104;&#30340;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#29289;&#29702;&#31283;&#23450;&#24615;&#12289;&#20351;&#29992;&#39044;&#26399;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#35299;&#24615;&#20197;&#21450;&#20351;&#29992;&#24847;&#22806;&#30456;&#20114;&#20316;&#29992;&#30340;&#20598;&#28982;&#21487;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing tasks in a physical environment is a crucial yet challenging problem for AI systems operating in the real world. Physics simulation-based tasks are often employed to facilitate research that addresses this challenge. In this paper, first, we present a systematic approach for defining a physical scenario using a causal sequence of physical interactions between objects. Then, we propose a methodology for generating tasks in a physics-simulating environment using these defined scenarios as inputs. Our approach enables a better understanding of the granular mechanics required for solving physics-based tasks, thereby facilitating accurate evaluation of AI systems' physical reasoning capabilities. We demonstrate our proposed task generation methodology using the physics-based puzzle game Angry Birds and evaluate the generated tasks using a range of metrics, including physical stability, solvability using intended physical interactions, and accidental solvability using unintended s
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#25216;&#26415;&#24050;&#32463;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#28789;&#27963;&#22320;&#25512;&#29702;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#25110;&#23545;&#25239;&#34892;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.02829</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#30340;&#22810;&#26234;&#33021;&#20307;&#39564;&#35777;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Verification and Control with Probabilistic Model Checking. (arXiv:2308.02829v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02829
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#25216;&#26415;&#24050;&#32463;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#39046;&#22495;&#65292;&#21487;&#29992;&#20110;&#28789;&#27963;&#22320;&#25512;&#29702;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#25110;&#23545;&#25239;&#34892;&#20026;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#22312;&#19981;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#29615;&#22659;&#20013;&#36816;&#20316;&#30340;&#36719;&#20214;&#25110;&#30828;&#20214;&#31995;&#32479;&#36827;&#34892;&#24418;&#24335;&#21270;&#33258;&#21160;&#25512;&#29702;&#30340;&#25216;&#26415;&#12290;&#23427;&#24314;&#31435;&#22312;&#20174;&#36923;&#36753;&#12289;&#33258;&#21160;&#26426;&#21644;&#22270;&#35770;&#21040;&#20248;&#21270;&#12289;&#25968;&#20540;&#26041;&#27861;&#21644;&#25511;&#21046;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#21644;&#25216;&#26415;&#19978;&#12290;&#36817;&#24180;&#26469;&#65292;&#27010;&#29575;&#27169;&#22411;&#26816;&#39564;&#36824;&#25193;&#23637;&#21040;&#38598;&#25104;&#21338;&#24328;&#35770;&#30340;&#24605;&#24819;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#35832;&#22914;&#38543;&#26426;&#21338;&#24328;&#21644;&#22343;&#34913;&#27010;&#24565;&#31561;&#27169;&#22411;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#22320;&#39564;&#35777;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#22810;&#20010;&#29702;&#24615;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#22320;&#24605;&#32771;&#26234;&#33021;&#20307;&#20197;&#25932;&#23545;&#25110;&#21327;&#20316;&#26041;&#24335;&#34892;&#21160;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#26032;&#38382;&#39064;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#19968;&#20123;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#24050;&#32463;&#34987;&#20351;&#29992;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
Probabilistic model checking is a technique for formal automated reasoning about software or hardware systems that operate in the context of uncertainty or stochasticity. It builds upon ideas and techniques from a diverse range of fields, from logic, automata and graph theory, to optimisation, numerical methods and control. In recent years, probabilistic model checking has also been extended to integrate ideas from game theory, notably using models such as stochastic games and solution concepts such as equilibria, to formally verify the interaction of multiple rational agents with distinct objectives. This provides a means to reason flexibly about agents acting in either an adversarial or a collaborative fashion, and opens up opportunities to tackle new problems within, for example, artificial intelligence, robotics and autonomous systems. In this paper, we summarise some of the advances in this area, and highlight applications for which they have already been used. We discuss how the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21495;&#24863;&#30693;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25506;&#32034;&#31526;&#21495;&#23383;&#31526;&#22312;&#25991;&#26412;&#21644;&#22270;&#34920;&#29702;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#26694;&#26550;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#26469;&#25552;&#39640;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02823</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#31526;&#21495;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Symbolic Character-Aware Model for Solving Geometry Problems. (arXiv:2308.02823v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21495;&#24863;&#30693;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25506;&#32034;&#31526;&#21495;&#23383;&#31526;&#22312;&#25991;&#26412;&#21644;&#22270;&#34920;&#29702;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#26694;&#26550;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#26469;&#25552;&#39640;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20960;&#20309;&#38382;&#39064;&#20381;&#36182;&#20110;&#25991;&#26412;&#21644;&#22270;&#34920;&#65292;&#22240;&#27492;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25991;&#26412;&#25551;&#36848;&#20013;&#65292;&#31526;&#21495;&#23383;&#31526;&#65288;&#22914;&#8220;$\triangle$ABC&#8221;&#65289;&#36890;&#24120;&#20316;&#20026;&#36830;&#25509;&#30456;&#24212;&#22270;&#34920;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#23383;&#31526;&#20165;&#20165;&#20998;&#35299;&#20026;&#21333;&#20010;&#23383;&#27597;&#65288;&#22914;'A'&#12289;'B'&#21644;'C'&#65289;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#26126;&#30830;&#30740;&#31350;&#23427;&#20204;&#65292;&#20174;&#32780;&#22833;&#21435;&#20102;&#19982;&#22270;&#34920;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31526;&#21495;&#24863;&#30693;&#27169;&#22411;&#65292;&#20197;&#23436;&#20840;&#25506;&#32034;&#36825;&#20123;&#23383;&#31526;&#22312;&#25991;&#26412;&#21644;&#22270;&#34920;&#29702;&#35299;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#26694;&#26550;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#25991;&#26412;&#32534;&#30721;&#22120;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20010;&#21035;&#31526;&#21495;&#23383;&#31526;&#19982;&#30456;&#24212;&#22270;&#34920;&#30340;&#20960;&#20309;&#20449;&#24687;&#21512;&#24182;&#24418;&#25104;&#19968;&#20010;&#35821;&#20041;&#21333;&#20803;&#12290;&#23545;&#20110;&#22270;&#34920;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#23383;&#31526;&#20316;&#20026;&#26631;&#31614;&#65292;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;&#19979;&#23545;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20854;&#22312;&#25991;&#26412;&#21644;&#22270;&#34920;&#20043;&#38388;&#36827;&#34892;&#20960;&#20309;&#19968;&#33268;&#24615;&#32422;&#26463;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI has made significant progress in solving math problems, but geometry problems remain challenging due to their reliance on both text and diagrams. In the text description, symbolic characters such as "$\triangle$ABC" often serve as a bridge to connect the corresponding diagram. However, by simply tokenizing symbolic characters into individual letters (e.g., 'A', 'B' and 'C'), existing works fail to study them explicitly and thus lose the semantic relationship with the diagram. In this paper, we develop a symbolic character-aware model to fully explore the role of these characters in both text and diagram understanding and optimize the model under a multi-modal reasoning framework. In the text encoder, we propose merging individual symbolic characters to form one semantic unit along with geometric information from the corresponding diagram. For the diagram encoder, we pre-train it under a multi-label classification framework with the symbolic characters as labels. In addition, we enha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#23494;&#24230;&#23450;&#20041;&#21644;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#39592;&#20687;&#32032;&#20998;&#24067;&#26469;&#27979;&#37327;&#39592;&#23494;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25104;&#24180;&#26399;39&#23681;&#21040;80&#23681;&#20043;&#38388;&#65292;&#39592;&#23494;&#24230;&#21576;&#32447;&#24615;&#19979;&#38477;&#65292;&#22899;&#24615;&#19979;&#38477;&#36895;&#24230;&#22823;&#32422;&#26159;&#30007;&#24615;&#30340;1.6&#20493;&#65292;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#20154;&#20307;&#34928;&#32769;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.02815</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#30340;&#39592;&#23494;&#24230;&#23450;&#20041;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#36827;&#34892;&#20154;&#39592;&#23494;&#24230;&#38543;&#24180;&#40836;&#21464;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The changing rule of human bone density with aging based on a novel definition and mensuration of bone density with computed tomography. (arXiv:2308.02815v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39592;&#23494;&#24230;&#23450;&#20041;&#21644;&#27979;&#37327;&#26041;&#27861;&#65292;&#21033;&#29992;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#39592;&#20687;&#32032;&#20998;&#24067;&#26469;&#27979;&#37327;&#39592;&#23494;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25104;&#24180;&#26399;39&#23681;&#21040;80&#23681;&#20043;&#38388;&#65292;&#39592;&#23494;&#24230;&#21576;&#32447;&#24615;&#19979;&#38477;&#65292;&#22899;&#24615;&#19979;&#38477;&#36895;&#24230;&#22823;&#32422;&#26159;&#30007;&#24615;&#30340;1.6&#20493;&#65292;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#20102;&#35299;&#20154;&#20307;&#34928;&#32769;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#21475;&#32769;&#40836;&#21270;&#65292;&#39592;&#36136;&#30095;&#26494;&#30151;&#21644;&#26131;&#30862;&#24615;&#39592;&#25240;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21452;&#33021;X&#23556;&#32447;&#21560;&#25910;&#27861;&#27979;&#37327;&#39592;&#23494;&#24230;&#22312;&#20010;&#24615;&#21270;&#39118;&#38505;&#35780;&#20272;&#26041;&#38754;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24178;&#25200;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32454;&#20998;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#30340;&#39592;&#20687;&#32032;&#20998;&#24067;&#26469;&#27979;&#37327;&#39592;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;39&#23681;&#21040;80&#23681;&#20043;&#38388;&#30340;&#25104;&#24180;&#26399;&#65292;&#39592;&#23494;&#24230;&#21576;&#32447;&#24615;&#19979;&#38477;&#65292;&#22899;&#24615;&#19979;&#38477;&#36895;&#24230;&#22823;&#32422;&#26159;&#30007;&#24615;&#30340;1.6&#20493;&#12290;&#36825;&#19982;&#24191;&#27867;&#25509;&#21463;&#30340;&#35266;&#28857;&#30456;&#30683;&#30462;&#65292;&#21363;&#39592;&#23494;&#24230;&#22312;&#22899;&#24615;&#32477;&#32463;&#26399;&#24320;&#22987;&#19979;&#38477;&#65292;&#22312;&#30007;&#24615;&#22823;&#32422;50&#23681;&#26102;&#24320;&#22987;&#19979;&#38477;&#12290;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#30340;&#32447;&#24615;&#24615;&#25552;&#20379;&#20102;&#23545;&#20154;&#20307;&#32769;&#21270;&#21160;&#24577;&#30340;&#36827;&#19968;&#27493;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Osteoporosis and fragility fractures have emerged as major public health concerns in an aging population. However, measuring age-related changes in bone density using dual-energy X-ray absorptiometry has limited personalized risk assessment due to susceptibility to interference from various factors. In this study, we propose an innovative statistical model of bone pixel distribution in fine-segmented computed tomography (CT) images, along with a novel approach to measuring bone density based on CT values of bone pixels. Our findings indicate that bone density exhibits a linear decline with age during adulthood between the ages of 39 and 80, with the rate of decline being approximately 1.6 times faster in women than in men. This contradicts the widely accepted notion that bone density starts declining in women at menopause and in men at around 50 years of age. The linearity of age-related changes provides further insights into the dynamics of the aging human body. Consequently, our find
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#20998;&#23376;&#36890;&#20449;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24494;&#22411;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#21307;&#23398;&#24212;&#29992;&#28508;&#21147;&#12290;&#20998;&#23376;&#36890;&#20449;&#36890;&#36807;&#36716;&#21270;&#25968;&#23383;&#20449;&#21495;&#20026;&#20998;&#23376;&#27987;&#24230;&#26469;&#36827;&#34892;&#65292;&#35299;&#35843;&#20449;&#21495;&#26102;&#38754;&#20020;&#30528;&#31934;&#30830;&#24314;&#27169;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02812</link><description>&lt;p&gt;
&#20998;&#23376;&#36890;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Molecular Communication. (arXiv:2308.02812v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02812
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#20998;&#23376;&#36890;&#20449;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24494;&#22411;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#38420;&#30340;&#21307;&#23398;&#24212;&#29992;&#28508;&#21147;&#12290;&#20998;&#23376;&#36890;&#20449;&#36890;&#36807;&#36716;&#21270;&#25968;&#23383;&#20449;&#21495;&#20026;&#20998;&#23376;&#27987;&#24230;&#26469;&#36827;&#34892;&#65292;&#35299;&#35843;&#20449;&#21495;&#26102;&#38754;&#20020;&#30528;&#31934;&#30830;&#24314;&#27169;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#36890;&#20449;&#26159;&#19968;&#31181;&#22312;&#24494;&#22411;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38656;&#35201;&#36991;&#20813;&#30005;&#20449;&#21495;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#36890;&#20449;&#26159;&#36890;&#36807;&#22312;&#32435;&#31859;&#32423;&#21035;&#19978;&#21457;&#36865;&#20998;&#23376;&#65288;&#25110;&#20854;&#20182;&#31890;&#23376;&#65289;&#32780;&#19981;&#26159;&#36890;&#36807;&#30005;&#32447;&#21457;&#36865;&#30005;&#23376;&#26469;&#23454;&#29616;&#30340;&#12290;&#20998;&#23376;&#36890;&#20449;&#35774;&#22791;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20026;&#23610;&#23544;&#12289;&#28201;&#24230;&#25110;&#36752;&#23556;&#32422;&#26463;&#19979;&#21487;&#33021;&#19981;&#36866;&#29992;&#30340;&#22825;&#32447;&#20256;&#36755;&#31995;&#32479;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#26696;&#12290;&#36890;&#20449;&#36890;&#36807;&#23558;&#25968;&#23383;&#20449;&#21495;&#36716;&#21270;&#20026;&#20998;&#23376;&#27987;&#24230;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#22312;&#36890;&#20449;&#20449;&#36947;&#30340;&#21478;&#19968;&#31471;&#26816;&#27979;&#21040;&#36825;&#20123;&#20998;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#22238;&#25968;&#23383;&#20449;&#21495;&#12290;&#30001;&#20110;&#20256;&#36755;&#20449;&#36947;&#30340;&#31934;&#30830;&#24314;&#27169;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#25110;&#20449;&#36947;&#30340;&#26102;&#21464;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;&#20329;&#25140;&#21307;&#30103;&#35774;&#22791;&#30340;&#20154;&#21592;&#30340;&#36816;&#21160;&#65289;&#25152;&#33268;&#12290;&#36825;&#20351;&#24471;&#20449;&#21495;&#30340;&#35299;&#35843;&#38750;&#24120;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular communication is a novel approach for data transmission between miniaturized devices, especially in contexts where electrical signals are to be avoided. The communication is based on sending molecules (or other particles) at nano scale through channel instead sending electrons over a wire. Molecular communication devices have a large potential in medical applications as they offer an alternative to antenna-based transmission systems that may not be applicable due to size, temperature, or radiation constraints. The communication is achieved by transforming a digital signal into concentrations of molecules. These molecules are then detected at the other end of the communication channel and transformed back into a digital signal. Accurately modeling the transmission channel is often not possible which may be due to a lack of data or time-varying parameters of the channel (e. g., the movements of a person wearing a medical device). This makes demodulation of the signal very diffi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#30340;Chimney&#28779;&#28798;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#36830;&#36143;&#19988;&#32467;&#26500;&#33391;&#22909;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02810</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#39044;&#27979;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A generative model for surrogates of spatial-temporal wildfire nowcasting. (arXiv:2308.02810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#30340;Chimney&#28779;&#28798;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#36830;&#36143;&#19988;&#32467;&#26500;&#33391;&#22909;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20840;&#29699;&#37326;&#28779;&#25968;&#37327;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#23545;&#23454;&#26102;&#28779;&#21183;&#39044;&#27979;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#22914;&#20803;&#32990;&#33258;&#21160;&#26426;&#21644;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#28779;&#21183;&#20256;&#25773;&#27169;&#25311;&#65292;&#20294;&#32791;&#26102;&#19988;&#35745;&#31639;&#22797;&#26434;&#12290;&#30446;&#21069;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28779;&#21183;&#39044;&#27979;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#29305;&#23450;&#20110;&#26576;&#20010;&#22320;&#21306;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#19981;&#21516;&#29983;&#24577;&#21306;&#22495;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#19977;&#32500;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#29983;&#25104;&#32473;&#23450;&#29983;&#24577;&#21306;&#22495;&#20869;&#26410;&#35265;&#30340;&#31354;&#38388;-&#26102;&#38388;&#37326;&#28779;&#29123;&#28903;&#21306;&#22495;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#22312;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#26368;&#36817;&#30340;&#19968;&#27425;&#22823;&#35268;&#27169;&#37326;&#28779;&#20107;&#20214; - Chimney&#28779;&#28798;&#30340;&#29983;&#24577;&#21306;&#22495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#29983;&#25104;&#20102;&#36830;&#36143;&#19988;&#26377;&#32467;&#26500;&#30340;&#28779;&#21183;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, ta
&lt;/p&gt;</description></item><item><title>MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02804</link><description>&lt;p&gt;
MiAMix: &#36890;&#36807;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixied Sample Data Augmentation Method. (arXiv:2308.02804v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02804
&lt;/p&gt;
&lt;p&gt;
MiAMix&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#24182;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#36807;&#25311;&#21512;&#20381;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#25968;&#25454;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#21313;&#20998;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#31574;&#30053;&#65292;&#20294;&#28151;&#21512;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#65288;MSDA&#65289;&#22312;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;MiAMix&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#22810;&#38454;&#27573;&#22686;&#24378;&#28151;&#21512;&#12290;MiAMix&#23558;&#22270;&#20687;&#22686;&#24378;&#38598;&#25104;&#21040;&#28151;&#21512;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#22810;&#26679;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#28151;&#21512;&#25513;&#27169;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#36827;&#28151;&#21512;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26174;&#33879;&#24615;&#20449;&#24687;&#65292;&#32780;MiAMix&#30340;&#35774;&#35745;&#20063;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20943;&#23569;&#20102;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#35757;&#32451;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#23545;MiAMix&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#22235;&#20010;&#22270;&#20687;&#22522;&#20934;&#21644;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;CMT&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20063;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.02793</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crowdsourcing Fraud Detection over Heterogeneous Temporal MMMA Graph. (arXiv:2308.02793v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;CMT&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#26102;&#38388;MMMA&#22270;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;&#22270;&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20063;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#20892;&#22330;&#19994;&#21153;&#30340;&#20852;&#36215;&#21033;&#29992;&#22810;&#29992;&#36884;&#28040;&#24687;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65288;MMMA&#65289;&#35825;&#20351;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#65292;&#32473;&#28857;&#20987;&#20892;&#22330;&#24037;&#20154;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMT&#30340;&#26032;&#22411;&#23545;&#27604;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MMMA&#30340;&#24322;&#26500;&#26102;&#38388;&#22270;&#65288;HTG&#65289;&#19978;&#36827;&#34892;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#12290;CMT&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#25429;&#25417;HTG&#30340;&#24322;&#36136;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#20247;&#21253;&#27450;&#35784;&#26816;&#27979;&#30340;&#39640;&#36136;&#37327;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;CMT&#22312;&#20195;&#34920;&#24615;MMMA&#24494;&#20449;&#30340;&#34892;&#19994;&#35268;&#27169;HTG&#19978;&#26816;&#27979;&#20247;&#21253;&#27450;&#35784;&#65292;&#24182;&#19988;&#20854;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;CMT&#36824;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#20844;&#20849;&#37329;&#34701;HTG&#19978;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#22270;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of the click farm business using Multi-purpose Messaging Mobile Apps (MMMAs) tempts cybercriminals to perpetrate crowdsourcing frauds that cause financial losses to click farm workers. In this paper, we propose a novel contrastive multi-view learning method named CMT for crowdsourcing fraud detection over the heterogeneous temporal graph (HTG) of MMMA. CMT captures both heterogeneity and dynamics of HTG and generates high-quality representations for crowdsourcing fraud detection in a self-supervised manner. We deploy CMT to detect crowdsourcing frauds on an industry-size HTG of a representative MMMA WeChat and it significantly outperforms other methods. CMT also shows promising results for fraud detection on a large-scale public financial HTG, indicating that it can be applied in other graph anomaly detection tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#27969;&#20013;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35299;&#20915;&#19981;&#21516;&#32500;&#24230;&#38382;&#39064;&#21644;&#20855;&#26377;&#19981;&#21516;&#35201;&#27714;&#30340;&#31665;&#23376;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20197;&#29289;&#27969;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Logistic-Oriented Bin Packing Problems Through a Hybrid Quantum-Classical Approach. (arXiv:2308.02787v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#27969;&#20013;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#25193;&#23637;&#20102;&#35299;&#20915;&#19981;&#21516;&#32500;&#24230;&#38382;&#39064;&#21644;&#20855;&#26377;&#19981;&#21516;&#35201;&#27714;&#30340;&#31665;&#23376;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35013;&#31665;&#38382;&#39064;&#26159;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24037;&#19994;&#24212;&#29992;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#20107;&#23454;&#19978;&#65292;&#23558;&#29289;&#21697;&#39640;&#25928;&#22320;&#35013;&#31665;&#26159;&#35768;&#22810;&#29289;&#27969;&#20844;&#21496;&#38754;&#20020;&#30340;&#26368;&#33392;&#24040;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20063;&#26159;&#38477;&#20302;&#23384;&#20648;&#25104;&#26412;&#25110;&#25913;&#21892;&#36710;&#36742;&#31354;&#38388;&#20998;&#37197;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#25105;&#20204;&#20808;&#21069;&#21457;&#34920;&#30340;&#37327;&#23376;-&#32463;&#20856;&#26694;&#26550;Q4RealBPP&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#35299;&#20915;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#29305;&#28857;&#65306;i&#65289;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31665;&#23376;&#65292;ii&#65289;&#25193;&#23637;&#26694;&#26550;&#20197;&#35299;&#20915;&#19977;&#32500;&#12289;&#20108;&#32500;&#21644;&#19968;&#32500;&#38382;&#39064;&#65292;iii&#65289;&#29289;&#21697;&#19982;&#31665;&#23376;&#20851;&#32852;&#30340;&#35201;&#27714;&#65292;iv&#65289;&#20132;&#20184;&#20248;&#20808;&#32423;&#12290;&#26412;&#25991;&#23545;&#25152;&#26377;&#36825;&#20123;&#29305;&#28857;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#21450;Q4RealBPP&#35299;&#20915;&#20197;&#23454;&#38469;&#20026;&#23548;&#21521;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bin Packing Problem is a classic problem with wide industrial applicability. In fact, the efficient packing of items into bins is one of the toughest challenges in many logistic corporations and is a critical issue for reducing storage costs or improving vehicle space allocation. In this work, we resort to our previously published quantum-classical framework known as Q4RealBPP, and elaborate on the solving of real-world oriented instances of the Bin Packing Problem. With this purpose, this paper gravitates on the following characteristics: i) the existence of heterogeneous bins, ii) the extension of the framework to solve not only three-dimensional, but also one- and two-dimensional instances of the problem, iii) requirements for item-bin associations, and iv) delivery priorities. All these features have been tested in this paper, as well as the ability of Q4RealBPP to solve real-world oriented instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#23547;&#25214;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#26410;&#35265;&#38754;&#37096;&#22270;&#20687;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.02784</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Contrastive Regression for Estimation of Eye Gaze. (arXiv:2308.02784v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23567;&#35268;&#27169;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#23547;&#25214;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#26410;&#35265;&#38754;&#37096;&#22270;&#20687;&#19978;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#23545;&#20154;&#26426;&#30028;&#38754;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#24320;&#21457;&#30524;&#29699;&#25511;&#21046;&#31995;&#32479;&#24050;&#25104;&#20026;&#24517;&#35201;&#12290;&#30524;&#29699;&#27880;&#35270;&#20316;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#65292;&#26159;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22522;&#20110;&#22806;&#35266;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#30524;&#29699;&#27880;&#35270;&#20272;&#35745;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#23436;&#20840;&#21463;&#21040;&#26631;&#35760;&#30340;&#30524;&#29699;&#27880;&#35270;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#20250;&#24433;&#21709;&#24615;&#33021;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;&#30524;&#29699;&#27880;&#35270;&#26041;&#21521;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#26631;&#35760;&#30340;&#30524;&#29699;&#27880;&#35270;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#27867;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#26159;&#30475;&#19981;&#35265;&#30340;&#20154;&#33080;&#22270;&#20687;&#20063;&#33021;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#33539;&#24335;&#65292;&#26082;&#26368;&#22823;&#21270;&#20102;&#30456;&#20284;&#22270;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#23884;&#20837;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#12290;&#25105;&#20204;&#30340;&#23545;&#27604;&#22238;&#24402;&#26694;&#26550;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20248;&#20110;&#20854;&#20182;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110; ORC &#36229;&#28909;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#12289;&#21487;&#34892;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.02765</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;ORC&#36229;&#28909;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Surrogate Empowered Sim2Real Transfer of Deep Reinforcement Learning for ORC Superheat Control. (arXiv:2308.02765v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110; ORC &#36229;&#28909;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#12289;&#21487;&#34892;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26426;&#26391;&#32943;&#24490;&#29615;(ORC)&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#21644;&#26131;&#20110;&#32500;&#25252;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#20313;&#28909;&#22238;&#25910;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36807;&#31243;&#24037;&#19994;&#30340;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#19979;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#26080;&#27861;&#36866;&#24212;ORC&#31995;&#32479;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#24037;&#20917;&#25110;&#31361;&#21457;&#24037;&#20316;&#27169;&#24335;&#30340;&#25913;&#21464;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#23427;&#36890;&#36807;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#30452;&#25509;&#23454;&#29616;&#25511;&#21046;&#30446;&#26631;&#65292;&#32780;&#26080;&#38656;&#23545;&#21463;&#25511;&#31995;&#32479;&#36827;&#34892;&#31934;&#30830;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;DRL&#24212;&#29992;&#20110;&#29289;&#29702;ORC&#31995;&#32479;&#23384;&#22312;&#19981;&#21487;&#25509;&#21463;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#19982;&#23454;&#38469;&#21463;&#25511;&#29615;&#22659;&#19981;&#21305;&#37197;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#20197;&#25903;&#25345;ORC&#25511;&#21046;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#25311;&#21040;&#30495;&#23454;&#36716;&#31227;&#23398;&#20064;&#30340;DRL&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;ORC&#36229;&#28909;&#25511;&#21046;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#12289;&#21487;&#34892;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Organic Rankine Cycle (ORC) is widely used in industrial waste heat recovery due to its simple structure and easy maintenance. However, in the context of smart manufacturing in the process industry, traditional model-based optimization control methods are unable to adapt to the varying operating conditions of the ORC system or sudden changes in operating modes. Deep reinforcement learning (DRL) has significant advantages in situations with uncertainty as it directly achieves control objectives by interacting with the environment without requiring an explicit model of the controlled plant. Nevertheless, direct application of DRL to physical ORC systems presents unacceptable safety risks, and its generalization performance under model-plant mismatch is insufficient to support ORC control requirements. Therefore, this paper proposes a Sim2Real transfer learning-based DRL control method for ORC superheat control, which aims to provide a new simple, feasible, and user-friendly solution 
&lt;/p&gt;</description></item><item><title>NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.02751</link><description>&lt;p&gt;
NeRFs: &#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
NeRFs: The Search for the Best 3D Representation. (arXiv:2308.02751v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02751
&lt;/p&gt;
&lt;p&gt;
NeRFs&#26159;&#35270;&#22270;&#21512;&#25104;&#21644;&#30456;&#20851;&#38382;&#39064;&#20013;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26597;&#35810;&#33719;&#21462;&#20307;&#31215;&#21442;&#25968;&#26469;&#25551;&#36848;&#36830;&#32493;&#20307;&#31215;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#25104;&#20026;&#35270;&#22270;&#21512;&#25104;&#25110;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#31561;&#38382;&#39064;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#20063;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;NeRFs&#36890;&#36807;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#35270;&#22270;&#30456;&#20851;&#36752;&#23556;&#21644;&#20307;&#31215;&#23494;&#24230;&#31561;&#20307;&#31215;&#21442;&#25968;&#65292;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#20307;&#31215;&#12290;&#35813;&#34920;&#31034;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#65292;&#27599;&#24180;&#26377;&#25968;&#21315;&#31687;&#35770;&#25991;&#22312;&#20854;&#22522;&#30784;&#19978;&#25193;&#23637;&#25110;&#30456;&#20851;&#30740;&#31350;&#65292;&#22810;&#20301;&#20316;&#32773;&#21644;&#32593;&#31449;&#25552;&#20379;&#27010;&#36848;&#21644;&#35843;&#30740;&#65292;&#24182;&#26377;&#20247;&#22810;&#24037;&#19994;&#24212;&#29992;&#21644;&#21019;&#19994;&#20844;&#21496;&#12290;&#26412;&#25991;&#31616;&#35201;&#22238;&#39038;&#20102;NeRFs&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#38271;&#36798;&#19977;&#21313;&#24180;&#30340;&#23547;&#25214;&#26368;&#20339;3D&#34920;&#31034;&#26041;&#27861;&#20197;&#21450;&#26368;&#32456;&#24341;&#20986;NeRFs&#35770;&#25991;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20498;&#31435;&#25670;&#25196;&#21319;&#22120;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#65292;&#35813;&#35774;&#35745;&#36866;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21644;&#20498;&#31435;&#25670;&#32452;&#21512;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02741</link><description>&lt;p&gt;
&#20498;&#31435;&#25670;&#25196;&#21319;&#22120;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Controller Design for a Quadrotor with Inverted Pendulum. (arXiv:2308.02741v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20498;&#31435;&#25670;&#25196;&#21319;&#22120;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#65292;&#35813;&#35774;&#35745;&#36866;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21644;&#20498;&#31435;&#25670;&#32452;&#21512;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#26159;&#19968;&#20010;&#20855;&#26377;&#20845;&#20010;&#33258;&#30001;&#24230;&#30340;&#27424;&#39537;&#21160;&#31995;&#32479;&#12290;&#22312;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#19978;&#22686;&#21152;&#19968;&#20010;&#29699;&#24418;&#25670;&#38180;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#22312;&#31283;&#23450;&#20854;&#20313;&#37096;&#20998;&#26102;&#23454;&#29616;&#20219;&#20309;&#36755;&#20986;&#36319;&#36394;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#21644;&#20498;&#31435;&#25670;&#32452;&#21512;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#19981;&#21516;&#31867;&#22411;&#25511;&#21046;&#22120;&#65292;&#21033;&#29992;&#21453;&#39304;&#32447;&#24615;&#21270;&#21644;&#20855;&#26377;&#20108;&#27425;&#35268;&#21010;&#30340;&#25511;&#21046;Lyapunov&#20989;&#25968;&#65288;CLF-QP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#29420;&#31435;&#24773;&#20917;&#19979;&#20197;&#21450;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;-&#20498;&#31435;&#25670;&#32452;&#21512;&#24773;&#20917;&#19979;&#30340;&#36712;&#36857;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quadrotor is a $6$ degrees-of-freedom (DoF) system with underactuation. Adding a spherical pendulum on top of a quadrotor further complicates the task of achieving any output tracking while stabilizing the rest. In this report, we present different types of controllers for the nonlinear dynamical system of quadrotor and pendulum combination, utilizing feedback-linearization and control Lyapunov function with quadratic programming (CLF-QP) approaches. We demonstrated trajectory tracking for quadrotor-only case as well as quadrotor-pendulum-combined case.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#24739;&#32773;&#20837;&#38498;&#30340;&#20303;&#38498;&#26102;&#38388;&#65292;&#24110;&#21161;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24739;&#32773;&#20837;&#38498;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#21270;&#39564;&#32467;&#26524;&#65292;&#21487;&#20197;&#30456;&#23545;&#20934;&#30830;&#22320;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.02730</link><description>&lt;p&gt;
&#35780;&#20272;&#24613;&#35786;&#30701;&#26399;&#20303;&#38498;&#37096;&#23545;&#20303;&#38498;&#26102;&#38388;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#21644;&#31163;&#25955;&#20107;&#20214;&#27169;&#25311;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing the impact of emergency department short stay units using length-of-stay prediction and discrete event simulation. (arXiv:2308.02730v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#36890;&#36807;&#39044;&#27979;&#24739;&#32773;&#20837;&#38498;&#30340;&#20303;&#38498;&#26102;&#38388;&#65292;&#24110;&#21161;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24739;&#32773;&#20837;&#38498;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#21270;&#39564;&#32467;&#26524;&#65292;&#21487;&#20197;&#30456;&#23545;&#20934;&#30830;&#22320;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#20837;&#38498;&#26102;&#30340;&#20303;&#38498;&#26102;&#38388;&#21487;&#20197;&#24110;&#21161;&#25351;&#23548;&#20020;&#24202;&#20915;&#31574;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#39044;&#27979;&#20174;&#24613;&#35786;&#31185;&#36716;&#20837;&#20869;&#31185;&#20303;&#38498;&#30340;&#24739;&#32773;&#30340;&#20303;&#38498;&#26102;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#26469;&#30830;&#23450;&#20135;&#29983;&#26368;&#20339;&#39044;&#27979;&#24615;&#33021;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31163;&#25955;&#20107;&#20214;&#27169;&#25311;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25512;&#33616;&#24615;&#33021;&#36890;&#24120;&#21487;&#25509;&#21463;&#65292;&#24182;&#19988;&#19981;&#21463;&#29305;&#24449;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#24739;&#32773;&#20837;&#38498;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#21270;&#39564;&#32467;&#26524;&#30456;&#23545;&#20934;&#30830;&#22320;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;&#65288;&#22914;&#65292;&#29992;&#20110;&#20998;&#31867;&#30701;&#26399;&#20303;&#38498;&#21644;&#38271;&#26399;&#20303;&#38498;&#24739;&#32773;&#30340;AUC&#20540;&#20026;0.69&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting hospital length-of-stay at the time a patient is admitted to hospital may help guide clinical decision making and resource allocation. In this study we aim to build a decision support system that predicts hospital length-of-stay for patients admitted to general internal medicine from the emergency department. We conduct an exploratory data analysis and employ feature selection methods to identify the attributes that result in the best predictive performance. We also develop a discrete-event simulation model to assess the performances of the prediction models in a practical setting. Our results show that the recommendation performances of the proposed approaches are generally acceptable and do not benefit from the feature selection. Further, the results indicate that hospital length-of-stay could be predicted with reasonable accuracy (e.g., AUC value for classifying short and long stay patients is 0.69) using patient admission demographics, laboratory test results,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#29305;&#24449;&#21644;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20462;&#25913;&#21253;&#25324;&#22686;&#24378;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#21644;&#35774;&#35745;&#21487;&#38450;&#27490;&#39044;&#27979;&#26497;&#30701;&#29255;&#27573;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#23545;&#20110;&#25552;&#39640;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#25928;&#26524;&#26377;&#23454;&#38469;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.02723</link><description>&lt;p&gt;
&#20026;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#25913;&#36827;&#35856;&#27874;&#25935;&#24863;&#24615;&#21644;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction. (arXiv:2308.02723v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20462;&#25913;&#36755;&#20837;&#29305;&#24449;&#21644;&#35757;&#32451;&#30446;&#26631;&#26469;&#25913;&#36827;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#20462;&#25913;&#21253;&#25324;&#22686;&#24378;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#21644;&#35774;&#35745;&#21487;&#38450;&#27490;&#39044;&#27979;&#26497;&#30701;&#29255;&#27573;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#20462;&#25913;&#23545;&#20110;&#25552;&#39640;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#30340;&#25928;&#26524;&#26377;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#35768;&#22810;&#26059;&#24459;&#25552;&#21462;&#27169;&#22411;&#20381;&#36182;&#20110;&#37325;&#26032;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#20551;&#35774;&#30340;&#36755;&#20837;&#29305;&#24449;&#20462;&#25913;&#21644;&#35757;&#32451;&#30446;&#26631;&#20462;&#25913;&#12290;&#39318;&#20808;&#65292;&#38899;&#39057;&#25968;&#25454;&#30340;&#39057;&#35889;&#22270;&#20013;&#30340;&#35856;&#27874;&#22312;&#39057;&#29575;&#36724;&#19978;&#36805;&#36895;&#34928;&#20943;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#23545;&#23614;&#37096;&#35856;&#27874;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;z-transform&#20462;&#25913;&#20102;&#32852;&#21512;&#39057;&#29575;&#21644;&#21608;&#26399;&#24615;(CFP)&#34920;&#31034;&#12290;&#20854;&#27425;&#65292;&#26497;&#30701;&#26102;&#38271;&#30340;&#20154;&#22768;&#21644;&#38750;&#20154;&#22768;&#29255;&#27573;&#24182;&#19981;&#24120;&#35265;&#12290;&#20026;&#20102;&#30830;&#20445;&#26356;&#31283;&#23450;&#30340;&#26059;&#24459;&#36718;&#24275;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#38450;&#27490;&#27169;&#22411;&#39044;&#27979;&#36825;&#20123;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#20462;&#25913;&#24212;&#29992;&#20110;&#22810;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;MSNet&#12289;FTANet&#21644;&#26032;&#24341;&#20837;&#30340;&#27169;&#22411;PianoNet&#65292;&#35813;&#27169;&#22411;&#26159;&#20174;&#38050;&#29748;&#36716;&#24405;&#32593;&#32476;&#25913;&#32534;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#20462;&#25913;&#22312;&#27468;&#21809;&#26059;&#24459;&#25552;&#21462;&#19978;&#20855;&#26377;&#23454;&#35777;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extrac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.02668</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02668
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#30340;&#24341;&#23548;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#21644;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#23548;&#24072;&#27169;&#22411;&#25351;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#20027;&#23548;&#33539;&#24335;&#26159;&#20381;&#36182;&#20110;&#23436;&#20840;&#24102;&#27880;&#37322;&#30340;&#35757;&#32451;&#22270;&#20687;&#65292;&#36825;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20381;&#36182;&#24182;&#25552;&#39640;&#32467;&#26524;&#65292;&#21322;&#30417;&#30563;&#26041;&#27861;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20197;&#38480;&#21046;&#23545;&#26631;&#35760;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#39062;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#26174;&#33879;&#25913;&#36827;&#24072;&#29983;&#33976;&#39311;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;(i)&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#8220;&#24341;&#23548;&#39044;&#28903;&#8221;&#38454;&#27573;&#25913;&#36827;&#20102;&#33976;&#39311;&#26041;&#27861;&#65292;(ii)&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#23454;&#20363;&#20998;&#21106;&#26550;&#26500;&#12289;&#20027;&#24178;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#19982;&#20043;&#21069;&#21482;&#20351;&#29992;&#30417;&#30563;&#25968;&#25454;&#26469;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#39044;&#28903;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#23548;&#24072;&#27169;&#22411;&#30340;&#25351;&#23548;&#22312;&#39044;&#28903;&#38454;&#27573;&#20013;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25913;&#36827;&#30340;&#33976;&#39311;&#26041;&#27861;&#22312;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillation approach by introducing a novel "guided burn-in" stage, and (ii) evaluate different instance segmentation architectures, as well as backbone networks and pre-training strategies. Contrary to previous work which uses only supervised data for the burn-in period of the student model, we also use guidance of the teacher model to exploit unlabeled data in the burn-in period. Our improved distillation approach leads to substantial improvements over previous state-of-the-art results. For example, on 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#21160;&#23398;&#20064;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#35859;&#35789;&#21152;&#36895;&#25628;&#32034;&#65292;&#35299;&#20915;&#12298;&#35265;&#35777;&#20154;&#12299;&#28216;&#25103;&#20013;&#30340;&#35868;&#39064;&#23454;&#20363;&#65292;&#27604;&#22522;&#20934;&#25628;&#32034;&#25552;&#21319;&#20845;&#20493;&#36895;&#24230;&#65292;&#19988;&#33021;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#35868;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02666</link><description>&lt;p&gt;
&#29992;&#33258;&#21160;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#20154;&#31867;&#35859;&#35789;&#26356;&#24555;&#35299;&#20915;&#35265;&#35777;&#22411;&#19977;&#35282;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Witness-type Triangle Puzzles Faster with an Automatically Learned Human-Explainable Predicate. (arXiv:2308.02666v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02666
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#23398;&#20064;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#35859;&#35789;&#21152;&#36895;&#25628;&#32034;&#65292;&#35299;&#20915;&#12298;&#35265;&#35777;&#20154;&#12299;&#28216;&#25103;&#20013;&#30340;&#35868;&#39064;&#23454;&#20363;&#65292;&#27604;&#22522;&#20934;&#25628;&#32034;&#25552;&#21319;&#20845;&#20493;&#36895;&#24230;&#65292;&#19988;&#33021;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35299;&#20915;&#28216;&#25103;&#12298;&#35265;&#35777;&#20154;&#12299;&#20013;&#30340;&#35868;&#39064;&#23454;&#20363;&#21487;&#20197;&#25351;&#23548;&#29609;&#23478;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#24110;&#21161;&#35868;&#39064;&#35774;&#35745;&#32773;&#29983;&#25104;&#26356;&#22909;&#30340;&#35868;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35868;&#39064;&#22312;&#32452;&#21512;&#19978;&#24456;&#38590;&#65292;&#24182;&#19988;&#22522;&#20110;&#25628;&#32034;&#30340;&#27714;&#35299;&#22120;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20154;&#31867;&#35859;&#35789;&#65292;&#21152;&#36895;&#25628;&#32034;&#65292;&#39044;&#27979;&#19981;&#21487;&#23436;&#25104;&#21040;&#35299;&#20915;&#36335;&#24452;&#30340;&#37096;&#20998;&#36335;&#24452;&#65292;&#20174;&#32780;&#25552;&#21319;&#25628;&#32034;&#25928;&#29575;&#12290;&#30456;&#36739;&#20110;&#22522;&#20934;&#25628;&#32034;&#65292;&#24179;&#22343;&#21152;&#36895;&#20845;&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#27599;&#20010;&#35868;&#39064;&#30340;&#22266;&#23450;&#25628;&#32034;&#26102;&#38388;&#39044;&#31639;&#65292;&#20351;&#29992;&#35859;&#35789;&#21152;&#36895;&#30340;&#25628;&#32034;&#33021;&#22815;&#35299;&#20915;&#26356;&#22823;&#35268;&#27169;&#30340;&#35868;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically solving puzzle instances in the game The Witness can guide players toward solutions and help puzzle designers generate better puzzles. In the latter case such an Artificial Intelligence puzzle solver can inform a human puzzle designer and procedural puzzle generator to produce better instances. The puzzles, however, are combinatorially difficult and search-based solvers can require large amounts of time and memory. We accelerate such search by automatically learning a human-explainable predicate that predicts whether a partial path to a Witness-type puzzle is not completable to a solution path. We prove a key property of the learned predicate which allows us to use it for pruning successor states in search thereby accelerating search by an average of six times while maintaining completeness of the underlying search. Conversely given a fixed search time budget per puzzle our predicate-accelerated search can solve more puzzle instances of larger sizes than the baseline sear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#24320;&#21457;&#34394;&#25311;&#29615;&#22659;&#20013;&#23545;&#35805;&#20195;&#29702;&#30340;&#24320;&#28304;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#21644;&#27785;&#28024;&#24335;&#20132;&#20114;&#25552;&#21319;&#19982;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2308.02665</link><description>&lt;p&gt;
&#22312;&#34394;&#25311;&#29616;&#23454;&#20013;&#35753;&#23545;&#35805;&#20195;&#29702;&#21457;&#20986;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
Let's Give a Voice to Conversational Agents in Virtual Reality. (arXiv:2308.02665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#24320;&#21457;&#34394;&#25311;&#29615;&#22659;&#20013;&#23545;&#35805;&#20195;&#29702;&#30340;&#24320;&#28304;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#21644;&#27785;&#28024;&#24335;&#20132;&#20114;&#25552;&#21319;&#19982;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#20307;&#39564;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#20013;&#30340;&#22810;&#27169;&#24577;&#21644;&#27785;&#28024;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#19982;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#26550;&#26500;&#65292;&#26088;&#22312;&#31616;&#21270;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#36816;&#34892;&#23545;&#35805;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#35813;&#26550;&#26500;&#25552;&#20379;&#20102;&#23558;&#19981;&#21516;&#39046;&#22495;&#30340;&#23545;&#35805;&#20195;&#29702;&#25554;&#20837;&#21644;&#28155;&#21152;&#33258;&#23450;&#20041;&#25110;&#22522;&#20110;&#20113;&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#22522;&#20110;&#35821;&#38899;&#30340;&#20132;&#20114;&#30340;&#21487;&#33021;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#26550;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#24320;&#21457;&#30340;&#20004;&#20010;&#23545;&#35805;&#21407;&#22411;&#65292;&#29992;&#20110;&#38750;&#27785;&#28024;&#24335;&#26174;&#31034;&#21644;&#34394;&#25311;&#29616;&#23454;&#22836;&#30420;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dialogue experience with conversational agents can be greatly enhanced with multimodal and immersive interactions in virtual reality. In this work, we present an open-source architecture with the goal of simplifying the development of conversational agents operating in virtual environments. The architecture offers the possibility of plugging in conversational agents of different domains and adding custom or cloud-based Speech-To-Text and Text-To-Speech models to make the interaction voice-based. Using this architecture, we present two conversational prototypes operating in the digital health domain developed in Unity for both non-immersive displays and VR headsets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#20840;&#29699;&#27969;&#26143;&#38632;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20114;&#21160;&#30340;&#32593;&#39029;&#24179;&#21488;&#65292;&#35753;&#20844;&#20247;&#21442;&#19982;&#27969;&#26143;&#25506;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27969;&#26143;&#38632;&#30340;&#21457;&#29616;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02664</link><description>&lt;p&gt;
&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#22788;&#29702;&#21644;&#21457;&#29616;&#32676;&#20307;&#22806;&#21253;&#29992;&#20110;&#27969;&#26143;&#38632;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
AI-Enhanced Data Processing and Discovery Crowd Sourcing for Meteor Shower Mapping. (arXiv:2308.02664v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22686;&#24378;&#22411;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#20840;&#29699;&#27969;&#26143;&#38632;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#20114;&#21160;&#30340;&#32593;&#39029;&#24179;&#21488;&#65292;&#35753;&#20844;&#20247;&#21442;&#19982;&#27969;&#26143;&#25506;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27969;&#26143;&#38632;&#30340;&#21457;&#29616;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;NASA&#33258;2010&#24180;&#24320;&#22987;&#36164;&#21161;&#30340;&#20840;&#29699;&#20840;&#22825;&#20505;&#27969;&#26143;&#35266;&#27979;(CAMS)&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23545;&#26469;&#33258;16&#20010;&#22269;&#23478;&#12289;&#21271;&#21322;&#29699;&#21644;&#21335;&#21322;&#29699;&#30340;&#22810;&#22320;&#20302;&#20809;&#35270;&#39057;&#25668;&#20687;&#26426;&#25506;&#27979;&#21040;&#30340;&#27969;&#26143;&#36712;&#36857;&#36827;&#34892;&#19977;&#35282;&#27979;&#37327;&#65292;&#21046;&#22270;&#20840;&#29699;&#27969;&#26143;&#38632;&#12290;&#35813;&#39033;&#30446;&#30340;&#20351;&#21629;&#26159;&#39564;&#35777;&#12289;&#21457;&#29616;&#24182;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#26143;&#38632;&#38477;&#20020;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#26045;&#22522;&#20110;&#20113;&#30340;&#33258;&#21160;&#21270;AI&#27969;&#31243;&#21644;&#25913;&#21892;&#25968;&#25454;&#21487;&#35270;&#21270;&#65292;&#23558;&#20844;&#20247;&#21442;&#19982;&#21040;&#30417;&#27979;&#27969;&#26143;&#25506;&#27979;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#21457;&#29616;&#29575;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#27963;&#21160;&#23398;&#20064;&#21644;AI&#27969;&#31243;&#33258;&#21160;&#21270;&#36827;&#34892;&#25968;&#25454;&#25668;&#21462;&#12289;&#22788;&#29702;&#21644;&#27934;&#23519;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#20114;&#21160;&#30340;Web&#38376;&#25143;&#65288;NASA&#27969;&#26143;&#38632;&#38376;&#25143;&#65289;&#65292;&#29992;&#20110;&#20419;&#36827;&#27969;&#26143;&#28304;&#22320;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;&#30446;&#21069;&#65292;CAMS&#24050;&#32463;&#21457;&#29616;&#20102;&#36229;&#36807;200&#20010;&#26032;&#30340;&#27969;&#26143;&#38632;
&lt;/p&gt;
&lt;p&gt;
The Cameras for Allsky Meteor Surveillance (CAMS) project, funded by NASA starting in 2010, aims to map our meteor showers by triangulating meteor trajectories detected in low-light video cameras from multiple locations across 16 countries in both the northern and southern hemispheres. Its mission is to validate, discover, and predict the upcoming returns of meteor showers. Our research aimed to streamline the data processing by implementing an automated cloud-based AI-enabled pipeline and improve the data visualization to improve the rate of discoveries by involving the public in monitoring the meteor detections. This article describes the process of automating the data ingestion, processing, and insight generation using an interpretable Active Learning and AI pipeline. This work also describes the development of an interactive web portal (the NASA Meteor Shower portal) to facilitate the visualization of meteor radiant maps. To date, CAMS has discovered over 200 new meteor showers and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02632</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks. (arXiv:2308.02632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#27627;&#31859;&#27874;&#38647;&#36798;&#25968;&#25454;&#30340;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#24320;&#21457;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#27169;&#25311;FMCW&#38647;&#36798;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22522;&#20110;&#23556;&#32447;&#36861;&#36394;&#65292;&#36890;&#24120;&#35745;&#31639;&#23494;&#38598;&#19988;&#19981;&#33021;&#32771;&#34385;&#32972;&#26223;&#22122;&#22768;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;FMCW&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#21512;&#25104;&#30340;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;&#35813;&#26041;&#27861;&#29983;&#25104;&#20102;16&#20010;&#21516;&#26102;&#30340;&#33033;&#20914;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#24320;&#21457;&#29992;&#20110;&#22788;&#29702;&#38647;&#36798;&#25968;&#25454;&#65288;&#28388;&#27874;&#21644;&#32858;&#31867;&#65289;&#30340;&#31639;&#27861;&#12290;&#36825;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#19981;&#21487;&#22797;&#29616;&#30340;&#19981;&#23384;&#22312;&#25110;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25705;&#25176;&#36710;&#30340;&#38647;&#36798;&#27979;&#37327;&#25968;&#25454;&#23545;GAN&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#29983;&#25104;&#25705;&#25176;&#36710;&#30452;&#32447;&#34892;&#39542;&#30340;&#21512;&#25104;&#21407;&#22987;&#38647;&#36798;&#25968;&#25454;&#12290;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#20102;&#25705;&#25176;&#36710;&#30340;&#36317;&#31163;&#21644;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;&#30340;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.02622</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring. (arXiv:2308.02622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21033;&#29992;&#32593;&#32476;&#21644;&#30693;&#35782;&#22270;&#35889;&#33258;&#21160;&#35780;&#20272;&#24433;&#21709;&#21147;&#25237;&#36164;&#30340;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDGs&#65289;&#26159;&#32852;&#21512;&#22269;&#25552;&#20986;&#30340;&#26088;&#22312;&#40723;&#21169;&#26377;&#21161;&#20110;&#20445;&#38556;&#20154;&#31867;&#32321;&#33635;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#25919;&#31574;&#21644;&#27963;&#21160;&#12290;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#19968;&#20010;&#20844;&#21496;&#19982;&#27599;&#20010;17&#20010;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#35774;&#35745;&#20102;SDG&#26694;&#26550;&#25552;&#20379;&#20998;&#25968;&#12290;&#36825;&#31181;&#35780;&#20998;&#33021;&#22815;&#23545;&#28508;&#22312;&#24314;&#31435;&#21253;&#23481;&#21644;&#21487;&#25345;&#32493;&#32463;&#27982;&#30340;&#25237;&#36164;&#36827;&#34892;&#19968;&#33268;&#30340;&#35780;&#20272;&#12290;&#30001;&#20110;&#27492;&#31867;&#26694;&#26550;&#38656;&#35201;&#39640;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#21019;&#24314;&#21644;&#32500;&#25252;&#23427;&#20204;&#30340;&#36807;&#31243;&#32791;&#26102;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#21019;&#24314;SDG&#26694;&#26550;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19981;&#21516;&#32593;&#32476;&#26469;&#28304;&#21644;&#19982;&#19968;&#32452;&#20844;&#21496;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;&#25910;&#38598;&#21644;&#36807;&#28388;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#25968;&#25454;&#35757;&#32451;&#21644;&#37096;&#32626;&#20102;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting score
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02618</link><description>&lt;p&gt;
ChatGPT&#29992;&#20110;GTFS: &#20174;&#25991;&#23383;&#21040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for GTFS: From Words to Information. (arXiv:2308.02618v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;ChatGPT&#35821;&#35328;&#27169;&#22411;&#20174;GTFS&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#65292;&#39564;&#35777;&#20102;ChatGPT&#65288;GPT-3.5&#65289;&#22312;GTFS&#35268;&#33539;&#29702;&#35299;&#21644;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20026;&#35299;&#20915;GTFS&#25968;&#25454;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20132;&#36890;&#34892;&#25968;&#25454;&#21457;&#24067;&#26631;&#20934;General Transit Feed Specification&#65288;GTFS&#65289;&#26159;&#34920;&#26684;&#25968;&#25454;&#65292;&#20449;&#24687;&#20998;&#25955;&#22312;&#19981;&#21516;&#30340;&#25991;&#20214;&#20013;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#24037;&#20855;&#25110;&#21253;&#26469;&#26816;&#32034;&#20449;&#24687;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#36235;&#21183;&#20063;&#22312;&#22686;&#38271;&#12290;&#26412;&#30740;&#31350;&#30340;&#24819;&#27861;&#26159;&#30475;&#30475;&#24403;&#21069;&#24191;&#27867;&#37319;&#29992;&#30340;LLMs&#65288;ChatGPT&#65289;&#26159;&#21542;&#33021;&#22815;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20174;GTFS&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#27979;&#35797;ChatGPT&#65288;GPT-3.5&#65289;&#26159;&#21542;&#29702;&#35299;GTFS&#35268;&#33539;&#12290;GPT-3.5&#22312;&#25105;&#20204;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65288;MCQ&#65289;&#20013;&#27491;&#30830;&#22238;&#31572;&#20102;77%&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#36807;&#28388;&#30340;GTFS&#25968;&#25454;&#38598;&#23545;LLM&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#38646;-shot&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#31243;&#24207;&#21512;&#25104;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#22312;&#31616;&#21333;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;90%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22797;&#26434;&#38382;&#39064;&#19978;&#36798;&#21040;&#20102;&#32422;40%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#30340;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#20248;&#21270;&#36710;&#36742;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.02614</link><description>&lt;p&gt;
&#36710;&#36742;&#25511;&#21046;&#65306;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;
&lt;/p&gt;
&lt;p&gt;
Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning. (arXiv:2308.02614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#30896;&#25758;&#36991;&#20813;&#30340;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#22312;&#20248;&#21270;&#36710;&#36742;&#25511;&#21046;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20154;&#21475;&#22686;&#38271;&#21644;&#36947;&#36335;&#19978;&#36710;&#36742;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#31649;&#29702;&#20132;&#36890;&#24182;&#30830;&#20445;&#23433;&#20840;&#24050;&#25104;&#20026;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24320;&#21457;&#36710;&#36742;&#26234;&#33021;&#25511;&#21046;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#36710;&#36742;&#30896;&#25758;&#36991;&#20813;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#21033;&#29992;&#32852;&#37030;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;FDRL&#65289;&#25216;&#26415;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#20248;&#20808;&#32771;&#34385;&#23433;&#20840;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20986;&#34892;&#24310;&#35823;&#21644;&#25552;&#39640;&#36710;&#36742;&#30340;&#24179;&#22343;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23545;&#26412;&#22320;&#27169;&#22411;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#21644;&#20840;&#29699;&#27169;&#22411;&#32852;&#37030;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;FDDPG&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#22312;&#20248;&#21270;&#36710;&#36742;&#30896;&#25758;&#36991;&#20813;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FDDPG&#31639;&#27861;&#22312;&#26377;&#25928;&#25511;&#21046;&#36710;&#36742;&#21644;&#39044;&#38450;&#30896;&#25758;&#26041;&#38754;&#20248;&#20110;DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and prev
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02613</link><description>&lt;p&gt;
&#29992;SyntHIR&#23454;&#29616;&#20114;&#25805;&#20316;&#24615;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;CDSS&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools. (arXiv:2308.02613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#24739;&#32773;&#26085;&#24535;&#21644;&#20581;&#24247;&#30331;&#35760;&#26469;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26377;&#24456;&#22823;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#23454;&#26045;CDSS&#24037;&#20855;&#65292;&#38656;&#35201;&#23558;&#35813;&#24037;&#20855;&#38598;&#25104;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#22312;&#29992;&#20110;&#23384;&#20648;&#21644;&#31649;&#29702;&#24739;&#32773;&#25968;&#25454;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21512;&#35268;&#27861;&#35268;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#33719;&#24471;&#23545;EHR&#31995;&#32479;&#30340;&#24517;&#35201;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21644;&#20351;&#29992;CDSS&#24037;&#20855;&#24320;&#21457;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#30340;&#20307;&#31995;&#26550;&#26500;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#22312;&#19968;&#20010;&#31216;&#20026;SyntHIR&#30340;&#31995;&#32479;&#20013;&#23454;&#29616;&#12290;SyntHIR&#31995;&#32479;&#20351;&#29992;Fast Healthcare Interoperability Resources (FHIR)&#26631;&#20934;&#36827;&#34892;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#65292;&#20351;&#29992;Gretel&#26694;&#26550;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;Microsoft Azure FHIR&#26381;&#21153;&#22120;&#20316;&#20026;&#22522;&#20110;FHIR&#30340;EHR&#31995;&#32479;&#65292;&#20197;&#21450;&#20351;&#29992;SMART on FHIR&#26694;&#26550;&#36827;&#34892;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;CDSS&#24037;&#20855;&#26469;&#23637;&#31034;SyntHIR&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02608</link><description>&lt;p&gt;
&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Unravelling Responsibility for AI. (arXiv:2308.02608v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#21512;&#29702;&#24605;&#32771;&#36131;&#20219;&#24212;&#35813;&#25918;&#22312;&#20309;&#22788;&#65292;&#25105;&#20204;&#39318;&#20808;&#38656;&#35201;&#19968;&#20010;&#36275;&#22815;&#28165;&#26224;&#21644;&#35814;&#32454;&#30340;&#36328;&#23398;&#31185;&#35789;&#27719;&#26469;&#35848;&#35770;&#36131;&#20219;&#12290;&#36131;&#20219;&#26159;&#19968;&#31181;&#19977;&#20803;&#20851;&#31995;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#34892;&#20026;&#32773;&#12289;&#19968;&#20010;&#20107;&#20214;&#21644;&#19968;&#31181;&#36131;&#20219;&#26041;&#24335;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24847;&#35782;&#30340;&#20026;&#20102;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#36827;&#34892;&#23454;&#36341;&#25512;&#29702;&#30340;&#8220;&#35299;&#26500;&#8221;&#36131;&#20219;&#27010;&#24565;&#30340;&#21162;&#21147;&#65292;&#26412;&#25991;&#37319;&#21462;&#20102;&#8220;&#34892;&#20026;&#32773;A&#23545;&#20107;&#20214;O&#36127;&#36131;&#8221;&#30340;&#19977;&#37096;&#20998;&#34920;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;A&#12289;&#36127;&#36131;&#12289;O&#30340;&#23376;&#31867;&#21035;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;&#36825;&#20123;&#26377;&#25928;&#32452;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36131;&#20219;&#20018;&#8221;&#65292;&#20998;&#20026;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#65306;&#35282;&#33394;&#36131;&#20219;&#12289;&#22240;&#26524;&#36131;&#20219;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36947;&#24503;&#36131;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36816;&#34892;&#31034;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#19968;&#20010;&#28041;&#21450;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#21478;&#19968;&#20010;&#28041;&#21450;AV&#19982;&#34892;&#20154;&#30340;&#33268;&#21629;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reason about where responsibility does and should lie in complex situations involving AI-enabled systems, we first need a sufficiently clear and detailed cross-disciplinary vocabulary for talking about responsibility. Responsibility is a triadic relation involving an actor, an occurrence, and a way of being responsible. As part of a conscious effort towards 'unravelling' the concept of responsibility to support practical reasoning about responsibility for AI, this paper takes the three-part formulation, 'Actor A is responsible for Occurrence O' and identifies valid combinations of subcategories of A, is responsible for, and O. These valid combinations - which we term "responsibility strings" - are grouped into four senses of responsibility: role-responsibility; causal responsibility; legal liability-responsibility; and moral responsibility. They are illustrated with two running examples, one involving a healthcare AI-based system and another the fatal collision of an AV with a pedes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22797;&#26434;&#30340;&#28237;&#27969;&#27969;&#21160;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANN&#21644;&#21253;&#35013;&#22120;&#26041;&#27861;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#23376;&#38598;&#36873;&#25321;&#25351;&#26631;&#65292;&#22312;&#27599;&#20010;&#28040;&#38500;&#27493;&#39588;&#20013;&#26368;&#23567;&#21270;&#24635;&#23548;&#25968;&#30340;&#25439;&#22833;&#25110;&#26041;&#21521;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#21435;&#38500;&#20887;&#20313;&#25110;&#26080;&#20851;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.02602</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#30340;&#21253;&#35013;&#22120;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#29992;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;ANN&#30340;&#28237;&#27969;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On stable wrapper-based parameter selection method for efficient ANN-based data-driven modeling of turbulent flows. (arXiv:2308.02602v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22797;&#26434;&#30340;&#28237;&#27969;&#27969;&#21160;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANN&#21644;&#21253;&#35013;&#22120;&#26041;&#27861;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#23376;&#38598;&#36873;&#25321;&#25351;&#26631;&#65292;&#22312;&#27599;&#20010;&#28040;&#38500;&#27493;&#39588;&#20013;&#26368;&#23567;&#21270;&#24635;&#23548;&#25968;&#30340;&#25439;&#22833;&#25110;&#26041;&#21521;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#26377;&#25928;&#21435;&#38500;&#20887;&#20313;&#25110;&#26080;&#20851;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27169;&#25311;&#22797;&#26434;&#30340;&#28237;&#27969;&#27969;&#21160;&#21644;&#20256;&#28909;&#29616;&#35937;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#21644;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANN)&#21644;&#21253;&#35013;&#22120;&#26041;&#27861;&#30340;&#31616;&#21270;&#24314;&#27169;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#36807;&#28388;&#26041;&#27861;&#65289;&#20855;&#26377;&#20248;&#21183;&#65292;&#21487;&#20197;&#22312;&#21442;&#25968;&#20043;&#38388;&#23384;&#22312;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#20887;&#20313;&#25110;&#26080;&#20851;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;ANN&#35757;&#32451;&#30340;&#36807;&#24230;&#25311;&#21512;&#21644;&#38543;&#26426;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#26356;&#39640;&#30340;&#29289;&#29702;&#32500;&#24230;&#19979;&#29305;&#21035;&#26159;&#22312;&#36873;&#25321;&#35797;&#39564;&#20013;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#23376;&#38598;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20960;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;ANN&#30340;&#21253;&#35013;&#22120;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#23376;&#38598;&#36873;&#25321;&#25351;&#26631;&#24320;&#21457;&#20102;&#20462;&#35746;&#29256;&#26041;&#27861;&#65292;&#20197;&#22312;&#27599;&#20010;&#28040;&#38500;&#27493;&#39588;&#20013;&#26368;&#23567;&#21270;&#24635;&#23548;&#25968;&#30340;&#25439;&#22833;&#25110;&#26041;&#21521;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#26816;&#39564;&#21442;&#25968;&#20943;&#23569;&#24615;&#33021;&#21644;&#35797;&#39564;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#21046;&#36896;&#30340;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#21363;&#28237;&#27969;&#27668;&#27873;&#27969;&#20013;&#27668;&#27873;&#23610;&#23544;&#30340;&#24314;&#27169;&#65292;&#20197;&#21450;&#31354;&#38388;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
To model complex turbulent flow and heat transfer phenomena, this study aims to analyze and develop a reduced modeling approach based on artificial neural network (ANN) and wrapper methods. This approach has an advantage over other methods such as the correlation-based filter method in terms of removing redundant or irrelevant parameters even under non-linearity among them. As a downside, the overfitting and randomness of ANN training may produce inconsistent subsets over selection trials especially in a higher physical dimension. This study analyzes a few existing ANN-based wrapper methods and develops a revised one based on the gradient-based subset selection indices to minimize the loss in the total derivative or the directional consistency at each elimination step. To examine parameter reduction performance and consistency-over-trials, we apply these methods to a manufactured subset selection problem, modeling of the bubble size in a turbulent bubbly flow, and modeling of the spati
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#65292;&#26088;&#22312;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.02597</link><description>&lt;p&gt;
&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36164;&#28304;&#39640;&#25928;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#65306;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#25552;&#39640;&#24739;&#32773;&#29983;&#23384;&#29575;
&lt;/p&gt;
&lt;p&gt;
Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries. (arXiv:2308.02597v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02597
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35786;&#26029;&#31995;&#32479;&#65292;&#29992;&#20110;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#30340;&#35786;&#26029;&#65292;&#26088;&#22312;&#20943;&#23569;&#21457;&#23637;&#20013;&#22269;&#23478;&#20020;&#24202;&#35786;&#26029;&#30340;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;&#30284;&#26159;&#30284;&#30151;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#21457;&#23637;&#20013;&#22269;&#23478;&#65292;&#29305;&#21035;&#26159;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#12289;&#21335;&#20122;&#21644;&#21335;&#32654;&#30340;&#20083;&#33146;&#30284;&#24739;&#32773;&#27515;&#20129;&#29575;&#26368;&#39640;&#12290;&#23548;&#33268;&#20840;&#29699;&#27515;&#20129;&#29575;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#30001;&#20110;&#21463;&#35757;&#30149;&#29702;&#23398;&#23478;&#20005;&#37325;&#30701;&#32570;&#32780;&#23548;&#33268;&#30340;&#35786;&#26029;&#38271;&#26102;&#38388;&#24310;&#36831;&#65292;&#20174;&#32780;&#23548;&#33268;&#35786;&#26029;&#26102;&#36739;&#22823;&#27604;&#20363;&#30340;&#26202;&#26399;&#34920;&#29616;&#12290;&#21021;&#22987;&#30151;&#29366;&#21457;&#23637;&#21040;&#35786;&#26029;&#25509;&#21463;&#26102;&#38388;&#21487;&#33021;&#38271;&#36798;15&#20010;&#26376;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#30340;&#21307;&#30103;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36716;&#31227;&#24615;&#20083;&#33146;&#30284;&#35786;&#26029;&#31995;&#32479;&#65292;&#26082;&#33021;&#36798;&#21040;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#65292;&#21448;&#33021;&#20445;&#35777;&#35745;&#31639;&#25928;&#29575;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#22522;&#20110;MobileNetV2&#30340;&#35786;&#26029;&#27169;&#22411;&#22312;&#35786;&#26029;&#20934;&#30830;&#24230;&#12289;&#27169;&#22411;&#27867;&#21270;&#24615;&#21644;&#27169;&#22411;&#35757;&#32451;&#26041;&#38754;&#36229;&#36807;&#20102;&#26356;&#22797;&#26434;&#30340;VGG16&#12289;ResNet50&#21644;ResNet101&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.02594</link><description>&lt;p&gt;
SMARLA&#65306;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents. (arXiv:2308.02594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;SMARLA&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#35813;&#26041;&#27861;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;&#65292;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#23545;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#12290;&#32463;&#39564;&#35777;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#21487;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;(DRL)&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#30830;&#20445;DRL&#26234;&#33021;&#20307;&#30340;&#23433;&#20840;&#24615;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20381;&#38752;&#27979;&#35797;&#26159;&#19981;&#36275;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#33021;&#25552;&#20379;&#20445;&#35777;&#12290;&#26500;&#24314;&#23433;&#20840;&#30417;&#27979;&#22120;&#26159;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SMARLA&#65292;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#30417;&#27979;&#26041;&#27861;&#65292;&#19987;&#20026;DRL&#26234;&#33021;&#20307;&#35774;&#35745;&#12290;&#20986;&#20110;&#23454;&#38469;&#21407;&#22240;&#65292;SMARLA&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#23376;(&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#35775;&#38382;&#26234;&#33021;&#20307;&#30340;&#20869;&#37096;)&#65292;&#24182;&#21033;&#29992;&#29366;&#24577;&#25277;&#35937;&#26469;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#65292;&#20174;&#32780;&#20419;&#36827;&#20174;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#23398;&#20064;&#23433;&#20840;&#36829;&#35268;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;RL&#26696;&#20363;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;SMARLA&#12290;&#32463;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;SMARLA&#20855;&#26377;&#20934;&#30830;&#30340;&#36829;&#35268;&#39044;&#27979;&#33021;&#21147;&#65292;&#35823;&#25253;&#29575;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26234;&#33021;&#20307;&#25191;&#34892;&#30340;&#19968;&#21322;&#24038;&#21491;&#30340;&#26089;&#26399;&#38454;&#27573;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#20855;&#26377;&#39640;&#19968;&#33268;&#24615;&#65292;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02575</link><description>&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20998;&#22120;&#21527;&#65311;&#35780;&#20272;GPT-4&#25991;&#26412;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is GPT-4 a reliable rater? Evaluating Consistency in GPT-4 Text Ratings. (arXiv:2308.02575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02575
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#20855;&#26377;&#39640;&#19968;&#33268;&#24615;&#65292;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;OpenAI&#30340;GPT-4&#22312;&#22810;&#20010;&#36845;&#20195;&#12289;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#20307;&#21464;&#21270;&#20013;&#29983;&#25104;&#30340;&#21453;&#39304;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#35813;&#27169;&#22411;&#26681;&#25454;&#20869;&#23481;&#21644;&#25991;&#20307;&#23545;&#23439;&#35266;&#32463;&#27982;&#23398;&#23398;&#31185;&#39046;&#22495;&#20869;&#30340;&#20219;&#21153;&#22238;&#31572;&#36827;&#34892;&#35780;&#20998;&#12290;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#12289;&#36845;&#20195;&#20043;&#38388;&#30340;&#35780;&#20998;&#30456;&#20851;&#24615;&#20197;&#21450;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#21516;&#26102;&#38388;&#36328;&#24230;&#30340;ICC&#20998;&#25968;&#22312;0.94&#21040;0.99&#20043;&#38388;&#65292;&#34920;&#26126;GPT-4&#33021;&#22815;&#22312;&#37325;&#22797;&#20219;&#21153;&#20013;&#29983;&#25104;&#19968;&#33268;&#30340;&#35780;&#20998;&#12290;&#20869;&#23481;&#21644;&#25991;&#20307;&#35780;&#20998;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20026;0.87&#12290;&#24403;&#24212;&#29992;&#19981;&#24688;&#24403;&#30340;&#25991;&#20307;&#26102;&#65292;&#24179;&#22343;&#20869;&#23481;&#35780;&#20998;&#20445;&#25345;&#19981;&#21464;&#65292;&#32780;&#25991;&#20307;&#35780;&#20998;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#19968;&#33268;&#35780;&#20998;&#26041;&#38754;&#20855;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.02570</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#23398;&#20064;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;NER
&lt;/p&gt;
&lt;p&gt;
Learning Implicit Entity-object Relations by Bidirectional Generative Alignment for Multimodal NER. (arXiv:2308.02570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#35821;&#20041;&#40511;&#27807;&#21644;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#38544;&#24335;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(MNER)&#38754;&#20020;&#30340;&#25361;&#25112;&#20027;&#35201;&#26377;&#20004;&#26041;&#38754;: (1) &#24357;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;; (2) &#21305;&#37197;&#23454;&#20307;&#19982;&#22270;&#20687;&#20013;&#20854;&#20851;&#32852;&#30340;&#29289;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22240;&#20026;&#32570;&#20047;&#30456;&#24212;&#30340;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BGA-MNER&#30340;&#21452;&#21521;&#29983;&#25104;&#23545;&#40784;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;BGA-MNER&#21253;&#25324;&#38024;&#23545;&#20004;&#31181;&#27169;&#24577;&#20013;&#30340;&#23454;&#20307;&#26174;&#33879;&#20869;&#23481;&#30340;\texttt{&#22270;&#20687;&#21040;&#25991;&#26412;}&#21644;\texttt{&#25991;&#26412;&#21040;&#22270;&#20687;}&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#20849;&#21516;&#20248;&#21270;&#21452;&#21521;&#37325;&#24314;&#30446;&#26631;&#26469;&#23545;&#40784;&#38544;&#21547;&#30340;&#23454;&#20307;-&#29289;&#20307;&#20851;&#31995;&#65292;&#22312;&#30452;&#25509;&#32780;&#24378;&#22823;&#30340;&#32422;&#26463;&#19979;&#23454;&#29616;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36890;&#24120;&#21253;&#21547;&#19981;&#21305;&#37197;&#30340;&#32452;&#20214;&#65292;&#23545;&#20110;&#29983;&#25104;&#26469;&#35828;&#26159;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38454;&#27573;&#24615;&#25913;&#36827;&#30340;&#19978;&#19979;&#25991;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#25552;&#21462;&#21305;&#37197;&#30340;&#36328;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge posed by multimodal named entity recognition (MNER) is mainly two-fold: (1) bridging the semantic gap between text and image and (2) matching the entity with its associated object in image. Existing methods fail to capture the implicit entity-object relations, due to the lack of corresponding annotation. In this paper, we propose a bidirectional generative alignment method named BGA-MNER to tackle these issues. Our BGA-MNER consists of \texttt{image2text} and \texttt{text2image} generation with respect to entity-salient content in two modalities. It jointly optimizes the bidirectional reconstruction objectives, leading to aligning the implicit entity-object relations under such direct and powerful constraints. Furthermore, image-text pairs usually contain unmatched components which are noisy for generation. A stage-refined context sampler is proposed to extract the matched cross-modal content for generation. Extensive experiments on two benchmarks demonstrate that our met
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;SimTeG&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.02565</link><description>&lt;p&gt;
SimTeG: &#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#25991;&#26412;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimTeG: A Frustratingly Simple Approach Improves Textual Graph Learning. (arXiv:2308.02565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02565
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;SimTeG&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#29305;&#24449;&#24037;&#31243;&#21644;&#27169;&#22411;&#35774;&#35745;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#65288;TGs&#65289;&#26159;&#25351;&#33410;&#28857;&#23545;&#24212;&#25991;&#26412;&#65288;&#21477;&#23376;&#25110;&#25991;&#26723;&#65289;&#30340;&#22270;&#24418;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;TGs&#30340;&#34920;&#31034;&#23398;&#20064;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#21644;&#65288;ii&#65289;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#21518;&#32773;&#38454;&#27573;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#24418;&#22522;&#20934;&#30340;&#21069;&#32773;&#38454;&#27573;&#20173;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#29305;&#24449;&#24037;&#31243;&#25216;&#26415;&#12290;&#26368;&#36817;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#21033;&#29992;LMs&#26469;&#20419;&#36827;TGs&#30340;&#23398;&#20064;&#65292;&#26041;&#27861;&#35201;&#20040;&#26159;&#22312;&#35745;&#31639;&#23494;&#38598;&#30340;&#26694;&#26550;&#20013;&#32852;&#21512;&#35757;&#32451;&#23427;&#20204;&#65288;&#21512;&#24182;&#20004;&#20010;&#38454;&#27573;&#65289;&#65292;&#35201;&#20040;&#26159;&#35774;&#35745;&#22797;&#26434;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#20219;&#21153;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65288;&#22686;&#24378;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimTeG&#65292;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#25991;&#26412;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#19981;&#21019;&#26032;&#20110;&#26694;&#26550;&#12289;&#27169;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual graphs (TGs) are graphs whose nodes correspond to text (sentences or documents), which are widely prevalent. The representation learning of TGs involves two stages: (i) unsupervised feature extraction and (ii) supervised graph representation learning. In recent years, extensive efforts have been devoted to the latter stage, where Graph Neural Networks (GNNs) have dominated. However, the former stage for most existing graph benchmarks still relies on traditional feature engineering techniques. More recently, with the rapid development of language models (LMs), researchers have focused on leveraging LMs to facilitate the learning of TGs, either by jointly training them in a computationally intensive framework (merging the two stages), or designing complex self-supervised training tasks for feature extraction (enhancing the first stage). In this work, we present SimTeG, a frustratingly Simple approach for Textual Graph learning that does not innovate in frameworks, models, and tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#26159;GenAI&#20013;&#30340;&#19979;&#19968;&#20010;&#28909;&#28857;&#65292;&#21487;&#20197;&#35299;&#20915;&#23398;&#20064;&#36164;&#28304;&#26377;&#38480;&#21644;&#36807;&#20110;&#20381;&#36182;&#31185;&#23398;&#21457;&#29616;&#32463;&#39564;&#20027;&#20041;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.02561</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#65306;&#29983;&#25104;AI&#20013;&#30340;&#19979;&#19968;&#20010;&#28909;&#28857;
&lt;/p&gt;
&lt;p&gt;
Large-scale Generative Simulation Artificial Intelligence: the Next Hotspot in Generative AI. (arXiv:2308.02561v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02561
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#26159;GenAI&#20013;&#30340;&#19979;&#19968;&#20010;&#28909;&#28857;&#65292;&#21487;&#20197;&#35299;&#20915;&#23398;&#20064;&#36164;&#28304;&#26377;&#38480;&#21644;&#36807;&#20110;&#20381;&#36182;&#31185;&#23398;&#21457;&#29616;&#32463;&#39564;&#20027;&#20041;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GenAI&#30340;&#27010;&#24565;&#24050;&#32463;&#21457;&#23637;&#20102;&#25968;&#21313;&#24180;&#12290;&#26368;&#36817;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#31215;&#26497;&#21442;&#19982;&#12290;&#37492;&#20110;&#29616;&#23454;&#25361;&#25112;&#65292;&#22914;&#26377;&#38480;&#30340;&#23398;&#20064;&#36164;&#28304;&#21644;&#36807;&#20110;&#20381;&#36182;&#31185;&#23398;&#21457;&#29616;&#32463;&#39564;&#20027;&#20041;&#65292;&#25105;&#20204;&#23558;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#65288;LS-GenAI&#65289;&#25552;&#21517;&#20026;GenAI&#30340;&#19979;&#19968;&#20010;&#36830;&#25509;&#28909;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of GenAI has been developed for decades. Until recently, it has impressed us with substantial breakthroughs in natural language processing and computer vision, actively engaging in industrial scenarios. Noticing the practical challenges, e.g., limited learning resources, and overly dependencies on scientific discovery empiricism, we nominate large-scale generative simulation artificial intelligence (LS-GenAI) as the next hotspot for GenAI to connect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#36807;&#21435;60&#24180;&#26469;&#20154;&#24037;&#26234;&#33021;&#20013;&#21457;&#29983;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#36716;&#21464;&#25152;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#21644;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2308.02558</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
The Paradigm Shifts in Artificial Intelligence. (arXiv:2308.02558v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#36807;&#21435;60&#24180;&#26469;&#20154;&#24037;&#26234;&#33021;&#20013;&#21457;&#29983;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#36716;&#21464;&#25152;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kuhn (1962) &#30340;&#31185;&#23398;&#36827;&#27493;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#29992;&#30340;&#35270;&#35282;&#65292;&#29992;&#20110;&#29702;&#35299;&#36807;&#21435;60&#24180;&#26469;&#20154;&#24037;&#26234;&#33021;&#20013;&#21457;&#29983;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#35813;&#26694;&#26550;&#21516;&#26679;&#26377;&#21161;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#36825;&#19968;&#36716;&#21464;&#30340;&#26631;&#24535;&#26159;&#22823;&#22411;&#39044;&#35757;&#32451;&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#22914;&#22522;&#20110;ChatGPT&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#36825;&#20123;&#31995;&#32479;&#20351;&#24471;&#26234;&#33021;&#25104;&#20026;&#19968;&#31181;&#21487;&#37197;&#32622;&#24212;&#29992;&#30340;&#36890;&#29992;&#25216;&#26415;&#20135;&#21697;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#24635;&#32467;&#20102;&#23548;&#33268;&#27599;&#20010;&#33539;&#24335;&#20852;&#36215;&#21644;&#34928;&#33853;&#30340;&#21147;&#37327;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#36716;&#21464;&#25152;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kuhn's framework of scientific progress (Kuhn, 1962) provides a useful framing of the paradigm shifts that have occurred in Artificial Intelligence over the last 60 years. The framework is also useful in understanding what is arguably a new paradigm shift in AI, signaled by the emergence of large pre-trained systems such as GPT-3, on which conversational agents such as ChatGPT are based. Such systems make intelligence a commoditized general purpose technology that is configurable to applications. In this paper, I summarize the forces that led to the rise and fall of each paradigm, and discuss the pressing issues and risks associated with the current paradigm shift in AI.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#24863;&#30693;&#21327;&#21516;&#36807;&#28388;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;KCF-PLM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#35780;&#35770;&#35780;&#20998;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;Transformer&#32593;&#32476;&#26469;&#32771;&#34385;&#35780;&#35770;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.02555</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#21327;&#21516;&#36807;&#28388;&#29992;&#20110;&#20010;&#24615;&#21270;&#35780;&#35770;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware Collaborative Filtering with Pre-trained Language Model for Personalized Review-based Rating Prediction. (arXiv:2308.02555v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02555
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#24863;&#30693;&#21327;&#21516;&#36807;&#28388;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;KCF-PLM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#35780;&#35770;&#35780;&#20998;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;Transformer&#32593;&#32476;&#26469;&#32771;&#34385;&#35780;&#35770;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#35780;&#35770;&#35780;&#20998;&#39044;&#27979;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#35780;&#35770;&#26469;&#24314;&#27169;&#29992;&#25143;&#20852;&#36259;&#21644;&#29289;&#21697;&#29305;&#24449;&#65292;&#20197;&#36827;&#34892;&#35780;&#20998;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#36935;&#21040;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#32771;&#34385;&#27599;&#20010;&#35780;&#35770;&#30340;&#32454;&#31890;&#24230;&#26041;&#38754;&#21644;&#30693;&#35782;&#22270;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#20197;&#34917;&#20805;&#32431;&#25991;&#26412;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#23578;&#26410;&#35748;&#30495;&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#35780;&#35770;&#35780;&#20998;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#24863;&#30693;&#21327;&#21516;&#36807;&#28388;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;KCF-PLM&#65289;&#30340;&#26041;&#27861;&#12290;&#38024;&#23545;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#20026;&#20102;&#21033;&#29992;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;KCF-PLM&#24320;&#21457;&#20102;&#19968;&#20010;Transformer&#32593;&#32476;&#65292;&#20197;&#23545;&#25552;&#21462;&#30340;&#26041;&#38754;&#19982;&#29992;&#25143;-&#29289;&#21697;&#23545;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#38024;&#23545;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#34920;&#31034;&#29992;&#25143;&#21644;&#29289;&#21697;&#65292;KCF-PLM&#23558;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#25152;&#26377;&#21382;&#21490;&#35780;&#35770;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.02542</link><description>&lt;p&gt;
&#20197;&#21327;&#21516;&#36807;&#28388;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#20316;&#20026;&#35268;&#33539;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering to capture AI user's preferences as norms. (arXiv:2308.02542v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;AI&#29992;&#25143;&#20559;&#22909;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;AI&#25216;&#26415;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#36827;&#34892;&#23450;&#21046;&#26159;&#20854;&#33391;&#22909;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#29992;&#25143;&#36807;&#22810;&#21442;&#19982;&#65292;&#24182;&#19988;&#26410;&#33021;&#30495;&#27491;&#25429;&#25417;&#21040;&#20182;&#20204;&#30340;&#30495;&#23454;&#20559;&#22909;&#12290;&#20107;&#23454;&#19978;&#65292;&#20026;&#20102;&#36991;&#20813;&#25163;&#21160;&#35774;&#32622;&#20559;&#22909;&#30340;&#40635;&#28902;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#25509;&#21463;&#40664;&#35748;&#35774;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#35774;&#32622;&#19982;&#20182;&#20204;&#30340;&#30495;&#23454;&#20559;&#22909;&#19981;&#31526;&#12290;&#35268;&#33539;&#21487;&#20197;&#29992;&#26469;&#35843;&#33410;&#34892;&#20026;&#65292;&#30830;&#20445;&#20854;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#20294;&#26159;&#23613;&#31649;&#25991;&#29486;&#24050;&#32463;&#35814;&#32454;&#30740;&#31350;&#20102;&#35268;&#33539;&#65292;&#22823;&#37096;&#20998;&#25552;&#35758;&#37117;&#26159;&#20174;&#24418;&#24335;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#12290;&#23454;&#38469;&#19978;&#65292;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#20851;&#20110;&#26500;&#24314;&#35268;&#33539;&#20197;&#25429;&#25417;&#29992;&#25143;&#38544;&#31169;&#20559;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#39046;&#22495;&#30693;&#35782;&#65292;&#22312;AI&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#24456;&#38590;&#33719;&#24471;&#21644;&#32500;&#25252;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26500;&#24314;&#35268;&#33539;&#26102;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#21363;&#21033;&#29992;&#31995;&#32479;&#20013;&#22823;&#37327;&#29992;&#25143;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;&#21463;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30456;&#20449;&#21327;&#21516;&#36807;&#28388;&#21487;&#20197;&#25104;&#20026;&#26500;&#24314;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customising AI technologies to each user's preferences is fundamental to them functioning well. Unfortunately, current methods require too much user involvement and fail to capture their true preferences. In fact, to avoid the nuisance of manually setting preferences, users usually accept the default settings even if these do not conform to their true preferences. Norms can be useful to regulate behaviour and ensure it adheres to user preferences but, while the literature has thoroughly studied norms, most proposals take a formal perspective. Indeed, while there has been some research on constructing norms to capture a user's privacy preferences, these methods rely on domain knowledge which, in the case of AI technologies, is difficult to obtain and maintain. We argue that a new perspective is required when constructing norms, which is to exploit the large amount of preference information readily available from whole systems of users. Inspired by recommender systems, we believe that co
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#32508;&#21512;&#20102;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36229;&#36234;&#31616;&#21333;&#20219;&#21153;&#65292;&#24182;&#26377;&#25928;&#27807;&#36890;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02541</link><description>&lt;p&gt;
&#36808;&#21521;&#26356;&#20855;&#20154;&#31867;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#20132;&#27969;&#65306;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards More Human-like AI Communication: A Review of Emergent Communication Research. (arXiv:2308.02541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#32508;&#21512;&#20102;&#32039;&#24613;&#27807;&#36890;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36229;&#36234;&#31616;&#21333;&#20219;&#21153;&#65292;&#24182;&#26377;&#25928;&#27807;&#36890;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21521;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#36716;&#21464;&#30340;&#26368;&#36817;&#65292;&#26426;&#22120;&#20934;&#30830;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#23398;&#20064;&#38169;&#20301;&#65292;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#20154;&#31867;&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26102;&#25152;&#20351;&#29992;&#30340;&#22522;&#26412;&#32467;&#26500;&#21644;&#25512;&#29702;&#26041;&#24335;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#25110;&#19981;&#21487;&#38752;&#30340;&#34892;&#20026;&#12290;&#32039;&#24613;&#27807;&#36890;&#65288;Emecom&#65289;&#26159;&#19968;&#20010;&#36817;&#24180;&#26469;&#21457;&#34920;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#20197;&#36229;&#36234;&#31616;&#21333;&#30340;&#21028;&#21035;&#24615;&#20219;&#21153;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#20132;&#27969;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#20351;&#29992;&#33021;&#21147;&#30340;&#20154;&#24037;&#20195;&#29702;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20197;&#20004;&#20010;&#26041;&#38754;&#20171;&#32461;Emecom&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30028;&#23450;&#20102;&#25991;&#29486;&#20013;&#21457;&#29616;&#30340;&#25152;&#26377;&#24120;&#35265;&#29305;&#24615;&#21450;&#20854;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23376;&#31867;&#21035;&#65292;&#24182;&#31361;&#20986;&#23427;&#20204;&#30340;&#29305;&#28857;&#21644;&#24320;&#25918;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (Emecom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present Emecom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open chall
&lt;/p&gt;</description></item><item><title>ALE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#27604;NLP&#20013;AL&#31574;&#30053;&#30340;&#20223;&#30495;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#23454;&#35777;&#22522;&#30784;&#21644;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.02537</link><description>&lt;p&gt;
ALE: &#29992;&#20110;NLP&#26597;&#35810;&#31574;&#30053;&#21442;&#25968;&#39537;&#21160;&#27604;&#36739;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ALE: A Simulation-Based Active Learning Evaluation Framework for the Parameter-Driven Comparison of Query Strategies for NLP. (arXiv:2308.02537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02537
&lt;/p&gt;
&lt;p&gt;
ALE&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#27604;NLP&#20013;AL&#31574;&#30053;&#30340;&#20223;&#30495;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#23454;&#35777;&#22522;&#30784;&#21644;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#36807;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#27880;&#37322;&#36807;&#31243;&#33719;&#21462;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#25361;&#25112;&#65292;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25552;&#20986;&#23558;&#26377;&#24076;&#26395;&#30340;&#25968;&#25454;&#28857;&#25512;&#33616;&#32473;&#27880;&#37322;&#21592;&#65292;&#20197;&#20415;&#20182;&#20204;&#27880;&#37322;&#25509;&#19979;&#26469;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#25110;&#36830;&#32493;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#33410;&#30465;&#27880;&#37322;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#32773;&#38754;&#20020;&#30528;&#35768;&#22810;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;AL&#31574;&#30053;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#23454;&#35777;&#22522;&#30784;&#26469;&#36873;&#25321;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35843;&#30740;&#23558;AL&#31574;&#30053;&#20998;&#31867;&#20026;&#27809;&#26377;&#24615;&#33021;&#25351;&#26631;&#30340;&#20998;&#31867;&#27861;&#12290;&#26032;&#22411;AL&#31574;&#30053;&#30340;&#20171;&#32461;&#24615;&#28436;&#31034;&#19982;&#23569;&#37327;&#31574;&#30053;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21487;&#37325;&#29616;&#30340;&#20027;&#21160;&#23398;&#20064;&#35780;&#20272;&#65288;ALE&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#27604;&#36739;&#35780;&#20272;NLP&#20013;&#30340;AL&#31574;&#30053;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#23454;&#29616;AL&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#27604;&#36739;&#36827;&#34892;&#20844;&#27491;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning and deep learning require a large amount of labeled data, which data scientists obtain in a manual, and time-consuming annotation process. To mitigate this challenge, Active Learning (AL) proposes promising data points to annotators they annotate next instead of a subsequent or random sample. This method is supposed to save annotation effort while maintaining model performance. However, practitioners face many AL strategies for different tasks and need an empirical basis to choose between them. Surveys categorize AL strategies into taxonomies without performance indications. Presentations of novel AL strategies compare the performance to a small subset of strategies. Our contribution addresses the empirical basis by introducing a reproducible active learning evaluation (ALE) framework for the comparative evaluation of AL strategies in NLP. The framework allows the implementation of AL strategies with low effort and a fair data-driven comparison through defin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#36741;&#21161;&#32986;&#32974;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#21152;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#25351;&#21335;&#65292;&#26088;&#22312;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.02534</link><description>&lt;p&gt;
&#25506;&#32034;&#21487;&#35299;&#37322;&#24615;&#22312;AI&#36741;&#21161;&#32986;&#32974;&#31579;&#36873;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring the Role of Explainability in AI-Assisted Embryo Selection. (arXiv:2308.02534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#36741;&#21161;&#32986;&#32974;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#21152;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#25351;&#21335;&#65292;&#26088;&#22312;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#22806;&#21463;&#31934;&#26159;&#27835;&#30103;&#19981;&#23381;&#30340;&#26368;&#24120;&#35265;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#35780;&#20272;&#21644;&#36873;&#25321;&#29992;&#20110;&#26893;&#20837;&#30340;&#32986;&#32974;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202;&#21307;&#29983;&#38388;&#21644;&#20869;&#37096;&#21464;&#24322;&#24615;&#30340;&#36807;&#31243;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#27491;&#22312;&#24341;&#36215;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#19981;&#36879;&#26126;&#24615;&#25439;&#23475;&#20102;&#23427;&#20204;&#22312;&#20020;&#24202;&#24773;&#22659;&#20013;&#30340;&#25509;&#21463;&#24230;&#65292;&#32780;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36879;&#26126;&#24615;&#26159;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;AI&#36741;&#21161;&#32986;&#32974;&#20998;&#26512;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#24403;&#21069;&#24037;&#20316;&#65292;&#35782;&#21035;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#20020;&#24202;&#24773;&#22659;&#20013;&#20316;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#24182;&#32771;&#34385;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#30340;&#38656;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#21152;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#25351;&#21335;&#65292;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#21521;&#25104;&#29087;&#30340;&#20020;&#24202;&#23454;&#36341;&#36808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Vitro Fertilization is among the most widespread treatments for infertility. One of its main challenges is the evaluation and selection of embryo for implantation, a process with large inter- and intra-clinician variability. Deep learning based methods are gaining attention, but their opaque nature compromises their acceptance in the clinical context, where transparency in the decision making is key. In this paper we analyze the current work in the explainability of AI-assisted embryo analysis models, identifying the limitations. We also discuss how these models could be integrated in the clinical context as decision support systems, considering the needs of clinicians and patients. Finally, we propose guidelines for the sake of increasing interpretability and trustworthiness, pushing this technology forward towards established clinical practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Choir Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#27880;&#24847;&#21147;&#26469;&#29983;&#25104;&#22810;&#22768;&#37096;&#38899;&#20048;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#31934;&#24230;&#21644;&#21644;&#22768;&#25351;&#26631;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02531</link><description>&lt;p&gt;
Choir Transformer: &#20351;&#29992;Transformer&#19978;&#30340;&#30456;&#23545;&#27880;&#24847;&#21147;&#29983;&#25104;&#22810;&#22768;&#37096;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Choir Transformer: Generating Polyphonic Music with Relative Attention on Transformer. (arXiv:2308.02531v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Choir Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#27880;&#24847;&#21147;&#26469;&#29983;&#25104;&#22810;&#22768;&#37096;&#38899;&#20048;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#31934;&#24230;&#21644;&#21644;&#22768;&#25351;&#26631;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26059;&#24459;&#21644;&#21644;&#22768;&#20043;&#38388;&#30340;&#27491;&#30830;&#32852;&#31995;&#65292;&#22810;&#22768;&#37096;&#38899;&#20048;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#21521;&#12290;&#22823;&#37096;&#20998;&#20197;&#21069;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#38590;&#20197;&#24314;&#31435;&#38271;&#36317;&#31163;&#38899;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Choir Transformer&#30340;&#22810;&#22768;&#37096;&#38899;&#20048;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#30456;&#23545;&#20301;&#32622;&#27880;&#24847;&#21147;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#38899;&#20048;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22810;&#22768;&#37096;&#38899;&#20048;&#29983;&#25104;&#30340;&#38899;&#20048;&#34920;&#31034;&#26041;&#27861;&#12290;Choir Transformer&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;4.06%&#30340;&#21069;&#26399;&#26368;&#20808;&#36827;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#22810;&#22768;&#37096;&#38899;&#20048;&#30340;&#21644;&#22768;&#25351;&#26631;&#65292;&#23454;&#39564;&#34920;&#26126;&#21644;&#22768;&#25351;&#26631;&#25509;&#36817;&#24052;&#36203;&#30340;&#38899;&#20048;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29983;&#25104;&#30340;&#26059;&#24459;&#21644;&#33410;&#22863;&#21487;&#20197;&#26681;&#25454;&#25351;&#23450;&#30340;&#36755;&#20837;&#36827;&#34892;&#35843;&#25972;&#65292;&#20855;&#26377;&#19981;&#21516;&#39118;&#26684;&#30340;&#38899;&#20048;&#65292;&#22914;&#27665;&#38388;&#38899;&#20048;&#25110;&#27969;&#34892;&#38899;&#20048;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyphonic music generation is still a challenge direction due to its correct between generating melody and harmony. Most of the previous studies used RNN-based models. However, the RNN-based models are hard to establish the relationship between long-distance notes. In this paper, we propose a polyphonic music generation neural network named Choir Transformer[ https://github.com/Zjy0401/choir-transformer], with relative positional attention to better model the structure of music. We also proposed a music representation suitable for polyphonic music generation. The performance of Choir Transformer surpasses the previous state-of-the-art accuracy of 4.06%. We also measures the harmony metrics of polyphonic music. Experiments show that the harmony metrics are close to the music of Bach. In practical application, the generated melody and rhythm can be adjusted according to the specified input, with different styles of music like folk music or pop music and so on.
&lt;/p&gt;</description></item><item><title>Gate-DAP&#26159;&#19968;&#31181;&#29992;&#20110;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#32593;&#32476;&#36830;&#25509;&#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#20449;&#24687;&#28304;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#39550;&#39542;&#20219;&#21153;&#24341;&#23548;&#19979;&#30340;&#20132;&#36890;&#22330;&#26223;&#29702;&#35299;&#21644;&#20107;&#25925;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02530</link><description>&lt;p&gt;
Gated Driver Attention Predictor &#65288;arXiv&#65306;2308.02530v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
Gated Driver Attention Predictor. (arXiv:2308.02530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02530
&lt;/p&gt;
&lt;p&gt;
Gate-DAP&#26159;&#19968;&#31181;&#29992;&#20110;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#32593;&#32476;&#36830;&#25509;&#38376;&#25511;&#26426;&#21046;&#65292;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#20449;&#24687;&#28304;&#22312;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#20102;&#39550;&#39542;&#20219;&#21153;&#24341;&#23548;&#19979;&#30340;&#20132;&#36890;&#22330;&#26223;&#29702;&#35299;&#21644;&#20107;&#25925;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#28041;&#21450;&#21040;&#23545;&#39550;&#39542;&#21592;&#30340;&#24847;&#22270;&#29702;&#35299;&#65292;&#21253;&#25324;&#39550;&#39542;&#21592;&#25171;&#31639;&#21435;&#21738;&#37324;&#20197;&#21450;&#39550;&#39542;&#21592;&#20851;&#27880;&#30340;&#23545;&#35937;&#65292;&#36890;&#24120;&#25552;&#20379;&#39550;&#39542;&#20219;&#21153;&#24341;&#23548;&#19979;&#30340;&#20132;&#36890;&#22330;&#26223;&#29702;&#35299;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32039;&#24613;&#25110;&#20107;&#25925;&#22330;&#26223;&#19979;&#30340;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#22312;&#24110;&#21161;&#20107;&#25925;&#39044;&#27979;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#20316;&#29992;&#65292;&#32780;&#20854;&#25552;&#21319;&#33021;&#21147;&#21463;&#21040;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#22320;&#22270;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#32593;&#32476;&#36830;&#25509;&#38376;&#25511;&#26426;&#21046;&#65288;Gate-DAP&#65289;&#12290;Gate-DAP&#26088;&#22312;&#23398;&#20064;&#19981;&#21516;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#27169;&#24577;&#20449;&#24687;&#22312;&#20855;&#26377;&#19981;&#21516;&#36947;&#36335;&#31867;&#22411;&#12289;&#22330;&#21512;&#20197;&#21450;&#20809;&#32447;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;Gate-DAP&#20013;&#30340;&#32593;&#32476;&#36830;&#25509;&#38376;&#25511;&#21253;&#25324;&#20102;&#31354;&#38388;&#32534;&#30721;&#32593;&#32476;&#38376;&#25511;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#38376;&#25511;&#21644;&#20449;&#24687;&#31867;&#22411;&#38376;&#25511;&#27169;&#22359;&#12290;&#27599;&#20010;&#36830;&#25509;&#38376;&#25511;&#25805;&#20316;&#37117;&#26159;&#21363;&#25554;&#21363;&#29992;&#30340;&#65292;&#21487;&#20197;&#28789;&#27963;&#32452;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver attention prediction implies the intention understanding of where the driver intends to go and what object the driver concerned about, which commonly provides a driving task-guided traffic scene understanding. Some recent works explore driver attention prediction in critical or accident scenarios and find a positive role in helping accident prediction, while the promotion ability is constrained by the prediction accuracy of driver attention maps. In this work, we explore the network connection gating mechanism for driver attention prediction (Gate-DAP). Gate-DAP aims to learn the importance of different spatial, temporal, and modality information in driving scenarios with various road types, occasions, and light and weather conditions. The network connection gating in Gate-DAP consists of a spatial encoding network gating, long-short-term memory network gating, and information type gating modules. Each connection gating operation is plug-and-play and can be flexibly assembled, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32452;&#20214;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#20986;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.02527</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#32452;&#20214;&#23545;&#31639;&#27861;&#34892;&#20026;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiobjective Evolutionary Component Effect on Algorithm behavior. (arXiv:2308.02527v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32452;&#20214;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#20986;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#22240;&#38382;&#39064;&#32780;&#24322;&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#26032;&#31639;&#27861;&#25110;&#23558;&#29616;&#26377;&#31639;&#27861;&#24212;&#29992;&#20110;&#26032;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#31616;&#21270;&#26032;&#30340;&#22810;&#30446;&#26631;&#31639;&#27861;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#65292;&#20154;&#20204;&#23545;&#20854;&#32452;&#20214;&#30340;&#33258;&#21160;&#35774;&#35745;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#33258;&#21160;&#35774;&#35745;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21487;&#20197;&#20248;&#20110;&#20154;&#24037;&#24320;&#21457;&#30340;&#23545;&#24212;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#32452;&#20214;&#26159;&#20160;&#20040;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#33258;&#21160;&#35774;&#35745;&#31639;&#27861;&#30340;&#26368;&#32456;&#37197;&#32622;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;&#20998;&#35299;&#30340;&#32463;&#36807;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;(MOEA/D)&#65292;&#35813;&#31639;&#27861;&#30001;&#36845;&#20195;&#31454;&#36895;(irace)&#37197;&#32622;&#21253;&#35774;&#35745;&#65292;&#24212;&#29992;&#20110;&#19977;&#32452;&#26377;&#32422;&#26463;&#38382;&#39064;&#65306;(1)&#20998;&#26512;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;(2)&#20998;&#26512;&#20154;&#24037;&#38382;&#39064;&#65292;(3)&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of multiobjective evolutionary algorithms (MOEAs) varies across problems, making it hard to develop new algorithms or apply existing ones to new problems. To simplify the development and application of new multiobjective algorithms, there has been an increasing interest in their automatic design from their components. These automatically designed metaheuristics can outperform their human-developed counterparts. However, it is still unknown what are the most influential components that lead to performance improvements. This study specifies a new methodology to investigate the effects of the final configuration of an automatically designed algorithm. We apply this methodology to a tuned Multiobjective Evolutionary Algorithm based on Decomposition (MOEA/D) designed by the iterated racing (irace) configuration package on constrained problems of 3 groups: (1) analytical real-world problems, (2) analytical artificial problems and (3) simulated real-world. We then compare the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#25968;&#25454;&#30417;&#25511;&#21644;&#28748;&#28297;&#31995;&#32479;&#25511;&#21046;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02524</link><description>&lt;p&gt;
&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chatbot Application to Support Smart Agriculture in Thailand. (arXiv:2308.02524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27888;&#22269;&#25903;&#25345;&#26234;&#33021;&#20892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#25968;&#25454;&#30417;&#25511;&#21644;&#28748;&#28297;&#31995;&#32479;&#25511;&#21046;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#24555;&#36895;&#22320;&#22238;&#22797;&#25991;&#26412;&#25110;&#35821;&#38899;&#23545;&#35805;&#30340;&#36719;&#20214;&#12290;&#22312;&#20892;&#19994;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#20892;&#19994;&#31995;&#32479;&#20165;&#20351;&#29992;&#26469;&#33258;&#24863;&#24212;&#21644;&#29289;&#32852;&#32593;&#25216;&#26415;&#30340;&#25968;&#25454;&#65292;&#19981;&#21253;&#25324;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#26469;&#25903;&#25345;&#20892;&#27665;&#30340;&#20915;&#31574;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#19968;&#28857;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#21487;&#20197;&#25104;&#20026;&#20892;&#27665;&#30340;&#21161;&#25163;&#65292;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#20316;&#20026;&#20449;&#24687;&#21644;&#30693;&#35782;&#34920;&#31034;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#20316;&#29289;&#31181;&#26893;&#24314;&#35758;&#12290;&#23427;&#19982;&#26234;&#33021;&#20892;&#19994;&#21644;&#25512;&#33616;&#31995;&#32479;&#37197;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#21253;&#25324;&#20116;&#20010;&#20027;&#35201;&#30340;&#21151;&#33021;&#65288;&#24320;&#22987;/&#20572;&#27490;&#33756;&#21333;&#12289;&#20027;&#39029;&#12289;&#28404;&#28748;&#39029;&#38754;&#12289;&#38654;&#28748;&#39029;&#38754;&#21644;&#30417;&#25511;&#39029;&#38754;&#65289;&#12290;&#20892;&#27665;&#20204;&#23558;&#20250;&#33719;&#24471;&#29992;&#20110;&#25968;&#25454;&#30417;&#25511;&#20197;&#25903;&#25345;&#20915;&#31574;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#21487;&#20197;&#36890;&#36807; LINE &#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#25511;&#21046;&#28748;&#28297;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A chatbot is a software developed to help reply to text or voice conversations automatically and quickly in real time. In the agriculture sector, the existing smart agriculture systems just use data from sensing and internet of things (IoT) technologies that exclude crop cultivation knowledge to support decision-making by farmers. To enhance this, the chatbot application can be an assistant to farmers to provide crop cultivation knowledge. Consequently, we propose the LINE chatbot application as an information and knowledge representation providing crop cultivation recommendations to farmers. It works with smart agriculture and recommendation systems. Our proposed LINE chatbot application consists of five main functions (start/stop menu, main page, drip irri gation page, mist irrigation page, and monitor page). Farmers will receive information for data monitoring to support their decision-making. Moreover, they can control the irrigation system via the LINE chatbot. Furthermore, farmer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20855;&#22791;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#30456;&#24403;&#30340;&#39640;&#32423;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.02522</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating ChatGPT and GPT-4 for Visual Programming. (arXiv:2308.02522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#20855;&#22791;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#30456;&#24403;&#30340;&#39640;&#32423;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#20010;&#24615;&#21270;&#21453;&#39304;&#21644;&#20869;&#23481;&#26469;&#26497;&#22823;&#25913;&#21892;&#35745;&#31639;&#26426;&#25945;&#32946;&#30340;&#26684;&#23616;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#32534;&#31243;&#25945;&#32946;&#22330;&#26223;&#19979;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#36825;&#20123;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#31243;&#65292;&#29305;&#21035;&#26159;Python&#32534;&#31243;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#35299;&#31572;&#36825;&#20123;&#27169;&#22411;&#22312;K-8&#32534;&#31243;&#25945;&#32946;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65306;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#19982;&#23427;&#20204;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;Python&#32534;&#31243;&#20013;&#30456;&#24403;&#30340;&#35270;&#35273;&#32534;&#31243;&#33021;&#21147;&#65311;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#21644;GPT-4&#65292;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#22312;&#35270;&#35273;&#32534;&#31243;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#19987;&#23478;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI and large language models have the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. Recent works have studied the capabilities of these models for different programming education scenarios; however, these works considered only text-based programming, in particular, Python programming. Consequently, they leave open the question of how well these models would perform in visual programming domains popularly used for K-8 programming education. The main research question we study is: Do state-of-the-art generative models show advanced capabilities in visual programming on par with their capabilities in text-based Python programming? In our work, we evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, in visual programming domains for various scenarios and assess performance using expert-based annotations. In particular, we base our evaluation using reference tasks from the domains of Hour
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.02514</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20027;&#26041;&#31243;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models as master equation solvers. (arXiv:2308.02514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#27714;&#35299;&#20027;&#26041;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32593;&#32476;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#31934;&#24230;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#26041;&#31243;&#22312;&#24314;&#27169;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#28982;&#32780;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#35299;&#20915;&#20027;&#26041;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24212;&#29992;&#20026;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#20027;&#26041;&#31243;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25552;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#36895;&#29575;&#21442;&#25968;&#12289;&#21021;&#22987;&#26465;&#20214;&#21644;&#26102;&#38388;&#20540;&#30452;&#25509;&#26144;&#23556;&#21040;&#19982;&#36755;&#20837;&#19978;&#19979;&#25991;&#23436;&#20840;&#21305;&#37197;&#30340;&#29366;&#24577;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#36817;&#20284;&#22320;&#27714;&#35299;&#20102;&#20027;&#26041;&#31243;&#30340;&#26368;&#19968;&#33324;&#24418;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23545;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21453;&#39304;&#22870;&#21169;&#30001;&#19968;&#32452;&#21464;&#20998;&#33258;&#22238;&#24402;&#27169;&#22411;&#25552;&#20379;&#12290;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#20110;&#22810;&#27169;&#32452;&#21644;&#39640;&#32500;&#31995;&#32479;&#65292;&#20934;&#30830;&#24615;&#24456;&#39640;&#12290;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Master equations are of fundamental importance in modeling stochastic dynamical systems.However, solving master equations is challenging due to the exponential increase in the number of possible states or trajectories with the dimension of the state space. In this study, we propose repurposing language models as a machine learning approach to solve master equations. We design a prompt-based neural network to map rate parameters, initial conditions, and time values directly to the state joint probability distribution that exactly matches the input contexts. In this way, we approximate the solution of the master equation in its most general form. We train the network using the policy gradient algorithm within the reinforcement learning framework, with feedback rewards provided by a set of variational autoregressive models. By applying this approach to representative examples, we observe high accuracy for both multi-module and high-dimensional systems. The trained network also exhibits ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#25581;&#31034;&#20154;&#33041;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#30693;&#35273;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroImagen&#30340;&#32508;&#21512;&#31649;&#36947;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.02510</link><description>&lt;p&gt;
&#31397;&#35270;&#22823;&#33041;&#65306;&#36890;&#36807;&#20154;&#33041;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#30693;&#35273;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals. (arXiv:2308.02510v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#25581;&#31034;&#20154;&#33041;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#30693;&#35273;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NeuroImagen&#30340;&#32508;&#21512;&#31649;&#36947;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35265;&#21040;&#23601;&#20449;&#65292;&#28982;&#32780;&#20154;&#31867;&#35270;&#35273;&#30693;&#35273;&#19982;&#35748;&#30693;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#26159;&#19968;&#20010;&#35868;&#12290;&#30001;&#20110;&#31070;&#32463;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#33021;&#22815;&#35760;&#24405;&#21040;&#35270;&#35273;&#35825;&#21457;&#30340;&#33041;&#27963;&#21160;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#27169;&#25311;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#36890;&#36807;&#37325;&#24314;&#35266;&#23519;&#21040;&#30340;&#22270;&#20687;&#26469;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#22522;&#20110;&#26131;&#20110;&#33719;&#24471;&#30340;&#33041;&#20449;&#21495;&#65292;&#21363;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#12290;&#30001;&#20110;&#33041;&#30005;&#22270;&#20449;&#21495;&#26159;&#21160;&#24577;&#30340;&#26102;&#38388;&#24207;&#21015;&#26684;&#24335;&#65292;&#21516;&#26102;&#20063;&#22240;&#26377;&#22122;&#38899;&#32780;&#33261;&#21517;&#26157;&#33879;&#65292;&#22788;&#29702;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#38656;&#35201;&#26356;&#22810;&#30340;&#19987;&#38376;&#24037;&#20316;&#65307;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#31649;&#36947;&#65292;&#21517;&#20026;NeuroImagen&#65292;&#29992;&#20110;&#20174;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#22270;&#20687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#26032;&#39062;&#30340;&#22810;&#23618;&#24863;&#30693;&#20449;&#24687;&#35299;&#30721;&#26041;&#27861;&#65292;&#20197;&#20174;&#32473;&#23450;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#33719;&#21462;&#22810;&#31890;&#24230;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NeuroImagen, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multi-grained outputs from the given EEG data. A latent diffusion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#22914;&#28909;&#28857;&#25968;&#25454;&#21644;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27431;&#27954;&#21355;&#26143;&#28779;&#28798;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.02508</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#27169;&#24577;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#27431;&#27954;&#21355;&#26143;&#28779;&#28798;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Supervised Machine Learning Approach for Satellite-based Wildfire Identification in Europe. (arXiv:2308.02508v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#22914;&#28909;&#28857;&#25968;&#25454;&#21644;&#22320;&#34920;&#35206;&#30422;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#27431;&#27954;&#21355;&#26143;&#28779;&#28798;&#30340;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#28798;&#38590;&#24615;&#33258;&#28982;&#20107;&#20214;&#65292;&#20363;&#22914;&#37326;&#28779;&#65292;&#21628;&#21505;&#24320;&#21457;&#24555;&#36895;&#21644;&#33258;&#21160;&#21270;&#30340;&#28779;&#28798;&#26816;&#27979;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37326;&#28779;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20449;&#24687;&#28304;&#25552;&#39640;&#33258;&#21160;&#21270;&#21355;&#26143;&#28909;&#28857;&#26816;&#27979;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;Moderate-resolution Imaging Spectroradiometer (MODIS)&#21644;Visible Infrared Imaging Radiometer Suite (VIIRS)&#28909;&#28857;&#26381;&#21153;&#26816;&#27979;&#21040;&#30340;&#28909;&#24322;&#24120;&#19982;&#27431;&#27954;&#26862;&#26519;&#28779;&#28798;&#20449;&#24687;&#31995;&#32479; (EFFIS) &#25968;&#25454;&#24211;&#36827;&#34892;&#20132;&#21449;&#24341;&#29992;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#27431;&#27954;&#28779;&#28798;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#28909;&#28857;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#28040;&#38500;&#28909;&#28857;&#26816;&#27979;&#20013;&#30340;&#27495;&#20041;&#65292;&#21306;&#20998;&#37326;&#28779;&#21644;&#20854;&#20182;&#20107;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#28304;&#65292;&#20363;&#22914;ERSI&#24180;&#24230;&#22303;&#22320;&#21033;&#29992;&#19982;&#22303;&#22320;&#35206;&#30422; (LULC) &#21644;Copernicus Sentinel-3&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
The increasing frequency of catastrophic natural events, such as wildfires, calls for the development of rapid and automated wildfire detection systems. In this paper, we propose a wildfire identification solution to improve the accuracy of automated satellite-based hotspot detection systems by leveraging multiple information sources. We cross-reference the thermal anomalies detected by the Moderate-resolution Imaging Spectroradiometer (MODIS) and the Visible Infrared Imaging Radiometer Suite (VIIRS) hotspot services with the European Forest Fire Information System (EFFIS) database to construct a large-scale hotspot dataset for wildfire-related studies in Europe. Then, we propose a novel multimodal supervised machine learning approach to disambiguate hotspot detections, distinguishing between wildfires and other events. Our methodology includes the use of multimodal data sources, such as the ERSI annual Land Use Land Cover (LULC) and the Copernicus Sentinel-3 data. Experimental results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.02312</link><description>&lt;p&gt;
&#35841;&#22238;&#31572;&#30340;&#26356;&#22909;&#65311;&#23545;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&amp;A&#24179;&#21488;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#19968;&#30452;&#26159;&#31243;&#24207;&#21592;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#33539;&#24335;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#23613;&#31649;ChatGPT&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#25110;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22238;&#31572;517&#20010;Stack Overflow&#65288;SO&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#23545;ChatGPT&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32508;&#21512;&#24615;&#21644;&#31616;&#27905;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#20998;&#26512;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;ChatGPT&#22238;&#31572;&#22312;&#35821;&#35328;&#21644;&#20154;&#31867;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;52&#65285;&#30340;ChatGPT&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;77&#65285;&#30340;&#22238;&#31572;&#20887;&#38271;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;ChatGPT&#22238;&#31572;&#20173;&#28982;&#22312;39.34&#65285;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#38738;&#30544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A platforms have been an integral part of the web-help-seeking behavior of programmers over the past decade. However, with the recent introduction of ChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift. Despite the popularity of ChatGPT, no comprehensive study has been conducted to evaluate the characteristics or usability of ChatGPT's answers to software engineering questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT's answers. Furthermore, we conducted a large-scale linguistic analysis, and a user study to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\% of ChatGPT answers are incorrect and 77\% are verbose. Nonetheless, ChatGPT answers are still preferred 39.34\% of the time due to their comprehensiveness and well-articulated langu
&lt;/p&gt;</description></item><item><title>OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;</title><link>http://arxiv.org/abs/2308.01390</link><description>&lt;p&gt;
OpenFlamingo: &#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01390
&lt;/p&gt;
&lt;p&gt;
OpenFlamingo&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;&#23545;&#24212;&#27169;&#22411;&#24615;&#33021;&#30340;80%&#33267;89%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;OpenFlamingo&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#33258;&#22238;&#24402;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;3B&#21040;9B&#12290; OpenFlamingo&#26159;&#19968;&#20010;&#25345;&#32493;&#21162;&#21147;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#22797;&#21046;DeepMind&#30340;Flamingo&#27169;&#22411;&#30340;&#24320;&#28304;&#29256;&#26412;&#12290;&#22312;&#19971;&#20010;&#35270;&#35273;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#65292;OpenFlamingo&#27169;&#22411;&#30340;&#24615;&#33021;&#20171;&#20110;&#23545;&#24212;&#30340;Flamingo&#24615;&#33021;&#30340;80%&#33267;89%&#20043;&#38388;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#36229;&#21442;&#25968;&#21644;&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;https://github.com/mlfoundations/open_flamingo&#19978;&#20998;&#20139;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;</title><link>http://arxiv.org/abs/2308.01157</link><description>&lt;p&gt;
LLMs&#29702;&#35299;&#29627;&#29827;&#30418;&#27169;&#22411;&#65292;&#21457;&#29616;&#24778;&#21916;&#24182;&#25552;&#20986;&#20462;&#22797;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01157
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#21644;&#33258;&#21160;&#21270;&#30340;&#24322;&#24120;&#26816;&#27979;&#12289;&#21407;&#22240;&#25551;&#36848;&#21644;&#20462;&#22797;&#24314;&#35758;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20316;&#20026;&#31034;&#20363;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#21253;$\texttt{TalkToEBM}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21487;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#23558;&#22797;&#26434;&#32467;&#26524;&#20998;&#35299;&#20026;&#21333;&#19968;&#21464;&#37327;&#30340;&#22270;&#34920;&#31034;&#32452;&#20214;&#12290;&#36890;&#36807;&#37319;&#29992;&#23618;&#27425;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#25972;&#20010;&#27169;&#22411;&#36866;&#24212;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20840;&#38754;&#30340;&#27169;&#22411;&#32423;&#24635;&#32467;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#33021;&#22815;&#24212;&#29992;&#20854;&#24191;&#27867;&#30340;&#32972;&#26223;&#30693;&#35782;&#26469;&#33258;&#21160;&#23436;&#25104;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#26816;&#27979;&#19982;&#20808;&#21069;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24322;&#24120;&#65292;&#25551;&#36848;&#24322;&#24120;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#21435;&#38500;&#24322;&#24120;&#30340;&#20462;&#22797;&#24314;&#35758;&#12290;&#25105;&#20204;&#20351;&#29992;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#22810;&#20010;&#31034;&#20363;&#26469;&#35777;&#26126;LLMs&#30340;&#36825;&#20123;&#26032;&#33021;&#21147;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;(GAMs)&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;$\texttt{TalkToEBM}$&#21253;&#20316;&#20026;&#19968;&#20010;&#24320;&#28304;&#30340;LLM-GAM&#25509;&#21475;&#36827;&#34892;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that large language models (LLMs) are remarkably good at working with interpretable models that decompose complex outcomes into univariate graph-represented components. By adopting a hierarchical approach to reasoning, LLMs can provide comprehensive model-level summaries without ever requiring the entire model to fit in context. This approach enables LLMs to apply their extensive background knowledge to automate common tasks in data science such as detecting anomalies that contradict prior knowledge, describing potential reasons for the anomalies, and suggesting repairs that would remove the anomalies. We use multiple examples in healthcare to demonstrate the utility of these new capabilities of LLMs, with particular emphasis on Generalized Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as an open-source LLM-GAM interface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01011</link><description>&lt;p&gt;
&#20351;&#29992;Floss&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#39057;&#22495;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;Floss&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#19978;&#23545;&#23398;&#21040;&#30340;&#34920;&#31034;&#36827;&#34892;&#27491;&#21017;&#21270;&#26469;&#22686;&#24378;&#21608;&#26399;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;Floss&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21608;&#26399;&#24615;&#24182;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#24448;&#24448;&#19981;&#33021;&#34987;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20805;&#20998;&#25429;&#25417;&#21040;&#12290;&#36825;&#23548;&#33268;&#23545;&#24863;&#20852;&#36259;&#30340;&#22522;&#30784;&#21160;&#24577;&#34892;&#20026;&#30340;&#34920;&#31034;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#21483;&#20570;Floss&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#21270;&#22320;&#22312;&#39057;&#22495;&#19978;&#35843;&#25972;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;Floss&#26041;&#27861;&#39318;&#20808;&#33258;&#21160;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20027;&#35201;&#21608;&#26399;&#24615;&#12290;&#28982;&#21518;&#65292;&#23427;&#21033;&#29992;&#21608;&#26399;&#31227;&#20301;&#21644;&#35889;&#23494;&#24230;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#20855;&#26377;&#21608;&#26399;&#19968;&#33268;&#24615;&#30340;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;Floss&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#21040;&#26377;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series analysis is a fundamental task in various application domains, and deep learning approaches have demonstrated remarkable performance in this area. However, many real-world time series data exhibit significant periodic or quasi-periodic dynamics that are often not adequately captured by existing deep learning-based solutions. This results in an incomplete representation of the underlying dynamic behaviors of interest. To address this gap, we propose an unsupervised method called Floss that automatically regularizes learned representations in the frequency domain. The Floss method first automatically detects major periodicities from the time series. It then employs periodic shift and spectral density similarity measures to learn meaningful representations with periodic consistency. In addition, Floss can be easily incorporated into both supervised, semi-supervised, and unsupervised learning frameworks. We conduct extensive experiments on common time series classification, for
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00628</link><description>&lt;p&gt;
&#20154;&#31867;-M3&#65306;&#19968;&#20010;&#29992;&#20110;&#23460;&#22806;&#22330;&#26223;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#22810;&#35270;&#35282;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;Human-M3&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23460;&#22806;&#22330;&#26223;3D&#20154;&#20307;&#23039;&#21183;&#25968;&#25454;&#38598;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#21482;&#20351;&#29992;&#19968;&#31181;&#27169;&#24577;&#65288;RGB&#22270;&#20687;&#25110;&#28857;&#20113;&#65289;&#65292;&#24182;&#19988;&#22330;&#26223;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#20154;&#12290;&#25968;&#25454;&#38598;&#22522;&#30784;&#30340;&#26377;&#38480;&#33539;&#22260;&#20005;&#37325;&#38459;&#30861;&#20102;&#21487;&#29992;&#25968;&#25454;&#30340;&#21464;&#21270;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Human-M3&#65292;&#36825;&#26159;&#19968;&#20010;&#23460;&#22806;&#22810;&#27169;&#24577;&#22810;&#35270;&#35282;&#22810;&#20154;&#31867;&#23039;&#21183;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#23460;&#22806;&#22330;&#26223;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#28857;&#20113;&#25968;&#25454;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#36755;&#20837;&#30340;&#31639;&#27861;&#26469;&#29983;&#25104;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#40065;&#26834;&#30340;&#28857;&#20113;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#23460;&#22806;&#22330;&#26223;&#20013;&#22810;&#20010;&#20154;&#30340;&#22810;&#35270;&#35282;RGB&#35270;&#39057;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#20934;&#30830;&#20154;&#20307;&#23450;&#20301;&#21644;&#21305;&#37197;&#27169;&#31946;&#38382;&#39064;&#65292;&#29983;&#25104;&#20102;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.00143</link><description>&lt;p&gt;
&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20869;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#37322;DNN&#30340;&#34892;&#20026;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#21453;&#24212;&#24335;&#31995;&#32479;&#20013;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;DNNs&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#36825;&#20351;&#24471;&#35299;&#37322;&#21644;&#35777;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#23545;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#25216;&#26415;&#33021;&#22815;&#25214;&#20986;&#23548;&#33268;DNN&#34892;&#20026;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;XAI&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;(i)&#23427;&#20204;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#35299;&#37322;&#27491;&#30830;&#24615;&#30340;&#27491;&#24335;&#20445;&#35777;&#65307;(ii)&#23427;&#20204;&#36890;&#24120;&#36866;&#29992;&#20110;&#8220;&#19968;&#27425;&#24615;&#8221;&#31995;&#32479;(&#21363;DNN&#29420;&#31435;&#20110;&#36807;&#21435;&#30340;&#35843;&#29992;)&#65292;&#32780;&#19981;&#26159;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#22987;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;DNN&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;XAI&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#22810;&#27493;&#39588;&#30340;&#21453;&#24212;&#24335;&#31995;&#32479;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#30340;&#36716;&#25442;&#32422;&#26463;&#26469;&#35745;&#31639;&#31616;&#27905;&#30340;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#20943;&#23569;&#24213;&#23618;&#39564;&#35777;&#22120;&#25152;&#25506;&#32034;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.00121</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#65306;AI&#20316;&#20026;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
Getting pwn'd by AI: Penetration Testing with Large Language Models. (arXiv:2308.00121v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#20316;&#20026;AI&#21161;&#25163;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#20004;&#31181;&#29992;&#20363;&#65292;&#21462;&#24471;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23433;&#20840;&#27979;&#35797;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28183;&#36879;&#27979;&#35797;&#26159;&#19968;&#39033;&#38656;&#35201;&#39640;&#27700;&#24179;&#19987;&#19994;&#30693;&#35782;&#30340;&#27963;&#21160;&#65292;&#24182;&#28041;&#21450;&#35768;&#22810;&#25163;&#21160;&#27979;&#35797;&#21644;&#20998;&#26512;&#27493;&#39588;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5&#65289;&#26469;&#22686;&#24378;&#28183;&#36879;&#27979;&#35797;&#20154;&#21592;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29992;&#20363;&#65306;&#29992;&#20110;&#23433;&#20840;&#27979;&#35797;&#20219;&#21153;&#30340;&#39640;&#32423;&#20219;&#21153;&#35268;&#21010;&#21644;&#22312;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#20013;&#36827;&#34892;&#20302;&#32423;&#28431;&#27934;&#23547;&#25214;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#38381;&#29615;&#21453;&#39304;&#65292;&#23558;&#30001;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20302;&#32423;&#25805;&#20316;&#19982;&#26131;&#21463;&#25915;&#20987;&#30340;&#34394;&#25311;&#26426;&#65288;&#36890;&#36807;SSH&#36830;&#25509;&#65289;&#30456;&#36830;&#65292;&#24182;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#34394;&#25311;&#26426;&#29366;&#24577;&#20197;&#23547;&#25214;&#28431;&#27934;&#65292;&#24182;&#25552;&#20379;&#20855;&#20307;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26377;&#21069;&#26223;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#36884;&#24452;&#65292;&#24182;&#23601;&#25552;&#20379;&#35813;&#25216;&#26415;&#30340;&#20262;&#29702;&#38382;&#39064;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of software security testing, more specifically penetration testing, is an activity that requires high levels of expertise and involves many manual testing and analysis steps. This paper explores the potential usage of large-language models, such as GPT3.5, to augment penetration testers with AI sparring partners. We explore the feasibility of supplementing penetration testers with AI models for two distinct use cases: high-level task planning for security testing assignments and low-level vulnerability hunting within a vulnerable virtual machine. For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine. We discuss promising initial results, detail avenues for improvement, and close deliberating on the ethics of providi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.16706</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD-Learning&#30340;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;ODE&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Distributed Dynamic Programming and an O.D.E. Framework of Distributed TD-Learning for Networked Multi-Agent Markov Decision Processes. (arXiv:2307.16706v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#21644;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#28857;&#12290;&#35813;&#20998;&#24067;&#24335;DP&#31639;&#27861;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#20998;&#24067;&#24335;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#21644;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#33258;&#24049;&#30340;&#22870;&#21169;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#26234;&#33021;&#20307;&#22870;&#21169;&#30340;&#20102;&#35299;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#33021;&#36890;&#36807;&#19968;&#20010;&#30001;&#22270;&#34920;&#31034;&#30340;&#36890;&#20449;&#32593;&#32476;&#19982;&#30456;&#37051;&#26234;&#33021;&#20307;&#20849;&#20139;&#20854;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21487;&#20197;&#24635;&#32467;&#20026;&#20004;&#20010;&#20851;&#38190;&#28857;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21463;&#36830;&#32493;&#26102;&#38388;&#21306;&#38388;&#20869;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#26041;&#27861;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;DP&#12290;&#36890;&#36807;&#25511;&#21046;&#29702;&#35770;&#30340;&#35270;&#35282;&#35780;&#20272;&#20102;&#35813;DP&#30340;&#25910;&#25947;&#24615;&#12290;2&#65289;&#22522;&#20110;&#19978;&#36848;DP&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;TD&#23398;&#20064;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;DP&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary objective of this paper is to investigate distributed dynamic programming (DP) and distributed temporal difference (TD) learning algorithms for networked multi-agent Markov decision problems (MAMDPs). In our study, we adopt a distributed multi-agent framework where individual agents have access only to their own rewards, lacking insights into the rewards of other agents. Additionally, each agent has the ability to share its parameters with neighboring agents through a communication network, represented by a graph. Our contributions can be summarized in two key points: 1) We introduce a novel distributed DP, inspired by the averaging consensus method in the continuous-time domain. The convergence of this DP is assessed through control theory perspectives. 2) Building upon the aforementioned DP, we devise a new distributed TD-learning algorithm and prove its convergence. A standout feature of our proposed distributed DP is its incorporation of two independent dynamic systems,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14361</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#30340;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#22240;&#31361;&#21464;&#22312;&#30284;&#30151;&#20013;&#30340;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#20256;&#32479;&#21644;&#26368;&#26032;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;LSTM&#12289;BiLSTM&#12289;CNN&#12289;GRU&#21644;GloVe&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#22312;Kaggle&#30340;&#8220;&#20010;&#24615;&#21270;&#21307;&#23398;&#65306;&#37325;&#26032;&#23450;&#20041;&#30284;&#30151;&#27835;&#30103;&#8221;&#25968;&#25454;&#38598;&#20013;&#23545;&#22522;&#22240;&#31361;&#21464;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#19982;BERT&#12289;Electra&#12289;Roberta&#12289;XLNet&#12289;Distilbert&#20197;&#21450;&#23427;&#20204;&#30340;LSTM&#38598;&#25104;&#31561;&#30693;&#21517;&#36716;&#25442;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;&#22343;&#26041;&#35823;&#24046;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#36824;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#30340;&#23436;&#32654;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#38598;&#25104;&#27169;&#22411;&#22312;&#22522;&#22240;&#31361;&#21464;&#20998;&#31867;&#31561;&#22256;&#38590;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#27668;&#20505;&#21464;&#21270;&#35780;&#20272;&#27169;&#22411;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20851;&#38190;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#65292;&#21516;&#26102;&#20063;&#23545;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#21457;&#23637;RICE-N&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.13894</link><description>&lt;p&gt;
AI4GCC - &#22242;&#38431;: &#20302;&#20110;&#28023;&#24179;&#38754;: &#35780;&#35770;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
AI4GCC - Team: Below Sea Level: Critiques and Improvements. (arXiv:2307.13894v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#27668;&#20505;&#21464;&#21270;&#35780;&#20272;&#27169;&#22411;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20851;&#38190;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24314;&#35758;&#65292;&#21516;&#26102;&#20063;&#23545;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#21457;&#23637;RICE-N&#26694;&#26550;&#65292;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#27169;&#25311;&#26694;&#26550;RICE-N&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#27668;&#20505;&#21464;&#21270;&#23545;&#32463;&#27982;&#24433;&#21709;&#30340;&#32508;&#21512;&#35780;&#20272;&#27169;&#22411;(IAM)&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;RICE-N&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#34892;&#21160;&#23631;&#34109;&#21644;&#26080;&#20851;&#30340;&#34892;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#24314;&#35758;&#65292;&#22914;&#21033;&#29992;&#20851;&#31246;&#25910;&#20837;&#21644;&#24809;&#32602;&#36807;&#37327;&#29983;&#20135;&#12290;&#25105;&#20204;&#36824;&#25209;&#21028;&#24615;&#22320;&#35752;&#35770;&#20102;IAM&#30340;&#29305;&#24449;&#65292;&#21363;&#36807;&#20110;&#20048;&#35266;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#19981;&#29616;&#23454;&#30340;&#20943;&#25490;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#19981;&#26029;&#25913;&#36827;RICE-N&#26694;&#26550;&#65292;&#20351;&#20854;&#20316;&#20026;&#25919;&#31574;&#21046;&#23450;&#32773;&#30340;&#28789;&#24863;&#26356;&#21152;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a critical analysis of the simulation framework RICE-N, an integrated assessment model (IAM) for evaluating the impacts of climate change on the economy. We identify key issues with RICE-N, including action masking and irrelevant actions, and suggest improvements such as utilizing tariff revenue and penalizing overproduction. We also critically engage with features of IAMs in general, namely overly optimistic damage functions and unrealistic abatement cost functions. Our findings contribute to the ongoing efforts to further develop the RICE-N framework in an effort to improve the simulation, making it more useful as an inspiration for policymakers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.13582</link><description>&lt;p&gt;
&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#20013;&#30340;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Argument Attribution Explanations in Quantitative Bipolar Argumentation Frameworks. (arXiv:2307.13582v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#20013;&#22635;&#34917;&#20102;&#35299;&#37322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26377;&#20960;&#20010;&#20154;&#25552;&#20513;&#35770;&#35777;&#24615;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#35770;&#35777;&#26694;&#26550;&#65288;AFs&#65289;&#30340;&#25512;&#29702;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#34429;&#28982;&#20851;&#20110;&#29992;&#36777;&#35770;/&#20105;&#35770;/&#23545;&#35805;&#30340;&#25193;&#23637;&#35821;&#20041;&#31934;&#31070;&#23450;&#24615;&#22320;&#35299;&#37322;AFs&#30340;&#25512;&#29702;&#32467;&#26524;&#30340;&#30740;&#31350;&#25104;&#26524;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#28176;&#36827;&#35821;&#20041;&#19979;&#35299;&#37322;AFs&#30340;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#21364;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#65292;&#23613;&#31649;&#22312;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#24402;&#22240;&#31934;&#31070;&#24341;&#20837;&#23450;&#37327;&#21452;&#26497;&#35770;&#35777;&#26694;&#26550;&#65288;QBAFs&#65289;&#30340;&#32972;&#26223;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#8220;&#35770;&#35777;&#24402;&#22240;&#35299;&#37322;&#65288;AAEs&#65289;&#8221;&#29702;&#35770;&#65292;&#29992;&#20110;&#30830;&#23450;&#35770;&#35777;&#23545;&#8220;&#20027;&#39064;&#35770;&#35777;&#8221;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#32780;&#29305;&#24449;&#24402;&#22240;&#21017;&#29992;&#20110;&#30830;&#23450;&#29305;&#24449;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentative explainable AI has been advocated by several in recent years, with an increasing interest on explaining the reasoning outcomes of Argumentation Frameworks (AFs). While there is a considerable body of research on qualitatively explaining the reasoning outcomes of AFs with debates/disputes/dialogues in the spirit of \emph{extension-based semantics}, explaining the quantitative reasoning outcomes of AFs under \emph{gradual semantics} has not received much attention, despite widespread use in applications. In this paper, we contribute to filling this gap by proposing a novel theory of \emph{Argument Attribution Explanations (AAEs)} by incorporating the spirit of feature attribution from machine learning in the context of Quantitative Bipolar Argumentation Frameworks (QBAFs): whereas feature attribution is used to determine the influence of features towards outputs of machine learning models, AAEs are used to determine the influence of arguments towards \emph{topic argument}s 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2307.13294</link><description>&lt;p&gt;
&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Imperceptible Physical Attack against Face Recognition Systems via LED Illumination Modulation. (arXiv:2307.13294v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LED&#29031;&#26126;&#35843;&#21046;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#36827;&#34892;&#19981;&#21487;&#23519;&#35273;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#33080;&#35782;&#21035;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#24320;&#22987;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#20294;&#25105;&#20204;&#38656;&#35201;&#27880;&#24847;&#21040;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20154;&#33080;&#35782;&#21035;&#35270;&#35273;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20004;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#25968;&#23383;&#25915;&#20987;&#21644;&#29289;&#29702;&#25915;&#20987;&#65292;&#37117;&#26377;&#32570;&#28857;&#65292;&#21069;&#32773;&#19981;&#23454;&#29992;&#65292;&#21518;&#32773;&#26174;&#30524;&#12289;&#35745;&#31639;&#37327;&#22823;&#19988;&#19981;&#21487;&#25191;&#34892;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#12289;&#21487;&#25191;&#34892;&#12289;&#19981;&#26174;&#30524;&#19988;&#35745;&#31639;&#37327;&#36739;&#20302;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#22522;&#20110;LED&#29031;&#26126;&#35843;&#21046;&#12290;&#20026;&#20102;&#27450;&#39575;&#31995;&#32479;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#36890;&#36807;&#23545;&#22330;&#26223;LED&#29031;&#26126;&#36827;&#34892;&#24555;&#36895;&#24378;&#24230;&#35843;&#21046;&#65292;&#22312;&#20154;&#30524;&#30475;&#19981;&#21040;&#30340;&#33539;&#22260;&#20869;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#20142;&#24230;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;CMOS&#22270;&#20687;&#20256;&#24863;&#22120;&#30340;&#21367;&#24088;&#24555;&#38376;&#25928;&#24212;&#65292;&#21521;&#25429;&#33719;&#30340;&#20154;&#33080;&#22270;&#20687;&#20013;&#27880;&#20837;&#20142;&#24230;&#20449;&#24687;&#25200;&#21160;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20154;&#33080;&#26816;&#27979;&#30340;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#20154;&#33080;&#39564;&#35777;&#30340;&#36530;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, the current two categories of adversarial attacks, namely digital attacks and physical attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and inexecutable. To address the issues, we propose a practical, executable, inconspicuous and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary,we present a denial-of-service (DoS) attack for face detection and a dodging attack for face verifica
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12306</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#32500;&#24230;&#35781;&#21650;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#39640;&#32500;&#24230;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#35781;&#21650;(CoD)&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#26469;&#26497;&#24230;&#31246;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#22312;&#35299;&#20915;&#39640;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#20013;&#38754;&#20020;&#26497;&#22823;&#25361;&#25112;&#65292;&#27491;&#22914;Richard Bellman&#22312;60&#24180;&#21069;&#39318;&#27425;&#25351;&#20986;&#30340;&#37027;&#26679;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#39640;&#32500;&#24230;&#19978;&#25968;&#20540;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#36825;&#26679;&#30340;&#35745;&#31639;&#20195;&#20215;&#36807;&#39640;&#65292;&#32780;&#23558;&#19968;&#33324;&#38750;&#32447;&#24615;PDEs&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#20174;&#26410;&#23454;&#29616;&#36807;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#20449;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25193;&#23637;&#21040;&#35299;&#20915;&#20219;&#24847;&#39640;&#32500;PDEs&#12290;&#35813;&#26032;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#32500;&#24230;&#26799;&#24230;&#19979;&#38477;(SDGD)&#65292;&#23558;PDE&#30340;&#26799;&#24230;&#20998;&#35299;&#20026;&#19982;&#19981;&#21516;&#32500;&#24230;&#23545;&#24212;&#30340;&#37096;&#20998;&#65292;&#24182;&#22312;&#35757;&#32451;PINNs&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#38543;&#26426;&#36873;&#25321;&#36825;&#20123;&#32500;&#24230;&#37096;&#20998;&#30340;&#23376;&#38598;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25910;&#25947;&#20445;&#35777;&#21644;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
&lt;/p&gt;</description></item><item><title>TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.08674</link><description>&lt;p&gt;
TableGPT&#65306;&#23558;&#34920;&#26684;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#32479;&#19968;&#21040;&#19968;&#20010;GPT&#20013;
&lt;/p&gt;
&lt;p&gt;
TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT. (arXiv:2307.08674v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08674
&lt;/p&gt;
&lt;p&gt;
TableGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#20351;LLMs&#33021;&#22815;&#26080;&#32541;&#22320;&#19982;&#34920;&#26684;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#32473;&#29992;&#25143;&#12290;&#20854;&#20013;&#30340;&#21019;&#26032;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24211;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#38656;&#35201;&#20154;&#20204;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#36827;&#34892;&#20998;&#26512;&#21644;&#25805;&#20316;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#19982;&#34920;&#26684;&#20132;&#20114;&#25104;&#20026;&#21487;&#33021;&#65292;&#20351;&#24471;&#36825;&#31181;&#33021;&#21147;&#26356;&#21152;&#25509;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TableGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#31934;&#35843;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#21151;&#33021;&#21629;&#20196;&#29702;&#35299;&#21644;&#25805;&#20316;&#34920;&#26684;&#12290;&#23427;&#24341;&#20837;&#20102;&#19982;&#34920;&#26684;&#26080;&#32541;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#24191;&#27867;&#30340;&#21151;&#33021;&#65292;&#22914;&#38382;&#31572;&#12289;&#25968;&#25454;&#25805;&#20316;&#65288;&#20363;&#22914;&#25554;&#20837;&#12289;&#21024;&#38500;&#12289;&#26597;&#35810;&#21644;&#20462;&#25913;&#25805;&#20316;&#65289;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#12289;&#20998;&#26512;&#25253;&#21578;&#29983;&#25104;&#21644;&#33258;&#21160;&#39044;&#27979;&#12290;TableGPT&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#20379;&#20415;&#21033;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;TableGPT&#30340;&#26680;&#24515;&#26159;&#20840;&#23616;&#34920;&#26684;&#34920;&#31034;&#30340;&#26032;&#27010;&#24565;&#65292;&#23427;&#20351;LLMs&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#20869;&#23481;&#65292;&#24182;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#21629;&#20196;&#25805;&#20316;&#23545;&#34920;&#26684;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the ent
&lt;/p&gt;</description></item><item><title>&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#23454;&#29616;&#30340;&#26234;&#33021;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#25216;&#26415;&#65292;&#22312;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.06687</link><description>&lt;p&gt;
&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#25361;&#25112;&#12289;&#26041;&#27861;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities. (arXiv:2307.06687v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06687
&lt;/p&gt;
&lt;p&gt;
&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#23454;&#29616;&#30340;&#26234;&#33021;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#25216;&#26415;&#65292;&#22312;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#34987;&#30740;&#31350;&#29992;&#26469;&#38761;&#26032;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#21644;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#32593;&#32476;&#34394;&#25311;&#20307;&#39564;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#20041;&#29702;&#35299;&#19982;&#34920;&#31034;&#26469;&#23454;&#29616;&#22312;&#28151;&#21512;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#26080;&#32541;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#20013;&#22235;&#20010;&#22522;&#26412;&#31995;&#32479;&#32452;&#20214;&#30340;&#26234;&#33021;&#21644;&#26102;&#31354;&#29305;&#24449;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#65288;STDR&#65289;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#65288;SIoT&#65289;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#65288;SDT&#65289;&#12290;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#36825;&#22235;&#20010;&#22522;&#26412;&#31995;&#32479;&#32452;&#20214;&#30340;&#20856;&#22411;&#25216;&#26415;&#65292;&#20351;&#24471;&#22312;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#20013;&#33021;&#22815;&#36827;&#34892;&#26234;&#33021;&#12289;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#65292;&#24182;&#32473;&#20986;&#20102;&#20856;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#22914;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatio-temporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatio-temporal data representation (STDR), semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01292</link><description>&lt;p&gt;
Pareto-&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PSML&#65289;&#65306;&#25351;&#32441;&#21644;&#20445;&#25252;&#25512;&#26029;&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#23558;&#26597;&#35810;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#24182;&#25351;&#23450;&#25152;&#38656;&#30340;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#31561;&#65289;&#12290;&#26381;&#21153;&#22120;&#22312;&#21518;&#31471;&#32500;&#25252;&#19968;&#32452;&#27169;&#22411;&#65288;&#27169;&#22411;&#24211;&#65289;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#25351;&#26631;&#25552;&#20379;&#26597;&#35810;&#26381;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#38544;&#34255;&#22312;&#25512;&#29702;&#26381;&#21153;&#25509;&#21475;&#32972;&#21518;&#30340;&#27169;&#22411;&#24211;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#30830;&#23450;&#20351;&#29992;&#30340;&#26159;&#21738;&#20010;&#27169;&#22411;&#12290;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#27493;&#39588;&#26469;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#37117;&#33021;&#24471;&#21040;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#27169;&#22411;&#25552;&#21462;&#21487;&#20197;&#20855;&#26377;&#20445;&#30495;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16703</link><description>&lt;p&gt;
&#24377;&#24615;&#32422;&#26463;&#19979;&#30340;&#20803;&#23398;&#20064;&#22120;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastically-Constrained Meta-Learner for Federated Learning. (arXiv:2306.16703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#23548;&#33268;&#20803;&#23398;&#20064;&#30340;&#19981;&#31283;&#23450;&#30446;&#26631;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#31105;&#27490;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#22240;&#20026;&#21333;&#20010;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20803;&#23398;&#20064;&#65288;&#22914;Per-FedAvg&#65289;&#12290;&#20803;&#23398;&#20064;&#23398;&#20064;&#36866;&#29992;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20849;&#20139;&#21021;&#22987;&#21442;&#25968;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#23558;&#21021;&#22987;&#21270;&#24555;&#36895;&#35843;&#25972;&#21040;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#27169;&#22411;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#37319;&#26679;&#26356;&#26032;&#30340;&#38543;&#26426;&#24615;&#65292;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#26412;&#22320;&#36866;&#24212;&#21516;&#19968;&#23458;&#25143;&#31471;&#26102;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#21516;&#36866;&#24212;&#26041;&#21521;&#30340;&#27874;&#21160;&#38459;&#30861;&#20102;&#20803;&#23398;&#20064;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21382;&#21490;&#26412;&#22320;&#35843;&#25972;&#30340;&#27169;&#22411;&#26469;&#38480;&#21046;&#20869;&#24490;&#29615;&#30340;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#32422;&#26463;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an approach to collaboratively training machine learning models for multiple parties that prohibit data sharing. One of the challenges in federated learning is non-IID data between clients, as a single model can not fit the data distribution for all clients. Meta-learning, such as Per-FedAvg, is introduced to cope with the challenge. Meta-learning learns shared initial parameters for all clients. Each client employs gradient descent to adapt the initialization to local data distributions quickly to realize model personalization. However, due to non-convex loss function and randomness of sampling update, meta-learning approaches have unstable goals in local adaptation for the same client. This fluctuation in different adaptation directions hinders the convergence in meta-learning. To overcome this challenge, we use the historical local adapted model to restrict the direction of the inner loop and propose an elastic-constrained method. As a result, the current round
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06651</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#21106;&#23398;&#20064;&#39044;&#27979;&#36719;&#20214;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Predicting Software Performance with Divide-and-Learn. (arXiv:2306.06651v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$DaL$&#30340;&#22522;&#20110;&#20998;&#21106;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#39640;&#24230;&#37197;&#32622;&#30340;&#36719;&#20214;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#24615;&#33021;&#27979;&#35797;&#21644;&#36136;&#37327;&#20445;&#35777;&#30340;&#22522;&#30784;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20381;&#38752;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#26469;&#24314;&#27169;&#36719;&#20214;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#28385;&#36275;&#37197;&#32622;&#26223;&#35266;&#20013;&#32487;&#25215;&#30340;&#31232;&#30095;&#24615;&#65306;&#37197;&#32622;&#36873;&#39033;&#65288;&#29305;&#24449;&#65289;&#30340;&#24433;&#21709;&#21644;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#24067;&#37117;&#38750;&#24120;&#31232;&#30095;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20998;&#21106;&#23398;&#20064;&#8221;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;$DaL$&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#20026;&#20102;&#22788;&#29702;&#26679;&#26412;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#23558;&#37197;&#32622;&#26223;&#35266;&#20013;&#30340;&#26679;&#26412;&#21010;&#20998;&#20026;&#36828;&#31163;&#30340;&#37096;&#20998;&#65292;&#23545;&#20110;&#27599;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26412;&#22320;&#27169;&#22411;&#26469;&#22788;&#29702;&#29305;&#24449;&#31232;&#30095;&#24615;&#12290;&#28982;&#21518;&#65292;&#26032;&#32473;&#23450;&#30340;&#37197;&#32622;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#32456;&#39044;&#27979;&#30340;&#27491;&#30830;&#27169;&#22411;&#12290;&#20843;&#20010;&#30495;&#23454;&#31995;&#32479;&#21644;&#20116;&#32452;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Predicting the performance of highly configurable software systems is the foundation for performance testing and quality assurance. To that end, recent work has been relying on machine/deep learning to model software performance. However, a crucial yet unaddressed challenge is how to cater for the sparsity inherited from the configuration landscape: the influence of configuration options (features) and the distribution of data samples are highly sparse.  In this paper, we propose an approach based on the concept of 'divide-and-learn', dubbed $DaL$. The basic idea is that, to handle sample sparsity, we divide the samples from the configuration landscape into distant divisions, for each of which we build a regularized Deep Neural Network as the local model to deal with the feature sparsity. A newly given configuration would then be assigned to the right model of division for the final prediction.  Experiment results from eight real-world systems and five sets of training data reveal that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;</title><link>http://arxiv.org/abs/2306.01953</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;AI&#65292;&#35777;&#26126;&#20102;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#26159;&#21487;&#28165;&#38500;&#30340;
&lt;/p&gt;
&lt;p&gt;
Invisible Image Watermarks Are Provably Removable Using Generative AI. (arXiv:2306.01953v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01953
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#21487;&#20197;&#28165;&#38500;&#38544;&#24418;&#22270;&#20687;&#27700;&#21360;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26041;&#27861;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#24182;&#38024;&#23545;&#19968;&#31181;&#20855;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24418;&#27700;&#21360;&#36890;&#36807;&#23884;&#20837;&#21482;&#26377;&#26435;&#21033;&#25317;&#26377;&#32773;&#21487;&#20197;&#26816;&#27979;&#21040;&#30340;&#38544;&#34255;&#20449;&#24687;&#26469;&#20445;&#25252;&#22270;&#20687;&#30340;&#29256;&#26435;&#12290;&#23427;&#20204;&#36824;&#38450;&#27490;&#20154;&#20204;&#28389;&#29992;&#30001;AI&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23478;&#26063;&#21270;&#20877;&#29983;&#25915;&#20987;&#26469;&#28165;&#38500;&#36825;&#20123;&#38544;&#24418;&#27700;&#21360;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#39318;&#20808;&#21521;&#22270;&#20687;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#30772;&#22351;&#27700;&#21360;&#65292;&#28982;&#21518;&#37325;&#24314;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#65292;&#21487;&#20197;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#22270;&#20687;&#38477;&#22122;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#23454;&#20363;&#21270;&#12290;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#38544;&#24418;&#27700;&#21360;&#37117;&#23481;&#26131;&#21463;&#21040;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#12290;&#23545;&#20110;&#19968;&#20010;&#29305;&#21035;&#26377;&#24377;&#24615;&#30340;&#27700;&#21360;RivaGAN&#65292;&#20877;&#29983;&#25915;&#20987;&#21487;&#20197;&#21435;&#38500;93-99%&#30340;&#38544;&#24418;&#27700;&#21360;&#65292;&#32780;&#22522;&#32447;&#25915;&#20987;&#21482;&#33021;&#21435;&#38500;&#19981;&#36229;&#36807;3%&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#35201;&#27714;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#21516;&#65292;&#20445;&#25345;&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#30340;&#27700;&#21360;&#21487;&#33021;&#26159;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invisible watermarks safeguard images' copyright by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and empirical results, we show that all invisible watermarks are vulnerable to the proposed attack. For a particularly resilient watermark, RivaGAN, regeneration attacks remove 93-99% of the invisible watermarks while the baseline attacks remove no more than 3%. However, if we do not require the watermarked image to look the same as the original one, watermarks that keep the image semantically similar can be an alternative
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.11769</link><description>&lt;p&gt;
&#25552;&#21319;&#32852;&#21512;&#23398;&#20064;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#31572;&#19982;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20316;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#38656;&#35201;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#35814;&#32454;&#30340;&#29702;&#35299;&#12290;&#23558;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#38598;&#25104;&#21040;&#39044;&#20808;&#35757;&#32451;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33719;&#21462;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#20197;&#21450;&#22270;&#20687;-&#20301;&#32622;-&#23383;&#24149;&#19977;&#20803;&#32452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20110;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#21512;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#65288;JADE&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
&lt;/p&gt;</description></item><item><title>Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2305.11473</link><description>&lt;p&gt;
Graphologue&#65306;&#29992;&#20132;&#20114;&#24335;&#22270;&#34920;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11473
&lt;/p&gt;
&lt;p&gt;
Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#26131;&#20110;&#33719;&#21462;&#21644;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#26234;&#33021;&#32780;&#36817;&#26469;&#39118;&#38753;&#19968;&#26102;&#12290;&#28982;&#32780;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#22312;&#25903;&#25345;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#23186;&#20171;&#21644;&#32447;&#24615;&#23545;&#35805;&#32467;&#26500;&#25552;&#20379;&#30340;&#21151;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#19982;&#21313;&#21517;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30028;&#38754;&#36890;&#24120;&#20250;&#21576;&#29616;&#20887;&#38271;&#30340;&#21709;&#24212;&#65292;&#20351;&#20154;&#20204;&#38590;&#20197;&#24555;&#36895;&#29702;&#35299;&#21644;&#28789;&#27963;&#22320;&#19982;&#21508;&#31181;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Graphologue&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;LLM&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20110;&#20449;&#24687;&#26597;&#25214;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;Graphologue&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#20174;LLM&#21709;&#24212;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24182;&#23454;&#26102;&#26500;&#24314;&#33410;&#28857;&#38142;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#25506;&#32034;&#30456;&#20851;&#20449;&#24687;&#21644;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30028;&#38754;&#30456;&#27604;&#65292;Graphologue&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#22312;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#12290;Graphologue&#20026;&#22686;&#24378;LLM&#22312;&#21508;&#31181;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#36319;&#36394;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#65292;&#25552;&#39640;&#20102;&#30456;&#26426;&#35266;&#23519;&#30340;&#25928;&#29992;&#65292;&#23545;&#20110;&#31574;&#30053;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.04718</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#32972;&#21467;&#65306;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The Treachery of Images: Bayesian Scene Keypoints for Deep Policy Learning in Robotic Manipulation. (arXiv:2305.04718v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340;&#28145;&#24230;&#31574;&#30053;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#22330;&#26223;&#20851;&#38190;&#28857;&#36319;&#36394;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#65292;&#25552;&#39640;&#20102;&#30456;&#26426;&#35266;&#23519;&#30340;&#25928;&#29992;&#65292;&#23545;&#20110;&#31574;&#30053;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20174;&#30456;&#26426;&#35266;&#23519;&#20013;&#23398;&#20064;&#21644;&#25552;&#21462;&#26356;&#32039;&#20945;&#30340;&#34920;&#31034;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#22330;&#26223;&#30340;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#22788;&#29702;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24773;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#24182;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#32463;&#24120;&#34987;&#36974;&#25377;&#25110;&#32773;&#20301;&#20110;&#30456;&#26426;&#30340;&#35270;&#37326;&#20043;&#22806;&#65292;&#23548;&#33268;&#30456;&#26426;&#35266;&#23519;&#22312;&#29289;&#20307;&#20301;&#32622;&#26041;&#38754;&#20135;&#29983;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BASK&#65292;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#36319;&#36394;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#23610;&#24230;&#19981;&#21464;&#20851;&#38190;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22270;&#20687;&#20013;&#22266;&#26377;&#30340;&#27495;&#20041;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#31216;&#29289;&#20307;&#21644;&#36974;&#25377;&#21644;&#35270;&#37326;&#20043;&#22806;&#30340;&#29289;&#20307;&#30340;&#20851;&#38190;&#28857;&#36319;&#36394;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25163;&#33109;&#30456;&#26426;&#35266;&#23519;&#20013;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#29289;&#20307;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#23637;&#31034;&#20102;&#26356;&#20986;&#33394;&#30340;&#31574;&#30053;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In policy learning for robotic manipulation, sample efficiency is of paramount importance. Thus, learning and extracting more compact representations from camera observations is a promising avenue. However, current methods often assume full observability of the scene and struggle with scale invariance. In many tasks and settings, this assumption does not hold as objects in the scene are often occluded or lie outside the field of view of the camera, rendering the camera observation ambiguous with regard to their location. To tackle this problem, we present BASK, a Bayesian approach to tracking scale-invariant keypoints over time. Our approach successfully resolves inherent ambiguities in images, enabling keypoint tracking on symmetrical objects and occluded and out-of-view objects. We employ our method to learn challenging multi-object robot manipulation tasks from wrist camera observations and demonstrate superior utility for policy learning compared to other representation learning te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.02394</link><description>&lt;p&gt;
&#22522;&#20110;&#24402;&#22240;&#30340;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending against Insertion-based Textual Backdoor Attacks via Attribution. (arXiv:2305.02394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#22240;&#30340;&#31649;&#36947;AttDef&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#65292;&#35813;&#31649;&#36947;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;56.59%&#33267;79.97%&#21644;15.25%&#33267;48.34%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#22411;&#25915;&#20987;&#27169;&#24335;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;&#27169;&#22411;&#28155;&#21152;&#21518;&#38376;&#26159;&#26377;&#25928;&#30340;&#12290;&#38450;&#24481;&#27492;&#31867;&#21518;&#38376;&#25915;&#20987;&#24050;&#21464;&#24471;&#32039;&#36843;&#21644;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AttDef&#30340;&#39640;&#25928;&#24402;&#22240;&#31649;&#36947;&#65292;&#29992;&#20110;&#38450;&#24481;&#20004;&#31181;&#25554;&#20837;&#24335;&#27745;&#26579;&#25915;&#20987;BadNL&#21644;InSent&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20855;&#26377;&#36739;&#22823;&#24402;&#22240;&#20998;&#25968;&#30340;&#20196;&#29260;&#35270;&#20026;&#28508;&#22312;&#35302;&#21457;&#22120;&#65292;&#22240;&#20026;&#36739;&#22823;&#30340;&#24402;&#22240;&#35789;&#23545;&#20110;&#38169;&#35823;&#39044;&#27979;&#32467;&#26524;&#20570;&#20986;&#36739;&#22823;&#36129;&#29486;&#65292;&#22240;&#27492;&#26356;&#26377;&#21487;&#33021;&#26159;&#27745;&#26579;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#36755;&#20837;&#26159;&#21542;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20004;&#31181;&#24120;&#35265;&#30340;&#25915;&#20987;&#22330;&#26223;&#65288;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#65289;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#27867;&#21270;&#24615;&#65292;&#36825;&#19968;&#28857;&#25345;&#32493;&#25913;&#21892;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;AttDef&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#20004;&#31181;&#25915;&#20987;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;79.97%&#65288;&#25552;&#39640;&#20102;56.59%&#65289;&#21644;48.34%&#65288;&#25552;&#39640;&#20102;15.25%&#65289;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#38450;&#24481;&#25554;&#20837;&#24335;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;RL&#26041;&#27861;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06056</link><description>&lt;p&gt;
&#21033;&#29992;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#20419;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Intrinsic Stochasticity of Real-Time Simulation to Facilitate Robust Reinforcement Learning for Robot Manipulation. (arXiv:2304.06056v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#26102;&#27169;&#25311;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;RL&#26041;&#27861;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;&#26426;&#22120;&#20154;&#25805;&#20316;&#36825;&#26679;&#30340;&#23433;&#20840;&#20851;&#38190;&#22411;&#24212;&#29992;&#65292;&#27169;&#25311;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23454;&#38469;&#23454;&#29616;&#21069;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#26159;RL&#20195;&#29702;&#24448;&#24448;&#23545;&#27169;&#25311;&#19982;&#23454;&#38469;&#19990;&#30028;&#20043;&#38388;&#30340;&#24046;&#24322;&#25935;&#24863;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#25104;&#27169;&#25311;&#36719;&#20214;&#23454;&#26102;&#27169;&#25311;&#65288;RT-IS&#65289;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#29305;&#24615;&#21450;&#20854;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;RL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#22495;&#38543;&#26426;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation is essential to reinforcement learning (RL) before implementation in the real world, especially for safety-critical applications like robot manipulation. Conventionally, RL agents are sensitive to the discrepancies between the simulation and the real world, known as the sim-to-real gap. The application of domain randomization, a technique used to fill this gap, is limited to the imposition of heuristic-randomized models. We investigate the properties of intrinsic stochasticity of real-time simulation (RT-IS) of off-the-shelf simulation software and its potential to improve the robustness of RL methods and the performance of domain randomization. Firstly, we conduct analytical studies to measure the correlation of RT-IS with the occupation of the computer hardware and validate its comparability with the natural stochasticity of a physical robot. Then, we apply the RT-IS feature in the training of an RL agent. The simulation and physical experiment results verify the feasibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04164</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#39640;&#25928;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gradient Sparsification for Efficient Wireless Federated Learning with Differential Privacy. (arXiv:2304.04164v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#32531;&#35299;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#19978;&#20256;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20351;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#19978;&#20256;&#27169;&#22411;&#32780;&#27844;&#28431;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#20256;&#36755;&#24102;&#23485;&#65292;&#35757;&#32451;&#24310;&#36831;&#22686;&#21152;&#65292;&#21516;&#26102;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20445;&#25252;&#26102;&#27169;&#22411;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#31232;&#30095;&#21270;&#21644;&#24046;&#20998;&#38544;&#31169;&#30340;&#26080;&#32447;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#32780;&#19981;&#25439;&#22833;&#25910;&#25947;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#38543;&#26426;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#19968;&#37096;&#20998;&#26799;&#24230;&#20803;&#32032;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;DP&#24341;&#36215;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#20943;&#23569;&#20102;&#26080;&#32447;&#20449;&#36947;&#19978;&#20256;&#36755;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#27169;&#38750;&#20984;FL&#38382;&#39064;&#20998;&#26512;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25910;&#25947;&#24230;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20351;&#29992;Alternating Direction Method of Multipliers&#65288;ADMM&#65289;&#35299;&#20915;&#20854;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables distributed clients to collaboratively train a machine learning model without sharing raw data with each other. However, it suffers the leakage of private information from uploading models. In addition, as the model size grows, the training latency increases due to limited transmission bandwidth and the model performance degrades while using differential privacy (DP) protection. In this paper, we propose a gradient sparsification empowered FL framework over wireless channels, in order to improve training efficiency without sacrificing convergence performance. Specifically, we first design a random sparsification algorithm to retain a fraction of the gradient elements in each client's local training, thereby mitigating the performance degradation induced by DP and and reducing the number of transmission parameters over wireless channels. Then, we analyze the convergence bound of the proposed algorithm, by modeling a non-convex FL problem. Next, we formula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2304.03442</link><description>&lt;p&gt;
&#29983;&#25104;&#20195;&#29702;: &#20154;&#31867;&#34892;&#20026;&#30340;&#20132;&#20114;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative Agents: Interactive Simulacra of Human Behavior. (arXiv:2304.03442v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#20195;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#20223;&#30495;&#20986;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#22635;&#20805;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#20026;&#21019;&#36896;&#26356;&#21152;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20307;&#39564;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#21487;&#36171;&#33021;&#20110;&#20174;&#27785;&#28024;&#24335;&#29615;&#22659;&#21040;&#20154;&#38469;&#20132;&#27969;&#25490;&#32451;&#31354;&#38388;&#21040;&#21407;&#22411;&#24037;&#20855;&#30340;&#20132;&#20114;&#24335;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#20195;&#29702;&#8212;&#8212;&#20855;&#26377;&#21487;&#20449;&#24230;&#30340;&#20154;&#31867;&#34892;&#20026;&#20223;&#30495;&#30340;&#35745;&#31639;&#26426;&#36719;&#20214;&#20195;&#29702;&#12290;&#29983;&#25104;&#20195;&#29702;&#20250;&#36215;&#24202;&#65292;&#20570;&#26089;&#39184;&#65292;&#21435;&#24037;&#20316;&#65307;&#33402;&#26415;&#23478;&#30011;&#30011;&#65292;&#20316;&#23478;&#20889;&#20316;&#65307;&#20182;&#20204;&#24418;&#25104;&#35266;&#28857;&#65292;&#20114;&#30456;&#27880;&#24847;&#65292;&#24182;&#24320;&#22987;&#20132;&#35848;&#65307;&#20182;&#20204;&#22238;&#24518;&#36807;&#21435;&#30340;&#26085;&#23376;&#24182;&#35745;&#21010;&#26410;&#26469;&#12290;&#20026;&#20102;&#20351;&#29983;&#25104;&#20195;&#29702;&#33021;&#22815;&#23454;&#29616;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23384;&#20648;&#20195;&#29702;&#30340;&#32463;&#21382;&#30340;&#23436;&#25972;&#35760;&#24405;&#65292;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32508;&#21512;&#36825;&#20123;&#35760;&#24518;&#21040;&#26356;&#39640;&#23618;&#27425;&#30340;&#21453;&#24605;&#65292;&#20197;&#21450;&#21160;&#24577;&#26816;&#32034;&#36825;&#20123;&#35760;&#24518;&#20197;&#35268;&#21010;&#34892;&#20026;&#12290;&#25105;&#20204;&#23454;&#20363;&#21270;&#29983;&#25104;&#20195;&#29702;&#20197;&#22635;&#20805;&#21463;&#12298;&#27169;&#25311;&#20154;&#29983;&#12299;&#21551;&#21457;&#30340;&#20132;&#20114;&#24335;&#27801;&#30418;&#29615;&#22659;&#65292;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#19982;25&#20010;&#20195;&#29702;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20219;&#21153;--&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;LV-VIS&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;MindVLT&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#36817;&#23454;&#26102;&#30340;&#36895;&#24230;&#23454;&#29616;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.01715</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Open-Vocabulary Video Instance Segmentation. (arXiv:2304.01715v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20219;&#21153;--&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;LV-VIS&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;MindVLT&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#36817;&#23454;&#26102;&#30340;&#36895;&#24230;&#23454;&#29616;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65288;VIS&#65289;&#26088;&#22312;&#20174;&#19968;&#32452;&#23553;&#38381;&#30340;&#35757;&#32451;&#31867;&#21035;&#20013;&#23545;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#32570;&#20047;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#26032;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#21516;&#26102;&#20174;&#24320;&#25918;&#38598;&#31867;&#21035;&#20013;&#23545;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#31867;&#21035;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#35780;&#27979;&#24320;&#25918;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#21253;&#21547;1,212&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;LV-VIS&#65289;&#65292;&#26174;&#33879;&#36229;&#20986;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#35268;&#27169;&#19968;&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35760;&#24518;&#39537;&#21160;&#35270;&#35273;&#35821;&#35328;&#21464;&#25442;&#22120;MindVLT&#65292;&#20197;&#23454;&#29616;&#36817;&#23454;&#26102;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MindVLT&#22312;&#23553;&#38381;&#38598;&#21644;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Instance Segmentation(VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset(LV-VIS), that contains well-annotated objects from 1,212 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Vision-Language Transformer, MindVLT, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive ex
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01008</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multimodal Learning: A Survey. (arXiv:2304.01008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01008
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#39033;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25361;&#25112;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#23398;&#20064;&#12289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#21644;&#19981;&#23545;&#40784;&#25968;&#25454;&#23398;&#20064;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#29702;&#35299;&#21644;&#20998;&#26512;&#26469;&#33258;&#22810;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19979;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#37197;&#23545;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#27169;&#22411;&#30340;&#25193;&#23637;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#37492;&#20110;&#37326;&#22806;&#26377;&#22823;&#35268;&#27169;&#26410;&#27880;&#37322;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#25104;&#20026;&#32531;&#35299;&#27880;&#37322;&#29942;&#39048;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#31574;&#30053;&#12290;&#33258;&#30417;&#30563;&#22810;&#27169;&#24577;&#23398;&#20064;&#65288;SSML&#65289;&#24314;&#31435;&#22312;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#20174;&#21407;&#22987;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;SSML&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#38754;&#20020;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#65292;&#65288;2&#65289;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19982;&#19981;&#23545;&#40784;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#65288;1&#65289;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning, which aims to understand and analyze information from multiple modalities, has achieved substantial progress in the supervised regime in recent years. However, the heavy dependence on data paired with expensive human annotations impedes scaling up models. Meanwhile, given the availability of large-scale unannotated data in the wild, self-supervised learning has become an attractive strategy to alleviate the annotation bottleneck. Building on these two directions, self-supervised multimodal learning (SSML) provides ways to learn from raw multimodal data. In this survey, we provide a comprehensive review of the state-of-the-art in SSML, in which we elucidate three major challenges intrinsic to self-supervised learning with multimodal data: (1) learning representations from multimodal data without labels, (2) fusion of different modalities, and (3) learning with unaligned data. We then detail existing solutions to these challenges. Specifically, we consider (1) object
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#31243;&#24207;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2304.00989</link><description>&lt;p&gt;
&#27867;&#22411;&#28304;&#20195;&#30721;&#30340;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Execution of Generic Source Code. (arXiv:2304.00989v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00989
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#33021;&#22815;&#26681;&#25454;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#31243;&#24207;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#21542;&#20351;&#29992;&#26681;&#25454;&#28304;&#20195;&#30721;&#32452;&#21512;&#32780;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#36880;&#35821;&#21477;&#25191;&#34892;Python&#31243;&#24207;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#25191;&#34892;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#35299;&#37322;&#65288;Neural Interpretation&#65292;NI&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25353;&#29031;&#28304;&#20195;&#30721;&#25191;&#34892;&#27867;&#22411;&#28304;&#20195;&#30721;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#20801;&#35768;&#32570;&#22833;&#23450;&#20041;&#12290;NI&#20445;&#30041;&#28304;&#20195;&#30721;&#32467;&#26500;&#65292;&#20854;&#20013;&#27599;&#20010;&#21464;&#37327;&#37117;&#26377;&#19968;&#20010;&#21521;&#37327;&#32534;&#30721;&#65292;&#27599;&#20010;&#20989;&#25968;&#25191;&#34892;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;NI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#20855;&#26377;&#32534;&#35793;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#32452;&#35013;&#30001;&#28304;&#20195;&#30721;&#8220;&#32534;&#31243;&#8221;&#30340;&#31070;&#32463;&#23618;&#12290;NI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25191;&#34892;Py150&#25968;&#25454;&#38598;&#31243;&#24207;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#21253;&#25324;&#27809;&#26377;&#20855;&#20307;&#36755;&#20837;&#30340;&#24211;&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#28789;&#27963;&#30340;&#20195;&#30721;&#29702;&#35299;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#21464;&#37327;&#35823;&#29992;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#26080;&#20855;&#20307;&#36755;&#20837;&#30340;&#30333;&#30418;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a Python program be executed statement-by-statement by neural networks composed according to the source code? We formulate the Neuro-Symbolic Execution Problem and introduce Neural Interpretation (NI), the first neural model for the execution of generic source code that allows missing definitions. NI preserves source code structure, where every variable has a vector encoding, and every function executes a neural network. NI is a novel neural model of computers with a compiler architecture that can assemble neural layers "programmed" by source code. NI is the first neural model capable of executing Py150 dataset programs, including library functions without concrete inputs, and it can be trained with flexible code understanding objectives. We demonstrate white-box execution without concrete inputs for variable misuse localization and repair.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16756</link><description>&lt;p&gt;
LLM&#29992;&#20110;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;: &#38754;&#21521;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#36866;&#21512;&#30340;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#21305;&#37197;&#26159;&#25512;&#36827;&#21307;&#23398;&#30740;&#31350;&#21644;&#25552;&#20379;&#26368;&#20339;&#25252;&#29702;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#26631;&#20934;&#21270;&#12289;&#20262;&#29702;&#32771;&#34385;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19982;&#20020;&#24202;&#35797;&#39564;&#26631;&#20934;&#20043;&#38388;&#20114;&#25805;&#20316;&#24615;&#32570;&#20047;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#21892;EHRs&#21644;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65288;LLM-PTM&#65289;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;LLMs&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#20445;&#23494;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;7.32&#65285;&#65292;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#20102;12.12&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08613</link><description>&lt;p&gt;
&#23398;&#20064;&#22870;&#21169;&#20449;&#24687;&#33719;&#21462;&#65306;&#27491;&#30830;&#35745;&#20998;&#35268;&#21017;&#36935;&#21040;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Incentivize Information Acquisition: Proper Scoring Rules Meet Principal-Agent Model. (arXiv:2303.08613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861;&#65288;Auer&#31561;&#20154;&#65292;2002&#65289;&#24212;&#29992;&#20110;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20114;&#21160;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#35745;&#20998;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22996;&#25176;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#28608;&#21169;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22996;&#25176;&#26041;&#21644;&#20195;&#29702;&#26041;&#20043;&#38388;&#30340; Stackelberg &#21338;&#24328;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#23459;&#24067;&#20102;&#19968;&#26465;&#24471;&#20998;&#35268;&#21017;&#26469;&#25351;&#23450;&#20184;&#27454;&#65292;&#28982;&#21518;&#20195;&#29702;&#26041;&#36873;&#25321;&#26368;&#22823;&#21270;&#20854;&#33258;&#36523;&#21033;&#28070;&#21644;&#25253;&#21578;&#20449;&#24687;&#30340;&#21162;&#21147;&#27700;&#24179;&#12290;&#25105;&#20204;&#20174;&#22996;&#25176;&#26041;&#30340;&#35282;&#24230;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#30340;&#22312;&#32447;&#35774;&#32622;&#65292;&#21363;&#36890;&#36807;&#19982;&#31574;&#30053;&#20195;&#29702;&#22810;&#27425;&#20132;&#20114;&#26469;&#35774;&#35745;&#26368;&#20248;&#35745;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;&#26679;&#26412;&#39640;&#25928;&#31639;&#27861;&#65292;&#23558; UCB &#31639;&#27861; (Auer et al., 2002) &#37327;&#36523;&#23450;&#21046;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#22312; T &#27425;&#36845;&#20195;&#21518;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615; $T^{2/3}$-&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#23545;&#22996;&#25176;&#26041;&#26368;&#20248;&#21033;&#28070;&#36827;&#34892;&#31934;&#32454;&#20272;&#35745;&#30340;&#36807;&#31243;&#20197;&#21450;&#20445;&#23432;&#32416;&#27491;&#26041;&#26696;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#26041;&#30340;&#34892;&#21160;&#24471;&#21040;&#26377;&#25928;&#28608;&#21169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#23427;&#26159;&#28176;&#36827;&#26368;&#23567;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the incentivized information acquisition problem, where a principal hires an agent to gather information on her behalf. Such a problem is modeled as a Stackelberg game between the principal and the agent, where the principal announces a scoring rule that specifies the payment, and then the agent then chooses an effort level that maximizes her own profit and reports the information. We study the online setting of such a problem from the principal's perspective, i.e., designing the optimal scoring rule by repeatedly interacting with the strategic agent. We design a provably sample efficient algorithm that tailors the UCB algorithm (Auer et al., 2002) to our model, which achieves a sublinear $T^{2/3}$-regret after $T$ iterations. Our algorithm features a delicate estimation procedure for the optimal profit of the principal, and a conservative correction scheme that ensures the desired agent's actions are incentivized. Furthermore, a key feature of our regret bound is that it is i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BriVL&#30340;&#31283;&#20581;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;WavBriVL&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.04585</link><description>&lt;p&gt;
&#20174;BriVL&#20013;&#25506;&#32034;&#39640;&#25928;&#35843;&#20248;&#30340;&#23398;&#20064;&#38899;&#39057;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Efficient-Tuned Learning Audio Representation Method from BriVL. (arXiv:2303.04585v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BriVL&#30340;&#31283;&#20581;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;WavBriVL&#65292;&#36890;&#36807;&#23558;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#38899;&#39057;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#36880;&#28176;&#24847;&#35782;&#21040;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#20114;&#32852;&#32593;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#35201;&#22909;&#20110;&#39640;&#36136;&#37327;/&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22810;&#27169;&#24577;/&#22823;&#27169;&#22411;&#35201;&#22909;&#20110;&#21333;&#19968;&#25110;&#21452;&#27169;&#24577;/&#23567;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bridging-Vision-and-Language&#65288;BriVL&#65289;&#30340;&#31283;&#20581;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;WavBriVL&#12290;WavBriVL&#23558;&#38899;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25237;&#24433;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#27169;&#24577;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20174;WavBriVL&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#23450;&#24615;&#35780;&#20272;&#65292;&#26469;&#23454;&#29616;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#30340;&#65306;&#65288;1&#65289;&#23398;&#20064;&#38899;&#39057;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65307;&#65288;2&#65289;&#25506;&#32034;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#24335;&#65292;&#21363;&#20351;&#29992;&#38899;&#39057;&#29983;&#25104;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#38899;&#39057;&#20013;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, researchers have gradually realized that in some cases, the self-supervised pre-training on large-scale Internet data is better than that of high-quality/manually labeled data sets, and multimodal/large models are better than single or bimodal/small models. In this paper, we propose a robust audio representation learning method WavBriVL based on Bridging-Vision-and-Language (BriVL). WavBriVL projects audio, image and text into a shared embedded space, so that multi-modal applications can be realized. We demonstrate the qualitative evaluation of the image generated from WavBriVL as a shared embedded space, with the main purposes of this paper:(1) Learning the correlation between audio and image;(2) Explore a new way of image generation, that is, use audio to generate pictures. Experimental results show that this method can effectively generate appropriate images from audio.
&lt;/p&gt;</description></item><item><title>DroNeRF&#26159;&#19968;&#31181;&#23454;&#26102;&#22810;&#20195;&#29702;&#26080;&#20154;&#26426;&#20301;&#23039;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35745;&#31639;&#29289;&#20307;&#20960;&#20309;&#26469;&#23454;&#29616;&#33258;&#20027;&#23450;&#20301;&#21644;3D&#37325;&#24314;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#29983;&#25104;&#29420;&#29305;&#32780;&#21160;&#24577;&#30340;&#26032;&#35270;&#22270;&#12290;</title><link>http://arxiv.org/abs/2303.04322</link><description>&lt;p&gt;
DroNeRF: &#23454;&#26102;&#22810;&#20195;&#29702;&#26080;&#20154;&#26426;&#20301;&#23039;&#20248;&#21270;&#29992;&#20110;&#35745;&#31639;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
DroNeRF: Real-time Multi-agent Drone Pose Optimization for Computing Neural Radiance Fields. (arXiv:2303.04322v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04322
&lt;/p&gt;
&lt;p&gt;
DroNeRF&#26159;&#19968;&#31181;&#23454;&#26102;&#22810;&#20195;&#29702;&#26080;&#20154;&#26426;&#20301;&#23039;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35745;&#31639;&#29289;&#20307;&#20960;&#20309;&#26469;&#23454;&#29616;&#33258;&#20027;&#23450;&#20301;&#21644;3D&#37325;&#24314;&#65292;&#24182;&#32467;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#25216;&#26415;&#29983;&#25104;&#29420;&#29305;&#32780;&#21160;&#24577;&#30340;&#26032;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;DroNeRF&#65292;&#29992;&#20110;&#33258;&#20027;&#23450;&#20301;&#21333;&#30446;&#26080;&#20154;&#26426;&#22260;&#32469;&#19968;&#20010;&#29289;&#20307;&#36827;&#34892;&#23454;&#26102;&#19977;&#32500;&#37325;&#24314;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#22270;&#20687;&#12290;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;Neural Radiance Fields&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#22270;&#21512;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#19968;&#32452;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;&#29289;&#20307;&#25110;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#12290;&#20351;&#29992;&#26080;&#20154;&#26426;&#32467;&#21512;NeRF&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#32780;&#21160;&#24577;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22330;&#26223;&#30340;&#26032;&#35270;&#22270;&#65292;&#23588;&#20854;&#26159;&#22312;&#22330;&#26223;&#33021;&#21147;&#26377;&#38480;&#19988;&#31227;&#21160;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#35745;&#31639;&#21508;&#20010;&#26080;&#20154;&#26426;&#30340;&#20248;&#21270;&#20301;&#23039;&#65292;&#20165;&#20381;&#36182;&#20110;&#29289;&#20307;&#20960;&#20309;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#22806;&#37096;&#23450;&#20301;&#31995;&#32479;&#12290;&#22312;&#25968;&#25454;&#25429;&#33719;&#38454;&#27573;&#26399;&#38388;&#29420;&#29305;&#30340;&#30456;&#26426;&#23450;&#20301;&#26174;&#33879;&#24433;&#21709;&#30528;3D&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#29983;&#25104;&#30340;&#26032;&#35270;&#22270;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#19981;&#21516;&#30340;&#24863;&#30693;&#24230;&#37327;&#65292;&#22914;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#27979;&#24230;&#65288;SSIM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel optimization algorithm called DroNeRF for the autonomous positioning of monocular camera drones around an object for real-time 3D reconstruction using only a few images. Neural Radiance Fields or NeRF, is a novel view synthesis technique used to generate new views of an object or scene from a set of input images. Using drones in conjunction with NeRF provides a unique and dynamic way to generate novel views of a scene, especially with limited scene capabilities of restricted movements. Our approach focuses on calculating optimized pose for individual drones while solely depending on the object geometry without using any external localization system. The unique camera positioning during the data-capturing phase significantly impacts the quality of the 3D model. To evaluate the quality of our generated novel views, we compute different perceptual metrics like the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure(SSIM). Our work demonstrates the 
&lt;/p&gt;</description></item><item><title>SG-LSTM&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#20154;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#31227;&#21160;&#65292;&#36890;&#36807;&#31038;&#20132;&#32676;&#32452;LSTM&#23454;&#29616;&#23545;&#20154;&#32676;&#21644;&#20114;&#21160;&#30340;&#24314;&#27169;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36712;&#36857;&#65292;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#25552;&#39640;&#20102;&#30896;&#25758;&#36335;&#24452;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.04320</link><description>&lt;p&gt;
SG-LSTM: &#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#23494;&#38598;&#20154;&#32676;&#20013;&#23548;&#33322;&#30340;&#31038;&#20132;&#32676;&#32452;LSTM
&lt;/p&gt;
&lt;p&gt;
SG-LSTM: Social Group LSTM for Robot Navigation Through Dense Crowds. (arXiv:2303.04320v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04320
&lt;/p&gt;
&lt;p&gt;
SG-LSTM&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#20154;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#31227;&#21160;&#65292;&#36890;&#36807;&#31038;&#20132;&#32676;&#32452;LSTM&#23454;&#29616;&#23545;&#20154;&#32676;&#21644;&#20114;&#21160;&#30340;&#24314;&#27169;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36712;&#36857;&#65292;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#25552;&#39640;&#20102;&#30896;&#25758;&#36335;&#24452;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20010;&#20154;&#26426;&#22120;&#20154;&#30340;&#21487;&#29992;&#24615;&#21644;&#20215;&#26684;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#23558;&#19981;&#20877;&#23616;&#38480;&#20110;&#22823;&#22411;&#20844;&#21496;&#20179;&#24211;&#25110;&#24037;&#21378;&#65292;&#32780;&#26159;&#39044;&#35745;&#23558;&#22312;&#36739;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#19982;&#26356;&#22823;&#30340;&#20154;&#32676;&#19968;&#36215;&#24037;&#20316;&#12290;&#38500;&#20102;&#30830;&#20445;&#23433;&#20840;&#21644;&#25928;&#29575;&#22806;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#20943;&#23569;&#26426;&#22120;&#20154;&#23545;&#20154;&#31867;&#21487;&#33021;&#20135;&#29983;&#30340;&#36127;&#38754;&#24515;&#29702;&#24433;&#21709;&#65292;&#24182;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36981;&#23432;&#26410;&#20070;&#20889;&#30340;&#31038;&#20132;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#25317;&#25380;&#29615;&#22659;&#20013;&#34892;&#20154;&#21644;&#24863;&#30693;&#31038;&#20132;&#32676;&#20307;&#30340;&#31227;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#20132;&#32676;&#32452;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;SG-LSTM&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#20855;&#26377;&#31038;&#20132;&#24847;&#35782;&#30340;LSTM&#26469;&#23545;&#23494;&#38598;&#29615;&#22659;&#20013;&#30340;&#20154;&#32676;&#21644;&#20114;&#21160;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#36712;&#36857;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#23548;&#33322;&#31639;&#27861;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#22320;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#35760;&#34892;&#20154;&#32676;&#20307;&#30340;&#22823;&#22411;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing availability and affordability of personal robots, they will no longer be confined to large corporate warehouses or factories but will instead be expected to operate in less controlled environments alongside larger groups of people. In addition to ensuring safety and efficiency, it is crucial to minimize any negative psychological impact robots may have on humans and follow unwritten social norms in these situations. Our research aims to develop a model that can predict the movements of pedestrians and perceptually-social groups in crowded environments. We introduce a new Social Group Long Short-term Memory (SG-LSTM) model that models human groups and interactions in dense environments using a socially-aware LSTM to produce more accurate trajectory predictions. Our approach enables navigation algorithms to calculate collision-free paths faster and more accurately in crowded environments. Additionally, we also release a large video dataset with labeled pedestrian gro
&lt;/p&gt;</description></item><item><title>NovPhy&#26159;&#19968;&#20010;&#20026;&#20102;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2303.01711</link><description>&lt;p&gt;
NovPhy&#65306;&#19968;&#20010;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#29289;&#29702;&#25512;&#29702;&#30340;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
NovPhy: A Testbed for Physical Reasoning in Open-world Environments. (arXiv:2303.01711v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01711
&lt;/p&gt;
&lt;p&gt;
NovPhy&#26159;&#19968;&#20010;&#20026;&#20102;&#24320;&#25918;&#19990;&#30028;&#29289;&#29702;&#25512;&#29702;&#32780;&#35774;&#35745;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#30340;AI&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#23558;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#34701;&#20837;&#36825;&#20123;AI&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20294;&#20165;&#20165;&#20855;&#22791;&#29289;&#29702;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#36275;&#20197;&#22312;&#30495;&#23454;&#29289;&#29702;&#29615;&#22659;&#20013;&#36816;&#34892;&#65311;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38754;&#23545;&#20043;&#21069;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#26032;&#39062;&#24773;&#20917;&#12290;&#20316;&#20026;&#20154;&#31867;&#65292;&#25105;&#20204;&#33021;&#22815;&#25104;&#21151;&#22320;&#36866;&#24212;&#36825;&#20123;&#24773;&#20917;&#12290;&#21516;&#26679;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#38656;&#35201;&#20855;&#22791;&#22312;&#38754;&#23545;&#26032;&#39062;&#24773;&#20917;&#26102;&#27491;&#24120;&#36816;&#20316;&#30340;&#33021;&#21147;&#65292;&#25165;&#33021;&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#29289;&#29702;&#29615;&#22659;&#20013;&#26377;&#25928;&#36816;&#34892;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#26679;&#30340;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#24179;&#21488;NovPhy&#65292;&#35201;&#27714;&#26234;&#33021;&#20307;&#22312;&#23384;&#22312;&#26032;&#39062;&#24773;&#20917;&#30340;&#29289;&#29702;&#22330;&#26223;&#20013;&#36827;&#34892;&#25512;&#29702;&#24182;&#30456;&#24212;&#22320;&#37319;&#21462;&#34892;&#21160;&#12290;&#27979;&#35797;&#24179;&#21488;&#21253;&#25324;&#38656;&#35201;&#26234;&#33021;&#20307;&#26816;&#27979;&#21644;&#36866;&#24212;&#29289;&#29702;&#22330;&#26223;&#20013;&#30340;&#26032;&#39062;&#24773;&#20917;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#21019;&#24314;&#27979;&#35797;&#24179;&#21488;&#20013;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20843;&#31181;&#20195;&#34920;&#19981;&#21516;&#26032;&#39062;&#24773;&#20917;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the emergence of AI systems that interact with the physical environment, there is an increased interest in incorporating physical reasoning capabilities into those AI systems. But is it enough to only have physical reasoning capabilities to operate in a real physical environment? In the real world, we constantly face novel situations we have not encountered before. As humans, we are competent at successfully adapting to those situations. Similarly, an agent needs to have the ability to function under the impact of novelties in order to properly operate in an open-world physical environment. To facilitate the development of such AI systems, we propose a new testbed, NovPhy, that requires an agent to reason about physical scenarios in the presence of novelties and take actions accordingly. The testbed consists of tasks that require agents to detect and adapt to novelties in physical scenarios. To create tasks in the testbed, we develop eight novelties representing a diverse novelt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.01254</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26641;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;(PETs)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#8212;&#8212;&#20840;&#21516;&#24577;&#21152;&#23494;(FHE)&#65292;&#23427;&#20801;&#35768;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FHE&#24212;&#29992;&#20110;&#22522;&#20110;&#26641;&#22411;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24471;&#21040;&#20102;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#65292;&#24182;&#24050;&#23454;&#29616;&#22312;Concrete-ML&#24211;&#20013;&#65292;&#35813;&#24211;&#22312;https://github.com/zama-ai/concrete-ml. &#24320;&#28304;&#12290;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;FHE&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.12012</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#30340;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#30340;&#32463;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#19981;&#21516;&#38477;&#32500;&#21644;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#31561;&#38477;&#32500;&#31639;&#27861;&#65292;&#36873;&#25321;&#29305;&#24449;&#26469;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#26816;&#26597;&#65292;&#35760;&#24405;&#22823;&#33041;&#30340;&#30005;&#27963;&#21160;&#12290;&#35813;&#26816;&#26597;&#29992;&#20110;&#24110;&#21161;&#35786;&#26029;&#21508;&#31181;&#33041;&#38382;&#39064;&#12290;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#30315;&#30187;&#26816;&#27979;&#12290;&#22312;&#30315;&#30187;&#21457;&#20316;&#26816;&#27979;&#20013;&#65292;&#20027;&#35201;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;EEG&#20449;&#21495;&#20013;&#30340;&#38544;&#34255;&#20449;&#24687;&#23545;&#20110;&#26816;&#27979;&#24433;&#21709;&#22823;&#33041;&#30340;&#30142;&#30149;&#24456;&#26377;&#29992;&#12290;&#26377;&#26102;&#65292;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20869;&#35782;&#21035;EEG&#30340;&#26368;&#23567;&#21464;&#21270;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;DWT&#21487;&#20197;&#22312;&#19981;&#21516;&#39057;&#24102;&#36827;&#34892;&#20449;&#21495;&#33391;&#22909;&#30340;&#20998;&#35299;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#38477;&#32500;&#31639;&#27861;&#65306;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#34701;&#21512;&#35268;&#21017;&#36873;&#25321;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Electroencephalogram (EEG) is a non-invasive exam that records the electrical activity of the brain. This exam is used to help diagnose conditions such as different brain problems. EEG signals are taken for the purpose of epilepsy detection and with Discrete Wavelet Transform (DWT) and machine learning classifier, they perform epilepsy detection. In Epilepsy seizure detection, mainly machine learning classifiers and statistical features are used. The hidden information in the EEG signal is useful for detecting diseases affecting the brain. Sometimes it is very difficult to identify the minimum changes in the EEG in the time and frequency domains purpose. The DWT can give a good decomposition of the signals in different frequency bands and feature extraction. We use the tri-dimensionality reduction algorithm.; Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA). Finally, features are selected by using a fusion rule and at t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#21160;&#21147;&#23398;&#23545;&#20110;&#33033;&#20914;&#20943;&#23569;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#36136;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#33033;&#20914;&#27963;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20248;&#21270;&#30340;&#24322;&#36136;&#33033;&#20914;&#20943;&#23569;&#32593;&#32476;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#33033;&#20914;&#27963;&#21160;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11618</link><description>&lt;p&gt;
&#24322;&#36136;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#21160;&#21147;&#23398;&#23545;&#20110;&#39640;&#25928;&#33258;&#20027;&#23398;&#20064;&#30340;&#33033;&#20914;&#20943;&#23569;&#32593;&#32476;&#30340;&#29702;&#35770;&#21644;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles. (arXiv:2302.11618v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#21160;&#21147;&#23398;&#23545;&#20110;&#33033;&#20914;&#20943;&#23569;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#36136;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#21644;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#38477;&#20302;&#33033;&#20914;&#27963;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20248;&#21270;&#30340;&#24322;&#36136;&#33033;&#20914;&#20943;&#23569;&#32593;&#32476;&#30340;&#24615;&#33021;&#25552;&#21319;&#21644;&#33033;&#20914;&#27963;&#21160;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26174;&#31034;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#21160;&#21147;&#23398;&#30340;&#24322;&#36136;&#24615;&#38477;&#20302;&#20102;&#36882;&#24402;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(RSNN)&#30340;&#33033;&#20914;&#27963;&#21160;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#33033;&#20914;&#20943;&#23569;(&#33258;&#20027;)&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#34920;&#26126;&#65292;&#31070;&#32463;&#20803;&#25972;&#21512;/&#25918;&#26494;&#21160;&#21147;&#23398;&#30340;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;RSNN&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#27169;&#24335;(&#26356;&#39640;&#30340;&#35760;&#24518;&#23481;&#37327;)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#31361;&#35302;&#30340;&#24322;&#36136;&#24615;&#26102;&#24207;&#30456;&#20851;&#21487;&#22609;&#24615;(STDP)&#21160;&#21147;&#23398;&#38477;&#20302;&#20102;&#33033;&#20914;&#27963;&#21160;&#20294;&#20445;&#25345;&#20102;&#35760;&#24518;&#23481;&#37327;&#12290;&#20998;&#26512;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30830;&#23450;&#31070;&#32463;&#20803;&#21644;&#31361;&#35302;&#30340;&#24322;&#36136;RSNN&#35774;&#35745;&#65292;&#20197;&#25552;&#39640;$\mathcal{E}$&#65292;&#21363;&#33033;&#20914;&#27963;&#21160;&#21644;&#35760;&#24518;&#23481;&#37327;&#30340;&#27604;&#20540;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;HRSNN&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#20943;&#23569;&#20102;&#33033;&#20914;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning. We analytically show that the diversity in neurons' integration/relaxation dynamics improves an RSNN's ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#65292;&#19988;&#19981;&#33021;&#24212;&#29992;&#20110;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#20197;&#21450;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25439;&#22833;&#36739;&#22823;&#12290;&#20026;&#27492;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASSET&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2302.11408</link><description>&lt;p&gt;
ASSET&#65306;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ASSET: Robust Backdoor Data Detection Across a Multiplicity of Deep Learning Paradigms. (arXiv:2302.11408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#20013;&#23454;&#29616;&#40065;&#26834;&#30340;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#30340;&#24615;&#33021;&#21464;&#21270;&#24456;&#22823;&#65292;&#19988;&#19981;&#33021;&#24212;&#29992;&#20110;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#65292;&#20197;&#21450;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25439;&#22833;&#36739;&#22823;&#12290;&#20026;&#27492;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASSET&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21518;&#38376;&#25968;&#25454;&#26816;&#27979;&#26159;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;&#26222;&#21450;&#24212;&#29992;&#22686;&#21152;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#36739;&#23569;&#12290;&#25104;&#21151;&#30340;&#21518;&#38376;&#25915;&#20987;&#20063;&#22312;&#36825;&#20123;&#26032;&#30340;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#36866;&#29992;&#24615;&#32570;&#20047;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#35780;&#20272;56&#31181;&#25915;&#20987;&#35774;&#32622;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#25915;&#20987;&#21644;&#27602;&#23475;&#27604;&#20363;&#19979;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#22312;&#26368;&#26032;&#30340;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#19979;&#20840;&#37096;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;SSL&#21644;TL&#26102;&#65292;&#23427;&#20204;&#35201;&#20040;&#21464;&#24471;&#19981;&#36866;&#29992;&#65292;&#35201;&#20040;&#36973;&#21463;&#36739;&#22823;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Active Separation via Offset (ASSET)&#30340;&#26032;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21518;&#38376;&#21644;&#24178;&#20928;&#26679;&#26412;&#20043;&#38388;&#20027;&#21160;&#24341;&#23548;&#19981;&#21516;&#30340;&#27169;&#22411;&#34892;&#20026;&#26469;&#20419;&#36827;&#23427;&#20204;&#30340;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor data detection is traditionally studied in an end-to-end supervised learning (SL) setting. However, recent years have seen the proliferating adoption of self-supervised learning (SSL) and transfer learning (TL), due to their lesser need for labeled data. Successful backdoor attacks have also been demonstrated in these new settings. However, we lack a thorough understanding of the applicability of existing detection methods across a variety of learning settings. By evaluating 56 attack settings, we show that the performance of most existing detection methods varies significantly across different attacks and poison ratios, and all fail on the state-of-the-art clean-label attack. In addition, they either become inapplicable or suffer large performance losses when applied to SSL and TL. We propose a new detection method called Active Separation via Offset (ASSET), which actively induces different model behaviors between the backdoor and clean samples to promote their separation. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.05880</link><description>&lt;p&gt;
TikTalk: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#30495;&#23454;&#19990;&#30028;&#38386;&#32842;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World. (arXiv:2301.05880v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05880
&lt;/p&gt;
&lt;p&gt;
TikTalk&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#26234;&#33021;&#19988;&#31867;&#20284;&#20154;&#31867;&#30340;&#38386;&#32842;&#26426;&#22120;&#20154;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#27969;&#34892;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#30340;38K&#20010;&#35270;&#39057;&#21644;367K&#20010;&#29992;&#25143;&#23545;&#35805;&#12290;&#19982;&#20854;&#20182;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#25968;&#25454;&#38598;&#20013;&#36824;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20013;&#26234;&#33021;&#21644;&#20154;&#31867;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;TikTalk&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#27969;&#34892;&#30340;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#25910;&#38598;&#20102;38K&#20010;&#35270;&#39057;&#65292;&#20197;&#21450;&#29992;&#25143;&#22312;&#20854;&#19979;&#21457;&#24067;&#30340;367K&#20010;&#23545;&#35805;&#12290;&#29992;&#25143;&#26681;&#25454;&#20182;&#20204;&#35266;&#30475;&#35270;&#39057;&#26102;&#30340;&#22810;&#27169;&#24577;&#32463;&#39564;&#36827;&#34892;&#33258;&#21457;&#24615;&#23545;&#35805;&#65292;&#36825;&#26377;&#21161;&#20110;&#37325;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#38386;&#32842;&#29615;&#22659;&#12290;&#19982;&#20043;&#21069;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;TikTalk&#20013;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#31867;&#22411;&#23548;&#33268;&#20102;&#26356;&#22810;&#26679;&#21270;&#30340;&#23545;&#35805;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#20174;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#20013;&#25429;&#25417;&#20154;&#31867;&#20852;&#36259;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#22238;&#31572;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20013;&#26356;&#39057;&#32321;&#22320;&#24341;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#12290;&#36825;&#20123;&#20107;&#23454;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#20102;TikTalk&#30340;&#29305;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#38386;&#32842;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#23545;&#35805;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04785</link><description>&lt;p&gt;
&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#12289;&#20351;&#29992;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#21450;&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#25915;&#20987;&#20013;&#26469;&#29983;&#25104;&#24378;&#26377;&#21147;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#39057;&#29575;&#21407;&#29702;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#21363;\textit{&#36739;&#20302;&#30340;&#39057;&#29575;&#20808;&#23398;&#20064;}&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20173;&#28982;&#25104;&#31435;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#65292;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CLIP&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01452</link><description>&lt;p&gt;
CLIP: &#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#26356;&#24555;&#22320;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CLIP&#65292;&#36890;&#36807;&#32467;&#21512;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#27491;&#20174;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#36716;&#21521;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#22312;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#20013;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#25913;&#36827;&#21644;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#65292;&#21363;&#20351;&#29992;&#36845;&#20195;&#25968;&#25454;&#20462;&#21098;&#30340;&#35838;&#31243;&#23398;&#20064;&#12290;CLIP&#32467;&#21512;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#36825;&#20004;&#31181;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#37319;&#29992;&#20102;&#26377;&#25439;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36845;&#20195;&#22320;&#21435;&#38500;&#26368;&#19981;&#37325;&#35201;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#28176;&#20943;&#23567;&#22312;&#35838;&#31243;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#12290;&#22312;&#20247;&#31609;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#29702;&#24565;&#65292;&#36890;&#36807;&#20943;&#23567;&#25910;&#25947;&#26102;&#38388;&#21644;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23558;&#35838;&#31243;&#23398;&#20064;&#21644;&#25968;&#25454;&#38598;&#20462;&#21098;&#32467;&#21512;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01450</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#23436;&#25972;&#26631;&#31614;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Crowd Density Estimation using Imperfect Labels. (arXiv:2212.01450v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#20272;&#35745;&#26159;&#20154;&#32676;&#35745;&#25968;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22836;&#37096;&#26631;&#27880;&#30340;&#20154;&#32676;&#22270;&#20687;&#26469;&#20272;&#35745;&#26410;&#35265;&#22270;&#20687;&#20013;&#30340;&#20154;&#32676;&#23494;&#24230;&#12290;&#36890;&#24120;&#65292;&#27169;&#22411;&#30340;&#23398;&#20064;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#26631;&#27880;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#19981;&#20934;&#30830;&#30340;&#26631;&#27880;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#23450;&#20301;&#21644;&#35745;&#25968;&#38169;&#35823;&#12290;&#24050;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#20851;&#27880;&#20351;&#29992;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#36827;&#34892;&#20154;&#32676;&#35745;&#25968;&#65292;&#20294;&#27809;&#26377;&#20154;&#25506;&#32034;&#27880;&#37322;&#38169;&#35823;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#23436;&#25972;&#26631;&#31614;&#65288;&#21253;&#25324;&#22122;&#22768;&#21644;&#32570;&#22833;&#26631;&#31614;&#65289;&#23545;&#20154;&#32676;&#35745;&#25968;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#31216;&#20026;&#26631;&#27880;&#22120;&#65289;&#33258;&#21160;&#29983;&#25104;&#19981;&#23436;&#25972;&#26631;&#31614;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#26032;&#30340;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#65288;&#30446;&#26631;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20154;&#32676;&#35745;&#25968;&#27169;&#22411;&#21644;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#23436;&#32654;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density estimation is one of the most widely used methods for crowd counting in which a deep learning model learns from head-annotated crowd images to estimate crowd density in unseen images. Typically, the learning performance of the model is highly impacted by the accuracy of the annotations and inaccurate annotations may lead to localization and counting errors during prediction. A significant amount of works exist on crowd counting using perfectly labelled datasets but none of these explore the impact of annotation errors on the model accuracy. In this paper, we investigate the impact of imperfect labels (both noisy and missing labels) on crowd counting accuracy. We propose a system that automatically generates imperfect labels using a deep learning model (called annotator) which are then used to train a new crowd counting model (target model). Our analysis on two crowd counting models and two benchmark datasets shows that the proposed scheme achieves accuracy closer to that of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#21644;&#22312;&#32447;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#32467;&#26524;&#31361;&#20986;&#20102;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#22312;&#26368;&#20248;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#20811;&#26381;&#20004;&#31181;&#26041;&#27861;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2211.15930</link><description>&lt;p&gt;
&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#19982;&#22312;&#32447;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#30340;&#27604;&#36739;&#30740;&#31350;&#21450;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21453;&#39304;&#25511;&#21046;&#30340;&#32479;&#19968;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control. (arXiv:2211.15930v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#21644;&#22312;&#32447;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#32467;&#26524;&#31361;&#20986;&#20102;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#22312;&#26368;&#20248;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#20811;&#26381;&#20004;&#31181;&#26041;&#27861;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#39640;&#25928;&#35299;&#20915;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#21453;&#39304;&#25511;&#21046;&#38382;&#39064;&#12290;&#39318;&#20808;&#23545;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#21644;&#22312;&#32447;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#36825;&#20004;&#31181;&#20027;&#27969;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#34429;&#28982;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#35757;&#32451;&#37096;&#20998;&#30456;&#23545;&#23481;&#26131;&#65292;&#20294;&#20854;&#25104;&#21151;&#19982;&#21542;&#20005;&#37325;&#20381;&#36182;&#20110;&#30001;&#24320;&#29615;&#26368;&#20248;&#25511;&#21046;&#27714;&#35299;&#22120;&#29983;&#25104;&#30340;&#26368;&#20248;&#25511;&#21046;&#25968;&#25454;&#38598;&#12290;&#30456;&#21453;&#65292;&#30452;&#25509;&#20248;&#21270;&#23558;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30452;&#25509;&#36716;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;&#65292;&#20294;&#22312;&#38382;&#39064;&#22797;&#26434;&#26102;&#65292;&#19982;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#30446;&#26631;&#21487;&#33021;&#38590;&#20197;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#31163;&#32447;&#30417;&#30563;&#23398;&#20064;&#22312;&#26368;&#20248;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#20026;&#20102;&#20811;&#26381;&#20004;&#31181;&#26041;&#27861;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#25968;&#25454;&#38598;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#20114;&#34917;&#23427;&#20204;&#65292;&#24182;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#20316;&#20026;&#19968;&#31181;&#32479;&#19968;&#30340;&#35757;&#32451;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is concerned with solving neural network-based feedback controllers efficiently for optimal control problems. We first conduct a comparative study of two mainstream approaches: offline supervised learning and online direct policy optimization. Albeit the training part of the supervised learning approach is relatively easy, the success of the method heavily depends on the optimal control dataset generated by open-loop optimal control solvers. In contrast, direct optimization turns the optimal control problem into an optimization problem directly without any requirement of pre-computing, but the dynamics-related objective can be hard to optimize when the problem is complicated. Our results highlight the priority of offline supervised learning in terms of both optimality and training time. To overcome the main challenges, dataset, and optimization, in the two approaches respectively, we complement them and propose the Pre-train and Fine-tune strategy as a unified training paradi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;CNN&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.07137</link><description>&lt;p&gt;
DroneNet: &#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DroneNet: Crowd Density Estimation using Self-ONNs for Drones. (arXiv:2211.07137v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07137
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#32452;&#32455;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;CNN&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#25512;&#26029;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#35270;&#39057;&#30417;&#25511;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#65292;&#30001;&#20110;&#26080;&#20154;&#26426;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#37096;&#32626;&#21644;&#31227;&#21160;&#27809;&#26377;&#38556;&#30861;&#12290;&#26080;&#20154;&#26426;&#35270;&#39057;&#30417;&#25511;&#30340;&#19968;&#20010;&#26377;&#36259;&#24212;&#29992;&#26159;&#20272;&#35745;&#20844;&#20849;&#22330;&#25152;&#30340;&#20154;&#32676;&#23494;&#24230;&#65288;&#21253;&#25324;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#12290;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#21033;&#29992;&#22270;&#20687;&#21644;&#35270;&#39057;&#36827;&#34892;&#33258;&#21160;&#20154;&#32676;&#35745;&#25968;&#21644;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#27169;&#22411;&#26550;&#26500;&#65292;&#21363;&#26356;&#28145;&#30340;CNN&#27169;&#22411;&#22312;&#22686;&#21152;&#25512;&#26029;&#26102;&#38388;&#30340;&#20195;&#20215;&#19979;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#20154;&#26426;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#27169;&#22411;&#65288;DroneNet&#65289;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#25805;&#20316;&#31070;&#32463;&#32593;&#32476;&#65288;Self-ONN&#65289;&#12290;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#65292;Self-ONN&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#22312;&#20004;&#20010;&#26080;&#20154;&#26426;&#35270;&#35282;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;DroneNet&#22312;&#20154;&#32676;&#23494;&#24230;&#20272;&#35745;&#19978;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
Video surveillance using drones is both convenient and efficient due to the ease of deployment and unobstructed movement of drones in many scenarios. An interesting application of drone-based video surveillance is to estimate crowd densities (both pedestrians and vehicles) in public places. Deep learning using convolution neural networks (CNNs) is employed for automatic crowd counting and density estimation using images and videos. However, the performance and accuracy of such models typically depend upon the model architecture i.e., deeper CNN models improve accuracy at the cost of increased inference time. In this paper, we propose a novel crowd density estimation model for drones (DroneNet) using Self-organized Operational Neural Networks (Self-ONN). Self-ONN provides efficient learning capabilities with lower computational complexity as compared to CNN-based models. We tested our algorithm on two drone-view public datasets. Our evaluation shows that the proposed DroneNet shows supe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#21487;&#20449;&#36182;&#30340;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#12289;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24403;&#21069;&#22522;&#20934;&#30340;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.16144</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#20449;&#22810;&#27169;&#24577;&#36816;&#21160;&#39044;&#27979;: &#36755;&#20986;&#30340;&#25972;&#20307;&#35780;&#20272;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards trustworthy multi-modal motion prediction: Holistic evaluation and interpretability of outputs. (arXiv:2210.16144v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#21487;&#20449;&#36182;&#30340;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#12289;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#36755;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#24403;&#21069;&#22522;&#20934;&#30340;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20854;&#20182;&#36947;&#36335;&#21442;&#19982;&#32773;&#30340;&#36816;&#21160;&#33021;&#20351;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25191;&#34892;&#23433;&#20840;&#39640;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;&#36825;&#20010;&#20219;&#21153;&#38750;&#24120;&#22797;&#26434;&#65292;&#22240;&#20026;&#36947;&#36335;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#21462;&#20915;&#20110;&#35768;&#22810;&#22240;&#32032;&#65292;&#24182;&#19988;&#21487;&#33021;&#30340;&#26410;&#26469;&#36712;&#36857;&#25968;&#37327;&#21487;&#35266;&#65288;&#22810;&#27169;&#24577;&#65289;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#36816;&#21160;&#39044;&#27979;&#38382;&#39064;&#30340;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35299;&#37322;&#24615;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20934;&#25152;&#20351;&#29992;&#30340;&#25351;&#26631;&#19981;&#35780;&#20272;&#38382;&#39064;&#30340;&#25152;&#26377;&#26041;&#38754;&#65292;&#20363;&#22914;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#25509;&#21463;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26681;&#25454;&#8220;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#8221;&#30340;&#19968;&#20123;&#35201;&#27714;&#65292;&#25512;&#21160;&#21487;&#20449;&#36816;&#21160;&#39044;&#27979;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#20851;&#27880;&#35780;&#20272;&#26631;&#20934;&#12289;&#40065;&#26834;&#24615;&#21644;&#36755;&#20986;&#21487;&#35299;&#37322;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#35780;&#20272;&#25351;&#26631;&#65292;&#30830;&#23450;&#24403;&#21069;&#22522;&#20934;&#30340;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the motion of other road agents enables autonomous vehicles to perform safe and efficient path planning. This task is very complex, as the behaviour of road agents depends on many factors and the number of possible future trajectories can be considerable (multi-modal). Most prior approaches proposed to address multi-modal motion prediction are based on complex machine learning systems that have limited interpretability. Moreover, the metrics used in current benchmarks do not evaluate all aspects of the problem, such as the diversity and admissibility of the output. In this work, we aim to advance towards the design of trustworthy motion prediction systems, based on some of the requirements for the design of Trustworthy Artificial Intelligence. We focus on evaluation criteria, robustness, and interpretability of outputs. First, we comprehensively analyse the evaluation metrics, identify the main gaps of current benchmarks, and propose a new holistic evaluation framework. We t
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#24212;&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2207.07563</link><description>&lt;p&gt;
QSAN: &#19968;&#31181;&#36817;&#26399;&#21487;&#23454;&#29616;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QSAN: A Near-term Achievable Quantum Self-Attention Network. (arXiv:2207.07563v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#24212;&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SAM&#65289;&#25797;&#38271;&#25429;&#25417;&#29305;&#24449;&#30340;&#20869;&#37096;&#36830;&#25509;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#23545;&#39640;&#32500;&#25968;&#25454;&#30340;&#39640;&#25928;&#29305;&#24449;&#25552;&#21462;&#21644;&#34920;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QSAN&#65289;&#65292;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25506;&#32034;&#20102;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;QSAM&#65289;&#65292;&#21253;&#25324;&#37327;&#23376;&#36923;&#36753;&#30456;&#20284;&#24230;&#65288;QLS&#65289;&#21644;&#37327;&#23376;&#20301;&#33258;&#27880;&#24847;&#21147;&#24471;&#20998;&#30697;&#38453;&#65288;QBSASM&#65289;&#65292;&#20316;&#20026;QSAN&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#22686;&#24378;SAM&#30340;&#25968;&#25454;&#34920;&#31034;&#33021;&#21147;&#12290;QLS&#29992;&#20110;&#38450;&#27490;&#27979;&#37327;&#33719;&#21462;&#20869;&#31215;&#65292;&#20351;&#24471;QSAN&#33021;&#22815;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23436;&#20840;&#23454;&#29616;&#65292;&#32780;QBSASM&#20316;&#20026;QSAN&#28436;&#36827;&#30340;&#32467;&#26524;&#65292;&#20135;&#29983;&#19968;&#20010;&#33021;&#26377;&#25928;&#21453;&#26144;&#36755;&#20986;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#23494;&#24230;&#30697;&#38453;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;QSAN&#30340;&#19968;&#27493;&#23454;&#29616;&#21644;&#37327;&#23376;&#30005;&#36335;&#26694;&#26550;&#65292;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#21387;&#32553;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Attention Mechanism (SAM) is good at capturing the internal connections of features and greatly improves the performance of machine learning models, espeacially requiring efficient characterization and feature extraction of high-dimensional data. A novel Quantum Self-Attention Network (QSAN) is proposed for image classification tasks on near-term quantum devices. First, a Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity (QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM) is explored as the theoretical basis of QSAN to enhance the data representation of SAM. QLS is employed to prevent measurements from obtaining inner products to allow QSAN to be fully implemented on quantum computers, and QBSASM as a result of the evolution of QSAN to produce a density matrix that effectively reflects the attention distribution of the output. Then, the framework for one-step realization and quantum circuits of QSAN are designed for fully considering the compression
&lt;/p&gt;</description></item><item><title>FairGrad&#26159;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#26435;&#37325;&#26469;&#23454;&#29616;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.10923</link><description>&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#65306;FairGrad
&lt;/p&gt;
&lt;p&gt;
FairGrad: Fairness Aware Gradient Descent. (arXiv:2206.10923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10923
&lt;/p&gt;
&lt;p&gt;
FairGrad&#26159;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#26435;&#37325;&#26469;&#23454;&#29616;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20998;&#31867;&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19981;&#20250;&#19981;&#20844;&#24179;&#27495;&#35270;&#20154;&#32676;&#23376;&#38598;&#30340;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#22823;&#22810;&#38480;&#20110;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#25110;&#32773;&#28041;&#21450;&#38590;&#20197;&#23454;&#29616;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#21487;&#24212;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairGrad&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#26041;&#26696;&#26469;&#24378;&#21270;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#32676;&#20307;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#21462;&#20915;&#20110;&#26159;&#21542;&#20855;&#26377;&#20248;&#21183;&#12290;FairGrad&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#24182;&#19988;&#24320;&#38144;&#26368;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FairGrad&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#20869;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#19982;&#26631;&#20934;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;FairGrad&#21487;&#20197;&#22312;https://pypi.org/project/fairgrad&#19978;&#20316;&#20026;&#19968;&#20010;PyPI&#21253;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.  FairGrad is available as a PyPI package at https://pypi.org/project/fairgrad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#22522;&#20934;- CARLANE&#65292;&#21253;&#25324;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#26377;&#27880;&#37322;&#30340;&#22270;&#29255;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.08083</link><description>&lt;p&gt;
CARLANE: &#20174;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#30340;&#36710;&#36947;&#26816;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains. (arXiv:2206.08083v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#22522;&#20934;- CARLANE&#65292;&#21253;&#25324;&#27169;&#25311;&#21040;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#39046;&#22495;&#30340;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#24182;&#21253;&#21547;&#22823;&#37327;&#26377;&#27880;&#37322;&#30340;&#22270;&#29255;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#29992;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#36890;&#36807;&#23558;&#27169;&#22411;&#20174;&#26377;&#26631;&#31614;&#30340;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#20943;&#36731;&#22495;&#28418;&#31227;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#22797;&#26434;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#36710;&#36947;&#26816;&#27979;&#19978;&#12290;&#36825;&#21487;&#20197;&#24402;&#22240;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#36825;&#20123;&#26041;&#21521;&#19978;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CARLANE&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;2D&#36710;&#36947;&#26816;&#27979;&#30340;3&#21521;&#27169;&#25311;&#21040;&#30495;&#23454;&#22495;&#36866;&#24212;&#22522;&#20934;&#12290;CARLANE&#21253;&#25324;&#21333;&#30446;&#26631;&#25968;&#25454;&#38598;MoLane&#21644;TuLane&#65292;&#20197;&#21450;&#22810;&#30446;&#26631;&#25968;&#25454;&#38598;MuLane&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#20849;&#21253;&#21547;16.3&#19975;&#24352;&#29420;&#29305;&#30340;&#22270;&#29255;&#65292;&#20854;&#20013;&#26377;11.8&#19975;&#24352;&#26377;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#21644;&#25253;&#21578;&#20102;&#31995;&#32479;&#22522;&#20934;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21407;&#22411;&#20132;&#21449;&#22495;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35780;&#20272;&#20013;&#30340;&#35823;&#25253;&#29575;&#21644;&#28431;&#25253;&#29575;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07813</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#20197;&#26816;&#27979;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#21644;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#29983;&#21629;&#23433;&#20840;&#29615;&#22659;&#20013;&#32463;&#24120;&#34920;&#29616;&#20986;&#38169;&#35823;&#34892;&#20026;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#37325;&#22823;&#38169;&#35823;&#65292;&#22240;&#27492;&#23427;&#20204;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;DRL&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#65292;&#20197;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#30340;&#25925;&#38556;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#27979;&#35797;DRL&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21644;&#36981;&#23432;&#23433;&#20840;&#35201;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#20195;&#29702;&#29983;&#25104;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#30340;&#29366;&#24577;&#24207;&#21015;&#21464;&#21270;&#65292;&#20197;&#25506;&#32034;&#29615;&#22659;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;DRL&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#22312;&#20445;&#25345;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#27979;&#35797;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21464;&#20998;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#24555;&#36895;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.03211</link><description>&lt;p&gt;
&#21464;&#20998;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational Meta Reinforcement Learning for Social Robotics. (arXiv:2206.03211v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21464;&#20998;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#21487;&#24555;&#36895;&#36866;&#24212;&#19981;&#21516;&#29615;&#22659;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31038;&#20132;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#20154;&#22312;&#25105;&#20204;&#26085;&#24120;&#29615;&#22659;&#20013;&#30340;&#26222;&#21450;&#65292;&#25552;&#39640;&#23427;&#20204;&#30340;&#31038;&#20132;&#25216;&#33021;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#20173;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#29942;&#39048;&#26159;&#65292;&#30001;&#20110;&#31038;&#20132;&#35268;&#33539;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29615;&#22659;&#65292;&#26426;&#22120;&#20154;&#34892;&#20026;&#38656;&#35201;&#32463;&#24120;&#36866;&#24212;&#12290;&#20363;&#22914;&#65292;&#22312;&#21307;&#38498;&#20013;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#27604;&#22312;&#21150;&#20844;&#23460;&#20013;&#26356;&#21152;&#23567;&#24515;&#22320;&#23548;&#33322;&#21608;&#22260;&#30340;&#30149;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#37324;&#65292;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#23398;&#20064;&#30340;&#65292;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#38024;&#23545;&#32473;&#23450;&#29615;&#22659;&#23398;&#20064;&#20986;&#21512;&#36866;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21464;&#20998;&#20803;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#24555;&#36895;&#22320;&#23558;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#36866;&#24212;&#21040;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#19978;&#12290;&#22240;&#27492;&#65292;&#22312;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#29615;&#22659;&#26102;&#65292;&#21487;&#20197;&#24555;&#36895;&#35780;&#20272;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#24182;&#36873;&#25321;&#36866;&#21512;&#30340;&#19968;&#20010;&#12290;&#35813;&#31243;&#24207;&#23398;&#20064;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#21644;&#19968;&#20123;&#20854;&#20182;&#30340;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing presence of robots in our every-day environments, improving their social skills is of utmost importance. Nonetheless, social robotics still faces many challenges. One bottleneck is that robotic behaviors need to be often adapted as social norms depend strongly on the environment. For example, a robot should navigate more carefully around patients in a hospital compared to workers in an office. In this work, we investigate meta-reinforcement learning (meta-RL) as a potential solution. Here, robot behaviors are learned via reinforcement learning where a reward function needs to be chosen so that the robot learns an appropriate behavior for a given environment. We propose to use a variational meta-RL procedure that quickly adapts the robots' behavior to new reward functions. As a result, given a new environment different reward functions can be quickly evaluated and an appropriate one selected. The procedure learns a vectorized representation for reward functions and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2010.08657</link><description>&lt;p&gt;
&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.08657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39044;&#20998;&#37197;&#22266;&#23450;&#20998;&#31867;&#22120;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23384;&#20648;&#22312;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#22312;&#23398;&#20064;&#38454;&#27573;&#30340;&#24320;&#22987;&#23601;&#23558;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#32435;&#20837;&#20998;&#31867;&#25439;&#22833;&#30340;&#35745;&#31639;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#20808;&#21069;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#38754;&#23545;&#19968;&#31995;&#21015;&#25968;&#25454;&#30340;&#20219;&#21153;&#26159;&#23398;&#20064;&#26032;&#31867;&#21035;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#31867;&#21035;&#12290;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24120;&#24120;&#20250;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#25928;&#30340;&#26041;&#27861;&#21033;&#29992;&#23384;&#20648;&#22312;&#19968;&#20010;&#24773;&#33410;&#24615;&#35760;&#24518;&#20013;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#21516;&#26102;&#25193;&#23637;&#26368;&#32456;&#20998;&#31867;&#22120;&#33410;&#28857;&#20197;&#23481;&#32435;&#26032;&#30340;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#22266;&#23450;&#20998;&#31867;&#22120;&#26367;&#20195;&#20102;&#25193;&#23637;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#19968;&#20123;&#39044;&#20998;&#37197;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#38454;&#27573;&#24320;&#22987;&#23601;&#21463;&#21040;&#20998;&#31867;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#19982;&#26631;&#20934;&#25193;&#23637;&#20998;&#31867;&#22120;&#30456;&#21453;&#65292;&#36825;&#26679;&#20570;&#26377;&#20197;&#19979;&#22909;&#22788;&#65306;(a)&#26410;&#26469;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#36755;&#20986;&#33410;&#28857;&#20174;&#23398;&#20064;&#30340;&#19968;&#24320;&#22987;&#23601;&#33021;&#30475;&#21040;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#30340;&#27491;&#26679;&#26412;&#65307;(b)&#33021;&#22815;&#23398;&#20064;&#19981;&#38543;&#30528;&#26032;&#31867;&#21035;&#30340;&#21152;&#20837;&#32780;&#25913;&#21464;&#20854;&#20960;&#20309;&#37197;&#32622;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In class-incremental learning, a learning agent faces a stream of data with the goal of learning new classes while not forgetting previous ones. Neural networks are known to suffer under this setting, as they forget previously acquired knowledge. To address this problem, effective methods exploit past data stored in an episodic memory while expanding the final classifier nodes to accommodate the new classes.  In this work, we substitute the expanding classifier with a novel fixed classifier in which a number of pre-allocated output nodes are subject to the classification loss right from the beginning of the learning phase. Contrarily to the standard expanding classifier, this allows: (a) the output nodes of future unseen classes to firstly see negative samples since the beginning of learning together with the positive samples that incrementally arrive; (b) to learn features that do not change their geometric configuration as novel classes are incorporated in the learning model.  Experi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307; TRPO (MATRPO) &#30340;&#20998;&#25955;&#24335; MARL &#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20219;&#21153;&#19978;&#20248;&#21270;&#20998;&#24067;&#24335;&#31574;&#30053;&#65292;&#24182;&#19988;&#26080;&#38656;&#26234;&#33021;&#20307;&#20043;&#38388;&#20849;&#20139;&#35266;&#27979;&#12289;&#22870;&#21169;&#12289;&#31574;&#30053;&#25110;&#20540;/&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#21512;&#20316;&#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2010.07916</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20449;&#20219;&#21306;&#22495;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Trust Region Policy Optimization. (arXiv:2010.07916v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.07916
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307; TRPO (MATRPO) &#30340;&#20998;&#25955;&#24335; MARL &#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20219;&#21153;&#19978;&#20248;&#21270;&#20998;&#24067;&#24335;&#31574;&#30053;&#65292;&#24182;&#19988;&#26080;&#38656;&#26234;&#33021;&#20307;&#20043;&#38388;&#20849;&#20139;&#35266;&#27979;&#12289;&#22870;&#21169;&#12289;&#31574;&#30053;&#25110;&#20540;/&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#21512;&#20316;&#28216;&#25103;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20449;&#20219;&#21306;&#22495;&#31574;&#30053;&#20248;&#21270; (TRPO) &#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TRPO &#30340;&#31574;&#30053;&#26356;&#26032;&#21487;&#20197;&#36716;&#21270;&#20026;&#22810;&#26234;&#33021;&#20307;&#26696;&#20363;&#19979;&#30340;&#20998;&#24067;&#24335;&#20849;&#35782;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#20849;&#35782;&#20248;&#21270;&#27169;&#22411;&#36827;&#34892;&#19968;&#31995;&#21015;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307; TRPO (MATRPO) &#30340;&#20998;&#25955;&#24335; MARL &#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#22522;&#20110;&#26412;&#22320;&#35266;&#27979;&#21644;&#31169;&#20154;&#22870;&#21169;&#20248;&#21270;&#20998;&#24067;&#24335;&#31574;&#30053;&#12290;&#26234;&#33021;&#20307;&#19981;&#38656;&#35201;&#20102;&#35299;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#35266;&#27979;&#12289;&#22870;&#21169;&#12289;&#31574;&#30053;&#25110;&#20540;/&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#19982;&#37051;&#23621;&#20849;&#20139;&#20284;&#28982;&#27604;&#12290;&#35813;&#31639;&#27861;&#23436;&#20840;&#20998;&#25955;&#19988;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#20316;&#28216;&#25103;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#22797;&#26434;&#30340; MARL &#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bundle-MVL-K&#30340;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#23478;&#26063;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31574;&#30053;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#35813;&#27169;&#22411;&#30340;&#20248;&#21270;&#25512;&#33616;&#12290;&#36825;&#26159;&#23545;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#25805;&#20316;&#21270;&#30340;&#39318;&#27425;&#23581;&#35797;&#20043;&#19968;&#65292;&#24182;&#39318;&#27425;&#24314;&#31435;&#20102;&#22810;&#27425;&#36141;&#20080;&#34892;&#20026;&#24314;&#27169;&#19982;&#25910;&#20837;&#22686;&#30410;&#20043;&#38388;&#30340;&#23450;&#37327;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2006.08055</link><description>&lt;p&gt;
&#22810;&#27425;&#36141;&#20080;&#34892;&#20026;&#65306;&#24314;&#27169;&#12289;&#20272;&#35745;&#21644;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Purchase Behavior: Modeling, Estimation and Optimization. (arXiv:2006.08055v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.08055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bundle-MVL-K&#30340;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#23478;&#26063;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31574;&#30053;&#26469;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#35813;&#27169;&#22411;&#30340;&#20248;&#21270;&#25512;&#33616;&#12290;&#36825;&#26159;&#23545;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#25805;&#20316;&#21270;&#30340;&#39318;&#27425;&#23581;&#35797;&#20043;&#19968;&#65292;&#24182;&#39318;&#27425;&#24314;&#31435;&#20102;&#22810;&#27425;&#36141;&#20080;&#34892;&#20026;&#24314;&#27169;&#19982;&#25910;&#20837;&#22686;&#30410;&#20043;&#38388;&#30340;&#23450;&#37327;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#27169;&#22810;&#20010;&#20135;&#21697;&#36141;&#20080;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#20026;&#22312;&#32447;&#38646;&#21806;&#21830;&#21644;&#30005;&#21830;&#24179;&#21488;&#25552;&#20379;&#20248;&#21270;&#25512;&#33616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#23478;&#26063;&#65292;&#31216;&#20026;Bundle-MVL-K&#23478;&#26063;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#20998;&#25628;&#32034;&#30340;&#36845;&#20195;&#31574;&#30053;&#65292;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#35813;&#27169;&#22411;&#30340;&#20248;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35745;&#31639;&#26368;&#20248;&#25512;&#33616;&#38598;&#30340;&#22256;&#38590;&#24615;&#65292;&#24182;&#25512;&#23548;&#20986;&#26368;&#20248;&#35299;&#30340;&#20960;&#20010;&#32467;&#26500;&#23646;&#24615;&#65292;&#20197;&#21152;&#36895;&#35745;&#31639;&#12290;&#36825;&#26159;&#23545;&#22810;&#27425;&#36141;&#20080;&#36873;&#25321;&#27169;&#22411;&#36827;&#34892;&#25805;&#20316;&#21270;&#30340;&#39318;&#27425;&#23581;&#35797;&#20043;&#19968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24314;&#27169;&#22810;&#27425;&#36141;&#20080;&#34892;&#20026;&#19982;&#25910;&#20837;&#22686;&#30410;&#20043;&#38388;&#30340;&#39318;&#20010;&#23450;&#37327;&#32852;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#24314;&#27169;&#21644;&#20248;&#21270;&#25216;&#26415;&#19982;&#31454;&#20105;&#35299;&#20915;&#26041;&#26696;&#22312;&#27169;&#22411;&#36866;&#37197;&#24230;&#12289;&#39044;&#26399;&#25910;&#20837;&#22686;&#30410;&#21644;&#36816;&#34892;&#26102;&#38388;&#20943;&#23569;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#30340;&#25928;&#26524;&#12290;&#20363;&#22914;&#65292;&#39044;&#26399;&#30340;&#25910;&#20837;&#22686;&#30410;&#27604;&#31454;&#20105;&#35299;&#20915;&#26041;&#26696;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of modeling purchase of multiple products and utilizing it to display optimized recommendations for online retailers and e-commerce platforms.  We present a parsimonious multi-purchase family of choice models called the Bundle-MVL-K family, and develop a binary search based iterative strategy that efficiently computes optimized recommendations for this model. We establish the hardness of computing optimal recommendation sets, and derive several structural properties of the optimal solution that aid in speeding up computation. This is one of the first attempts at operationalizing multi-purchase class of choice models. We show one of the first quantitative links between modeling multiple purchase behavior and revenue gains. The efficacy of our modeling and optimization techniques compared to competing solutions is shown using several real world datasets on multiple metrics such as model fitness, expected revenue gains and run-time reductions. For example, the expecte
&lt;/p&gt;</description></item></channel></rss>