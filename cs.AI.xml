<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>OntoChat&#26159;&#19968;&#20010;&#25903;&#25345;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.05921</link><description>&lt;p&gt;
OntoChat: &#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
OntoChat: a Framework for Conversational Ontology Engineering using Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05921
&lt;/p&gt;
&lt;p&gt;
OntoChat&#26159;&#19968;&#20010;&#25903;&#25345;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39033;&#30446;&#20013;&#30340;&#26412;&#20307;&#24037;&#31243;(OE)&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#30340;&#25361;&#25112;&#28304;&#20110;&#21508;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#12289;&#39046;&#22495;&#19987;&#23478;&#21450;&#20854;&#19982;&#26412;&#20307;&#35774;&#35745;&#32773;&#30340;&#22797;&#26434;&#20114;&#21160;&#32972;&#26223;&#30340;&#24322;&#36136;&#24615;&#12290;&#36825;&#31181;&#22810;&#26041;&#20114;&#21160;&#32463;&#24120;&#20250;&#22312;&#26412;&#20307;&#38656;&#27714;&#30340;&#24341;&#30003;&#20013;&#20135;&#29983;&#31995;&#32479;&#24615;&#30340;&#27495;&#20041;&#21644;&#20559;&#35265;&#65292;&#30452;&#25509;&#24433;&#21709;&#35774;&#35745;&#12289;&#35780;&#20272;&#24182;&#21487;&#33021;&#21361;&#21450;&#30446;&#26631;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30446;&#21069;&#30340;OE&#26041;&#27861;&#24378;&#28872;&#20381;&#36182;&#20110;&#25163;&#24037;&#27963;&#21160;&#65288;&#22914;&#37319;&#35775;&#12289;&#35752;&#35770;&#39029;&#38754;&#65289;&#12290;&#22312;&#25910;&#38598;&#20102;&#20851;&#20110;&#26368;&#20851;&#38190;OE&#27963;&#21160;&#30340;&#35777;&#25454;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;OntoChat&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#38656;&#27714;&#24341;&#30003;&#12289;&#20998;&#26512;&#21644;&#27979;&#35797;&#30340;&#23545;&#35805;&#26412;&#20307;&#24037;&#31243;&#26694;&#26550;&#12290;&#36890;&#36807;&#19982;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#20114;&#21160;&#65292;&#29992;&#25143;&#21487;&#20197;&#24341;&#23548;&#29992;&#25143;&#25925;&#20107;&#30340;&#21019;&#24314;&#21644;&#33021;&#21147;&#38382;&#39064;&#30340;&#25552;&#21462;&#65292;&#21516;&#26102;&#25509;&#25910;&#35745;&#31639;&#25903;&#25345;&#20197;&#20998;&#26512;&#25972;&#20307;&#38656;&#27714;&#24182;&#36827;&#34892;&#26089;&#26399;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05921v1 Announce Type: new  Abstract: Ontology engineering (OE) in large projects poses a number of challenges arising from the heterogeneous backgrounds of the various stakeholders, domain experts, and their complex interactions with ontology designers. This multi-party interaction often creates systematic ambiguities and biases from the elicitation of ontology requirements, which directly affect the design, evaluation and may jeopardise the target reuse. Meanwhile, current OE methodologies strongly rely on manual activities (e.g., interviews, discussion pages). After collecting evidence on the most crucial OE activities, we introduce OntoChat, a framework for conversational ontology engineering that supports requirement elicitation, analysis, and testing. By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving computational support to analyse the overall requirements and test early
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;</title><link>https://arxiv.org/abs/2403.03627</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models to Support Real-World Fact-Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03627
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25903;&#25345;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#35299;&#37322;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22768;&#26126;&#30340;&#19981;&#21512;&#29702;&#20043;&#22788;&#21644;&#28508;&#22312;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#28508;&#21147;&#25903;&#25345;&#20154;&#31867;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#12290;&#34429;&#28982;MLLMs&#24050;&#32463;&#34987;&#29992;&#20316;&#20107;&#23454;&#26680;&#26597;&#24037;&#20855;&#65292;&#20294;&#23601;&#20854;&#22312;&#27492;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#32780;&#35328;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#31995;&#32479;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;&#27169;&#22411;&#20419;&#36827;&#29616;&#23454;&#19990;&#30028;&#20107;&#23454;&#26680;&#26597;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26159;&#26080;&#38656;&#35777;&#25454;&#30340;&#65292;&#20165;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#26377;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#35774;&#35745;&#33021;&#22815;&#25552;&#21462;&#27169;&#22411;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#32622;&#20449;&#27700;&#24179;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20851;&#20110;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#22833;&#36133;&#21407;&#22240;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#21457;&#29616;&#65292;(1) GPT-4V&#22312;&#35782;&#21035;&#24694;&#24847;&#21644;&#35823;&#23548;&#24615;&#22810;&#27169;&#24577;&#22768;&#26126;&#26041;&#38754;&#34920;&#29616;&#20986;&#36229;&#20961;&#24615;&#33021;&#65292;&#33021;&#22815;&#35299;&#37322;&#19981;&#21512;&#29702;&#30340;&#26041;&#38754;&#21644;&#28508;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;(2)&#29616;&#26377;&#30340;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03627v1 Announce Type: cross  Abstract: Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing o
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;</title><link>https://arxiv.org/abs/2403.00843</link><description>&lt;p&gt;
&#21033;&#29992;&#21452;&#23618;&#21487;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35268;&#21010;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00843
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#26469;&#22686;&#24378;&#38271;&#26399;&#25512;&#33616;&#65292;&#20351;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20542;&#21521;&#20110;&#36807;&#20998;&#36814;&#21512;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#32780;&#24573;&#35270;&#20182;&#20204;&#30340;&#38271;&#26399;&#21442;&#19982;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25512;&#33616;&#20915;&#31574;&#36807;&#31243;&#20013;&#21512;&#24182;&#35268;&#21010;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24320;&#21457;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#21363;&#26102;&#20852;&#36259;&#21644;&#38271;&#26399;&#21442;&#19982;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#31232;&#30095;&#25968;&#25454;&#30340;&#26174;&#33879;&#35268;&#21010;&#33021;&#21147;&#29992;&#20110;&#38271;&#26399;&#25512;&#33616;&#12290;&#20851;&#38190;&#22312;&#20110;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#22330;&#26223;&#20013;&#26377;&#25928;&#29702;&#35299;&#21644;&#24212;&#29992;&#20219;&#21153;&#35299;&#20915;&#21407;&#21017;&#65292;&#22240;&#20026;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21487;&#33021;&#24182;&#26410;&#33258;&#28982;&#21253;&#21547;&#36825;&#20123;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.00642</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking The Uniformity Metric in Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00642
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#24182;&#28385;&#36275;&#29616;&#26377;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#36798;&#26631;&#30340;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#32500;&#24230;&#23849;&#28291;&#25935;&#24863;&#30340;&#26032;&#22343;&#21248;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#21248;&#24615;&#22312;&#35780;&#20272;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#29702;&#35299;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#20043;&#21069;&#30340;&#19968;&#39033;&#24320;&#21019;&#24615;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#23450;&#37327;&#34913;&#37327;&#23398;&#20064;&#34920;&#31034;&#30340;&#23849;&#28291;&#31243;&#24230;&#12290;&#30452;&#25509;&#20248;&#21270;&#36825;&#19968;&#24230;&#37327;&#19982;&#23545;&#40784;&#19968;&#36215;&#65292;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#27490;&#19981;&#26029;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#36825;&#19968;&#24230;&#37327;&#32570;&#20047;&#23545;&#32500;&#24230;&#23849;&#28291;&#30340;&#25935;&#24863;&#24615;&#65292;&#20984;&#26174;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#24182;&#35774;&#35745;&#19968;&#20010;&#26356;&#26377;&#25928;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20116;&#20010;&#22522;&#26412;&#24615;&#36136;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#26410;&#33021;&#28385;&#36275;&#20854;&#20013;&#30340;&#19968;&#20123;&#12290;&#25105;&#20204;&#38543;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22343;&#21248;&#24615;&#24230;&#37327;&#65292;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#65292;&#24182;&#19988;&#23545;&#32500;&#24230;&#23849;&#28291;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00642v1 Announce Type: cross  Abstract: Uniformity plays a crucial role in the assessment of learned representations, contributing to a deeper comprehension of self-supervised learning. The seminal work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that quantitatively measures the collapse degree of learned representations. Directly optimizing this metric together with alignment proves to be effective in preventing constant collapse. However, we present both theoretical and empirical evidence revealing that this metric lacks sensitivity to dimensional collapse, highlighting its limitations. To address this limitation and design a more effective uniformity metric, this paper identifies five fundamental properties, some of which the existing uniformity metric fails to meet. We subsequently introduce a novel uniformity metric that satisfies all of these desiderata and exhibits sensitivity to dimensional collapse. When applied as an auxiliary loss in various 
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12237</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20869;&#23481;&#23457;&#26680;&#20013;&#25512;&#36831;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer in Content Moderation: The Human-AI Interplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#22312;&#32447;&#24179;&#21488;&#20869;&#23481;&#23457;&#26680;&#20381;&#36182;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#25429;&#25417;&#20869;&#23481;&#23457;&#26680;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31639;&#27861;&#35266;&#23519;&#21040;&#21363;&#23558;&#21457;&#24067;&#30340;&#24086;&#23376;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20570;&#20986;&#20998;&#31867;&#21644;&#20934;&#20837;&#20915;&#31574;&#65292;&#24182;&#23433;&#25490;&#24086;&#23376;&#36827;&#34892;&#20154;&#24037;&#23457;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.09877</link><description>&lt;p&gt;
&#35745;&#31639;&#20855;&#26377;&#32479;&#19968;&#21160;&#20316;&#25104;&#26412;&#30340;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
On Computing Plans with Uniform Action Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09877
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#30340;&#19977;&#31181;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#21487;&#20197;&#29983;&#25104;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35268;&#21010;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#26377;&#20852;&#36259;&#25214;&#21040;&#21160;&#20316;&#25104;&#26412;&#23613;&#21487;&#33021;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;&#36825;&#26679;&#30340;&#35745;&#21010;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#39044;&#27979;&#24615;&#65292;&#36825;&#22312;&#20154;&#31867;&#25191;&#34892;&#35268;&#21010;&#24037;&#20855;&#24314;&#35758;&#30340;&#35745;&#21010;&#26102;&#26159;&#20851;&#38190;&#29305;&#24449;&#12290;&#26412;&#25991;&#23558;&#19977;&#20010;&#19968;&#33268;&#24615;&#24230;&#37327;&#24212;&#29992;&#20110;&#33258;&#21160;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#35268;&#21010;&#30340;&#32534;&#35793;&#25216;&#26415;&#65292;&#20801;&#35768;&#20197;&#21160;&#20316;&#25104;&#26412;&#24635;&#21644;&#21644;&#21160;&#20316;&#25104;&#26412;&#19968;&#33268;&#24615;&#36827;&#34892;&#35789;&#20856;&#25490;&#24207;&#26368;&#20248;&#21270;&#12290;&#22312;&#30693;&#21517;&#21644;&#26032;&#39062;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#37325;&#26500;&#30340;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#33268;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09877v1 Announce Type: new  Abstract: In many real-world planning applications, agents might be interested in finding plans whose actions have costs that are as uniform as possible. Such plans provide agents with a sense of stability and predictability, which are key features when humans are the agents executing plans suggested by planning tools. This paper adapts three uniformity metrics to automated planning, and introduce planning-based compilations that allow to lexicographically optimize sum of action costs and action costs uniformity. Experimental results both in well-known and novel planning benchmarks show that the reformulated tasks can be effectively solved in practice to generate uniform plans.
&lt;/p&gt;</description></item><item><title>PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2401.03855</link><description>&lt;p&gt;
PythonSaga&#65306;&#37325;&#26032;&#23450;&#20041;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;LLM&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PythonSaga: Redefining the Benchmark to Evaluate Code Generating LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03855
&lt;/p&gt;
&lt;p&gt;
PythonSaga&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#65292;&#38024;&#23545;Python&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;,&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20934;&#23384;&#22312;&#30340;&#32534;&#31243;&#27010;&#24565;&#20559;&#35265;&#21644;&#31616;&#21333;&#20219;&#21153;&#26222;&#36941;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#20195;&#30721;&#28608;&#22686;&#30340;&#25512;&#21160;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#23545;HumanEval&#21644;MBPP&#20004;&#20010;&#27969;&#34892;&#30340;Python&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20154;&#24037;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#22810;&#26679;&#24615;&#21644;&#38590;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23545;&#19968;&#32452;&#26377;&#38480;&#30340;&#32534;&#31243;&#27010;&#24565;&#23384;&#22312;&#20005;&#37325;&#20559;&#35265;&#65292;&#23436;&#20840;&#24573;&#35270;&#20102;&#22823;&#22810;&#25968;&#20854;&#20182;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#22823;&#37327;&#31616;&#21333;&#20219;&#21153;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#22840;&#22823;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;PythonSaga&#65292;&#21253;&#21547;&#20102;185&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#65292;&#28085;&#30422;&#20102;38&#20010;&#19981;&#21516;&#38590;&#24230;&#32423;&#21035;&#30340;&#32534;&#31243;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03855v2 Announce Type: replace-cross  Abstract: Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities. We conducted a large-scale human evaluation of HumanEval and MBPP, two popular benchmarks for Python code generation, analyzing their diversity and difficulty. Our findings unveil a critical bias towards a limited set of programming concepts, neglecting most of the other concepts entirely. Furthermore, we uncover a worrying prevalence of easy tasks, potentially inflating model performance estimations. To address these limitations, we propose a novel benchmark, PythonSaga, featuring 185 hand-crafted prompts on a balanced representation of 38 programming concepts across diverse difficulty levels.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.13668</link><description>&lt;p&gt;
MAIRA-1&#65306;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAIRA-1: A specialised large multimodal model for radiology report generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13668
&lt;/p&gt;
&lt;p&gt;
MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20351;&#22810;&#27169;&#24577;&#27169;&#22411;&#33719;&#24471;&#22270;&#20687;&#29702;&#35299;&#21644;&#25551;&#36848;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65288;MAIRA-1&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#22522;&#20110;Vicuna-7B&#30340;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;&#29305;&#21035;&#22320;&#65292;MAIRA-1&#22312;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#40784;&#30340;RadCliQ&#24230;&#37327;&#21644;&#32771;&#34385;&#30340;&#25152;&#26377;&#35789;&#27719;&#24230;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25163;&#21160;&#23457;&#26680;&#26174;&#31034;&#20986;&#20102;&#20135;&#29983;&#25253;&#21578;&#30340;&#27969;&#30021;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#25152;&#26410;&#25429;&#25417;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#26356;&#22810;&#20449;&#24687;&#21644;&#36164;&#28304;&#21487;&#22312;&#39033;&#30446;&#32593;&#31449;&#19978;&#25214;&#21040;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.08941</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25551;&#36848;&#36923;&#36753;&#35821;&#22659;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning over Description Logic-based Contexts with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#20016;&#23500;&#35821;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34913;&#37327;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#21512;&#25104;&#35821;&#22659;&#20013;&#23545;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25110;&#35777;&#26126;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20351;&#29992;&#30340;&#35821;&#22659;&#38750;&#24120;&#31616;&#21333;&#65307;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26159;&#30001;&#20165;&#21547;&#26377;&#23569;&#37327;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#37327;&#35789;&#30340;&#30701;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#29983;&#25104;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#22522;&#20110;Transformer&#27169;&#22411;&#33021;&#22815;&#22312;&#34920;&#36798;&#20016;&#23500;&#35821;&#22659;&#20013;&#25191;&#34892;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#25551;&#36848;&#36923;&#36753;&#30693;&#35782;&#24211;&#29983;&#25104;&#30340;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#20026;&#29983;&#25104;&#30693;&#35782;&#24211;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#36798;&#24335;&#35821;&#35328;$\mathcal{ALCQ$&#12290;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;384K&#20010;&#31034;&#20363;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#32500;&#24230;&#19978;&#22686;&#21152;&#65306;i) &#25512;&#29702;&#28145;&#24230;&#65292;&#21644;ii) &#21477;&#23376;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08941v2 Announce Type: replace-cross  Abstract: One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that t
&lt;/p&gt;</description></item><item><title>ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.20431</link><description>&lt;p&gt;
&#25552;&#21319;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Raising the ClaSS of Streaming Time Series Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20431
&lt;/p&gt;
&lt;p&gt;
ClaSS&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#65292;&#26222;&#36941;&#23384;&#22312;&#30340;&#20256;&#24863;&#22120;&#21457;&#23556;&#39640;&#39057;&#25968;&#20540;&#27979;&#37327;&#27969;&#65292;&#21453;&#26144;&#20102;&#20154;&#31867;&#12289;&#21160;&#29289;&#12289;&#24037;&#19994;&#12289;&#21830;&#19994;&#21644;&#33258;&#28982;&#36807;&#31243;&#30340;&#29305;&#24615;&#12290;&#36825;&#20123;&#36807;&#31243;&#30340;&#21464;&#21270;&#65292;&#20363;&#22914;&#30001;&#22806;&#37096;&#20107;&#20214;&#25110;&#20869;&#37096;&#29366;&#24577;&#21464;&#21270;&#24341;&#36215;&#30340;&#65292;&#20250;&#34920;&#29616;&#20026;&#35760;&#24405;&#20449;&#21495;&#20013;&#30340;&#21464;&#21270;&#12290;&#27969;&#24335;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#65288;STSS&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#27969;&#20998;&#21106;&#20026;&#23545;&#24212;&#20110;&#25152;&#35266;&#23519;&#30340;&#36807;&#31243;&#25110;&#23454;&#20307;&#29366;&#24577;&#30340;&#36830;&#32493;&#21487;&#21464;&#22823;&#23567;&#30340;&#20998;&#27573;&#12290;&#20998;&#21106;&#25805;&#20316;&#26412;&#36523;&#24517;&#39035;&#33021;&#22815;&#24212;&#23545;&#36755;&#20837;&#20449;&#21495;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ClaSS&#65292;&#19968;&#31181;&#26032;&#39062;&#12289;&#39640;&#25928;&#19988;&#39640;&#31934;&#24230;&#30340;STSS&#31639;&#27861;&#12290;ClaSS&#20351;&#29992;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35780;&#20272;&#28508;&#22312;&#20998;&#21106;&#30340;&#21516;&#36136;&#24615;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#33879;&#30340;&#21464;&#21270;&#28857;&#65288;CPs&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#35780;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22823;&#22411;&#22522;&#20934;&#21644;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26723;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.20431v2 Announce Type: replace-cross  Abstract: Ubiquitous sensors today emit high frequency streams of numerical measurements that reflect properties of human, animal, industrial, commercial, and natural processes. Shifts in such processes, e.g. caused by external events or internal state changes, manifest as changes in the recorded signals. The task of streaming time series segmentation (STSS) is to partition the stream into consecutive variable-sized segments that correspond to states of the observed processes or entities. The partition operation itself must in performance be able to cope with the input frequency of the signals. We introduce ClaSS, a novel, efficient, and highly accurate algorithm for STSS. ClaSS assesses the homogeneity of potential partitions using self-supervised time series classification and applies statistical tests to detect significant change points (CPs). In our experimental evaluation using two large benchmarks and six real-world data archives, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2310.00724</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;:&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subtractive Mixture Models via Squaring: Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00724
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#23558;&#20960;&#20010;&#20998;&#24067;&#20316;&#20026;&#32452;&#20214;&#30456;&#21152;&#26469;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#12290;&#20801;&#35768;&#28151;&#21512;&#20943;&#21435;&#27010;&#29575;&#36136;&#37327;&#25110;&#23494;&#24230;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#25152;&#38656;&#30340;&#32452;&#20214;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#31181;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#24182;&#30830;&#20445;&#23427;&#20204;&#20173;&#28982;&#32534;&#30721;&#38750;&#36127;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24179;&#26041;&#26469;&#23398;&#20064;&#21644;&#25191;&#34892;&#28145;&#24230;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#30005;&#36335;&#26694;&#26550;&#20013;&#36827;&#34892;&#36825;&#20123;&#30740;&#31350;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#24352;&#37327;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#24182;&#27867;&#21270;&#20854;&#20182;&#20943;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20801;&#35768;&#20943;&#27861;&#30340;&#24179;&#26041;&#30005;&#36335;&#31867;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#20855;&#34920;&#36798;&#21147;&#65307;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#19978;&#23454;&#35777;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#21152;&#30340;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#20276;&#38543;&#21464;&#37327;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36880;&#28176;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2111.08108</link><description>&lt;p&gt;
&#20351;&#29992;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Control with Stochastic Models of Hamiltonian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.08108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#20276;&#38543;&#21464;&#37327;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36880;&#28176;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#65292;&#28982;&#21518;&#27714;&#35299;&#21704;&#23494;&#39039;&#21160;&#21147;&#31995;&#32479;&#65292;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#24212;&#29992;&#20110;&#21407;&#22987;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#23398;&#20064;&#37325;&#28857;&#36716;&#31227;&#21040;&#20102;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#30456;&#24212;&#30340;&#20276;&#38543;&#21464;&#37327;&#19978;&#12290;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21521;&#21518;&#25512;&#36827;&#26102;&#38388;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#20174;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#20219;&#21153;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.08108v2 Announce Type: replace-cross  Abstract: Optimal control problems can be solved by applying the Pontryagin maximum principle and then solving for a Hamiltonian dynamical system. In this paper, we propose novel learning frameworks to tackle optimal control problems. By applying the Pontryagin maximum principle to the original optimal control problem, the learning focus shifts to reduced Hamiltonian dynamics and corresponding adjoint variables. The reduced Hamiltonian networks can be learned by going backward in time and then minimizing loss function deduced from the Pontryagin maximum principle's conditions. The learning process is further improved by progressively learning a posterior distribution of reduced Hamiltonians, utilizing a variational autoencoder which leads to more effective path exploration process. We apply our learning frameworks to control tasks and obtain competitive results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.13555</link><description>&lt;p&gt;
&#22270;&#20687;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Fairness of Image Upsampling Methods. (arXiv:2401.13555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13555
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#24212;&#29992;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#29616;&#20195;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21019;&#24314;&#21512;&#25104;&#23186;&#20307;&#65288;&#22914;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#35825;&#20154;&#65292;&#20294;&#35780;&#20272;&#20854;&#20844;&#24179;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#21463;&#30417;&#30563;&#20844;&#24179;&#24615;&#30340;&#28789;&#24863;&#26469;&#28304;&#8212;&#8212;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;&#22270;&#20687;&#19978;&#37319;&#26679;&#36825;&#20010;&#29305;&#23450;&#24212;&#29992;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#29616;&#20195;&#19978;&#37319;&#26679;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20316;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UnfairFace&#65292;&#36825;&#26159;FairFace&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#22797;&#21046;&#20102;&#24120;&#35265;&#22823;&#35268;&#27169;&#20154;&#33080;&#25968;&#25454;&#38598;&#30340;&#31181;&#26063;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#26080;&#20559;&#35757;&#32451;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#31639;&#27861;&#23545;&#35813;&#38382;&#39064;&#30340;&#21709;&#24212;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid development of deep generative models for creating synthetic media, such as images and videos. While the practical applications of these models in everyday tasks are enticing, it is crucial to assess the inherent risks regarding their fairness. In this work, we introduce a comprehensive framework for benchmarking the performance and fairness of conditional generative models. We develop a set of metrics$\unicode{x2013}$inspired by their supervised fairness counterparts$\unicode{x2013}$to evaluate the models on their fairness and diversity. Focusing on the specific application of image upsampling, we create a benchmark covering a wide variety of modern upsampling methods. As part of the benchmark, we introduce UnfairFace, a subset of FairFace that replicates the racial distribution of common large-scale face datasets. Our empirical study highlights the importance of using an unbiased training set and reveals variations in how the algorithms respond to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.10805</link><description>&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#30340;&#27010;&#24565;&#65288;CATE&#65289;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#65292;&#20294;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#35813;&#30740;&#31350;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;&#36830;&#25509;&#21160;&#20316;&#21644;&#20854;&#25928;&#26524;&#65288;CATE&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#12290;CATE&#21487;&#20197;&#22312;&#20219;&#21153;&#35268;&#21010;&#21644;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31561;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#22522;&#20110;CATE&#30340;&#20219;&#21153;&#24418;&#24335;&#65292;&#22914;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#65292;&#20854;&#20013;&#35270;&#39057;&#29702;&#35299;&#27169;&#22411;&#20197;&#35821;&#20041;&#21644;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#36830;&#25509;&#21160;&#20316;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#24418;&#24335;&#20135;&#29983;&#20102;&#25429;&#25417;&#30452;&#35266;&#21160;&#20316;&#29305;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#29992;&#20110;&#21160;&#20316;&#36873;&#25321;&#21644;&#21160;&#20316;&#25351;&#23450;&#12290;&#23613;&#31649;&#20219;&#21153;&#20855;&#26377;&#30452;&#35266;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#22256;&#38590;&#37325;&#37325;&#65292;&#20154;&#31867;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#21162;&#21147;&#22880;&#23450;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#36830;&#25509;&#35270;&#39057;&#29702;&#35299;&#20013;&#21160;&#20316;&#21644;&#25928;&#26524;&#30340;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#24076;&#26395;&#33021;&#28608;&#21457;&#20986;&#39640;&#32423;&#24418;&#24335;&#21644;&#27169;&#22411;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce the novel concept of visually Connecting Actions and Their Effects (CATE) in video understanding. CATE can have applications in areas like task planning and learning from demonstration. We propose different CATE-based task formulations, such as action selection and action specification, where video understanding models connect actions and effects at semantic and fine-grained levels. We observe that different formulations produce representations capturing intuitive action properties. We also design various baseline models for action selection and action specification. Despite the intuitive nature of the task, we observe that models struggle, and humans outperform them by a large margin. The study aims to establish a foundation for future efforts, showcasing the flexibility and versatility of connecting actions and effects in video understanding, with the hope of inspiring advanced formulations and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2401.10711</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#24369;&#30417;&#30563;&#39640;&#26031;&#23545;&#27604;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#20316;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#26102;&#21051;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#24378;&#21046;LMMs&#36827;&#34892;&#25512;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#26469;&#23398;&#20064;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26088;&#22312;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#35270;&#39057;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22270;&#20687;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#22788;&#29702;&#35270;&#39057;&#38382;&#31572;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#65292;&#20165;&#20165;&#26159;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35270;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#38024;&#23545;&#38382;&#39064;&#20851;&#38190;&#26102;&#38388;&#25139;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#24378;&#21046;LMMs&#20351;&#29992;&#38382;&#39064;&#20851;&#38190;&#26102;&#21051;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#25512;&#29702;&#20986;&#31572;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#21512;&#24182;&#20026;&#20107;&#20214;&#25551;&#36848;&#65292;&#20197;&#25214;&#21040;&#22810;&#20010;&#20851;&#38190;&#24103;&#20316;&#20026;&#30446;&#26631;&#26102;&#21051;&#65292;&#36825;&#20123;&#26102;&#21051;&#23558;&#20316;&#20026;&#20266;&#26631;&#31614;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#20266;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#30340;&#24369;&#30417;&#30563;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#23545;&#27604;&#22522;&#30784;&#27169;&#22359;&#65288;GCG&#65289;&#12290;GCG&#23398;&#20064;&#22810;&#20010;&#39640;&#26031;&#20989;&#25968;&#26469;&#25551;&#36848;&#26102;&#25928;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Question Answering (VideoQA) aims to answer natural language questions based on the information observed in videos. Despite the recent success of Large Multimodal Models (LMMs) in image-language understanding and reasoning, they deal with VideoQA insufficiently by simply taking uniformly sampled frames as visual inputs, which ignores question-relevant visual clues. Moreover, there are no human annotations for question-critical timestamps in existing VideoQA datasets. In light of this, we propose a novel weakly supervised framework to enforce the LMMs to reason out the answers with question-critical moments as visual inputs. Specifically, we fuse the question and answer pairs as event descriptions to find multiple keyframes as target moments, which will be pseudo-labels. With these pseudo-labels as additionally weak supervision, we devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG learns multiple Gaussian functions to characterize the temporal structure o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.02416</link><description>&lt;p&gt;
ODIN: &#19968;&#20010;&#29992;&#20110;2D&#21644;3D&#24863;&#30693;&#30340;&#21333;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02416
&lt;/p&gt;
&lt;p&gt;
ODIN&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#36827;&#34892;2D&#21644;3D&#35270;&#22270;&#38388;&#30340;&#20449;&#24687;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#20808;&#36827;&#27169;&#22411;&#22312;&#20687;ScanNet&#36825;&#26679;&#30340;&#24403;&#20195;3D&#24863;&#30693;&#22522;&#20934;&#19978;&#20351;&#29992;&#24182;&#26631;&#35760;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;3D&#28857;&#20113;&#65292;&#35813;&#28857;&#20113;&#26159;&#36890;&#36807;&#23545;&#24863;&#30693;&#21040;&#30340;&#22810;&#35270;&#35282;RGB-D&#22270;&#20687;&#36827;&#34892;&#21518;&#22788;&#29702;&#33719;&#24471;&#30340;&#12290;&#23427;&#20204;&#36890;&#24120;&#22312;&#39046;&#22495;&#20869;&#36827;&#34892;&#35757;&#32451;&#65292;&#25918;&#24323;&#20102;&#22823;&#35268;&#27169;&#30340;2D&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#32988;&#36807;&#23558;&#23039;&#24577;RGB-D&#22810;&#35270;&#35282;&#22270;&#20687;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28040;&#32791;&#23039;&#24577;&#22270;&#20687;&#21644;&#21518;&#22788;&#29702;&#30340;3D&#28857;&#20113;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21152;&#21095;&#20102;2D&#21644;3D&#24863;&#30693;&#38656;&#35201;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#35266;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#36825;&#20010;&#35266;&#28857;&#65292;&#24182;&#25552;&#20986;ODIN&#65288;Omni-Dimensional INstance segmentation&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#20351;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#23545;2D RGB&#22270;&#20687;&#21644;3D&#28857;&#20113;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20132;&#26367;&#30340;2D&#35270;&#22270;&#20869;&#21644;3D&#35270;&#22270;&#38388;&#20449;&#24687;&#34701;&#21512;&#26469;&#21306;&#20998;2D&#21644;3D&#29305;&#24449;&#25805;&#20316;&#65292;&#21033;&#29992;&#28041;&#21450;&#30340;&#20196;&#29260;&#30340;&#20301;&#32622;&#32534;&#30721;&#26469;&#25429;&#25417;2D&#34917;&#19969;&#20196;&#29260;&#21644;3D&#22352;&#26631;&#30340;&#20687;&#32032;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01373</link><description>&lt;p&gt;
&#20351;&#29992;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01373
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#26469;&#25552;&#39640;&#21046;&#36896;&#19994;&#20013;&#30340;&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#31561;&#25928;CNN&#27169;&#22411;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#25351;&#26631;&#26041;&#38754;&#65292;T-CNN&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#21482;&#26377;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#23454;&#38469;&#21046;&#36896;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#26816;&#27979;&#26159;&#21046;&#36896;&#19994;&#36136;&#37327;&#25511;&#21046;&#38454;&#27573;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24352;&#37327;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;T-CNN&#65289;&#65292;&#24182;&#22312;&#32599;&#20271;&#29305;&#183;&#21338;&#19990;&#21046;&#36896;&#21378;&#29983;&#20135;&#30340;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#32452;&#20214;&#30340;&#30495;&#23454;&#32570;&#38519;&#26816;&#27979;&#24212;&#29992;&#20013;&#32771;&#23519;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;T-CNN&#22312;&#20943;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#19978;&#36816;&#34892;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#31561;&#25928;CNN&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;T-CNN&#21487;&#20197;&#36890;&#36807;&#36136;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#19982;&#20256;&#32479;CNN&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#21482;&#26377;&#20854;15&#20493;&#23569;&#65292;&#35757;&#32451;&#26102;&#38388;&#24555;4%&#33267;19%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;T-CNN&#22823;&#22823;&#36229;&#36234;&#20102;&#20256;&#32479;&#20154;&#31867;&#35270;&#35273;&#26816;&#26597;&#30340;&#32467;&#26524;&#65292;&#22312;&#24403;&#21069;&#21046;&#36896;&#24212;&#29992;&#20013;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
&lt;/p&gt;</description></item><item><title>FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2312.17163</link><description>&lt;p&gt;
FENet: &#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17163
&lt;/p&gt;
&lt;p&gt;
FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#39550;&#39542;&#27880;&#24847;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24320;&#21457;&#20102;&#22686;&#24378;&#32858;&#28966;&#37319;&#26679;&#12289;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#12289;&#22686;&#24378;FPN&#26550;&#26500;&#21644;&#23450;&#21521;IoU&#25439;&#22833;&#30340;&#32593;&#32476;&#21019;&#26032;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#26816;&#27979;&#20013;&#30340;&#31934;&#20934;&#24615;&#38556;&#30861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32858;&#28966;&#37319;&#26679;&#31574;&#30053;&#65292;&#24378;&#35843;&#36828;&#22788;&#37325;&#35201;&#32454;&#33410;&#65292;&#19982;&#22343;&#21248;&#26041;&#27861;&#30456;&#27604;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20934;&#21644;&#23454;&#38469;&#26354;&#32447;/&#36828;&#36317;&#31163;&#36710;&#36947;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;FENetV1&#36890;&#36807;&#27169;&#25311;&#39550;&#39542;&#21592;&#35270;&#35273;&#30340;&#36879;&#35270;&#24863;&#30693;&#19978;&#19979;&#25991;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#24230;&#37327;&#24615;&#33021;&#65292;&#20294;FENetV2&#22312;&#25552;&#20986;&#30340;&#37096;&#20998;&#35270;&#37326;&#20998;&#26512;&#20013;&#35777;&#26126;&#26159;&#26368;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#25512;&#33616;V2&#29992;&#20110;&#23454;&#38469;&#36710;&#36947;&#23548;&#33322;&#65292;&#23613;&#31649;&#22312;&#26631;&#20934;&#30340;&#25972;&#24352;&#22270;&#20687;&#27979;&#37327;&#19978;&#26377;&#36731;&#24494;&#30340;&#38477;&#32423;&#12290;&#26410;&#26469;&#30340;&#26041;&#21521;&#21253;&#25324;&#25910;&#38598;&#23454;&#38469;&#36947;&#36335;&#25968;&#25454;&#21644;&#38598;&#25104;&#20114;&#34917;&#30340;&#21452;&#37325;&#26694;&#26550;&#65292;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#25351;&#23548;&#23454;&#29616;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.02246</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#26088;&#22312;&#20174;&#35266;&#27979;&#20013;&#30830;&#23450;&#21442;&#25968;&#65292;&#36825;&#26159;&#24037;&#31243;&#21644;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36924;&#30495;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#33391;&#22909;&#30340;&#25968;&#23398;&#29305;&#24615;&#32780;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#23545;&#26041;&#24046;&#35843;&#24230;&#30340;&#36873;&#25321;&#25935;&#24863;&#65292;&#35813;&#35843;&#24230;&#25511;&#21046;&#30528;&#25193;&#25955;&#36807;&#31243;&#30340;&#21160;&#24577;&#12290;&#20026;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#24494;&#35843;&#36825;&#20010;&#35843;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#26102;&#38388;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#35843;&#24230;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25903;&#25345;&#23545;&#25968;&#25454;&#30340;&#27010;&#29575;&#26465;&#20214;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#24320;&#38144;&#19979;&#36866;&#24212;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#30456;&#20851;&#30340;&#36870;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;&#36229;&#20998;&#36776;&#29575;&#26174;&#24494;&#38236;&#21644;&#23450;&#37327;&#30456;&#20301;&#25104;&#20687;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#36739;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems aim to determine parameters from observations, a crucial task in engineering and science. Lately, generative models, especially diffusion models, have gained popularity in this area for their ability to produce realistic solutions and their good mathematical properties. Despite their success, an important drawback of diffusion models is their sensitivity to the choice of variance schedule, which controls the dynamics of the diffusion process. Fine-tuning this schedule for specific applications is crucial but time-costly and does not guarantee an optimal result. We propose a novel approach for learning the schedule as part of the training process. Our method supports probabilistic conditioning on data, provides high-quality solutions, and is flexible, proving able to adapt to different applications with minimum overhead. This approach is tested in two unrelated inverse problems: super-resolution microscopy and quantitative phase imaging, yielding comparable or superior 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.01717</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#30340;&#38598;&#25104;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Ensemble Distillation for Unsupervised Constituency Parsing. (arXiv:2310.01717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#21477;&#27861;&#35299;&#26512;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#33976;&#39311;&#23558;&#38598;&#25104;&#30693;&#35782;&#36716;&#31227;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#23558;&#21477;&#23376;&#30340;&#35789;&#21644;&#30701;&#35821;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#19981;&#20351;&#29992;&#35821;&#35328;&#23398;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#22120;&#25429;&#25417;&#21040;&#20102;&#35299;&#26512;&#32467;&#26500;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#26469;&#25552;&#39640;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26641;&#24179;&#22343;&#8221;&#30340;&#27010;&#24565;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35299;&#26512;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#38598;&#25104;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#20013;&#65307;&#36825;&#31181;&#38598;&#25104;-&#33976;&#39311;&#30340;&#36807;&#31243;&#26159;&#32531;&#35299;&#24120;&#35265;&#30340;&#22810;&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22987;&#32456;&#34920;&#29616;&#20986;&#20854;&#22312;&#19981;&#21516;&#38598;&#25104;&#32452;&#20214;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the unsupervised constituency parsing task, which organizes words and phrases of a sentence into a hierarchical structure without using linguistically annotated data. We observe that existing unsupervised parsers capture differing aspects of parsing structures, which can be leveraged to enhance unsupervised parsing performance. To this end, we propose a notion of "tree averaging," based on which we further propose a novel ensemble method for unsupervised parsing. To improve inference efficiency, we further distill the ensemble knowledge into a student model; such an ensemble-then-distill process is an effective approach to mitigate the over-smoothing problem existing in common multi-teacher distilling methods. Experiments show that our method surpasses all previous approaches, consistently demonstrating its effectiveness and robustness across various runs, with different ensemble components, and under domain-shift conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.06306</link><description>&lt;p&gt;
DocumentCLIP&#65306;&#38142;&#25509;&#25442;&#34892;&#25991;&#20214;&#20013;&#30340;&#22270;&#20687;&#21644;&#27491;&#25991;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06306
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#32780;&#22312;&#25903;&#25345;&#22810;&#23186;&#20307;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#29702;&#35299;&#21333;&#20010;&#22270;&#20687;&#19982;&#19968;&#27573;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#32780;&#24448;&#24448;&#24573;&#30053;&#20102;&#22810;&#21477;&#35805;&#19982;&#22810;&#20010;&#22270;&#20687;&#32452;&#25104;&#30340;&#25991;&#26723;&#20869;&#37096;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DocumentCLIP&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24378;&#21046;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#23545;&#26032;&#38395;&#25991;&#31456;&#12289;&#26434;&#24535;&#12289;&#20135;&#21697;&#25551;&#36848;&#31561;&#20855;&#26377;&#26356;&#20016;&#23500;&#35821;&#35328;&#21644;&#35270;&#35273;&#20869;&#23481;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25506;&#32034;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#30340;&#20154;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;Wikipedia&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22521;&#35757;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.17249</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#26816;&#27979;&#30340;&#27169;&#22411;&#26080;&#20851;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic explainable artificial intelligence for object detection in image data. (arXiv:2303.17249v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#40657;&#30418;&#35299;&#37322;&#26041;&#27861;&#8212;&#8212;BODEM&#65292;&#23427;&#37319;&#29992;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#29983;&#25104;&#22810;&#20010;&#29256;&#26412;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#27604;&#30446;&#21069;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#36890;&#36807;&#24320;&#21457;&#22823;&#22411;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#36879;&#26126;&#24230;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#22952;&#30861;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24320;&#21457;&#26041;&#27861;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#36923;&#36753;&#21644;&#28431;&#27934;&#12290;&#26412;&#25991;&#20026;&#20102;&#35299;&#37322;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35937;&#26816;&#27979;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;Black-box Object Detection Explanation by Masking&#65288;BODEM&#65289;&#30340;&#40657;&#30418;&#35828;&#26126;&#26041;&#27861;&#65292;&#37319;&#29992;&#26032;&#30340;&#25513;&#34109;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23616;&#37096;&#21644;&#36828;&#31243;&#25513;&#34109;&#26469;&#29983;&#25104;&#36755;&#20837;&#22270;&#20687;&#30340;&#22810;&#20010;&#29256;&#26412;&#12290;&#23616;&#37096;&#25513;&#34109;&#29992;&#20110;&#24178;&#25200;&#30446;&#26631;&#23545;&#35937;&#20869;&#30340;&#20687;&#32032;&#65292;&#20197;&#20102;&#35299;&#23545;&#35937;&#26816;&#27979;&#22120;&#23545;&#36825;&#20123;&#21464;&#21270;&#30340;&#21453;&#24212;&#65292;&#32780;&#36828;&#31243;&#25513;&#34109;&#21017;&#29992;&#20110;&#30740;&#31350;&#23545;&#35937;&#26816;&#27979;&#22120;&#22312;&#22270;&#20687;&#32972;&#26223;&#19978;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29992;&#20110;&#35299;&#37322;&#23545;&#35937;&#26816;&#27979;&#30340;&#20854;&#20182;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BODEM&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#21644;&#26377;&#29992;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection is a fundamental task in computer vision, which has been greatly progressed through developing large and intricate deep learning models. However, the lack of transparency is a big challenge that may not allow the widespread adoption of these models. Explainable artificial intelligence is a field of research where methods are developed to help users understand the behavior, decision logics, and vulnerabilities of AI-based systems. Black-box explanation refers to explaining decisions of an AI system without having access to its internals. In this paper, we design and implement a black-box explanation method named Black-box Object Detection Explanation by Masking (BODEM) through adopting a new masking approach for AI-based object detection systems. We propose local and distant masking to generate multiple versions of an input image. Local masks are used to disturb pixels within a target object to figure out how the object detector reacts to these changes, while distant ma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#35813;&#25237;&#31080;&#36807;&#31243;&#30340;&#19968;&#20123;&#37325;&#35201;&#29305;&#24449;&#65292;&#21253;&#25324;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#25237;&#31080;&#25968;&#21644;&#38656;&#27714;&#30340;&#26041;&#24046;&#31561;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#38408;&#20540;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#21442;&#19982;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;</title><link>http://arxiv.org/abs/2111.06390</link><description>&lt;p&gt;
&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#30340;&#20840;&#38754;&#29305;&#24449;&#21270;
&lt;/p&gt;
&lt;p&gt;
Full Characterization of Adaptively Strong Majority Voting in Crowdsourcing. (arXiv:2111.06390v2 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#23545;&#20247;&#21253;&#20013;&#33258;&#36866;&#24212;&#24378;&#22810;&#25968;&#25237;&#31080;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#35813;&#25237;&#31080;&#36807;&#31243;&#30340;&#19968;&#20123;&#37325;&#35201;&#29305;&#24449;&#65292;&#21253;&#25324;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#25237;&#31080;&#25968;&#21644;&#38656;&#27714;&#30340;&#26041;&#24046;&#31561;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#38408;&#20540;&#26469;&#23454;&#29616;&#22312;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#21442;&#19982;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#20013;&#65292;&#36890;&#36807;&#35753;&#24037;&#20154;&#26816;&#26597;&#39033;&#30446;&#24182;&#23545;&#20854;&#27491;&#30830;&#24615;&#36827;&#34892;&#25237;&#31080;&#65292;&#21487;&#20197;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#19981;&#21487;&#38752;&#24037;&#20154;&#21709;&#24212;&#30340;&#24433;&#21709;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;$\delta$-&#36793;&#30028;&#25237;&#31080;&#36807;&#31243;&#65292;&#30452;&#21040;&#36229;&#36807;&#20102;&#24037;&#20154;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#39044;&#23450;&#38408;&#20540;$\delta$&#20026;&#27490;&#65292;&#39069;&#22806;&#30340;&#25237;&#31080;&#23558;&#34987;&#24449;&#27714;&#24847;&#35265;&#12290;&#35813;&#36807;&#31243;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#20165;&#20316;&#20026;&#19968;&#31181;&#32463;&#39564;&#27861;&#21017;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21560;&#25910;&#24335;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#20998;&#26512;&#22312;&#20247;&#21253;&#36807;&#31243;&#20013;&#19982;&#36825;&#31181;&#25237;&#31080;&#36807;&#31243;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38381;&#24335;&#26041;&#31243;&#65292;&#29992;&#20110;&#25551;&#36848;&#25152;&#24471;&#20986;&#30340;&#20849;&#35782;&#25237;&#31080;&#30340;&#36136;&#37327;&#12289;&#36798;&#25104;&#20849;&#35782;&#25152;&#38656;&#30340;&#24179;&#22343;&#25237;&#31080;&#25968;&#12289;&#25237;&#31080;&#38656;&#27714;&#30340;&#26041;&#24046;&#21644;&#20854;&#20182;&#20998;&#24067;&#30697;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#35843;&#25972;&#38408;&#20540;$\delta$&#20197;&#23454;&#29616;&#22312;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20934;&#30830;&#24615;&#27700;&#24179;&#30340;&#24037;&#20154;&#30340;&#25237;&#31080;&#36807;&#31243;&#20013;&#30340;&#36136;&#37327;&#31561;&#25928;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19982;&#25237;&#31080;&#36807;&#31243;&#25928;&#29575;&#30456;&#31561;&#30340;&#25903;&#20184;&#36153;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In crowdsourcing, quality control is commonly achieved by having workers examine items and vote on their correctness. To minimize the impact of unreliable worker responses, a $\delta$-margin voting process is utilized, where additional votes are solicited until a predetermined threshold $\delta$ for agreement between workers is exceeded. The process is widely adopted but only as a heuristic. Our research presents a modeling approach using absorbing Markov chains to analyze the characteristics of this voting process that matter in crowdsourced processes. We provide closed-form equations for the quality of resulting consensus vote, the expected number of votes required for consensus, the variance of vote requirements, and other distribution moments. Our findings demonstrate how the threshold $\delta$ can be adjusted to achieve quality equivalence across voting processes that employ workers with varying accuracy levels. We also provide efficiency-equalizing payment rates for voting proces
&lt;/p&gt;</description></item></channel></rss>