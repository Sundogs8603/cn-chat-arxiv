<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.06757</link><description>&lt;p&gt;
&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#20102;&#35299;&#34892;&#20154;&#26159;&#21542;&#23558;&#27178;&#31359;&#22312;&#33258;&#20027;&#36710;&#36742;&#21069;&#26041;&#23545;&#20110;&#25191;&#34892;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#25805;&#25511;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#24207;&#21015;&#22270;&#20687;&#20013;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#39044;&#27979;&#27492;&#31867;&#24847;&#22270;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23548;&#33268;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#21270;&#27178;&#31359;&#21644;&#38750;&#27178;&#31359;&#65288;C/NC&#65289;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;ARCANE&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#22320;&#29983;&#25104;&#21253;&#21547;C/NC&#35270;&#39057;&#21098;&#36753;&#26679;&#26412;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;ARCANE&#29983;&#25104;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PedSynth&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;PedSynth&#22914;&#20309;&#34917;&#20805;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#22914;JAAD&#21644;PIE&#65292;&#20174;&#32780;&#20026;C/NC&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;C/NC&#39044;&#27979;&#27169;&#22411;&#30340;&#36710;&#36733;&#37096;&#32626;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PedGNN&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#23427;&#36895;&#24230;&#24555;&#19988;&#20869;&#23384;&#21344;&#29992;&#38750;&#24120;&#20302;&#12290;PedGNN&#22522;&#20110;GNN-G&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-G
&lt;/p&gt;</description></item><item><title>&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06751</link><description>&lt;p&gt;
Easy Training Data&#23545;&#20110;&#22256;&#38590;&#20219;&#21153;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06751
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#22312;&#23450;&#20041;&#19978;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#22312;&#22256;&#38590;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30417;&#30563;&#38382;&#39064;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#36807;&#31243;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#35770;&#65292;&#21363;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#30456;&#23545;&#33391;&#22909;&#65292;&#29978;&#33267;&#34920;&#29616;&#24471;&#21644;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#8220;oracle&#8221;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#21644;QLoRA&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#20174;&#26131;&#21040;&#38590;&#30340;&#27867;&#21270;&#65292;&#38024;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#38590;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20845;&#20010;&#32463;&#39564;&#22810;&#26679;&#30340;&#20154;&#31867;&#38590;&#24230;&#24230;&#37327;&#65288;&#22914;&#24180;&#32423;&#27700;&#24179;&#65289;&#21644;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#65288;&#22522;&#20110;&#25439;&#22833;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#20851;&#24515;&#27169;&#22411;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#25910;&#38598;&#24182;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#65292;&#22240;&#20026;&#22256;&#38590;&#25968;&#25454;&#36890;&#24120;&#26356;&#22024;&#26434;&#21644;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#36866;&#24212;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#20013;&#35282;&#33394;&#25552;&#21462;&#30340;&#22810;&#26679;&#24615;&#21644;&#38750;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06742</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#25913;&#36827;&#23545;&#35805;&#20013;&#30340;&#35282;&#33394;&#25552;&#21462;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain. (arXiv:2401.06742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#36866;&#24212;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#20013;&#35282;&#33394;&#25552;&#21462;&#30340;&#22810;&#26679;&#24615;&#21644;&#38750;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#22914;PersonaChat&#20026;&#35757;&#32451;&#26377;&#22522;&#20110;&#35282;&#33394;&#30340;&#23545;&#35805;&#20195;&#29702;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35805;&#21644;&#21465;&#20107;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#32570;&#20047;&#65292;&#20027;&#35201;&#23384;&#22312;&#20110;&#8220;&#30495;&#23454;&#8221;&#19990;&#30028;&#20013;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#29420;&#29305;&#35282;&#33394;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;&#22312;&#32473;&#23450;&#29305;&#23450;&#35282;&#33394;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#35805;&#65292;&#20294;&#25163;&#24037;&#21046;&#20316;&#36825;&#20123;&#35282;&#33394;&#21487;&#33021;&#32791;&#26102;&#65292;&#22240;&#27492;&#23384;&#22312;&#20174;&#29616;&#26377;&#29305;&#23450;&#35282;&#33394;&#23545;&#35805;&#20013;&#33258;&#21160;&#25552;&#21462;&#35282;&#33394;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#20063;&#26159;&#22312;&#20174;PersonaChat&#34893;&#29983;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#24456;&#38590;&#20174;&#38750;&#30495;&#23454;&#19990;&#30028;&#30340;&#23545;&#35805;&#35774;&#32622;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20363;&#22914;&#20197;&#24187;&#24819;&#20026;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;LIGHT&#12290;&#21019;&#24314;&#26032;&#25968;&#25454;&#20197;&#35757;&#32451;&#29305;&#23450;&#35774;&#32622;&#30340;&#27169;&#22411;&#26159;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#65292;&#22240;&#27492;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#21518;&#26399;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the "real" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06730</link><description>&lt;p&gt;
&#19981;&#21487;&#38752;&#30340;&#20381;&#36182;&#65306;&#35821;&#35328;&#27169;&#22411;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#40664;&#35748;&#25509;&#21475;&#65292;&#35821;&#35328;&#27169;&#22411;&#36866;&#24403;&#22320;&#20256;&#36798;&#19979;&#28216;&#24212;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#23545;&#20854;&#22238;&#31572;&#30340;&#32622;&#20449;&#24230;&#65292;&#20197;&#21450;&#19979;&#28216;&#29992;&#25143;&#23545;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#21453;&#24212;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20844;&#24320;&#37096;&#32626;&#30340;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21363;&#20351;&#20135;&#29983;&#20102;&#38169;&#35823;&#31572;&#26696;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#12290;&#34429;&#28982;&#21487;&#20197;&#26126;&#30830;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#32622;&#20449;&#24230;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#22312;&#32622;&#20449;&#30340;&#22238;&#31572;&#20013;&#38169;&#35823;&#29575;&#39640;&#36798;&#24179;&#22343;47%&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;RLHF&#23545;&#40784;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#23545;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25991;&#26412;&#26377;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work hig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.06715</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#31246;&#27861;&#25512;&#35770;&#20026;&#31867;&#27604;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reframing Tax Law Entailment as Analogical Reasoning. (arXiv:2401.06715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#23450;&#25512;&#29702;&#26159;&#25351;&#23558;&#31435;&#27861;&#35268;&#23450;&#24212;&#29992;&#20110;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#19968;&#31995;&#21015;&#26696;&#20363;&#20107;&#23454;&#12290;&#25105;&#20204;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#31867;&#27604;&#20219;&#21153;&#23454;&#20363;&#28041;&#21450;&#20004;&#20010;&#27861;&#23450;&#25512;&#29702;&#23454;&#20363;&#30340;&#32452;&#21512;&#12290;&#36825;&#26679;&#20570;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#22823;&#23567;&#22686;&#21152;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#20219;&#21153;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#35828;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#26469;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statutory reasoning refers to the application of legislative provisions to a series of case facts described in natural language. We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude, and introduces an element of interpretability. We show that this task is roughly as difficult to Natural Language Processing models as the original task. Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27880;&#35299;LoST&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#34920;&#26126;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;NLP&#27169;&#22411;&#23545;&#35302;&#21457;&#35789;&#12289;LoST&#25351;&#26631;&#21644;&#21518;&#26524;&#36825;&#19977;&#31867;&#25991;&#26412;&#25552;&#31034;&#26356;&#21152;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.06709</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text. (arXiv:2401.06709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27880;&#35299;LoST&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#34920;&#26126;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;NLP&#27169;&#22411;&#23545;&#35302;&#21457;&#35789;&#12289;LoST&#25351;&#26631;&#21644;&#21518;&#26524;&#36825;&#19977;&#31867;&#25991;&#26412;&#25552;&#31034;&#26356;&#21152;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;NLP&#30740;&#31350;&#31038;&#21306;&#26368;&#36817;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#35745;&#31639;&#36827;&#23637;&#20013;&#35265;&#35777;&#20102;&#19968;&#27874;&#22797;&#26434;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#33258;&#25105;&#24863;&#30693;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;AI&#27169;&#22411;&#30340;&#24314;&#31435;&#12290;&#36825;&#20123;&#36127;&#36131;&#20219;&#30340;AI&#27169;&#22411;&#26377;&#21161;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#37327;&#21270;&#24515;&#29702;&#27010;&#24565;&#12290;&#22312;&#36229;&#36234;&#20302;&#32423;&#65288;&#20998;&#31867;&#65289;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#25552;&#21319;&#21040;&#26356;&#39640;&#32423;&#21035;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#35299;&#37322;&#30340;&#35282;&#24230;&#23558;&#20043;&#20316;&#20026;&#19968;&#31181;&#23433;&#20840;&#25514;&#26045;&#12290;&#25105;&#20204;&#27880;&#37322;&#20102;LoST&#25968;&#25454;&#38598;&#65292;&#20197;&#25429;&#25417;&#34920;&#26126;Reddit&#29992;&#25143;&#21457;&#24086;&#20013;&#23384;&#22312;&#20302;&#33258;&#23562;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#29992;&#20110;&#30830;&#23450;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;NLP&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#25552;&#31034;&#65306;&#65288;i&#65289;&#35302;&#21457;&#35789;&#65306;&#35302;&#21457;&#24515;&#29702;&#25200;&#21160;&#30340;&#35789;&#27719;&#65292;&#65288;ii&#65289;LoST&#25351;&#26631;&#65306;&#24378;&#35843;&#20302;&#33258;&#23562;&#30340;&#25991;&#26412;&#25351;&#26631;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21518;&#26524;&#65306;&#25551;&#36848;&#24773;&#32490;&#31283;&#23450;&#24615;&#21518;&#26524;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social NLP research community witness a recent surge in the computational advancements of mental health analysis to build responsible AI models for a complex interplay between language use and self-perception. Such responsible AI models aid in quantifying the psychological concepts from user-penned texts on social media. On thinking beyond the low-level (classification) task, we advance the existing binary classification dataset, towards a higher-level task of reliability analysis through the lens of explanations, posing it as one of the safety measures. We annotate the LoST dataset to capture nuanced textual cues that suggest the presence of low self-esteem in the posts of Reddit users. We further state that the NLP models developed for determining the presence of low self-esteem, focus more on three types of textual cues: (i) Trigger: words that triggers mental disturbance, (ii) LoST indicators: text indicators emphasizing low self-esteem, and (iii) Consequences: words describing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06699</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#38381;&#21512;&#35299;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#21644;&#38142;&#24335;&#35268;&#21017;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;&#26435;&#37325;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#26159;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65292;&#26032;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#19968;&#32452;&#26435;&#37325;&#65292;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#20197;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#20248;&#21270;&#26435;&#37325;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#19981;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#20998;&#31867;&#38382;&#39064;&#65289;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#20960;&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#26368;&#32456;&#35299;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#26159;&#36825;&#20123;&#35745;&#31639;&#65288;&#23545;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#65289;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#35805;&#20195;&#29702;&#20316;&#20026;&#34913;&#37327;&#20915;&#31574;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35748;&#30693;&#20559;&#24046;&#36827;&#34892;&#27979;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#35805;&#20195;&#29702;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#27979;&#37327;&#26694;&#26550;&#21644;&#25439;&#22833;&#21388;&#24694;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06686</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#35805;&#20195;&#29702;&#20316;&#20026;&#34913;&#37327;&#20915;&#31574;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#26377;&#25928;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Exploring Conversational Agents as an Effective Tool for Measuring Cognitive Biases in Decision-Making. (arXiv:2401.06686v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#35805;&#20195;&#29702;&#20316;&#20026;&#34913;&#37327;&#20915;&#31574;&#20013;&#35748;&#30693;&#20559;&#24046;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35748;&#30693;&#20559;&#24046;&#36827;&#34892;&#27979;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#35805;&#20195;&#29702;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#27979;&#37327;&#26694;&#26550;&#21644;&#25439;&#22833;&#21388;&#24694;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#21457;&#27861;&#21644;&#35748;&#30693;&#20559;&#24046;&#26159;&#20154;&#31867;&#20915;&#31574;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#30340;&#35748;&#30693;&#20559;&#24046;&#21487;&#20197;&#20351;&#26234;&#33021;&#24037;&#20855;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#25903;&#25345;&#12290;&#30446;&#21069;&#65292;&#26816;&#27979;&#35748;&#30693;&#20559;&#24046;&#30340;&#23384;&#22312;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#23454;&#39564;&#21644;&#20154;&#24037;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#23545;&#35805;&#20195;&#29702;&#20316;&#20026;&#34913;&#37327;&#19981;&#21516;&#39046;&#22495;&#20013;&#21508;&#31181;&#35748;&#30693;&#20559;&#24046;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#20195;&#29702;&#21547;&#26377;&#19968;&#20010;&#26681;&#25454;&#29616;&#26377;&#23454;&#39564;&#35774;&#35745;&#21644;&#25991;&#29486;&#20013;&#30830;&#23450;&#30340;&#21508;&#31181;&#23454;&#39564;&#20219;&#21153;&#30340;&#20559;&#24046;&#27979;&#37327;&#26426;&#21046;&#12290;&#25105;&#20204;&#23545;&#34913;&#37327;&#26694;&#26550;&#21644;&#25439;&#22833;&#21388;&#24694;&#20559;&#24046;&#30340;&#21021;&#22987;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#35805;&#20195;&#29702;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heuristics and cognitive biases are an integral part of human decision-making. Automatically detecting a particular cognitive bias could enable intelligent tools to provide better decision-support. Detecting the presence of a cognitive bias currently requires a hand-crafted experiment and human interpretation. Our research aims to explore conversational agents as an effective tool to measure various cognitive biases in different domains. Our proposed conversational agent incorporates a bias measurement mechanism that is informed by the existing experimental designs and various experimental tasks identified in the literature. Our initial experiments to measure framing and loss-aversion biases indicate that the conversational agents can be effectively used to measure the biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>LLMRS&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#35780;&#35770;&#32534;&#30721;&#20026;&#35780;&#35770;&#20998;&#25968;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMRS&#22312;&#36719;&#20214;&#36141;&#20080;&#26041;&#38754;&#30340;&#25512;&#33616;&#24615;&#33021;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#25429;&#25417;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2401.06676</link><description>&lt;p&gt;
LLMRS: &#35299;&#38145;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#23545;&#36719;&#20214;&#36141;&#20080;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase. (arXiv:2401.06676v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06676
&lt;/p&gt;
&lt;p&gt;
LLMRS&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#35780;&#35770;&#32534;&#30721;&#20026;&#35780;&#35770;&#20998;&#25968;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMRS&#22312;&#36719;&#20214;&#36141;&#20080;&#26041;&#38754;&#30340;&#25512;&#33616;&#24615;&#33021;&#36229;&#36807;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#25429;&#25417;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26080;&#22788;&#19981;&#22312;&#65292;&#20174;Spotify&#30340;&#27468;&#21333;&#25512;&#33616;&#21040;&#20122;&#39532;&#36874;&#30340;&#20135;&#21697;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;&#26041;&#27861;&#25110;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#29983;&#25104;&#26222;&#36866;&#30340;&#25512;&#33616;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20026;&#20998;&#26512;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#25552;&#39640;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMRS&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38646;-shot&#25512;&#33616;&#31995;&#32479;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#23558;&#29992;&#25143;&#35780;&#35770;&#32534;&#30721;&#20026;&#35780;&#35770;&#20998;&#25968;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#20122;&#39532;&#36874;&#20135;&#21697;&#35780;&#35770;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23545;LLMRS&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#29992;&#20110;&#36719;&#20214;&#36141;&#20080;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLMRS&#20248;&#20110;&#22522;&#20110;&#25490;&#21517;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#21516;&#26102;&#25104;&#21151;&#20174;&#20135;&#21697;&#35780;&#35770;&#20013;&#25429;&#25417;&#21040;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are ubiquitous, from Spotify playlist suggestions to Amazon product suggestions. Nevertheless, depending on the methodology or the dataset, these systems typically fail to capture user preferences and generate general recommendations. Recent advancements in Large Language Models (LLM) offer promising results for analyzing user queries. However, employing these models to capture user preferences and efficiency remains an open question. In this paper, we propose LLMRS, an LLM-based zero-shot recommender system where we employ pre-trained LLM to encode user reviews into a review score and generate user-tailored recommendations. We experimented with LLMRS on a real-world dataset, the Amazon product reviews, for software purchase use cases. The results show that LLMRS outperforms the ranking-based baseline model while successfully capturing meaningful information from product reviews, thereby providing more reliable recommendations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35270;&#35282;&#26469;&#35299;&#20915;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#21644;&#36974;&#25377;&#31574;&#30053;&#26469;&#25552;&#39640;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06654</link><description>&lt;p&gt;
&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#19982;&#36974;&#25377;&#31574;&#30053;&#20197;&#23454;&#29616;&#19968;&#33268;&#30340;XAI&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks. (arXiv:2401.06654v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#35270;&#35282;&#26469;&#35299;&#20915;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#26041;&#27861;&#20013;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#32806;&#20687;&#32032;&#32763;&#36716;&#21644;&#36974;&#25377;&#31574;&#30053;&#26469;&#25552;&#39640;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#31227;&#38500;&#26159;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65292;&#26082;&#36866;&#29992;&#20110;&#22522;&#20110;&#36974;&#25377;&#30340;&#35299;&#37322;&#65288;Shapley&#20540;&#65289;&#65292;&#20063;&#36866;&#29992;&#20110;&#23427;&#20204;&#30340;&#35780;&#20272;&#65288;&#20687;&#32032;&#32763;&#36716;&#65292;PF&#65289;&#12290;&#28982;&#32780;&#65292;&#36974;&#25377;&#31574;&#30053;&#21487;&#20197;&#20174;&#31616;&#21333;&#30340;&#22343;&#20540;&#26367;&#25442;&#21040;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20462;&#22797;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#38480;&#21046;&#20102;&#36974;&#25377;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;PF&#22522;&#20934;&#20250;&#23548;&#33268;&#30683;&#30462;&#30340;&#25490;&#21517;&#12290;&#36825;&#19968;&#38382;&#39064;&#36824;&#22240;&#31454;&#20105;&#30340;PF&#24230;&#37327;&#32780;&#21464;&#24471;&#22797;&#26434;&#65306;&#29305;&#24449;&#35201;&#20040;&#20174;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#24320;&#22987;&#31227;&#38500;&#65288;MIF&#65289;&#65292;&#35201;&#20040;&#20174;&#26368;&#26080;&#24433;&#21709;&#21147;&#30340;&#24320;&#22987;&#31227;&#38500;&#65288;LIF&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#22522;&#20110;&#36974;&#25377;&#30340;XAI&#30340;&#24120;&#35265;&#25209;&#35780;&#65292;&#21363;&#20154;&#24037;&#26679;&#26412;&#23548;&#33268;&#27169;&#22411;&#35780;&#20272;&#19981;&#21487;&#38752;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#29992;R-OMS&#24471;&#20998;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#65288;&#21442;&#32771;&#27169;&#22411;&#33539;&#22260;&#20043;&#22806;&#30340;R-OMS&#24471;&#20998;&#65289;&#12290;R-OMS&#24471;&#20998;&#33021;&#22815;&#23545;&#36974;&#25377;&#31574;&#30053;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#65292;&#24182;&#35299;&#20915;&#20102;&#30683;&#30462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.06640</link><description>&lt;p&gt;
&#23454;&#39564;&#29615;&#22659;&#33021;&#22815;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#31283;&#20581;&#30340;&#35821;&#20041;&#23646;&#24615;&#25512;&#26029;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#20154;&#30417;&#30563;&#35780;&#20272;&#20984;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25191;&#34892;&#24847;&#20041;&#25552;&#21462;&#26041;&#38754;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#24341;&#20837;&#23454;&#39564;&#29615;&#22659;&#65288;&#22914;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;LMs&#30340;&#34920;&#29616;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#37027;&#20040;&#36825;&#26159;&#21542;&#36866;&#29992;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#24847;&#20041;&#25935;&#24863;&#20219;&#21153;&#21602;&#65311;&#25105;&#20204;&#22312;&#25511;&#21046;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#20869;&#23481;&#30340;&#21069;&#25552;&#19979;&#65292;&#23545;&#23454;&#39564;&#29615;&#22659;&#23545;&#20110;&#25552;&#39640;LMs&#22312;&#25191;&#34892;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#20219;&#21153;&#26159;&#39044;&#20808;&#34920;&#26126;LMs&#26080;&#27861;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23454;&#39564;&#29615;&#22659;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;LMs&#22312;&#23646;&#24615;&#32487;&#25215;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#65306;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#26368;&#23567;&#25913;&#20889;&#65292;&#21457;&#29616;&#19968;&#20123;LMs&#20174;&#36755;&#20837;&#20013;&#25429;&#25417;&#21040;&#27973;&#23618;&#30340;&#38750;&#35821;&#20041;&#24335;&#21551;&#21457;&#24335;&#20449;&#24687;&#65292;&#36825;&#34920;&#26126;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.06634</link><description>&lt;p&gt;
CCFC&#65306;&#26725;&#25509;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CCFC: Bridging Federated Clustering and Contrastive Learning. (arXiv:2401.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26725;&#25509;&#20102;&#32852;&#37030;&#32858;&#31867;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CCFC&#30340;&#26032;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32858;&#31867;&#24615;&#33021;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;CCFC&#30340;NMI&#24471;&#20998;&#25552;&#39640;&#20102;0.4155&#12290;&#21516;&#26102;&#65292;CCFC&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#32852;&#37030;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#32858;&#31867;&#26159;&#23545;&#20110;&#32852;&#37030;&#22330;&#26223;&#20013;&#38598;&#20013;&#32858;&#31867;&#30340;&#37325;&#35201;&#25193;&#23637;&#65292;&#21487;&#20197;&#35753;&#22810;&#20010;&#25968;&#25454;&#25345;&#26377;&#23458;&#25143;&#31471;&#22312;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#36827;&#34892;&#25968;&#25454;&#20998;&#32452;&#12290;&#22312;&#38598;&#20013;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#39537;&#21160;&#30340;&#32858;&#31867;&#22312;&#22788;&#29702;&#39640;&#32500;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#32858;&#31867;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#32467;&#21512;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#34920;&#31034;&#23450;&#21046;&#20102;&#19968;&#20010;&#32858;&#31867;&#23545;&#27604;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27169;&#22411;&#20316;&#20026;&#25552;&#20986;&#26032;&#30340;&#32852;&#37030;&#32858;&#31867;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#31216;&#20026;&#32858;&#31867;&#23545;&#27604;&#32852;&#37030;&#32858;&#31867;&#65288;CCFC&#65289;&#12290;&#21463;&#30410;&#20110;&#34920;&#31034;&#23398;&#20064;&#65292;CCFC&#30340;&#32858;&#31867;&#24615;&#33021;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#26159;&#26368;&#20339;&#22522;&#20934;&#26041;&#27861;&#30340;&#20004;&#20493;&#12290;&#19982;&#26368;&#30456;&#20851;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#26368;&#26174;&#33879;&#30340;&#26696;&#20363;&#20013;&#65292;&#36825;&#31181;&#25910;&#30410;&#23548;&#33268;NMI&#24471;&#20998;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#26368;&#39640;&#36798;&#21040;0.4155&#12290;&#27492;&#22806;&#65292;CCFC&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22312;&#32852;&#37030;&#22330;&#26223;&#19979;&#20986;&#29616;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#36136;&#37327;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCF
&lt;/p&gt;</description></item><item><title>Ada-Retrieval&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#29992;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29289;&#21697;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#65292;&#24182;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.06633</link><description>&lt;p&gt;
Ada-Retrieval&#65306;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#29992;&#20110;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations. (arXiv:2401.06633v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06633
&lt;/p&gt;
&lt;p&gt;
Ada-Retrieval&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#29992;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#29289;&#21697;&#20505;&#36873;&#32773;&#36873;&#25321;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#65292;&#24182;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#27169;&#22411;&#26088;&#22312;&#36873;&#25321;&#19982;&#32473;&#23450;&#29992;&#25143;&#20559;&#22909;&#21305;&#37197;&#30340;&#19968;&#23567;&#32452;&#29289;&#21697;&#20505;&#36873;&#32773;&#12290;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#21518;&#32493;&#30340;&#27169;&#22411;&#65288;&#22914;&#25490;&#21517;&#22120;&#65289;&#39640;&#24230;&#20381;&#36182;&#20110;&#29289;&#21697;&#20505;&#36873;&#32773;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#27169;&#22411;&#37319;&#29992;&#21333;&#36718;&#25512;&#29702;&#33539;&#20363;&#65292;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#30340;&#21160;&#24577;&#24615;&#24182;&#22266;&#23450;&#22312;&#29289;&#21697;&#31354;&#38388;&#30340;&#26576;&#20010;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-Retrieval&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#22810;&#36718;&#26816;&#32034;&#33539;&#20363;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25913;&#36827;&#29992;&#25143;&#34920;&#31034;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#23436;&#25972;&#30340;&#29289;&#21697;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#20505;&#36873;&#32773;&#12290;Ada-Retrieval&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#29289;&#21697;&#34920;&#31034;&#36866;&#37197;&#22120;&#21644;&#29992;&#25143;&#34920;&#31034;&#36866;&#37197;&#22120;&#65292;&#26088;&#22312;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#27880;&#20837;&#29289;&#21697;&#21644;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#65292;&#21487;&#20197;&#19982;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;RNN&#25110;Transformer&#65289;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval models aim at selecting a small set of item candidates which match the preference of a given user. They play a vital role in large-scale recommender systems since subsequent models such as rankers highly depend on the quality of item candidates. However, most existing retrieval models employ a single-round inference paradigm, which may not adequately capture the dynamic nature of user preferences and stuck in one area in the item space. In this paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for recommender systems that iteratively refines user representations to better capture potential candidates in the full item space. Ada-Retrieval comprises two key modules: the item representation adapter and the user representation adapter, designed to inject context information into items' and users' representations. The framework maintains a model-agnostic design, allowing seamless integration with various backbone models such as RNNs or Transformers. We pe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#24182;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06595</link><description>&lt;p&gt;
&#27599;&#20010;&#33410;&#28857;&#37117;&#19981;&#21516;&#65306;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering. (arXiv:2401.06595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#20219;&#21153;&#24182;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#34701;&#21512;&#26469;&#25552;&#39640;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#22270;&#32858;&#31867;&#26159;&#19968;&#39033;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#23558;&#33410;&#28857;&#20998;&#20026;&#19981;&#21516;&#30340;&#32452;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;SSL&#20219;&#21153;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#65292;&#19981;&#21516;&#30340;SSL&#20219;&#21153;&#34987;&#20998;&#37197;&#32473;&#25152;&#26377;&#22270;&#33410;&#28857;&#30456;&#21516;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20123;&#22270;&#33410;&#28857;&#20854;&#37051;&#23621;&#22312;&#19981;&#21516;&#30340;&#32452;&#20013;&#65292;&#23545;SSL&#20219;&#21153;&#38656;&#35201;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#24378;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23398;&#20064;&#19981;&#21516;&#33410;&#28857;&#30340;SSL&#20219;&#21153;&#26435;&#37325;&#24182;&#34701;&#21512;&#20174;&#19981;&#21516;SSL&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#23884;&#20837;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21363;&#21160;&#24577;&#34701;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;DyFSS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DyFSS&#20351;&#29992;&#20174;&#38376;&#25511;&#32593;&#32476;&#25512;&#23548;&#20986;&#30340;&#19981;&#21516;&#26435;&#37325;&#34701;&#21512;&#20174;&#22810;&#26679;&#30340;SSL&#20219;&#21153;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#26377;&#25928;&#23398;&#20064;&#38376;&#25511;&#32593;&#32476;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#23618;&#33258;&#30417;&#30563;&#31574;&#30053;&#65292;&#20854;&#20013;&#21253;&#21547;...
&lt;/p&gt;
&lt;p&gt;
Attributed graph clustering is an unsupervised task that partitions nodes into different groups. Self-supervised learning (SSL) shows great potential in handling this task, and some recent studies simultaneously learn multiple SSL tasks to further boost performance. Currently, different SSL tasks are assigned the same set of weights for all graph nodes. However, we observe that some graph nodes whose neighbors are in different groups require significantly different emphases on SSL tasks. In this paper, we propose to dynamically learn the weights of SSL tasks for different nodes and fuse the embeddings learned from different SSL tasks to boost performance. We design an innovative graph clustering approach, namely Dynamically Fusing Self-Supervised Learning (DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks using distinct weights derived from a gating network. To effectively learn the gating network, we design a dual-level self-supervised strategy that incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#65292;&#22312;&#23454;&#26102;&#25512;&#23548;&#21512;&#25104;&#38754;&#37096;&#21767;&#37096;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#20134;&#20851;&#27880;&#20102;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#36716;&#31227;&#27169;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06588</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#30340;&#21160;&#24577;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints. (arXiv:2401.06588v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#36830;&#25509;&#20027;&#20041;&#35821;&#38899;&#35782;&#21035;&#65292;&#22312;&#23454;&#26102;&#25512;&#23548;&#21512;&#25104;&#38754;&#37096;&#21767;&#37096;&#36816;&#21160;&#30340;&#21516;&#26102;&#65292;&#20134;&#20851;&#27880;&#20102;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#36716;&#31227;&#27169;&#22411;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22312;&#24378;&#24310;&#36831;&#32422;&#26463;&#26465;&#20214;&#19979;&#20351;&#29992;&#36830;&#25509;&#20027;&#20041;&#25216;&#26415;&#36827;&#34892;&#35821;&#38899;&#35782;&#21035;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#32422;&#26463;&#26159;&#36890;&#36807;&#23558;&#35821;&#38899;&#20449;&#21495;&#36755;&#20837;&#21040;&#21475;&#33108;&#36816;&#21160;&#21512;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#26102;&#25512;&#23548;&#20986;&#21512;&#25104;&#38754;&#37096;&#30340;&#21767;&#37096;&#36816;&#21160;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#30340;&#26102;&#38388;&#28436;&#21270;&#27169;&#22411;&#19982;&#32500;&#29305;&#27604;&#35793;&#30721;&#22120;&#24378;&#21046;&#30340;&#36716;&#31227;&#27169;&#22411;&#22312;&#19981;&#21516;&#24310;&#36831;&#26465;&#20214;&#19979;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#36890;&#36807;&#21442;&#25968;&#25511;&#21046;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12289;&#35821;&#35328;&#27169;&#22411;&#20013;&#26102;&#38388;&#20381;&#36182;&#30340;&#38271;&#24230;&#21644;&#35299;&#30721;&#22120;&#24310;&#36831;&#20043;&#38388;&#30340;&#24378;&#28872;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.06583</link><description>&lt;p&gt;
&#23558;&#21464;&#24418;&#22120;&#25216;&#26415;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#25991;&#26723;&#24050;&#32463;&#25104;&#20026;&#22312;&#32593;&#32476;&#19978;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#33616;&#38750;&#26597;&#35810;&#35821;&#35328;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#21487;&#33021;&#20250;&#24573;&#35270;&#38750;&#27597;&#35821;&#30340;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#26469;&#34920;&#31034;&#36328;&#35821;&#35328;&#25991;&#26723;&#12290;&#35780;&#20272;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#65288;mBERT&#65292;mT5 XLM RoBERTa&#65292;ErnieM&#65289;&#22312;20&#31181;&#35821;&#35328;&#23545;&#19978;&#20351;&#29992;&#19977;&#31181;&#26144;&#23556;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#35821;&#35328;&#23545;&#20195;&#34920;&#20102;&#27431;&#30431;&#36873;&#25321;&#30340;&#20116;&#31181;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;Mate&#26816;&#32034;&#29575;&#21644;&#20114;&#24800;&#25490;&#24207;&#31561;&#25351;&#26631;&#26469;&#34913;&#37327;&#26144;&#23556;TLDRs&#19982;&#26410;&#26144;&#23556;TLDRs&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#21644;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#20026;&#25193;&#23637;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06568</link><description>&lt;p&gt;
&#22312;&#28304;&#35821;&#35328;&#20013;&#36855;&#22833;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23545;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#32763;&#35793;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28085;&#30422;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#21644;&#27169;&#22411;&#31867;&#22411;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26469;&#21306;&#20998;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#30340;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;LLMs&#36827;&#34892;&#20102;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;&#30340;&#20803;&#35780;&#20272;&#65292;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#21442;&#32771;&#20449;&#24687;&#26469;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2401.06559</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20934;&#26694;&#26550;&#23545;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
A General Benchmark Framework is Dynamic Graph Neural Network Need. (arXiv:2401.06559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23398;&#20064;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#28436;&#21270;&#20851;&#31995;&#21644;&#26102;&#38388;&#21160;&#24577;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20013;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#23548;&#33268;&#21160;&#24577;&#22270;&#27169;&#22411;&#30340;&#35780;&#20272;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#24378;&#35843;&#20102;&#23545;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#12289;&#28436;&#21270;&#22270;&#32467;&#26500;&#21644;&#19979;&#28216;&#20219;&#21153;&#38656;&#27714;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#26694;&#26550;&#30340;&#38656;&#27714;&#12290;&#24314;&#31435;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#29616;&#26377;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20419;&#36827;&#21019;&#26032;&#65292;&#24182;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#30830;&#23450;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#26694;&#26550;&#26159;&#24403;&#21069;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#30340;&#23616;&#38480;&#20043;&#22788;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#65292;&#25512;&#21160;&#21160;&#24577;&#22270;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#24182;&#20026;&#29983;&#25104;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#25171;&#19979;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph learning is crucial for modeling real-world systems with evolving relationships and temporal dynamics. However, the lack of a unified benchmark framework in current research has led to inaccurate evaluations of dynamic graph models. This paper highlights the significance of dynamic graph learning and its applications in various domains. It emphasizes the need for a standardized benchmark framework that captures temporal dynamics, evolving graph structures, and downstream task requirements. Establishing a unified benchmark will help researchers understand the strengths and limitations of existing models, foster innovation, and advance dynamic graph learning. In conclusion, this paper identifies the lack of a standardized benchmark framework as a current limitation in dynamic graph learning research . Such a framework will facilitate accurate model evaluation, drive advancements in dynamic graph learning techniques, and enable the development of more effective models for re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#65292;&#29992;&#20110;&#21033;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#23398;&#20064;&#31038;&#20132;&#32593;&#32476;&#20013;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.06557</link><description>&lt;p&gt;
&#23545;&#24453;&#31038;&#20132;&#32593;&#32476;&#20013;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#30340;&#36229;&#34746;&#26059;&#34920;&#31034;&#23398;&#20064;&#30340;&#20851;&#27880;
&lt;/p&gt;
&lt;p&gt;
Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks. (arXiv:2401.06557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#65292;&#29992;&#20110;&#21033;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#23398;&#20064;&#31038;&#20132;&#32593;&#32476;&#20013;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#24212;&#65288;ITE&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#12290;&#22914;&#20309;&#35782;&#21035;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#22312;ITE&#20272;&#35745;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#31038;&#20132;&#32593;&#32476;&#30340;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23398;&#20064;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#34920;&#31034;&#65292;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#31038;&#20132;&#32593;&#32476;&#32463;&#24120;&#34920;&#29616;&#20986;&#26080;&#26631;&#24230;&#32467;&#26500;&#65292;&#32780;&#27431;&#20960;&#37324;&#24471;&#23884;&#20837;&#22312;&#23884;&#20837;&#27492;&#31867;&#22270;&#26102;&#20250;&#20135;&#29983;&#39640;&#24230;&#22833;&#30495;&#65292;&#65288;2&#65289;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#33258;&#25105;&#20013;&#24515;&#32593;&#32476;&#37117;&#34920;&#29616;&#20986;&#19982;&#27835;&#30103;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24847;&#21619;&#30528;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#26174;&#33879;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Treatment-Aware Hyperbolic Representation Learning&#65288;TAHyper&#65289;&#12290;&#39318;&#20808;&#65292;TAHyper&#20351;&#29992;&#36229;&#34746;&#26059;&#31354;&#38388;&#36827;&#34892;&#23884;&#20837;&#23398;&#20064;&#65292;&#20197;&#25429;&#25417;&#31038;&#20132;&#32593;&#32476;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#27835;&#30103;&#30456;&#20851;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the individual treatment effect (ITE) from observational data is a crucial research topic that holds significant value across multiple domains. How to identify hidden confounders poses a key challenge in ITE estimation. Recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. However, these methods utilize graph neural networks to learn the representation of hidden confounders in Euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while Euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. To address these issues, we propose a novel method called Treatment-Aware Hyperbolic Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic space to en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.06550</link><description>&lt;p&gt;
&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#26816;&#27979;&#22478;&#24066;&#21151;&#33021;&#21306;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#22478;&#24066;&#21151;&#33021;&#21306;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20852;&#36259;&#21306;&#65288;AOI&#65289;&#26159;&#25351;&#20855;&#26377;&#23450;&#20041;&#36793;&#30028;&#30340;&#25972;&#21512;&#30340;&#22478;&#24066;&#21151;&#33021;&#21306;&#22495;&#12290;&#22478;&#24066;&#21830;&#19994;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#23450;&#20041;AOI&#30340;&#26356;&#31934;&#30830;&#35201;&#27714;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22478;&#24066;&#35268;&#21010;&#25110;&#21306;&#22495;&#32463;&#27982;&#20998;&#26512;&#30340;&#24191;&#27867;AOI&#25366;&#25496;&#65292;&#26410;&#33021;&#28385;&#36275;&#31227;&#21160;&#20114;&#32852;&#32593;&#22312;&#32447;&#21040;&#31163;&#32447;&#19994;&#21153;&#30340;&#31934;&#30830;&#35201;&#27714;&#12290;&#36825;&#20123;&#19994;&#21153;&#38656;&#35201;&#21040;&#20855;&#20307;&#30340;&#31038;&#21306;&#12289;&#23398;&#26657;&#25110;&#21307;&#38498;&#30340;&#20934;&#30830;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#36965;&#24863;&#22270;&#20687;&#21644;&#22810;&#35821;&#20041;&#21442;&#32771;&#20449;&#24687;&#26816;&#27979;AOI&#22260;&#26639;&#22810;&#36793;&#24418;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#21547;&#21160;&#24577;&#20154;&#21592;&#27969;&#21160;&#21644;&#29289;&#27969;&#22320;&#22336;&#20449;&#24687;&#30340;&#32423;&#32852;&#27169;&#22359;&#26469;&#35780;&#20272;&#20854;&#26102;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36873;&#25321;&#29305;&#23450;&#31867;&#21035;&#30340;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#24320;&#22987;&#65292;&#24182;&#29992;&#23427;&#26469;&#21484;&#22238;&#30456;&#24212;&#30340;&#36965;&#24863;&#22270;&#20687;&#12289;&#38468;&#36817;&#30340;POI&#12289;&#36947;&#36335;n
&lt;/p&gt;
&lt;p&gt;
Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined boundaries. The rapid development of urban commerce has resulted in an increased demand for more precise requirements in defining AOIs. However, existing research primarily concentrates on broad AOI mining for urban planning or regional economic analysis, failing to cater to the precise requirements of mobile Internet online-to-offline businesses. These businesses necessitate accuracy down to a specific community, school, or hospital. In this paper, we propose an end-to-end multimodal deep learning algorithm for detecting AOI fence polygon using remote sensing images and multi-semantics reference information. We then evaluate its timeliness through a cascaded module that incorporates dynamic human mobility and logistics address information. Specifically, we begin by selecting a point-of-interest (POI) of specific category, and use it to recall corresponding remote sensing images, nearby POIs, road n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#23454;&#29616;&#20102;&#23545;&#21307;&#30103;&#23545;&#35805;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24314;&#31435;&#37492;&#21035;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#32454;&#21270;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06541</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#29983;&#25104;&#21307;&#30103;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis. (arXiv:2401.06541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#23454;&#29616;&#20102;&#23545;&#21307;&#30103;&#23545;&#35805;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24314;&#31435;&#37492;&#21035;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#32454;&#21270;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#35786;&#26029;&#12289;&#27835;&#30103;&#26041;&#26696;&#21644;&#20581;&#24247;&#21672;&#35810;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#65292;&#27491;&#30830;&#30340;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20026;&#26410;&#26469;&#30340;&#21672;&#35810;&#24314;&#31435;&#20102;&#22522;&#30784;&#12290;&#20020;&#24202;&#21307;&#29983;&#36890;&#24120;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24418;&#25104;&#37492;&#21035;&#35786;&#26029;&#12290;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#20551;&#35774;&#21644;&#39564;&#35777;&#20102;&#21508;&#31181;&#21487;&#33021;&#30340;&#30142;&#30149;&#65292;&#24182;&#21162;&#21147;&#29983;&#25104;&#20840;&#38754;&#32780;&#20005;&#35880;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#24314;&#27169;&#37492;&#21035;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#30340;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#30340;&#30452;&#35273;&#32852;&#24819;&#36827;&#34892;&#37492;&#21035;&#35786;&#26029;&#65292;&#28982;&#21518;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diagnoses, treatment plans, and health consultations. In medical dialogues, a proper diagnosis is crucial as it establishes the foundation for future consultations. Clinicians typically employ both intuitive and analytic reasoning to formulate a differential diagnosis. This reasoning process hypothesizes and verifies a variety of possible diseases and strives to generate a comprehensive and rigorous diagnosis. However, recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis, which hinders the practical application of these systems. To address the above issue, we propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced anal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#36827;&#34892;&#26234;&#33021;&#30340;&#36164;&#28304;&#21644;&#21151;&#33021;&#32534;&#25490;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.06538</link><description>&lt;p&gt;
&#26234;&#33021;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#20999;&#29255;&#20307;&#31995;&#32467;&#26500;&#29305;&#24449;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Intelligent Data-Driven Architectural Features Orchestration for Network Slicing. (arXiv:2401.06538v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#36827;&#34892;&#26234;&#33021;&#30340;&#36164;&#28304;&#21644;&#21151;&#33021;&#32534;&#25490;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20999;&#29255;&#26159;&#19979;&#19968;&#20195;&#31227;&#21160;&#32593;&#32476;&#65288;NGMN&#65289;&#20197;&#21450;&#20854;&#20182;&#26032;&#31995;&#32479;&#65288;&#22914;&#29289;&#32852;&#32593;&#21644;&#24037;&#19994;&#29289;&#32852;&#32593;&#65289;&#30340;&#20851;&#38190;&#25512;&#21160;&#22240;&#32032;&#21644;&#36235;&#21183;&#12290;&#32534;&#25490;&#21644;&#26426;&#22120;&#23398;&#20064;&#26159;&#32593;&#32476;&#20999;&#29255;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#30340;&#20803;&#32032;&#65292;&#22240;&#20026;&#20999;&#29255;&#36807;&#31243;&#38656;&#35201;&#32534;&#25490;&#36164;&#28304;&#21644;&#21151;&#33021;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#20248;&#21270;&#32534;&#25490;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#32570;&#20047;&#23450;&#20041;&#26234;&#33021;&#26041;&#27861;&#26469;&#32534;&#25490;&#20999;&#29255;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#21644;&#36164;&#28304;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20999;&#29255;&#26550;&#26500;&#20013;&#30340;&#29305;&#24449;&#21644;&#33021;&#21147;&#32534;&#25490;&#12290;&#39318;&#20808;&#65292;&#23545;&#20999;&#29255;&#35268;&#21010;&#12289;&#37197;&#32622;&#12289;&#35843;&#35797;&#21644;&#36816;&#34892;&#38454;&#27573;&#30340;&#20999;&#29255;&#36164;&#28304;&#32534;&#25490;&#21644;&#20998;&#37197;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;&#26550;&#26500;&#29305;&#24449;&#32534;&#25490;&#30340;&#38656;&#27714;&#65292;&#24182;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23884;&#20837;&#30340;&#20195;&#29702;&#12289;&#32852;&#37030;&#23398;&#20064;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;RGB-&#39640;&#20809;&#35889;&#21360;&#21046;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#24223;&#24323;&#30005;&#23376;&#20135;&#21697;&#22238;&#25910;&#36807;&#31243;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#27934;&#23519;&#21147;&#65292;&#20197;&#20248;&#21270;&#22238;&#25910;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06528</link><description>&lt;p&gt;
PCB-Vision: &#19968;&#20010;&#22810;&#22330;&#26223;&#30340;&#21360;&#21046;&#30005;&#36335;&#26495;RGB-&#39640;&#20809;&#35889;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards. (arXiv:2401.06528v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;RGB-&#39640;&#20809;&#35889;&#21360;&#21046;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#24223;&#24323;&#30005;&#23376;&#20135;&#21697;&#22238;&#25910;&#36807;&#31243;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#27934;&#23519;&#21147;&#65292;&#20197;&#20248;&#21270;&#22238;&#25910;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#24320;&#21457;&#20808;&#36827;&#30340;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#27969;&#27700;&#32447;&#65292;&#20316;&#20026;&#20915;&#31574;&#21644;&#36807;&#31243;&#25511;&#21046;&#30340;&#22522;&#30784;&#65292;&#20197;&#24212;&#23545;&#24223;&#24323;&#30005;&#23376;&#20135;&#21697;&#65288;&#30005;&#23376;&#22403;&#22334;&#65289;&#22238;&#25910;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#19982;&#24490;&#29615;&#32463;&#27982;&#21644;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#30340;&#26356;&#24191;&#27867;&#30446;&#26631;&#19968;&#33268;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;RGB&#21644;&#39640;&#20809;&#35889;&#25104;&#20687;&#25968;&#25454;&#65292;&#20026;&#20248;&#21270;&#22238;&#25910;&#25928;&#29575;&#25552;&#20379;&#23450;&#37327;&#21644;&#23450;&#24615;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;PCB-Vision&#8221;&#65307;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;RGB-&#39640;&#20809;&#35889;&#21360;&#21046;&#30005;&#36335;&#26495;&#65288;PCB&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;53&#24352;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;RGB&#22270;&#20687;&#65292;&#37197;&#23545;&#20854;&#23545;&#24212;&#30340;&#21487;&#35265;&#20809;&#21644;&#36817;&#32418;&#22806;&#65288;VNIR&#65289;&#33539;&#22260;&#20869;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#31435;&#26041;&#20307;&#12290;&#22522;&#20110;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#36164;&#28304;&#65292;&#37325;&#28857;&#20851;&#27880;&#30005;&#23376;&#22403;&#22334;&#27969;&#30340;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the critical theme of recycling electronic waste (E-waste), this contribution is dedicated to developing advanced automated data processing pipelines as a basis for decision-making and process control. Aligning with the broader goals of the circular economy and the United Nations (UN) Sustainable Development Goals (SDG), our work leverages non-invasive analysis methods utilizing RGB and hyperspectral imaging data to provide both quantitative and qualitative insights into the E-waste stream composition for optimizing recycling efficiency. In this paper, we introduce 'PCB-Vision'; a pioneering RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53 RGB images of high spatial resolution paired with their corresponding high spectral resolution hyperspectral data cubes in the visible and near-infrared (VNIR) range. Grounded in open science principles, our dataset provides a comprehensive resource for researchers through high-quality ground truths, focusing 
&lt;/p&gt;</description></item><item><title>ML-On-Rails&#26159;&#19968;&#20010;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#35758;&#65292;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20445;&#25252;ML&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06513</link><description>&lt;p&gt;
ML-On-Rails: &#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study. (arXiv:2401.06513v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06513
&lt;/p&gt;
&lt;p&gt;
ML-On-Rails&#26159;&#19968;&#20010;&#26088;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21327;&#35758;&#65292;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#31561;&#25361;&#25112;&#65292;&#24182;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#19968;&#39033;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20445;&#25252;ML&#27169;&#22411;&#22312;&#29983;&#20135;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#26174;&#33879;&#25913;&#21464;&#20102;&#21508;&#20010;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#27169;&#22411;&#30340;&#21407;&#22411;&#35774;&#35745;&#21040;&#22312;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#23384;&#22312;&#30528;&#35768;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28041;&#21450;&#30830;&#20445;&#23433;&#20840;&#24615;&#12289;&#20445;&#35777;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;ML&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ML-On-Rails&#30340;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;ML&#27169;&#22411;&#65292;&#24182;&#20026;&#19981;&#21516;&#30340;ML&#20219;&#21153;&#24314;&#31435;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#31471;&#28857;&#25509;&#21475;&#65292;&#20197;&#20419;&#36827;ML&#25552;&#20379;&#32773;&#21644;ML&#28040;&#36153;&#32773;&#65288;&#36719;&#20214;&#24037;&#31243;&#24072;&#65289;&#20043;&#38388;&#30340;&#28165;&#26224;&#27807;&#36890;&#12290;ML-On-Rails&#36890;&#36807;&#28155;&#21152;&#26816;&#27979;&#33021;&#21147;&#26469;&#25552;&#39640;ML&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#35782;&#21035;&#19982;&#23454;&#38469;&#29983;&#20135;&#20013;&#30340;ML&#30456;&#20851;&#30340;&#29305;&#23450;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;MoveReminder&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#26469;&#35780;&#20272;ML-On-Rails&#21327;&#35758;&#12290;&#36890;&#36807;&#36825;&#20010;&#35780;&#20272;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#29983;&#20135;&#20013;&#20445;&#25252;ML&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in produ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#20102;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#39057;&#29575;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.06506</link><description>&lt;p&gt;
&#39057;&#29575;&#23631;&#34109;&#29992;&#20110;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#20102;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#39057;&#29575;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19968;&#31995;&#21015;&#29983;&#25104;&#22411;AI&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#23588;&#20854;&#26159;&#22312;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#26366;&#35265;&#36807;&#30340;&#26032;&#20852;&#26041;&#27861;&#12290;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#38656;&#35201;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#26368;&#36817;&#25552;&#20986;&#30340;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#25506;&#32034;&#23558;&#23631;&#34109;&#22270;&#20687;&#24314;&#27169;&#24212;&#29992;&#20110;&#36890;&#29992;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#22312;&#35757;&#32451;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#26102;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#23631;&#34109;&#12290;&#22522;&#20110;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39057;&#29575;&#23631;&#34109;&#30340;&#26032;&#22411;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#39057;&#29575;&#22495;&#65292;&#19982;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#31354;&#38388;&#22495;&#26816;&#27979;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#20998;&#26512;&#25581;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study universal deepfake detection. Our goal is to detect synthetic images from a range of generative AI approaches, particularly from emerging ones which are unseen during training of the deepfake detector. Universal deepfake detection requires outstanding generalization capability. Motivated by recently proposed masked image modeling which has demonstrated excellent generalization in self-supervised pre-training, we make the first attempt to explore masked image modeling for universal deepfake detection. We study spatial and frequency domain masking in training deepfake detectors. Based on empirical analysis, we propose a novel deepfake detector via frequency masking. Our focus on frequency domain is different from the majority, which primarily target spatial domain detection. Our comparative analyses reveal substantial performance gains over existing methods. Code and models are publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#26377;&#26041;&#21521;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#20934;&#30830;&#22320;&#26816;&#27979;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#23567;&#22411;&#26377;&#26041;&#21521;&#29289;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33322;&#31354;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06503</link><description>&lt;p&gt;
&#25913;&#36827;&#33322;&#31354;&#22270;&#20687;&#20013;&#23567;&#22411;&#26377;&#26041;&#21521;&#29289;&#20307;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving the Detection of Small Oriented Objects in Aerial Images. (arXiv:2401.06503v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#26377;&#26041;&#21521;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20219;&#21153;&#65292;&#20934;&#30830;&#22320;&#26816;&#27979;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#23567;&#22411;&#26377;&#26041;&#21521;&#29289;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33322;&#31354;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#23610;&#23544;&#21644;&#26041;&#21521;&#65292;&#22823;&#35268;&#27169;&#33322;&#31354;&#22270;&#20687;&#20013;&#20195;&#34920;&#36739;&#23567;&#20687;&#32032;&#21306;&#22495;&#30340;&#23567;&#22411;&#26377;&#26041;&#21521;&#29289;&#20307;&#24456;&#38590;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26377;&#26041;&#21521;&#33322;&#31354;&#26816;&#27979;&#22120;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#29289;&#20307;&#30340;&#26041;&#21521;&#24314;&#27169;&#65292;&#23545;&#29289;&#20307;&#30340;&#23610;&#23544;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#26377;&#26041;&#21521;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#20934;&#30830;&#22320;&#26816;&#27979;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#23567;&#22411;&#26377;&#26041;&#21521;&#29289;&#20307;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;Attention-Points&#32593;&#32476;&#65292;&#21253;&#25324;&#20004;&#20010;&#25439;&#22833;&#65306;&#24341;&#23548;&#27880;&#24847;&#21147;&#25439;&#22833;&#65288;GALoss&#65289;&#21644;&#26694;&#28857;&#25439;&#22833;&#65288;BPLoss&#65289;&#12290;GALoss&#20351;&#29992;&#23454;&#20363;&#20998;&#21106;&#25513;&#27169;&#20316;&#20026;&#22522;&#26412;&#30495;&#20540;&#65292;&#23398;&#20064;&#25552;&#39640;&#23567;&#22411;&#29289;&#20307;&#26816;&#27979;&#25152;&#38656;&#30340;&#27880;&#24847;&#21147;&#29305;&#24449;&#12290;&#36825;&#20123;&#27880;&#24847;&#21147;&#29305;&#24449;&#28982;&#21518;&#29992;&#20110;&#39044;&#27979;BPLoss&#20013;&#30340;&#26694;&#28857;&#65292;&#30830;&#23450;&#28857;&#30456;&#23545;&#20110;&#30446;&#26631;&#26377;&#26041;&#21521;&#36793;&#30028;&#26694;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Attention-Points&#32593;&#32476;&#22312;&#26631;&#20934;&#30340;&#33322;&#31354;&#25968;&#25454;&#38598;&#19978;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small oriented objects that represent tiny pixel-area in large-scale aerial images are difficult to detect due to their size and orientation. Existing oriented aerial detectors have shown promising results but are mainly focused on orientation modeling with less regard to the size of the objects. In this work, we proposed a method to accurately detect small oriented objects in aerial images by enhancing the classification and regression tasks of the oriented object detection model. We designed the Attention-Points Network consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss (BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn the attention features needed to improve the detection of small objects. These attention features are then used to predict box points for BPLoss, which determines the points' position relative to the target oriented bounding box. Experimental results show the effectiveness of our Attention-Points Network on a standar
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#27010;&#29575;&#29615;&#22659;&#30340;&#31867;Shapley&#20998;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#30740;&#31350;&#24067;&#23572;&#20989;&#25968;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06493</link><description>&lt;p&gt;
&#39044;&#26399;&#30340;&#31867;Shapley&#20998;&#25968;: &#22797;&#26434;&#24615;&#19982;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Expected Shapley-Like Scores of Boolean Functions: Complexity and Applications to Probabilistic Databases. (arXiv:2401.06493v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06493
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#27010;&#29575;&#29615;&#22659;&#30340;&#31867;Shapley&#20998;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#30740;&#31350;&#24067;&#23572;&#20989;&#25968;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#28304;&#33258;&#21338;&#24328;&#35770;&#24182;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#24211;&#20013;&#20107;&#23454;&#23545;&#26597;&#35810;&#22238;&#31572;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#20854;&#20182;&#31867;&#20284;&#30340;&#26435;&#37325;&#25351;&#26631;&#65292;&#22914;Banzhaf&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#31867;Shapley&#20998;&#25968;&#36866;&#24212;&#21040;&#27010;&#29575;&#29615;&#22659;&#20013;&#65292;&#30446;&#26631;&#26159;&#35745;&#31639;&#23427;&#20204;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#26399;Shapley&#20540;&#21644;&#24067;&#23572;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#30340;&#35745;&#31639;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#30456;&#20114;&#21487;&#24402;&#32422;&#65292;&#20174;&#32780;&#33719;&#24471;&#30456;&#21516;&#30340;&#21487;&#22788;&#29702;&#24615;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#24067;&#23572;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#30830;&#23450;&#24615;&#21487;&#20998;&#35299;&#30005;&#36335;&#30340;&#21487;&#22788;&#29702;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#24211;&#26469;&#28304;&#35299;&#37322;&#22312;&#27010;&#29575;&#25968;&#25454;&#24211;&#20013;&#24212;&#29992;&#36825;&#20010;&#31639;&#27861;&#65292;&#24182;&#22312;ProvSQL&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#23454;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values, originating in game theory and increasingly prominent in explainable AI, have been proposed to assess the contribution of facts in query answering over databases, along with other similar power indices such as Banzhaf values. In this work we adapt these Shapley-like scores to probabilistic settings, the objective being to compute their expected value. We show that the computations of expected Shapley values and of the expected values of Boolean functions are interreducible in polynomial time, thus obtaining the same tractability landscape. We investigate the specific tractable case where Boolean functions are represented as deterministic decomposable circuits, designing a polynomial-time algorithm for this setting. We present applications to probabilistic databases through database provenance, and an effective implementation of this algorithm within the ProvSQL system, which experimentally validates its feasibility over a standard benchmark.
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#27010;&#24565;&#23398;&#20064;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;&#22810;&#24863;&#23448;&#34920;&#31034;&#21644;&#25991;&#26412;&#23548;&#20986;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#25511;&#21046;&#31995;&#32479;&#21327;&#35843;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#27169;&#25311;&#20102;&#20154;&#31867;&#27010;&#24565;&#20064;&#24471;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.06471</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#20154;&#31867;&#27010;&#24565;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Brain-inspired Computational Model for Human-like Concept Learning. (arXiv:2401.06471v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#27010;&#24565;&#23398;&#20064;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;&#22810;&#24863;&#23448;&#34920;&#31034;&#21644;&#25991;&#26412;&#23548;&#20986;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#25511;&#21046;&#31995;&#32479;&#21327;&#35843;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#27169;&#25311;&#20102;&#20154;&#31867;&#27010;&#24565;&#20064;&#24471;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#23398;&#20064;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#22312;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35760;&#24518;&#21644;&#20915;&#31574;&#31561;&#24515;&#29702;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26469;&#33258;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#21644;&#35748;&#30693;&#24515;&#29702;&#23398;&#30340;&#30740;&#31350;&#20154;&#21592;&#30528;&#30524;&#20110;&#25581;&#31034;&#20010;&#20307;&#30340;&#27010;&#24565;&#20064;&#24471;&#26426;&#21046;&#12290;&#36825;&#20123;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#22823;&#33041;&#23545;&#27010;&#24565;&#30340;&#34920;&#31034;&#20381;&#36182;&#20110;&#20004;&#20010;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65306;&#22810;&#24863;&#23448;&#34920;&#31034;&#21644;&#25991;&#26412;&#23548;&#20986;&#30340;&#34920;&#31034;&#12290;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#34920;&#31034;&#30001;&#19968;&#20010;&#35821;&#20041;&#25511;&#21046;&#31995;&#32479;&#21327;&#35843;&#65292;&#26368;&#32456;&#23548;&#33268;&#27010;&#24565;&#30340;&#20064;&#24471;&#12290;&#20511;&#37492;&#36825;&#31181;&#26426;&#21046;&#65292;&#26412;&#30740;&#31350;&#22522;&#20110;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#20102;&#19968;&#31181;&#20154;&#31867;&#27010;&#24565;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#35299;&#20915;&#22810;&#28304;&#25968;&#25454;&#21644;&#32500;&#24230;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#27010;&#24565;&#20064;&#24471;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept learning is a fundamental aspect of human cognition and plays a critical role in mental processes such as categorization, reasoning, memory, and decision-making. Researchers across various disciplines have shown consistent interest in the process of concept acquisition in individuals. To elucidate the mechanisms involved in human concept learning, this study examines the findings from computational neuroscience and cognitive psychology. These findings indicate that the brain's representation of concepts relies on two essential components: multisensory representation and text-derived representation. These two types of representations are coordinated by a semantic control system, ultimately leading to the acquisition of concepts. Drawing inspiration from this mechanism, the study develops a human-like computational model for concept learning based on spiking neural networks. By effectively addressing the challenges posed by diverse sources and imbalanced dimensionality of the two
&lt;/p&gt;</description></item><item><title>PersianMind&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#29616;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#24320;&#28304;&#27169;&#22411;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#19978;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06466</link><description>&lt;p&gt;
PersianMind: &#19968;&#20010;&#36328;&#35821;&#35328;&#30340;&#27874;&#26031;&#35821;-&#33521;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06466
&lt;/p&gt;
&lt;p&gt;
PersianMind&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#29616;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#24320;&#28304;&#27169;&#22411;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#19978;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#20855;&#22791;&#24191;&#27867;&#30340;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#33521;&#35821;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#65292;&#20294;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#20063;&#24456;&#26174;&#33879;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#22914;LLaMa&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#20027;&#35201;&#26159;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PersianMind&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#31034;&#20102;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;LLaMa2&#30340;&#35789;&#27719;&#34920;&#25193;&#23637;10,000&#20010;&#27874;&#26031;&#35821;&#26631;&#35760;&#65292;&#24182;&#35757;&#32451;&#32422;20&#20159;&#20010;&#27874;&#26031;&#35821;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33521;&#35821;&#30693;&#35782;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#20462;&#22797;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT &#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#35777;&#35299;&#37322;&#30340;&#26041;&#27861;&#35770;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06465</link><description>&lt;p&gt;
Sanity Checks Revisited: &#20462;&#22797;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test. (arXiv:2401.06465v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797;&#36827;&#34892;&#20102;&#25506;&#32034;&#21644;&#20462;&#22797;&#65292;&#36890;&#36807;&#24341;&#20837;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT &#20004;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#23454;&#35777;&#35299;&#37322;&#30340;&#26041;&#27861;&#35770;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021; (XAI) &#31038;&#21306;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#38543;&#26426;&#21270;&#27979;&#35797; (MPRT) &#20973;&#20511;&#20854;&#26377;&#21147;&#30340;&#35780;&#20272;&#21407;&#21017;&#32780;&#24191;&#21463;&#35748;&#21487;&#65306;&#35299;&#37322;&#20989;&#25968;&#24212;&#23545;&#27169;&#22411;&#20989;&#25968;&#21442;&#25968;&#30340;&#21464;&#21270;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#20851;&#20110; MPRT &#30340;&#20960;&#20010;&#26041;&#27861;&#35770;&#19978;&#30340;&#27880;&#24847;&#20107;&#39033;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27880;&#24847;&#20107;&#39033;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23545;&#21407;&#22987; MPRT &#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#8212;&#8212;&#24179;&#28369; MPRT &#21644;&#39640;&#25928; MPRT&#65292;&#21069;&#32773;&#36890;&#36807;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22122;&#38899;&#23545;&#35780;&#20272;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21518;&#32773;&#36890;&#36807;&#22312;&#23436;&#20840;&#21442;&#25968;&#38543;&#26426;&#21270;&#21518;&#35299;&#37322;&#30340;&#22797;&#26434;&#24230;&#19978;&#21319;&#26469;&#32469;&#36807;&#23545;&#20559;&#20506;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#21464;&#20307;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#21487;&#38752;&#24615;&#65292;&#20174;&#32780;&#26356;&#21487;&#20449;&#22320;&#24212;&#29992; XAI &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the eXplainable Artificial Intelligence (XAI) community for its well-motivated evaluative principle: that the explanation function should be sensitive to changes in the parameters of the model function. However, recent works have identified several methodological caveats for the empirical interpretation of MPRT. To address these caveats, we introduce two adaptations to the original MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact that noise has on the evaluation results through sampling and the latter circumvents the need for biased similarity measurements by re-interpreting the test through the explanation's rise in complexity, after full parameter randomisation. Our experimental results demonstrate that these proposed variants lead to improved metric reliability, thus enabling a more trustworthy application of XAI methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#25805;&#25511;3D&#36719;&#20214;&#29983;&#25104;&#20855;&#26377;&#23574;&#38160;&#29305;&#24449;&#21644;&#21442;&#25968;&#25511;&#21046;&#30340;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#27969;&#31243;&#26469;&#25506;&#31350;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;3D&#21442;&#25968;&#24314;&#27169;&#20013;&#25581;&#31034;&#20102;LLMs&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06437</link><description>&lt;p&gt;
3D-PreMise&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29983;&#25104;&#20855;&#26377;&#23574;&#38160;&#29305;&#24449;&#21644;&#21442;&#25968;&#25511;&#21046;&#30340;3D&#24418;&#29366;&#65311;
&lt;/p&gt;
&lt;p&gt;
3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?. (arXiv:2401.06437v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#25805;&#25511;3D&#36719;&#20214;&#29983;&#25104;&#20855;&#26377;&#23574;&#38160;&#29305;&#24449;&#21644;&#21442;&#25968;&#25511;&#21046;&#30340;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#21644;&#27969;&#31243;&#26469;&#25506;&#31350;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;3D&#21442;&#25968;&#24314;&#27169;&#20013;&#25581;&#31034;&#20102;LLMs&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#38544;&#24335;3D&#34920;&#31034;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;3D&#29289;&#20307;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#21442;&#25968;&#25511;&#21046;&#19979;&#20934;&#30830;&#24314;&#27169;&#20855;&#26377;&#23450;&#20041;&#30340;&#23574;&#38160;&#29305;&#24449;&#30340;&#20960;&#20309;&#24418;&#29366;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#22312;&#24037;&#19994;&#35774;&#35745;&#21644;&#21046;&#36896;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31243;&#24207;&#21512;&#25104;&#25805;&#25511;3D&#36719;&#20214;&#29983;&#25104;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#24418;&#29366;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;3D-PreMise&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#24037;&#19994;&#24418;&#29366;&#30340;3D&#21442;&#25968;&#24314;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#20013;&#25506;&#32034;&#26368;&#20808;&#36827;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#26377;&#25928;&#30340;&#29983;&#25104;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#21487;&#35270;&#30028;&#38754;&#25506;&#32034;&#20102;LLMs&#30340;&#33258;&#25105;&#20462;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;LLMs&#22312;&#24037;&#19994;&#24212;&#29992;&#30340;3D&#21442;&#25968;&#24314;&#27169;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in implicit 3D representations and generative models have markedly propelled the field of 3D object generation forward. However, it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls, which is crucial in fields like industrial design and manufacturing. To bridge this gap, we introduce a framework that employs Large Language Models (LLMs) to generate text-driven 3D shapes, manipulating 3D software via program synthesis. We present 3D-PreMise, a dataset specifically tailored for 3D parametric modeling of industrial shapes, designed to explore state-of-the-art LLMs within our proposed pipeline. Our work reveals effective generation strategies and delves into the self-correction capabilities of LLMs using a visual interface. Our work highlights both the potential and limitations of LLMs in 3D parametric modeling for industrial applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;GCN&#27169;&#22411;&#20013;&#24341;&#20837;Transformer&#23618;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06436</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#31038;&#20132;&#32593;&#32476;&#30340;&#29289;&#21697;&#25512;&#33616;&#20013;&#65292;&#29992;Transformer&#23618;&#25913;&#36827;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation. (arXiv:2401.06436v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;GCN&#27169;&#22411;&#20013;&#24341;&#20837;Transformer&#23618;&#65292;&#22312;&#33410;&#28857;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;GCN&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#39044;&#27979;&#35780;&#20998;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26631;&#20934;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#25193;&#23637;&#20102;&#20960;&#23618;Transformer&#26550;&#26500;&#12290;&#35770;&#25991;&#30340;&#20027;&#35201;&#28966;&#28857;&#26159;&#32593;&#32476;&#20013;&#33410;&#28857;&#23884;&#20837;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#20351;&#29992;&#26469;&#33258;&#22522;&#20110;&#22270;&#30340;&#21367;&#31215;&#23618;&#30340;&#23884;&#20837;&#23618;&#65292;&#27880;&#24847;&#26426;&#21046;&#21487;&#20197;&#37325;&#26032;&#25490;&#21015;&#29305;&#24449;&#31354;&#38388;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#33719;&#21462;&#26356;&#39640;&#25928;&#30340;&#23884;&#20837;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#20256;&#32479;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;GCN&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06431</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21270;&#21040;&#22686;&#24378;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#25509;&#25910;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#38750;&#24120;&#37325;&#35201;&#65292;&#24403;&#20154;&#31867;&#25945;&#24072;&#26080;&#27861;&#25552;&#20379;&#26102;&#65292;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#26159;&#19968;&#31181;&#37325;&#35201;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#65292;&#20316;&#20026;AES&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#31361;&#20986;&#20102;LLM AES&#31995;&#32479;&#30340;&#26174;&#30528;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#36229;&#36234;&#20102;&#20256;&#32479;&#35780;&#20998;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;LLM&#36741;&#21161;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#28041;&#21450;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#35780;&#20998;&#21592;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;LLM&#19981;&#20165;&#33021;&#33258;&#21160;&#21270;&#35780;&#20998;&#36807;&#31243;&#65292;&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;&#24403;&#21021;&#23398;&#32773;&#35780;&#20998;&#21592;&#33719;&#24471;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#27700;&#24179;&#30456;&#24403;&#65292;&#21516;&#26102;&#19987;&#23478;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more effici
&lt;/p&gt;</description></item><item><title>UPDP&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;CNN&#21644;Vision Transformer&#30340;&#32479;&#19968;&#28176;&#36827;&#28145;&#24230;&#20462;&#21098;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22359;&#20462;&#21098;&#31574;&#30053;&#21644;&#28176;&#36827;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#30340;&#20462;&#21098;&#26041;&#27861;&#22312;&#20462;&#21098;&#39640;&#25928;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#21508;&#31181;&#20462;&#21098;&#37197;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06426</link><description>&lt;p&gt;
UPDP: &#19968;&#31181;&#36866;&#29992;&#20110;CNN&#21644;Vision Transformer&#30340;&#32479;&#19968;&#28176;&#36827;&#28145;&#24230;&#20462;&#21098;&#22120;
&lt;/p&gt;
&lt;p&gt;
UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer. (arXiv:2401.06426v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06426
&lt;/p&gt;
&lt;p&gt;
UPDP&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;CNN&#21644;Vision Transformer&#30340;&#32479;&#19968;&#28176;&#36827;&#28145;&#24230;&#20462;&#21098;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22359;&#20462;&#21098;&#31574;&#30053;&#21644;&#28176;&#36827;&#35757;&#32451;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#30340;&#20462;&#21098;&#26041;&#27861;&#22312;&#20462;&#21098;&#39640;&#25928;&#27169;&#22411;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#21508;&#31181;&#20462;&#21098;&#37197;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36880;&#36890;&#36947;&#20462;&#21098;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#32593;&#32476;&#36890;&#36947;&#26469;&#20462;&#21098;CNN&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#21367;&#31215;&#23618;&#21644;&#26576;&#20123;&#39640;&#25928;&#27169;&#22359;&#65292;&#22914;&#27969;&#34892;&#30340;&#21453;&#21521;&#27531;&#24046;&#22359;&#12290;&#20043;&#21069;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#32593;&#32476;&#28145;&#24230;&#19981;&#33021;&#24456;&#22909;&#22320;&#20462;&#21098;&#19968;&#20123;&#39640;&#25928;&#27169;&#22411;&#65292;&#22240;&#20026;&#23384;&#22312;&#26576;&#20123;&#24402;&#19968;&#21270;&#23618;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#30452;&#25509;&#21024;&#38500;&#28608;&#27963;&#23618;&#26469;&#24494;&#35843;&#23376;&#32593;&#32476;&#20250;&#30772;&#22351;&#21407;&#22987;&#27169;&#22411;&#26435;&#37325;&#65292;&#38459;&#30861;&#20462;&#21098;&#27169;&#22411;&#30340;&#39640;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;&#27169;&#22411;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#20462;&#21098;&#31574;&#30053;&#21644;&#23376;&#32593;&#32476;&#30340;&#28176;&#36827;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20462;&#21098;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;Vision Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20462;&#21098;&#37197;&#32622;&#19978;&#25345;&#32493;&#20248;&#20110;&#29616;&#26377;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19977;&#20010;&#20462;&#21098;&#21518;&#30340;ConvNeXtV1&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06421</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#22320;&#29699;&#35266;&#27979;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. (arXiv:2401.06421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22320;&#29699;&#35266;&#27979;&#39046;&#22495;&#20013;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#65292;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#21487;&#33021;&#20250;&#23548;&#33268;&#36127;&#38754;&#21518;&#26524;&#12290;&#31526;&#21512;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#65292;&#26080;&#35770;&#20854;&#20998;&#24067;&#22914;&#20309;&#12290;&#19982;&#20854;&#20182;&#20687;&#32032;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#31526;&#21512;&#39044;&#27979;&#19981;&#38656;&#35201;&#35775;&#38382;&#24213;&#23618;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#25552;&#20379;&#32479;&#35745;&#19978;&#26377;&#25928;&#21644;&#26377;&#20449;&#24687;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unreliable predictions can occur when using artificial intelligence (AI) systems with negative consequences for downstream applications, particularly when employed for decision-making. Conformal prediction provides a model-agnostic framework for uncertainty quantification that can be applied to any dataset, irrespective of its distribution, post hoc. In contrast to other pixel-level uncertainty quantification methods, conformal prediction operates without requiring access to the underlying model and training dataset, concurrently offering statistically valid and informative prediction regions, all while maintaining computational efficiency. In response to the increased need to report uncertainty alongside point predictions, we bring attention to the promise of conformal prediction within the domain of Earth Observation (EO) applications. To accomplish this, we assess the current state of uncertainty quantification in the EO domain and found that only 20% of the reviewed Google Earth En
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.06406</link><description>&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review. (arXiv:2401.06406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06406
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;&#35813;&#26041;&#27861;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31687;&#32508;&#36848;&#22238;&#39038;&#20102;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#20173;&#28982;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#27835;&#30103;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#30142;&#30149;&#20043;&#19968;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#20351;&#24471;&#23545;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#20016;&#23500;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12289;&#39640;&#32500;&#25968;&#25454;&#31867;&#22411;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12289;&#24739;&#32773;&#20869;&#37096;&#21644;&#32959;&#30244;&#20869;&#37096;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#20197;&#21450;&#19982;&#29616;&#26377;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#35299;&#37322;&#21644;&#19968;&#33268;&#24615;&#12290;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#65292;&#36825;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32508;&#36848;&#20102;&#37319;&#29992;&#20102;&#34701;&#21512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#30340;&#30693;&#35782;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#39044;&#21518;&#20013;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#31361;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four prima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06394</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Data Augmentation for Aspect Sentiment Quad Prediction. (arXiv:2401.06394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#21477;&#23376;&#30340;&#22235;&#20803;&#24773;&#24863;&#20803;&#32032;&#65292;&#36825;&#26159;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;ASQP&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#22235;&#20803;&#27169;&#24335;&#19981;&#24179;&#34913;&#21644;&#26041;&#38754;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20855;&#26377;&#26465;&#20214;&#20989;&#25968;&#30340;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#33258;&#36866;&#24212;&#22686;&#24378;&#23614;&#37096;&#22235;&#20803;&#27169;&#24335;&#21644;&#26041;&#38754;&#31867;&#21035;&#65292;&#32531;&#35299;ASQP&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#21644;&#35821;&#27861;&#24341;&#23548;&#35299;&#30721;&#30446;&#26631;&#65292;&#25552;&#21462;&#23436;&#25972;&#30340;&#22235;&#20803;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#25152;&#25552;&#20986;&#30340;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment elements for a given sentence, which is a critical task in the field of aspect-based sentiment analysis. However, the data imbalance issue has not received sufficient attention in ASQP task. In this paper, we divide the issue into two-folds, quad-pattern imbalance and aspect-category imbalance, and propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance issue. Specifically, a data augmentation process with a condition function adaptively enhances the tail quad patterns and aspect categories, alleviating the data imbalance in ASQP. Following previous studies, we also further explore the generative framework for extracting complete quads by introducing the category prior knowledge and syntax-guided decoding target. Experimental results demonstrate that data augmentation for imbalance in ASQP task can improve the performance, and the proposed ADA method is superior to naive data oversampling.
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#19982;AI&#30340;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#20154;&#31867;&#19982;AI&#20132;&#20114;&#36807;&#31243;&#20013;&#24515;&#26234;&#34920;&#24449;&#30340;&#24314;&#31435;&#65292;&#26088;&#22312;&#24110;&#21161;&#23454;&#29616;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2401.06382</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#35828;&#20160;&#20040;&#65311;-&#19982;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What should I say? -- Interacting with AI and Natural Language Interfaces. (arXiv:2401.06382v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06382
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#19982;AI&#30340;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#20154;&#31867;&#19982;AI&#20132;&#20114;&#36807;&#31243;&#20013;&#24515;&#26234;&#34920;&#24449;&#30340;&#24314;&#31435;&#65292;&#26088;&#22312;&#24110;&#21161;&#23454;&#29616;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65288;HAI&#65289;&#23376;&#39046;&#22495;&#20174;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#26088;&#22312;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#20132;&#20114;&#27169;&#24335;&#24050;&#32463;&#23454;&#26045;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#36825;&#20123;&#26356;&#20687;&#20154;&#31867;&#26412;&#36136;&#30340;&#26367;&#20195;&#30028;&#38754;&#25152;&#38656;&#35748;&#30693;&#30340;&#21464;&#21270;&#20197;&#21450;&#20351;&#29992;&#36825;&#20123;&#30028;&#38754;&#30340;&#35748;&#30693;&#31185;&#23398;&#24433;&#21709;&#65292;&#20102;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#20851;&#38190;&#22312;&#20110;&#24515;&#26234;&#34920;&#24449;&#65292;&#28982;&#32780;&#65292;&#22312;&#19982;AI&#20132;&#20114;&#26102;&#65292;&#24515;&#26234;&#34920;&#24449;&#26159;&#22914;&#20309;&#24314;&#31435;&#30340;&#20173;&#19981;&#29978;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) technology becomes more and more prevalent, it becomes increasingly important to explore how we as humans interact with AI. The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer Interaction (HCI) field and aims to examine this very notion. Many interaction patterns have been implemented without fully understanding the changes in required cognition as well as the cognitive science implications of using these alternative interfaces that aim to be more human-like in nature. Prior research suggests that theory of mind representations are crucial to successful and effortless communication, however very little is understood when it comes to how theory of mind representations are established when interacting with AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;&#65292;&#20026;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#21644;&#35299;&#37322;&#20854;&#19982;&#23884;&#20837;&#31354;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26041;&#20415;&#30340;&#35821;&#35328;&#21644;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.06379</link><description>&lt;p&gt;
Vehicle: &#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;
Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs. (arXiv:2401.06379v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#22312;&#39564;&#35777;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#20013;&#24357;&#21512;&#23884;&#20837;&#32570;&#21475;&#65292;&#20026;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#21644;&#35299;&#37322;&#20854;&#19982;&#23884;&#20837;&#31354;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#26041;&#20415;&#30340;&#35821;&#35328;&#21644;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#26159;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#21644;&#20256;&#32479;&#31526;&#21495;&#21270;&#20195;&#30721;&#30340;&#31243;&#24207;&#65292;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#36825;&#20123;&#31243;&#24207;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#27491;&#30830;&#24615;&#21462;&#20915;&#20110;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#8220;&#23884;&#20837;&#32570;&#21475;&#8221;&#8212;&#8212;&#20197;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#8220;&#38382;&#39064;&#31354;&#38388;&#8221;&#23646;&#24615;&#19982;&#31561;&#25928;&#30340;&#8220;&#23884;&#20837;&#31354;&#38388;&#8221;&#23646;&#24615;&#20043;&#38388;&#32570;&#20047;&#25216;&#26415;&#30340;&#38382;&#39064;&#8212;&#8212;&#35270;&#20026;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#20010;&#21517;&#20026;Vehicle&#30340;&#24037;&#20855;&#65292;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20419;&#36827;&#31070;&#32463;&#31526;&#21495;&#21270;&#31243;&#24207;&#30340;&#31471;&#21040;&#31471;&#39564;&#35777;&#12290;Vehicle&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#25351;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#38382;&#39064;&#31354;&#38388;&#8221;&#23646;&#24615;&#24182;&#22768;&#26126;&#23427;&#20204;&#19982;&#8220;&#23884;&#20837;&#31354;&#38388;&#8221;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19968;&#20010;&#24378;&#22823;&#30340;&#32534;&#35793;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#36825;&#20123;&#23646;&#24615;&#35299;&#37322;&#20026;&#25152;&#36873;&#25321;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#29615;&#22659;&#12289;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#25903;&#25345;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic programs -- programs containing both machine learning components and traditional symbolic code -- are becoming increasingly widespread. However, we believe that there is still a lack of a general methodology for verifying these programs whose correctness depends on the behaviour of the machine learning components. In this paper, we identify the ``embedding gap'' -- the lack of techniques for linking semantically-meaningful ``problem-space'' properties to equivalent ``embedding-space'' properties -- as one of the key issues, and describe Vehicle, a tool designed to facilitate the end-to-end verification of neural-symbolic programs in a modular fashion. Vehicle provides a convenient language for specifying ``problem-space'' properties of neural networks and declaring their relationship to the ``embedding-space", and a powerful compiler that automates interpretation of these properties in the language of a chosen machine-learning training environment, neural network verifie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;ProcessGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#35748;&#30693;&#38480;&#21046;&#19979;&#31649;&#29702;&#19994;&#21153;&#27969;&#31243;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#35748;&#30693;&#38556;&#30861;&#20010;&#20307;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;ProcessGPT&#25913;&#21892;&#20102;&#19981;&#21516;&#35748;&#30693;&#33021;&#21147;&#30340;&#20010;&#20307;&#30340;&#27969;&#31243;&#21487;&#29992;&#24615;&#65292;&#23545;&#32452;&#32455;&#26469;&#35828;&#20063;&#33021;&#24102;&#26469;&#26356;&#39640;&#30340;&#29983;&#20135;&#21147;&#12289;&#22763;&#27668;&#21644;&#21253;&#23481;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06375</link><description>&lt;p&gt;
&#35748;&#30693;BPM&#20316;&#20026;&#24179;&#31561;&#21270;&#22240;&#32032;&#65306;&#25552;&#39640;&#20855;&#26377;&#65288;&#21644;&#27809;&#26377;&#65289;&#35748;&#30693;&#38556;&#30861;&#30340;&#21592;&#24037;&#30340;&#35775;&#38382;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Cognitive BPM as an Equalizer: Improving Access and Efficiency for Employees with (and without) Cognitive Disabilities. (arXiv:2401.06375v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;ProcessGPT&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#35748;&#30693;&#38480;&#21046;&#19979;&#31649;&#29702;&#19994;&#21153;&#27969;&#31243;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#35748;&#30693;&#38556;&#30861;&#20010;&#20307;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;ProcessGPT&#25913;&#21892;&#20102;&#19981;&#21516;&#35748;&#30693;&#33021;&#21147;&#30340;&#20010;&#20307;&#30340;&#27969;&#31243;&#21487;&#29992;&#24615;&#65292;&#23545;&#32452;&#32455;&#26469;&#35828;&#20063;&#33021;&#24102;&#26469;&#26356;&#39640;&#30340;&#29983;&#20135;&#21147;&#12289;&#22763;&#27668;&#21644;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;ProcessGPT&#65292;&#19968;&#20010;&#26088;&#22312;&#33258;&#21160;&#21270;&#12289;&#22686;&#24378;&#21644;&#25913;&#21892;&#19994;&#21153;&#27969;&#31243;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20197;&#30740;&#31350;&#22312;&#20154;&#31867;&#21171;&#21160;&#21147;&#30340;&#35748;&#30693;&#38480;&#21046;&#19979;&#31649;&#29702;&#19994;&#21153;&#27969;&#31243;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#35748;&#30693;&#38556;&#30861;&#30340;&#20010;&#20307;&#12290;ProcessGPT&#25552;&#20379;&#20102;&#35774;&#35745;&#39640;&#25928;&#19994;&#21153;&#27969;&#31243;&#30340;&#34013;&#22270;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#38480;&#21046;&#12290;&#36890;&#36807;&#20174;&#35748;&#30693;&#38556;&#30861;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#65292;&#25105;&#20204;&#34920;&#26126;ProcessGPT&#25913;&#21892;&#20102;&#20855;&#26377;&#21644;&#27809;&#26377;&#35748;&#30693;&#38556;&#30861;&#30340;&#20010;&#20307;&#30340;&#27969;&#31243;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#23454;&#26045;&#31867;&#20284;ProcessGPT&#21151;&#33021;&#30340;&#32452;&#32455;&#23558;&#23454;&#29616;&#22686;&#21152;&#30340;&#29983;&#20135;&#21147;&#12289;&#22763;&#27668;&#21644;&#21253;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine ProcessGPT, an AI model designed to automate, augment, and improve business processes, to study the challenges of managing business processes within the cognitive limitations of the human workforce, particularly individuals with cognitive disabilities. ProcessGPT provides a blueprint for designing efficient business processes that take into account human cognitive limitations. By viewing this through the lens of cognitive disabilities, we show that ProcessGPT improves process usability for individuals with and without cognitive disabilities. We also demonstrate that organizations implementing ProcessGPT-like capabilities will realize increased productivity, morale, and inclusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06373</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;Johnny&#35828;&#26381;LLMs&#36234;&#29425;&#65306;&#36890;&#36807;&#20154;&#24615;&#21270;LLMs&#37325;&#26032;&#24605;&#32771;&#23545;AI&#23433;&#20840;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;AI&#23433;&#20840;&#30740;&#31350;&#23558;AI&#27169;&#22411;&#35270;&#20026;&#26426;&#22120;&#65292;&#24182;&#38598;&#20013;&#22312;&#30001;&#23433;&#20840;&#19987;&#23478;&#24320;&#21457;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#19978;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#31454;&#20105;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#20063;&#21487;&#33021;&#20135;&#29983;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#20316;&#20026;&#31867;&#20284;&#20154;&#31867;&#30340;&#20132;&#27969;&#32773;&#26469;&#36234;&#29425;&#65292;&#20197;&#25506;&#32034;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#34987;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#35828;&#26381;LLMs&#36234;&#29425;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#20960;&#21313;&#24180;&#30340;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#35828;&#26381;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#35828;&#26381;&#23545;&#25239;&#25552;&#31034;&#65288;PAP&#65289;&#26469;&#36234;&#29425;LLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#39118;&#38505;&#31867;&#21035;&#19978;PAP&#22312;Llama 2-7b Chat&#12289;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#22312;10&#27425;&#35797;&#39564;&#20013;&#22343;&#36229;&#36807;92%&#65292;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#29289;&#21307;&#23398;&#23454;&#20363;&#20998;&#21106;&#30340;&#22270;&#20851;&#31995;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#32423;&#29305;&#24449;&#12289;&#23454;&#20363;&#20851;&#31995;&#21644;&#20687;&#32032;&#32423;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#23454;&#20363;&#22270;&#33976;&#39311;&#21644;&#20146;&#21644;&#22270;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#38656;&#27714;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06370</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#29983;&#29289;&#21307;&#23398;&#23454;&#20363;&#20998;&#21106;&#30340;&#22270;&#20851;&#31995;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Graph Relation Distillation for Efficient Biomedical Instance Segmentation. (arXiv:2401.06370v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#29289;&#21307;&#23398;&#23454;&#20363;&#20998;&#21106;&#30340;&#22270;&#20851;&#31995;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#32423;&#29305;&#24449;&#12289;&#23454;&#20363;&#20851;&#31995;&#21644;&#20687;&#32032;&#32423;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#20351;&#29992;&#23454;&#20363;&#22270;&#33976;&#39311;&#21644;&#20146;&#21644;&#22270;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#38656;&#27714;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#23454;&#20363;&#24863;&#30693;&#23884;&#20837;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#23454;&#20363;&#20998;&#21106;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#20854;&#36164;&#28304;&#38656;&#27714;&#24040;&#22823;&#12290;&#30693;&#35782;&#33976;&#39311;&#36890;&#36807;&#23558;&#26469;&#33258;&#37325;&#22411;&#25945;&#24072;&#32593;&#32476;&#30340;&#33976;&#39311;&#30693;&#35782;&#36716;&#31227;&#32473;&#36731;&#37327;&#32423;&#20294;&#39640;&#24615;&#33021;&#30340;&#23398;&#29983;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#25552;&#21462;&#29992;&#20110;&#21306;&#20998;&#23454;&#20363;&#30340;&#30693;&#35782;&#21644;&#24573;&#30053;&#20840;&#23616;&#20851;&#31995;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#29289;&#21307;&#23398;&#23454;&#20363;&#20998;&#21106;&#30340;&#22270;&#20851;&#31995;&#33976;&#39311;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#19977;&#31181;&#20851;&#38190;&#31867;&#22411;&#30340;&#30693;&#35782;&#65306;&#23454;&#20363;&#32423;&#29305;&#24449;&#12289;&#23454;&#20363;&#20851;&#31995;&#21644;&#20687;&#32032;&#32423;&#36793;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#22270;&#33976;&#39311;&#26041;&#26696;&#65292;&#20998;&#21035;&#37096;&#32626;&#22312;&#22270;&#20687;&#20869;&#37096;&#21644;&#22270;&#20687;&#38388;&#65306;&#23454;&#20363;&#22270;&#33976;&#39311;&#65288;IGD&#65289;&#21644;&#20146;&#21644;&#22270;&#33976;&#39311;&#65288;AGD&#65289;&#12290;IGD&#26500;&#24314;&#20102;&#19968;&#20010;&#34920;&#31034;&#23454;&#20363;&#29305;&#24449;&#21644;&#20851;&#31995;&#30340;&#22270;&#65292;&#24182;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#32473;&#23398;&#29983;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance-aware embeddings predicted by deep neural networks have revolutionized biomedical instance segmentation, but its resource requirements are substantial. Knowledge distillation offers a solution by transferring distilled knowledge from heavy teacher networks to lightweight yet high-performance student networks. However, existing knowledge distillation methods struggle to extract knowledge for distinguishing instances and overlook global relation information. To address these challenges, we propose a graph relation distillation approach for efficient biomedical instance segmentation, which considers three essential types of knowledge: instance-level features, instance relations, and pixel-level boundaries. We introduce two graph distillation schemes deployed at both the intra-image level and the inter-image level: instance graph distillation (IGD) and affinity graph distillation (AGD). IGD constructs a graph representing instance features and relations, transferring these two typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for Enhancing RSVP-BCI Decoding. (arXiv:2401.06340v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer (TSformer-SA) &#29992;&#20110;&#22686;&#24378;RSVP-BCI&#35299;&#30721;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#22810;&#35270;&#22270;&#20449;&#24687;&#24182;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#65292;&#23454;&#29616;&#20102;&#35299;&#30721;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20018;&#32852;&#35270;&#35273;&#21576;&#29616;&#65288;RSVP&#65289;&#22522;&#20110;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#30446;&#26631;&#26816;&#32034;&#30340;&#39640;&#25928;&#25216;&#26415;&#12290;&#20256;&#32479;&#35299;&#30721;&#26041;&#27861;&#30340;&#24615;&#33021;&#25913;&#36827;&#20381;&#36182;&#20110;&#22823;&#37327;&#26469;&#33258;&#26032;&#27979;&#35797;&#23545;&#35937;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#22686;&#21152;&#20102;BCI&#31995;&#32479;&#30340;&#20934;&#22791;&#26102;&#38388;&#12290;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#26469;&#33258;&#29616;&#26377;&#23545;&#35937;&#30340;&#25968;&#25454;&#20197;&#20943;&#23569;&#24615;&#33021;&#25913;&#36827;&#23545;&#26032;&#23545;&#35937;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;&#23427;&#20204;&#22522;&#20110;&#23545;&#25239;&#23398;&#20064;&#30340;&#20248;&#21270;&#31574;&#30053;&#20197;&#21450;&#22823;&#37327;&#25968;&#25454;&#30340;&#35757;&#32451;&#22686;&#21152;&#20102;&#20934;&#22791;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#33041;&#30005;&#20449;&#21495;&#30340;&#21333;&#35270;&#22270;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#25913;&#21892;&#24615;&#33021;&#12290;&#20026;&#20102;&#22312;&#20943;&#23569;&#20934;&#22791;&#26102;&#38388;&#30340;&#21516;&#26102;&#25552;&#39640;&#35299;&#30721;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20027;&#39064;&#19987;&#29992;&#36866;&#37197;&#22120;&#30340;&#26102;&#38388;-&#39057;&#35889;&#34701;&#21512;Transformer&#65288;TSformer-SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface (BCI) is an efficient technology for target retrieval using electroencephalography (EEG) signals. The performance improvement of traditional decoding methods relies on a substantial amount of training data from new test subjects, which increases preparation time for BCI systems. Several studies introduce data from existing subjects to reduce the dependence of performance improvement on data from new subjects, but their optimization strategy based on adversarial learning with extensive data increases training time during the preparation procedure. Moreover, most previous methods only focus on the single-view information of EEG signals, but ignore the information from other views which may further improve performance. To enhance decoding performance while reducing preparation time, we propose a Temporal-Spectral fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a cross-view interaction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#21160;&#24577;&#20154;&#21475;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.06318</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#31995;&#32479;&#20844;&#24179;&#24615;&#20013;&#21462;&#24471;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning. (arXiv:2401.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#21160;&#24577;&#20154;&#21475;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20915;&#31574;&#27169;&#22411;&#22312;&#38745;&#24577;&#20154;&#32676;&#19978;&#36816;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20915;&#31574;&#27169;&#22411;&#25805;&#20316;&#30340;&#20154;&#21475;&#26159;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#12290;&#27599;&#20010;&#20915;&#31574;&#21487;&#33021;&#20250;&#25913;&#21464;&#29305;&#24449;&#25110;&#29992;&#25143;&#34892;&#20026;&#30340;&#22522;&#30784;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24314;&#27169;&#21160;&#24577;&#31995;&#32479;&#12290;&#22312;&#25215;&#35748;&#20256;&#32479;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#38271;&#26399;&#20844;&#24179;&#24615;&#26159;&#19981;&#21516;&#35201;&#27714;&#19988;&#21487;&#33021;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#21508;&#31181;&#20844;&#24179;&#32771;&#34385;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20351;&#29992;&#39044;&#22788;&#29702;&#21644;&#22788;&#29702;&#20013;&#30340;&#26041;&#27861;&#12290;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20256;&#32479;&#20844;&#24179;&#24615;&#12289;&#38271;&#26399;&#20844;&#24179;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
While significant advancements have been made in the field of fair machine learning, the majority of studies focus on scenarios where the decision model operates on a static population. In this paper, we study fairness in dynamic systems where sequential decisions are made. Each decision may shift the underlying distribution of features or user behavior. We model the dynamic system through a Markov Decision Process (MDP). By acknowledging that traditional fairness notions and long-term fairness are distinct requirements that may not necessarily align with one another, we propose an algorithmic framework to integrate various fairness considerations with reinforcement learning using both pre-processing and in-processing approaches. Three case studies show that our method can strike a balance between traditional fairness notions, long-term fairness, and utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06308</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;&#12289;&#21160;&#24577;6G&#24212;&#29992;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#24863;&#30693;&#22810;&#22336;&#35775;&#38382;&#26041;&#26696;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#24182;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#28385;&#36275;&#26410;&#26469;6G&#24212;&#29992;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#24863;&#30693;&#33539;&#24335;&#30340;&#20986;&#29616;&#20026;&#21019;&#26032;&#30340;&#26381;&#21153;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#23588;&#20854;&#26159;&#22312;&#22522;&#20110;6G&#30340;&#24212;&#29992;&#29615;&#22659;&#20013;&#12290;&#23613;&#31649;&#22312;&#35821;&#20041;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23558;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#36164;&#28304;&#20998;&#37197;&#20915;&#31574;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#32570;&#20047;&#23545;&#26410;&#26469;&#31995;&#32479;&#38656;&#27714;&#21644;&#29305;&#24615;&#30340;&#32771;&#34385;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#32447;&#39057;&#35889;&#22810;&#22336;&#35775;&#38382;&#38382;&#39064;&#30340;&#24314;&#27169;&#12290;&#23427;&#26088;&#22312;&#20248;&#21270;&#21033;&#29992;&#29575;&#19982;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#65292;&#20351;&#29992;&#945;-&#20844;&#24179;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#21534;&#21520;&#37327;&#21644;&#21327;&#21161;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#26469;&#32771;&#34385;&#29992;&#25143;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#12290;&#39318;&#20808;&#65292;&#20998;&#26512;&#20102;&#35813;&#38382;&#39064;&#65292;&#25214;&#20986;&#20102;&#26368;&#20248;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#20027;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35821;&#20041;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#21452;&#37325;&#21644;&#20915;&#26007;&#28145;&#24230;Q&#23398;&#20064; (SAMA-D3QL) &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#22312;&#35299;&#30721;&#37327;&#23376;&#20449;&#24687;&#26102;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QNN&#35299;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#20960;&#20046;&#20855;&#26377;&#20108;&#27425;&#25913;&#36827;&#12290;&#36825;&#20351;&#24471;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26102;&#21487;&#20197;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06300</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#37327;&#23376;&#20449;&#24687;&#35793;&#30721;&#22120;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Advantage of Quantum Neural Networks as Quantum Information Decoders. (arXiv:2401.06300v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#22312;&#35299;&#30721;&#37327;&#23376;&#20449;&#24687;&#26102;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QNN&#35299;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#20960;&#20046;&#20855;&#26377;&#20108;&#27425;&#25913;&#36827;&#12290;&#36825;&#20351;&#24471;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26102;&#21487;&#20197;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#20445;&#25252;&#37327;&#23376;&#20449;&#24687;&#20813;&#21463;&#22122;&#22768;&#24341;&#36215;&#30340;&#38169;&#35823;&#30340;&#26377;&#24076;&#26395;&#31574;&#30053;&#26159;&#23558;&#20854;&#32534;&#30721;&#21040;&#25299;&#25169;&#37327;&#23376;&#23384;&#20648;&#35774;&#22791;&#30340;&#20302;&#33021;&#24577;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#36825;&#31181;&#23384;&#20648;&#22120;&#30340;&#35835;&#21462;&#38169;&#35823;&#30340;&#24773;&#20917;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#27867;&#20989;&#25200;&#21160;&#65288;&#22914;&#30636;&#24577;&#22833;&#35843;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#30721;&#32534;&#30721;&#22312;&#25299;&#25169;&#31283;&#23450;&#22120;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#20013;&#30340;&#37327;&#23376;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;&#31283;&#23450;&#22120;&#22411;&#38169;&#35823;&#26657;&#27491;&#21644;&#35299;&#30721;&#26041;&#26696;&#22312;&#36825;&#31181;&#25200;&#21160;&#30340;&#37327;&#23376;&#30721;&#20013;&#24037;&#20316;&#24471;&#30456;&#24403;&#22909;&#65292;&#36890;&#36807;&#23637;&#31034;&#35299;&#30721;&#38169;&#35823;&#22312;&#24213;&#23618;&#26080;&#25200;&#21160;&#30721;&#30340;&#36317;&#31163;&#19978;&#21576;&#25351;&#25968;&#34928;&#20943;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNN&#65289;&#35793;&#30721;&#22120;&#22312;&#35835;&#21462;&#38169;&#35823;&#26041;&#38754;&#25552;&#20379;&#20102;&#20960;&#20046;&#20108;&#27425;&#30340;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#23454;&#38469;&#30340;&#37327;&#23376;&#32416;&#38169;&#30721;&#26041;&#38754;&#20351;&#29992;QNN&#30340;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#24471;&#25506;&#32034;&#26356;&#24191;&#27867;&#30340;&#38750;&#31283;&#23450;&#22120;&#30721;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising strategy to protect quantum information from noise-induced errors is to encode it into the low-energy states of a topological quantum memory device. However, readout errors from such memory under realistic settings is less understood. We study the problem of decoding quantum information encoded in the groundspaces of topological stabilizer Hamiltonians in the presence of generic perturbations, such as quenched disorder. We first prove that the standard stabilizer-based error correction and decoding schemes work adequately well in such perturbed quantum codes by showing that the decoding error diminishes exponentially in the distance of the underlying unperturbed code. We then prove that Quantum Neural Network (QNN) decoders provide an almost quadratic improvement on the readout error. Thus, we demonstrate provable advantage of using QNNs for decoding realistic quantum error-correcting codes, and our result enables the exploration of a wider range of non-stabilizer codes in 
&lt;/p&gt;</description></item><item><title>&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#22120;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21516;&#26102;&#20248;&#21270;&#30456;&#20851;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#40092;&#24230;&#12290;&#23427;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#21644;&#21033;&#29992;&#22810;&#20010;&#30446;&#26631;&#30340;&#31532;&#20108;&#27425;&#25490;&#24207;&#24471;&#20998;&#26469;&#25552;&#39640;&#31163;&#32447;AUC&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#22238;&#25918;&#29702;&#35770;&#22312;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#36827;&#19968;&#27493;&#25913;&#21892;&#31163;&#32447;&#22238;&#25918;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06293</link><description>&lt;p&gt;
&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#22120;: &#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in Recommendation Systems. (arXiv:2401.06293v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06293
&lt;/p&gt;
&lt;p&gt;
&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#22120;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21516;&#26102;&#20248;&#21270;&#30456;&#20851;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#40092;&#24230;&#12290;&#23427;&#36890;&#36807;&#24314;&#27169;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#21644;&#21033;&#29992;&#22810;&#20010;&#30446;&#26631;&#30340;&#31532;&#20108;&#27425;&#25490;&#24207;&#24471;&#20998;&#26469;&#25552;&#39640;&#31163;&#32447;AUC&#65292;&#24182;&#36890;&#36807;&#31163;&#32447;&#22238;&#25918;&#29702;&#35770;&#22312;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#36827;&#19968;&#27493;&#25913;&#21892;&#31163;&#32447;&#22238;&#25918;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;&#8212;&#8212;&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#23427;&#21516;&#26102;&#20248;&#21270;&#30456;&#20851;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#26032;&#40092;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#39034;&#24207;&#36138;&#24515;&#31639;&#27861;&#65288;SGA&#65289;&#36275;&#22815;&#39640;&#25928;&#65288;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#65289;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#20135;&#25512;&#33616;&#24341;&#25806;&#12290;&#23427;&#22312;&#31163;&#32447;AUC&#65288;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65289;&#19978;&#21462;&#24471;&#20102;6%&#33267;10%&#30340;&#25552;&#21319;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#21015;&#34920;&#20013;&#26126;&#30830;&#24314;&#27169;&#20102;&#29289;&#21697;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20102;&#22810;&#20010;&#30446;&#26631;&#30340;&#31532;&#20108;&#27425;&#25490;&#24207;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31163;&#32447;&#22238;&#25918;&#29702;&#35770;&#25512;&#24191;&#21040;&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#26469;&#25913;&#21892;&#31163;&#32447;&#22238;&#25918;&#32467;&#26524;&#12290;&#31163;&#32447;&#22238;&#25918;&#32467;&#26524;&#36824;&#21487;&#20197;&#36890;&#36807;Pareto&#26368;&#20248;&#24615;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;OpenAI Gym&#21644;Ray&#26694;&#26550;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#25554;&#27133;&#37325;&#26032;&#25490;&#24207;&#27169;&#25311;&#22120;&#12290;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#37197;&#32622;&#65292;&#24555;&#36895;&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a generic model-based re-ranking framework, MultiSlot ReRanker, which simultaneously optimizes relevance, diversity, and freshness. Specifically, our Sequential Greedy Algorithm (SGA) is efficient enough (linear time complexity) for large-scale production recommendation engines. It achieved a lift of $+6\%$ to $ +10\%$ offline Area Under the receiver operating characteristic Curve (AUC) which is mainly due to explicitly modeling mutual influences among items of a list, and leveraging the second pass ranking scores of multiple objectives. In addition, we have generalized the offline replay theory to multi-slot re-ranking scenarios, with trade-offs among multiple objectives. The offline replay results can be further improved by Pareto Optimality. Moreover, we've built a multi-slot re-ranking simulator based on OpenAI Gym integrated with the Ray framework. It can be easily configured for different assumptions to quickly benchmark both reinforcement learning and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.06256</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36866;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26222;&#36866;&#30340;&#30693;&#35782;&#27169;&#22411;&#21644;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#21407;&#22411;&#24320;&#21457;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;42&#31181;&#35748;&#30693;&#26550;&#26500;&#21644;&#19968;&#32452;&#21151;&#33021;&#27169;&#22359;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#30693;&#35782;&#34920;&#31034;&#25972;&#21512;&#21040;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#20102;42&#31181;&#29992;&#20110;&#21019;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#21151;&#33021;&#27169;&#22359;&#65292;&#36825;&#20123;&#27169;&#22359;&#26159;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#25152;&#24212;&#20855;&#22791;&#30340;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#27809;&#26377;&#25214;&#21040;&#25152;&#38656;&#30340;&#21151;&#33021;&#27169;&#22359;&#38598;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#29992;&#20110;&#25509;&#36817;AGI&#33021;&#21147;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#20316;&#20026;&#26550;&#26500;&#26694;&#26550;&#20013;&#30340;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#21508;&#31181;&#38750;&#24418;&#24335;&#21270;&#12289;&#37096;&#20998;&#21644;&#23436;&#20840;&#24418;&#24335;&#21270;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#27861;&#32467;&#21512;&#22312;&#19968;&#20010;&#30693;&#35782;&#24211;&#20013;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#35760;&#24405;&#12289;&#22270;&#24418;&#12289;&#31639;&#27861;&#12289;&#25968;&#25454;&#24211;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#30693;&#35782;&#22270;&#12289;&#26412;&#20307;&#12289;&#26694;&#26550;&#12289;&#23454;&#36136;-&#23646;&#24615;-&#20851;&#31995;&#27169;&#22411;&#12289;&#25512;&#29702;&#31995;&#32479;&#12289;&#35859;&#35789;&#28436;&#31639;&#27169;&#22411;&#12289;&#27010;&#24565;&#27169;&#22411;&#31561;&#12290;&#20026;&#20102;&#32452;&#21512;&#21644;&#32467;&#26500;&#21270;&#21508;&#20010;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
The article identified 42 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a new cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragment
&lt;/p&gt;</description></item><item><title>WISE&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#19979;&#25193;&#23637;&#36827;&#34892;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06230</link><description>&lt;p&gt;
WISE: &#22522;&#20110;&#22320;&#19979;&#25193;&#23637;&#30340;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WISE: full-Waveform variational Inference via Subsurface Extensions. (arXiv:2401.06230v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06230
&lt;/p&gt;
&lt;p&gt;
WISE&#26159;&#19968;&#31181;&#36890;&#36807;&#22320;&#19979;&#25193;&#23637;&#36827;&#34892;&#20840;&#27874;&#24418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#25512;&#26029;&#21644;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#37327;&#21270;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#21450;&#20854;&#23545;&#25104;&#20687;&#30340;&#24433;&#21709;&#30340;&#20840;&#27874;&#24418;&#21453;&#28436;&#27010;&#29575;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#20849;&#20139;&#22270;&#20687;&#25910;&#38598;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#23545;&#20934;&#30830;&#30340;&#21021;&#22987;&#36895;&#24230;&#27169;&#22411;&#30340;&#20381;&#36182;&#12290;&#32771;&#34385;&#21040;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#26681;&#25454;&#25968;&#25454;&#29983;&#25104;&#20559;&#31227;&#36895;&#24230;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#26469;&#37327;&#21270;&#21518;&#32493;&#25104;&#20687;&#36807;&#31243;&#20013;&#30340;&#25391;&#24133;&#21644;&#23450;&#20301;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a probabilistic technique for full-waveform inversion, employing variational inference and conditional normalizing flows to quantify uncertainty in migration-velocity models and its impact on imaging. Our approach integrates generative artificial intelligence with physics-informed common-image gathers, reducing reliance on accurate initial velocity models. Considered case studies demonstrate its efficacy producing realizations of migration-velocity models conditioned by the data. These models are used to quantify amplitude and positioning effects during subsequent imaging.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06210</link><description>&lt;p&gt;
&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#34920;&#31034;&#26159;&#26426;&#22120;&#29702;&#35299;&#20013;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30340;&#19968;&#33324;&#34920;&#31034;&#20445;&#30041;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#34987;&#35748;&#20026;&#19982;&#35821;&#20041;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#19968;&#33324;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#24207;&#21015;&#26041;&#27861;&#65288;&#26174;&#24335;&#32771;&#34385;&#21333;&#35789;&#30340;&#39034;&#24207;&#65289;&#21644;&#38750;&#24207;&#21015;&#26041;&#27861;&#65288;&#19981;&#26174;&#24335;&#32771;&#34385;&#39034;&#24207;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#26377;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#31867;&#26041;&#27861;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27969;&#34892;&#30340;SA&#25968;&#25454;&#38598;&#21644;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;SA&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#20013;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ADS-B&#25968;&#25454;&#23545;LLaMA 2&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#36807;&#28388;&#22122;&#38899;&#21644;&#20272;&#35745;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#25581;&#31034;&#20102;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#28304;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;LLMs&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06204</link><description>&lt;p&gt;
&#23545;LLM&#22312;&#39134;&#34892;&#36712;&#36857;&#37325;&#24314;&#20998;&#26512;&#20013;&#28508;&#21147;&#30340;&#25506;&#32034;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Exploratory Assessment of LLM's Potential Toward Flight Trajectory Reconstruction Analysis. (arXiv:2401.06204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#20013;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ADS-B&#25968;&#25454;&#23545;LLaMA 2&#27169;&#22411;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#22312;&#36807;&#28388;&#22122;&#38899;&#21644;&#20272;&#35745;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20063;&#25581;&#31034;&#20102;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#21487;&#33021;&#28304;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;LLMs&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33322;&#31354;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#26041;&#38754;&#20855;&#26377;&#38761;&#21629;&#24615;&#28508;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#19968;&#28508;&#21147;&#65292;&#22522;&#20110;LLMs&#22312;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#21644;&#35299;&#35835;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24320;&#28304;LLM&#27169;&#22411;LLaMA 2&#65292;&#26412;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#33258;&#21160;&#30456;&#20851;&#30417;&#35270;&#24191;&#25773;&#65288;ADS-B&#65289;&#25968;&#25454;&#37325;&#24314;&#39134;&#34892;&#36712;&#36857;&#65292;&#35813;&#25968;&#25454;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#19981;&#35268;&#21017;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#36807;&#28388;&#22122;&#38899;&#65292;&#24182;&#20272;&#35745;&#20986;&#32447;&#24615;&#21644;&#26354;&#32447;&#22411;&#39134;&#34892;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#22312;&#22788;&#29702;&#36739;&#38271;&#25968;&#25454;&#24207;&#21015;&#26041;&#38754;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;LLM&#27169;&#22411;&#30340;&#26631;&#35760;&#38271;&#24230;&#38480;&#21046;&#12290;&#30740;&#31350;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;LLMs&#22312;&#39134;&#34892;&#36712;&#36857;&#37325;&#24314;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#20854;&#22312;&#33322;&#31354;&#21644;&#20132;&#36890;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold transformative potential in aviation, particularly in reconstructing flight trajectories. This paper investigates this potential, grounded in the notion that LLMs excel at processing sequential data and deciphering complex data structures. Utilizing the LLaMA 2 model, a pre-trained open-source LLM, the study focuses on reconstructing flight trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with irregularities inherent in real-world scenarios. The findings demonstrate the model's proficiency in filtering noise and estimating both linear and curved flight trajectories. However, the analysis also reveals challenges in managing longer data sequences, which may be attributed to the token length limitations of LLM models. The study's insights underscore the promise of LLMs in flight trajectory reconstruction and open new avenues for their broader application across the aviation and transportation sectors.
&lt;/p&gt;</description></item><item><title>xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06199</link><description>&lt;p&gt;
xTrimoPGLM: &#32479;&#19968;&#30340;&#30334;&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06199
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#29983;&#29289;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#23616;&#38480;&#20110;&#33258;&#32534;&#30721;&#25110;&#33258;&#22238;&#24402;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#26102;&#24456;&#38590;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;xTrimoPGLM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25506;&#32034;&#36825;&#20004;&#31867;&#30446;&#26631;&#30340;&#20860;&#23481;&#24615;&#21644;&#32852;&#21512;&#20248;&#21270;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#65292;&#20351;&#29992;1000&#20159;&#21442;&#25968;&#21644;1&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#26469;&#35757;&#32451;xTrimoPGLM&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;1&#65289;xTrimoPGLM&#22312;&#22235;&#20010;&#31867;&#21035;&#30340;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#22522;&#32447;&#12290;&#35813;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
&lt;/p&gt;</description></item><item><title>NeuSpin&#26159;&#19968;&#20010;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#22312;&#36793;&#32536;&#36827;&#34892;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65292;NeuSPIN&#21487;&#20197;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#20197;&#21450;&#21487;&#38752;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.06195</link><description>&lt;p&gt;
NeuSpin&#65306;&#22522;&#20110;&#33258;&#26059;&#30005;&#23376;&#23398;&#30340;&#21487;&#38752;&#36793;&#32536;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#35774;&#35745;&#29992;&#20110;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
NeuSpin: Design of a Reliable Edge Neuromorphic System Based on Spintronics for Green AI. (arXiv:2401.06195v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06195
&lt;/p&gt;
&lt;p&gt;
NeuSpin&#26159;&#19968;&#20010;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#30340;&#39033;&#30446;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#22312;&#36793;&#32536;&#36827;&#34892;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65292;NeuSPIN&#21487;&#20197;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#20197;&#21450;&#21487;&#38752;&#24615;&#30340;&#35201;&#27714;&#65292;&#20174;&#32780;&#20419;&#36827;&#29289;&#32852;&#32593;&#21644;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#20010;&#24615;&#21270;&#21307;&#30103;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#23558;&#38656;&#35201;&#23384;&#20648;&#21644;&#22788;&#29702;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#35774;&#22791;&#30340;&#20851;&#38190;&#35201;&#27714;&#26159;&#36229;&#20302;&#21151;&#32791;&#12289;&#39640;&#22788;&#29702;&#33021;&#21147;&#12289;&#20302;&#25104;&#26412;&#30340;&#33258;&#20027;&#24615;&#65292;&#20197;&#21450;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20197;&#23454;&#29616;&#36793;&#32536;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#65292;&#23545;&#36164;&#28304;&#30340;&#38656;&#27714;&#39640;&#65292;&#24182;&#19988;&#30001;&#20110;&#20869;&#23384;&#22681;&#38382;&#39064;&#65292;&#38754;&#20020;&#30528;&#20256;&#32479;&#35745;&#31639;&#26550;&#26500;&#30340;&#25361;&#25112;&#12290;&#21033;&#29992;&#26032;&#22411;&#21487;&#21464;&#30005;&#38459;&#35760;&#24518;&#30340;&#35745;&#31639;&#20869;&#23384;&#19968;&#20307;&#21270;&#65288;CIM&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20869;&#23384;&#22359;&#21644;&#35745;&#31639;&#21333;&#20803;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#38477;&#20302;&#20102;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;CIM&#30828;&#20214;&#19978;&#23454;&#29616;BayNNs&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#33258;&#26059;&#30005;&#23376;&#23398;&#25216;&#26415;&#65292;&#30001;&#20110;&#21487;&#21464;&#24615;&#21644;&#21046;&#36896;&#32570;&#38519;&#32780;&#23384;&#22312;&#25216;&#26415;&#25361;&#25112;&#12290;NeuSPIN&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#20840;&#26632;&#30828;&#20214;&#21644;&#36719;&#20214;&#20849;&#21516;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24320;&#21457;&#26032;&#39062;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet of Things (IoT) and smart wearable devices for personalized healthcare will require storing and computing ever-increasing amounts of data. The key requirements for these devices are ultra-low-power, high-processing capabilities, autonomy at low cost, as well as reliability and accuracy to enable Green AI at the edge. Artificial Intelligence (AI) models, especially Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges with traditional computing architectures due to the memory wall problem. Computing-in-Memory (CIM) with emerging resistive memories offers a solution by combining memory blocks and computing units for higher efficiency and lower power consumption. However, implementing BayNNs on CIM hardware, particularly with spintronic technologies, presents technical challenges due to variability and manufacturing defects. The NeuSPIN project aims to address these challenges through full-stack hardware and software co-design, developing novel algorithmic 
&lt;/p&gt;</description></item><item><title>CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2401.06194</link><description>&lt;p&gt;
CrisisKAN: &#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06194
&lt;/p&gt;
&lt;p&gt;
CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#23454;&#26102;&#20449;&#24687;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#20108;&#32773;&#20860;&#26377;&#65289;&#35782;&#21035;&#21508;&#31181;&#20107;&#20214;&#30340;&#26032;&#20852;&#26469;&#28304;&#12290;&#23613;&#31649;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20107;&#20214;&#20998;&#31867;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#24357;&#21512;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#32534;&#30721;&#23548;&#33268;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#26080;&#27861;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26080;&#27861;&#22312;&#28798;&#38590;&#12289;&#22823;&#27969;&#34892;&#31561;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#24314;&#31435;&#20449;&#20219;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#23383;&#25968;&#38480;&#21046;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#20107;&#20214;&#24341;&#20837;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrisisKAN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#19982;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20998;&#31867;&#21361;&#26426;&#20107;&#20214;&#12290;&#20026;&#20102;&#20016;&#23500;&#23545;&#25991;&#26412;&#20449;&#24687;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25552;&#20986;&#30340;&#32500;&#22522;&#30334;&#31185;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.06183</link><description>&lt;p&gt;
&#20351;&#29992;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#30340;&#31471;&#21040;&#31471;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#35821;&#38899;&#19968;&#30452;&#26159;&#26377;&#25928;&#27807;&#36890;&#21644;&#36830;&#25509;&#30340;&#38556;&#30861;&#65292;&#22312;&#25105;&#20204;&#26085;&#30410;&#20114;&#32852;&#30340;&#19990;&#30028;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#37327;&#36523;&#23450;&#21046;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#33521;&#25991;&#38899;&#39057;&#30340;&#21512;&#25104;&#12290;&#36890;&#36807;&#25972;&#21512;XLSR Wav2Vec2&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;mBART&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#32452;&#20214;&#31561;&#23574;&#31471;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#36328;&#35821;&#35328;&#20132;&#27969;&#26041;&#24335;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#20010;&#21035;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20114;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#21360;&#22320;&#35821;&#21475;&#35821;&#21040;&#21512;&#25104;&#33521;&#25991;&#38899;&#39057;&#30340;&#27969;&#30021;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20998;&#26512;&#12289;&#35777;&#23454;&#21644;&#25209;&#35780;&#20102;&#21830;&#19994;&#39046;&#23548;&#32773;&#20351;&#29992;AI&#22270;&#20687;&#29983;&#25104;&#21462;&#20195;&#20154;&#31867;&#33402;&#26415;&#21171;&#21160;&#21147;&#30340;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;AI&#22270;&#20687;&#29983;&#25104;&#28041;&#21450;&#19968;&#31181;&#19981;&#36947;&#24503;&#30340;&#21171;&#21160;&#24615;&#30423;&#31363;&#65292;&#36825;&#23545;&#35768;&#22810;&#20854;&#20182;&#30340;AI&#24212;&#29992;&#20063;&#20855;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06178</link><description>&lt;p&gt;
AI&#33402;&#26415;&#26159;&#30423;&#31363;&#65306;&#21171;&#21160;&#12289;&#25552;&#21462;&#21644;&#21093;&#21066;&#65292;&#25110;&#32773;&#35828;&#20851;&#20110;&#38543;&#26426;&#27874;&#27931;&#20811;&#30340;&#21361;&#38505;
&lt;/p&gt;
&lt;p&gt;
AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the Dangers of Stochastic Pollocks. (arXiv:2401.06178v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06178
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20998;&#26512;&#12289;&#35777;&#23454;&#21644;&#25209;&#35780;&#20102;&#21830;&#19994;&#39046;&#23548;&#32773;&#20351;&#29992;AI&#22270;&#20687;&#29983;&#25104;&#21462;&#20195;&#20154;&#31867;&#33402;&#26415;&#21171;&#21160;&#21147;&#30340;&#34892;&#20026;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;AI&#22270;&#20687;&#29983;&#25104;&#28041;&#21450;&#19968;&#31181;&#19981;&#36947;&#24503;&#30340;&#21171;&#21160;&#24615;&#30423;&#31363;&#65292;&#36825;&#23545;&#35768;&#22810;&#20854;&#20182;&#30340;AI&#24212;&#29992;&#20063;&#20855;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;DALL-E&#12289;Midjourney&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#24212;&#29992;&#25512;&#20986;&#20197;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#21019;&#20316;&#33402;&#26415;&#30340;&#24037;&#20855;&#19968;&#30452;&#22791;&#21463;&#20105;&#35758;&#12290;&#19968;&#20123;&#20154;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#25216;&#26415;&#30340;&#38271;&#26399;&#25285;&#24551;&#65292;&#35748;&#20026;&#23427;&#20204;&#39044;&#31034;&#30528;&#21363;&#23558;&#21040;&#26469;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#26410;&#26469;&#65292;&#20294;&#26356;&#32039;&#36843;&#30340;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23545;&#24403;&#21069;&#21019;&#20316;&#21171;&#21160;&#21147;&#30340;&#24433;&#21709;&#12290;&#21830;&#19994;&#39046;&#23548;&#32773;&#24050;&#32463;&#24320;&#22987;&#29992;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21462;&#20195;&#20154;&#31867;&#33402;&#26415;&#21171;&#21160;&#21147;&#12290;&#20316;&#23478;&#30340;&#24037;&#20250;&#21457;&#36215;&#20102;&#19968;&#22330;&#25239;&#35758;&#36816;&#21160;&#65292;&#35748;&#20026;AI&#22270;&#20687;&#29983;&#25104;&#26159;&#19968;&#31181;&#30423;&#31363;&#34892;&#20026;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#35770;&#28857;&#36827;&#34892;&#20102;&#20998;&#26512;&#12289;&#35777;&#23454;&#21644;&#25209;&#35780;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#65292;AI&#22270;&#20687;&#29983;&#25104;&#28041;&#21450;&#19968;&#31181;&#19981;&#36947;&#24503;&#30340;&#21171;&#21160;&#24615;&#30423;&#31363;&#12290;&#22914;&#26524;&#27491;&#30830;&#30340;&#35805;&#65292;&#35768;&#22810;&#20854;&#20182;&#30340;AI&#24212;&#29992;&#20063;&#20381;&#36182;&#20110;&#36825;&#31181;&#30423;&#31363;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the launch of applications such as DALL-E, Midjourney, and Stable Diffusion, generative artificial intelligence has been controversial as a tool for creating artwork. While some have presented longtermist worries about these technologies as harbingers of fully automated futures to come, more pressing is the impact of generative AI on creative labour in the present. Already, business leaders have begun replacing human artistic labour with AI-generated images. In response, the artistic community has launched a protest movement, which argues that AI image generation is a kind of theft. This paper analyzes, substantiates, and critiques these arguments, concluding that AI image generators involve an unethical kind of labour theft. If correct, many other AI applications also rely upon theft.
&lt;/p&gt;</description></item><item><title>GOODAT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#26102;&#22270;&#24418;&#39046;&#22495;&#22806;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25968;&#25454;&#39537;&#21160;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.06176</link><description>&lt;p&gt;
GOODAT: &#38754;&#21521;&#27979;&#35797;&#26102;&#22270;&#24418;&#30340;&#39046;&#22495;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GOODAT: Towards Test-time Graph Out-of-Distribution Detection. (arXiv:2401.06176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06176
&lt;/p&gt;
&lt;p&gt;
GOODAT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#26102;&#22270;&#24418;&#39046;&#22495;&#22806;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#25968;&#25454;&#39537;&#21160;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#22312;&#24314;&#27169;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;GNN&#22312;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#31526;&#21512;&#21516;&#19968;&#20998;&#24067;(&#21363;&#20998;&#24067;&#20869;,ID)&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#38754;&#23545;&#26469;&#33258;&#19981;&#29087;&#24713;&#20998;&#24067;(&#21363;&#39046;&#22495;&#22806;,OOD)&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#29992;GNNs&#35782;&#21035;&#21644;&#25298;&#32477;OOD&#26679;&#26412;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22270;OOD&#26816;&#27979;&#65292;&#36890;&#24120;&#38598;&#20013;&#22312;&#35757;&#32451;&#29305;&#23450;&#27169;&#22411;&#25110;&#22312;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;GNN&#20043;&#19978;&#20462;&#25913;&#25968;&#25454;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#37325;&#22411;&#30340;&#35757;&#32451;&#36164;&#28304;&#21644;&#25104;&#26412;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20248;&#21270;&#22522;&#20110;GNN&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#20462;&#25913;&#21407;&#22987;GNN&#21644;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26222;&#36866;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#26816;&#27979;&#22270;&#24418;&#30340;&#26041;&#27861;(GOODAT)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#12289;&#26080;&#30417;&#30563;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#27604;&#36739;&#21644;&#37325;&#26032;&#23454;&#29616;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06175</link><description>&lt;p&gt;
MTAD: &#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24037;&#20855;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly Detection. (arXiv:2401.06175v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06175
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#27604;&#36739;&#21644;&#37325;&#26032;&#23454;&#29616;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26159;&#30830;&#20445;&#35768;&#22810;&#36719;&#20214;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#37325;&#35201;&#26102;&#38388;&#24207;&#21015;&#25351;&#26631;&#12290;&#23427;&#20204;&#24544;&#23454;&#35760;&#24405;&#36816;&#34892;&#26102;&#29366;&#24577;&#65292;&#20415;&#20110;&#29702;&#35299;&#24322;&#24120;&#31995;&#32479;&#34892;&#20026;&#65292;&#24182;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#23450;&#20301;&#26681;&#26412;&#21407;&#22240;&#30340;&#26377;&#29992;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#21069;&#25152;&#26410;&#26377;&#65292;&#23548;&#33268;KPI&#25968;&#37327;&#28608;&#22686;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#20256;&#32479;&#30340;KPI&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21464;&#24471;&#19981;&#23454;&#29992;&#65292;&#36825;&#20419;&#20351;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#23545;&#36825;&#20123;KPI&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20005;&#26684;&#27604;&#36739;&#65292;&#24182;&#19988;&#37325;&#26032;&#23454;&#29616;&#38656;&#35201;&#20184;&#20986;&#30456;&#24403;&#22823;&#30340;&#24037;&#20316;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#30740;&#31350;&#37319;&#29992;&#29420;&#31435;&#30340;&#35780;&#20272;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#25351;&#26631;&#12290;&#20854;&#20013;&#19968;&#20123;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#23637;&#31034;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26377;&#20123;&#21017;&#20135;&#29983;&#20102;&#36827;&#23637;&#30340;&#38169;&#35273;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key Performance Indicators (KPIs) are essential time-series metrics for ensuring the reliability and stability of many software systems. They faithfully record runtime states to facilitate the understanding of anomalous system behaviors and provide informative clues for engineers to pinpoint the root causes. The unprecedented scale and complexity of modern software systems, however, make the volume of KPIs explode. Consequently, many traditional methods of KPI anomaly detection become impractical, which serves as a catalyst for the fast development of machine learning-based solutions in both academia and industry. However, there is currently a lack of rigorous comparison among these KPI anomaly detection methods, and re-implementation demands a non-trivial effort. Moreover, we observe that different works adopt independent evaluation processes with different metrics. Some of them may not fully reveal the capability of a model and some are creating an illusion of progress. To better und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#27954;&#20892;&#19994;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#31934;&#20934;&#20892;&#19994;&#12289;&#20316;&#29289;&#30417;&#27979;&#21644;&#27668;&#20505;&#36866;&#24212;&#24615;&#23454;&#36341;&#31561;&#26426;&#20250;&#65292;&#24182;&#20998;&#26512;&#20102;&#25216;&#26415;&#22522;&#30784;&#35774;&#26045;&#12289;&#25968;&#25454;&#33719;&#21462;&#21644;&#25216;&#33021;&#32570;&#21475;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#23567;&#20892;&#25143;&#12289;&#20379;&#24212;&#38142;&#21644;&#21253;&#23481;&#24615;&#22686;&#38271;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06171</link><description>&lt;p&gt;
&#12298;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#38750;&#27954;&#21487;&#25345;&#32493;&#20892;&#19994;&#21457;&#23637;&#65306;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#24433;&#21709;&#12299;
&lt;/p&gt;
&lt;p&gt;
Harnessing Artificial Intelligence for Sustainable Agricultural Development in Africa: Opportunities, Challenges, and Impact. (arXiv:2401.06171v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#27954;&#20892;&#19994;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#31934;&#20934;&#20892;&#19994;&#12289;&#20316;&#29289;&#30417;&#27979;&#21644;&#27668;&#20505;&#36866;&#24212;&#24615;&#23454;&#36341;&#31561;&#26426;&#20250;&#65292;&#24182;&#20998;&#26512;&#20102;&#25216;&#26415;&#22522;&#30784;&#35774;&#26045;&#12289;&#25968;&#25454;&#33719;&#21462;&#21644;&#25216;&#33021;&#32570;&#21475;&#31561;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#23567;&#20892;&#25143;&#12289;&#20379;&#24212;&#38142;&#21644;&#21253;&#23481;&#24615;&#22686;&#38271;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#27954;&#21508;&#22320;&#21487;&#25345;&#32493;&#20892;&#19994;&#21457;&#23637;&#20013;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#36890;&#36807;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#26426;&#20250;&#12289;&#25361;&#25112;&#21644;&#24433;&#21709;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#32034;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#20892;&#19994;&#39046;&#22495;&#30340;&#21160;&#24577;&#21457;&#23637;&#12290;&#25991;&#31456;&#20998;&#26512;&#20102;&#31934;&#20934;&#20892;&#19994;&#12289;&#20316;&#29289;&#30417;&#27979;&#21644;&#27668;&#20505;&#36866;&#24212;&#24615;&#23454;&#36341;&#31561;&#26041;&#38754;&#30340;&#26426;&#20250;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#19982;&#25216;&#26415;&#22522;&#30784;&#35774;&#26045;&#12289;&#25968;&#25454;&#33719;&#21462;&#21644;&#25216;&#33021;&#32570;&#21475;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#20154;&#24037;&#26234;&#33021;&#23545;&#23567;&#20892;&#25143;&#12289;&#20379;&#24212;&#38142;&#21644;&#21253;&#23481;&#24615;&#22686;&#38271;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#20262;&#29702;&#32771;&#34385;&#21644;&#25919;&#31574;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20154;&#24037;&#26234;&#33021;&#25972;&#21512;&#30340;&#36127;&#36131;&#20219;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20379;&#32454;&#33268;&#30340;&#35748;&#35782;&#65292;&#26412;&#25991;&#23545;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#38750;&#27954;&#20892;&#19994;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#35752;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the transformative potential of artificial intelligence (AI) in the context of sustainable agricultural development across diverse regions in Africa. Delving into opportunities, challenges, and impact, the study navigates through the dynamic landscape of AI applications in agriculture. Opportunities such as precision farming, crop monitoring, and climate-resilient practices are examined, alongside challenges related to technological infrastructure, data accessibility, and skill gaps. The article analyzes the impact of AI on smallholder farmers, supply chains, and inclusive growth. Ethical considerations and policy implications are also discussed, offering insights into responsible AI integration. By providing a nuanced understanding, this paper contributes to the ongoing discourse on leveraging AI for fostering sustainability in African agriculture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#65292;&#27604;&#36739;&#20102;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#21644;&#21093;&#21066;&#25169;&#20811;&#30340;&#24046;&#24322;&#65292;&#35752;&#35770;&#20102;&#25169;&#20811;&#26426;&#22120;&#20154;&#25152;&#37319;&#29992;&#30340;&#29305;&#23450;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;2&#20154;&#23545;&#22810;&#20154;&#28216;&#25103;&#30340;&#19981;&#21516;&#20197;&#21450;&#19982;&#29609;&#26356;&#22810;&#29609;&#23478;&#26102;&#20986;&#29616;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29702;&#35770;&#26041;&#27861;&#22312;&#24320;&#21457;&#33719;&#32988;&#31574;&#30053;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06168</link><description>&lt;p&gt;
&#20851;&#20110;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Game Theory Optimal Poker. (arXiv:2401.06168v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#65292;&#27604;&#36739;&#20102;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#21644;&#21093;&#21066;&#25169;&#20811;&#30340;&#24046;&#24322;&#65292;&#35752;&#35770;&#20102;&#25169;&#20811;&#26426;&#22120;&#20154;&#25152;&#37319;&#29992;&#30340;&#29305;&#23450;&#31574;&#30053;&#65292;&#25506;&#35752;&#20102;2&#20154;&#23545;&#22810;&#20154;&#28216;&#25103;&#30340;&#19981;&#21516;&#20197;&#21450;&#19982;&#29609;&#26356;&#22810;&#29609;&#23478;&#26102;&#20986;&#29616;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#36824;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29702;&#35770;&#26041;&#27861;&#22312;&#24320;&#21457;&#33719;&#32988;&#31574;&#30053;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25169;&#20811;&#26159;&#19968;&#31181;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#65292;&#19982;&#22269;&#38469;&#35937;&#26827;&#12289;&#22235;&#36830;&#26827;&#31561;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#19981;&#21516;&#12290;&#34429;&#28982;&#35768;&#22810;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#24050;&#32463;&#34987;&#35299;&#20915;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#35299;&#20915;&#20219;&#20309;&#38750;&#24179;&#20961;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#28216;&#25103;&#12290;&#36825;&#20351;&#24471;&#25169;&#20811;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#26497;&#22909;&#35797;&#39564;&#24179;&#21488;&#12290;&#26412;&#25991;&#39318;&#20808;&#27604;&#36739;&#20102;&#21338;&#24328;&#35770;&#26368;&#20248;&#25169;&#20811;&#21644;&#21093;&#21066;&#25169;&#20811;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25277;&#35937;&#25216;&#26415;&#12289;&#25237;&#27880;&#27169;&#22411;&#21644;&#25104;&#21151;&#30340;&#25169;&#20811;&#26426;&#22120;&#20154;&#65288;&#22914;Tartanian[1]&#21644;Pluribus[6]&#65289;&#25152;&#37319;&#29992;&#30340;&#29305;&#23450;&#31574;&#30053;&#30340;&#22797;&#26434;&#24615;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;2&#20154;&#23545;&#22810;&#20154;&#28216;&#25103;&#30340;&#24046;&#24322;&#20197;&#21450;&#19982;&#29609;&#26356;&#22810;&#29609;&#23478;&#26102;&#20986;&#29616;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#29702;&#35770;&#26041;&#27861;&#22312;&#24320;&#21457;&#33719;&#32988;&#31574;&#30053;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poker is in the family of imperfect information games unlike other games such as chess, connect four, etc which are perfect information game instead. While many perfect information games have been solved, no non-trivial imperfect information game has been solved to date. This makes poker a great test bed for Artificial Intelligence research. In this paper we firstly compare Game theory optimal poker to Exploitative poker. Secondly, we discuss the intricacies of abstraction techniques, betting models, and specific strategies employed by successful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also explore 2-player vs multi-player games and the limitations that come when playing with more players. Finally, this paper discusses the role of machine learning and theoretical approaches in developing winning strategies and suggests future directions for this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#24335;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#65292;&#36827;&#32780;&#22686;&#24378;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06167</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;CLIP&#30340;&#22270;&#20687;&#21040;&#25991;&#26412;&#36716;&#25442;&#22686;&#24378;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multimodal Understanding with CLIP-Based Image-to-Text Transformation. (arXiv:2401.06167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#24335;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26469;&#23454;&#29616;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#36716;&#25442;&#65292;&#36827;&#32780;&#22686;&#24378;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#25991;&#26412;&#35299;&#37322;&#30340;&#36807;&#31243;&#26159;&#19968;&#39033;&#20851;&#38190;&#19988;&#22797;&#26434;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#24335;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of transforming input images into corresponding textual explanations stands as a crucial and complex endeavor within the domains of computer vision and natural language processing. In this paper, we propose an innovative ensemble approach that harnesses the capabilities of Contrastive Language-Image Pretraining models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22320;&#28857;&#30340;&#31639;&#27861;&#24033;&#36923;&#31649;&#29702;&#31995;&#32479;&#30340;&#21435;&#20559;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#31181;&#26063;&#20559;&#35265;&#29305;&#24449;&#24182;&#20445;&#30041;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06162</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22320;&#28857;&#30340;&#31639;&#27861;&#24033;&#36923;&#31649;&#29702;&#30340;&#21435;&#20559;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A debiasing technique for place-based algorithmic patrol management. (arXiv:2401.06162v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06162
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22320;&#28857;&#30340;&#31639;&#27861;&#24033;&#36923;&#31649;&#29702;&#31995;&#32479;&#30340;&#21435;&#20559;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#31181;&#26063;&#20559;&#35265;&#29305;&#24449;&#24182;&#20445;&#30041;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#35686;&#21153;&#24037;&#20316;&#21457;&#29983;&#20102;&#38761;&#21629;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23545;&#21382;&#21490;&#25968;&#25454;&#20013;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#31639;&#27861;&#20915;&#31574;&#30340;&#23457;&#26597;&#12290;&#22312;&#36825;&#39033;&#25506;&#32034;&#24615;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22320;&#28857;&#30340;&#31639;&#27861;&#24033;&#36923;&#31649;&#29702;&#31995;&#32479;&#30340;&#21435;&#20559;&#25216;&#26415;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#22312;&#20445;&#30041;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#28040;&#38500;&#20102;&#31181;&#26063;&#20559;&#35265;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#30340;&#22312;&#20844;&#24179;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#35686;&#21153;&#39046;&#22495;&#20013;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#38271;&#20018;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a revolution in data-driven policing. With that has come scrutiny on how bias in historical data affects algorithmic decision making. In this exploratory work, we introduce a debiasing technique for place-based algorithmic patrol management systems. We show that the technique efficiently eliminates racially biased features while retaining high accuracy in the models. Finally, we provide a lengthy list of potential future research in the realm of fairness and data-driven policing which this work uncovered.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#22312;&#24403;&#20195;&#31038;&#20250;&#21644;&#26410;&#26469;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20197;&#21450;&#22312;&#37096;&#32626;ADS&#26102;&#30340;&#35268;&#33539;&#12289;&#36879;&#26126;&#24230;&#21644;&#20262;&#29702;&#34892;&#20026;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06161</link><description>&lt;p&gt;
&#20154;&#31867;&#20026;&#20013;&#24515;&#30340;&#21487;&#20449;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Trustworthy human-centric based Automated Decision-Making Systems. (arXiv:2401.06161v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06161
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#22312;&#24403;&#20195;&#31038;&#20250;&#21644;&#26410;&#26469;&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20197;&#21450;&#22312;&#37096;&#32626;ADS&#26102;&#30340;&#35268;&#33539;&#12289;&#36879;&#26126;&#24230;&#21644;&#20262;&#29702;&#34892;&#20026;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#65288;ADS&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#12289;&#27963;&#21160;&#21644;&#32844;&#19994;&#20013;&#24050;&#26222;&#36941;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#37319;&#29992;&#24341;&#20837;&#20102;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;ADS&#30340;&#28389;&#29992;&#12290;&#24403;ADS&#22312;&#19981;&#24517;&#35201;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#25110;&#32773;&#24573;&#35270;&#20102;&#37325;&#35201;&#35201;&#27714;&#12289;&#26465;&#20214;&#21644;&#26465;&#27454;&#65292;&#23548;&#33268;&#24847;&#22806;&#21518;&#26524;&#26102;&#65292;&#21487;&#33021;&#20250;&#20986;&#29616;&#36825;&#31181;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#23545;&#25968;&#23383;&#21270;&#12289;&#25968;&#23383;&#36716;&#22411;&#20197;&#21450;&#22312;&#24403;&#20195;&#31038;&#20250;&#21644;&#26410;&#26469;&#29615;&#22659;&#20013;&#21033;&#29992;ADS&#25152;&#28041;&#21450;&#30340;&#21547;&#20041;&#12289;&#21306;&#21035;&#21644;&#20262;&#29702;&#32771;&#34385;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#37096;&#32626;ADS&#26102;&#30340;&#35268;&#33539;&#12289;&#36879;&#26126;&#24230;&#21644;&#20262;&#29702;&#34892;&#20026;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Decision-Making Systems (ADS) have become pervasive across various fields, activities, and occupations, to enhance performance. However, this widespread adoption introduces potential risks, including the misuse of ADS. Such misuse may manifest when ADS is employed in situations where it is unnecessary or when essential requirements, conditions, and terms are overlooked, leading to unintended consequences. This research paper presents a thorough examination of the implications, distinctions, and ethical considerations associated with digitalization, digital transformation, and the utilization of ADS in contemporary society and future contexts. Emphasis is placed on the imperative need for regulation, transparency, and ethical conduct in the deployment of ADS.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21475;&#35797;&#30340;&#21407;&#22411;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#25945;&#32946;&#27665;&#20027;&#21270;&#12289;&#21253;&#23481;&#22810;&#26679;&#30340;&#23398;&#29983;&#32676;&#20307;&#20197;&#21450;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06160</link><description>&lt;p&gt;
&#26410;&#26469;&#25945;&#32946;&#30340;&#38450;&#39118;&#38505;&#35774;&#35745;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21475;&#35797;&#30340;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
Future-proofing Education: A Prototype for Simulating Oral Examinations Using Large Language Models. (arXiv:2401.06160v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06160
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21475;&#35797;&#30340;&#21407;&#22411;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#25945;&#32946;&#27665;&#20027;&#21270;&#12289;&#21253;&#23481;&#22810;&#26679;&#30340;&#23398;&#29983;&#32676;&#20307;&#20197;&#21450;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#21407;&#22411;&#36827;&#34892;&#33258;&#21160;&#21475;&#35797;&#27169;&#25311;&#12290;&#25551;&#36848;&#20102;&#21407;&#22411;&#30340;&#35774;&#35745;&#32771;&#34385;&#65292;&#24182;&#36890;&#36807;&#19968;&#32452;&#36873;&#25321;&#30340;&#25945;&#32946;&#32773;&#21644;&#23398;&#29983;&#23545;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35752;&#35770;&#20102;&#25216;&#26415;&#21644;&#25945;&#23398;&#35266;&#23519;&#12290;&#21407;&#22411;&#22312;&#27169;&#25311;&#21475;&#35797;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#21644;&#31616;&#21270;&#25945;&#32946;&#24037;&#20316;&#36127;&#25285;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21407;&#22411;&#30340;&#26377;&#26395;&#32467;&#26524;&#26174;&#31034;&#20102;LLMs&#22312;&#27665;&#20027;&#25945;&#32946;&#12289;&#21253;&#23481;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#20197;&#21450;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the impact of Large Language Models (LLMs) in higher education, focusing on an automated oral examination simulation using a prototype. The design considerations of the prototype are described, and the system is evaluated with a select group of educators and students. Technical and pedagogical observations are discussed. The prototype proved to be effective in simulating oral exams, providing personalized feedback, and streamlining educators' workloads. The promising results of the prototype show the potential for LLMs in democratizing education, inclusion of diverse student populations, and improvement of teaching quality and efficiency.
&lt;/p&gt;</description></item><item><title>UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.06157</link><description>&lt;p&gt;
UDEEP: &#22522;&#20110;&#36793;&#32536;&#30340;&#27700;&#19979;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection. (arXiv:2401.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06157
&lt;/p&gt;
&lt;p&gt;
UDEEP&#26159;&#19968;&#20010;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20837;&#20405;&#20449;&#21495;&#40857;&#34430;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#23545;&#29983;&#24577;&#31995;&#32479;&#36896;&#25104;&#20102;&#19981;&#21033;&#24433;&#21709;&#12290;&#23427;&#20204;&#20256;&#25773;&#20102;&#23545;&#33521;&#22269;&#21807;&#19968;&#30340;&#26412;&#22320;&#30333;&#29226;&#40857;&#34430;&#33268;&#21629;&#30340;&#30495;&#33740;&#22411;&#40857;&#34430;&#30239;&#30123;&#30149;(Aphanomyces astaci)&#12290;&#20837;&#20405;&#30340;&#20449;&#21495;&#40857;&#34430;&#24191;&#27867;&#25366;&#25496;&#27934;&#31348;&#65292;&#30772;&#22351;&#26646;&#24687;&#22320;&#65292;&#20405;&#34432;&#27827;&#23736;&#24182;&#23545;&#27700;&#36136;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#21516;&#26102;&#31454;&#20105;&#26412;&#22320;&#29289;&#31181;&#30340;&#36164;&#28304;&#24182;&#23548;&#33268;&#26412;&#22320;&#31181;&#32676;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#27745;&#26579;&#20063;&#20351;&#30333;&#29226;&#40857;&#34430;&#26356;&#21152;&#23481;&#26131;&#21463;&#21040;&#25439;&#23475;&#65292;&#20854;&#31181;&#32676;&#22312;&#33521;&#22269;&#26576;&#20123;&#22320;&#21306;&#19979;&#38477;&#36229;&#36807;90&#65285;&#65292;&#20351;&#20854;&#26497;&#26131;&#28626;&#20020;&#28781;&#32477;&#12290;&#20026;&#20102;&#20445;&#25252;&#27700;&#29983;&#29983;&#24577;&#31995;&#32479;&#65292;&#35299;&#20915;&#20837;&#20405;&#29289;&#31181;&#21644;&#24223;&#24323;&#22609;&#26009;&#23545;&#33521;&#22269;&#27827;&#27969;&#29983;&#24577;&#31995;&#32479;&#30340;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;UDEEP&#24179;&#21488;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#20998;&#31867;&#20449;&#21495;&#40857;&#34430;&#21644;&#22609;&#26009;&#30862;&#29255;&#65292;&#20805;&#24403;&#29615;&#22659;&#30417;&#27979;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources and leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90% in certain English counties, making them highly susceptible to extinction. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and discarded plastics in the United Kingdom's river ecosystem's. The UDEEP platform can play a crucial role in environmental monitoring by performing on-the-fly classification of Signal crayfish and plastic debris while leveragi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06153</link><description>&lt;p&gt;
&#22522;&#20110;k-&#32858;&#31867;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm. (arXiv:2401.06153v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06153
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20248;&#21270;&#32463;&#24120;&#22312;&#24037;&#31243;&#38382;&#39064;&#20013;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#22312;&#23547;&#25214;&#19981;&#21516;&#21644;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#26102;&#12290;&#36827;&#21270;&#31639;&#27861;&#36890;&#36807;&#31181;&#32676;&#30340;&#27010;&#24565;&#12289;&#25506;&#32034;/&#24320;&#21457;&#21151;&#33021;&#21644;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#31561;&#29305;&#28857;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#35299;&#20915;&#22810;&#27169;&#24577;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#22810;&#27169;&#24577;&#20248;&#21270;Big Bang-Big Crunch&#31639;&#27861;&#30340;&#29256;&#26412;&#65292;&#31216;&#20026;k-BBBC&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#25972;&#20010;&#31181;&#32676;&#30340;&#23436;&#20840;&#25910;&#25947;&#65292;&#23545;&#20110;&#29305;&#23450;&#38382;&#39064;&#24179;&#22343;&#26816;&#32034;&#21040;99\%&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;(i)&#22312;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30830;&#23450;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20197;&#21450;(ii)&#23450;&#37327;&#27979;&#37327;&#27491;&#30830;&#26816;&#32034;&#21040;&#30340;&#26368;&#20248;&#35299;&#25968;&#37327;&#19982;&#39044;&#26399;&#25968;&#37327;&#20043;&#38388;&#30340;&#27604;&#29575;&#65288;&#21363;&#25104;&#21151;&#29575;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;k-BBBC&#22312;&#20855;&#26377;&#22823;&#37327;&#26368;&#20248;&#35299;&#65288;&#27979;&#35797;&#20102;379&#20010;&#26368;&#20248;&#35299;&#65289;&#21644;&#39640;&#32500;&#24230;&#30340;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal optimization is often encountered in engineering problems, especially when different and alternative solutions are sought. Evolutionary algorithms can efficiently tackle multi-modal optimization thanks to their features such as the concept of population, exploration/exploitation, and being suitable for parallel computation.  This paper introduces a multi-modal optimization version of the Big Bang-Big Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees a complete convergence of the entire population, retrieving on average the 99\% of local optima for a specific problem. Additionally, we introduce two post-processing methods to (i) identify the local optima in a set of retrieved solutions (i.e., a population), and (ii) quantify the number of correctly retrieved optima against the expected ones (i.e., success rate).  Our results show that k-BBBC performs well even with problems having a large number of optima (tested on 379 optima) and high dimensio
&lt;/p&gt;</description></item><item><title>MMDiff&#26159;&#19968;&#20010;&#32852;&#21512;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#20363;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06151</link><description>&lt;p&gt;
&#36235;&#21521;&#20110;&#20351;&#29992;SE(3)-&#31163;&#25955;&#25193;&#25955;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#30340;&#32852;&#21512;&#24207;&#21015;-&#32467;&#26500;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion. (arXiv:2401.06151v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06151
&lt;/p&gt;
&lt;p&gt;
MMDiff&#26159;&#19968;&#20010;&#32852;&#21512;&#29983;&#25104;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#24212;&#29992;&#20215;&#20540;&#65292;&#24182;&#36890;&#36807;&#39564;&#35777;&#23454;&#20363;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20998;&#23376;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#34507;&#30333;&#36136;&#24037;&#31243;&#30340;&#24037;&#19994;&#21644;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20855;&#26377;&#20016;&#23500;&#19988;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#30446;&#21069;&#20165;&#38480;&#20110;&#29420;&#31435;&#25110;&#32852;&#21512;&#22320;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#25110;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#32771;&#34385;&#34507;&#30333;&#36136;&#21644;&#20854;&#20182;&#22823;&#20998;&#23376;&#20043;&#38388;&#24120;&#35265;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMDiff&#65292;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;SE(3)-&#31163;&#25955;&#25193;&#25955;&#22122;&#22768;&#35774;&#35745;&#26680;&#37240;&#21644;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#32467;&#26500;&#22522;&#30784;&#36716;&#24405;&#22240;&#23376;&#35774;&#35745;&#21644;&#38750;&#32534;&#30721;RNA&#24207;&#21015;&#35774;&#35745;&#31561;&#26032;&#20852;&#30340;&#22823;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26412;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;&#20005;&#26684;&#26032;&#30340;&#22823;&#20998;&#23376;&#22797;&#21512;&#29289;&#29983;&#25104;&#35774;&#35745;&#22522;&#20934;&#26469;&#23637;&#31034;&#20102;MMDiff&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MMDiff&#33021;&#22815;&#25104;&#21151;&#29983;&#25104;&#24494;&#22411;RNA&#21644;&#21333;&#38142;DNA&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA mole
&lt;/p&gt;</description></item><item><title>D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06150</link><description>&lt;p&gt;
D-STGCNT:&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#23494;&#38598;&#26102;&#31354;&#22270;&#21367;&#31215;GRU&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06150
&lt;/p&gt;
&lt;p&gt;
D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35780;&#20272;&#26080;&#20020;&#24202;&#30417;&#30563;&#24773;&#20917;&#19979;&#24739;&#32773;&#36827;&#34892;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#30340;&#25361;&#25112;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#20379;&#36136;&#37327;&#35780;&#20998;&#20197;&#30830;&#20445;&#27491;&#30830;&#25191;&#34892;&#21644;&#33719;&#24471;&#26399;&#26395;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;Dense Spatio-Temporal Graph Conv-GRU Network with Transformer&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;STGCN&#21644;transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#27599;&#20010;&#24247;&#22797;&#38203;&#28860;&#20013;&#36215;&#20027;&#35201;&#20316;&#29992;&#30340;&#20851;&#33410;&#12290;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#29992;&#20110;&#24555;&#36895;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#24182;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#12290;transformer&#32534;&#30721;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20391;&#37325;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#20351;&#20854;&#22312;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#23454;&#36341;&#21644;&#21457;&#29616;&#26032;&#29983;&#29289;&#26631;&#35760;&#29289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#23637;&#26395;&#65292;&#35813;&#39046;&#22495;&#20250;&#28041;&#21450;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#21644;&#30740;&#31350;&#20219;&#21153;&#65292;&#20197;&#21450;&#19981;&#26029;&#22686;&#22810;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.06148</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#23383;&#21644;&#35745;&#31639;&#30149;&#29702;&#23398;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Digital and Computational Pathology. (arXiv:2401.06148v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#22312;&#33258;&#21160;&#21270;&#20020;&#24202;&#23454;&#36341;&#21644;&#21457;&#29616;&#26032;&#29983;&#29289;&#26631;&#35760;&#29289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#23637;&#26395;&#65292;&#35813;&#39046;&#22495;&#20250;&#28041;&#21450;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#21644;&#30740;&#31350;&#20219;&#21153;&#65292;&#20197;&#21450;&#19981;&#26029;&#22686;&#22810;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#32452;&#32455;&#20999;&#29255;&#30340;&#36827;&#23637;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#20013;&#28145;&#24230;&#23398;&#20064;&#31561;&#24555;&#36895;&#36827;&#23637;&#25512;&#21160;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#35813;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#20020;&#24202;&#35786;&#26029;&#65292;&#39044;&#27979;&#24739;&#32773;&#30340;&#39044;&#21518;&#21644;&#27835;&#30103;&#21453;&#24212;&#65292;&#24182;&#20174;&#32452;&#32455;&#22270;&#20687;&#20013;&#21457;&#29616;&#26032;&#30340;&#24418;&#24577;&#23398;&#29983;&#29289;&#26631;&#35760;&#29289;&#12290;&#19968;&#20123;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#29616;&#22312;&#33719;&#24471;&#25209;&#20934;&#29992;&#20110;&#36741;&#21161;&#20020;&#24202;&#35786;&#26029;&#65307;&#28982;&#32780;&#65292;&#25216;&#26415;&#38556;&#30861;&#20173;&#28982;&#23384;&#22312;&#65292;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#30740;&#31350;&#24037;&#20855;&#30340;&#24191;&#27867;&#20020;&#24202;&#24212;&#29992;&#21644;&#25972;&#21512;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#22312;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20020;&#24202;&#32456;&#28857;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#23398;&#36827;&#23637;&#65292;&#24182;&#24378;&#35843;&#36825;&#20123;&#21457;&#23637;&#22914;&#20309;&#23454;&#29616;&#20020;&#24202;&#23454;&#36341;&#30340;&#33258;&#21160;&#21270;&#21644;&#26032;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#26410;&#26469;&#23637;&#26395;&#65292;&#38543;&#30528;&#35813;&#39046;&#22495;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#20020;&#24202;&#21644;&#30740;&#31350;&#20219;&#21153;&#65292;&#20197;&#21450;&#26085;&#30410;&#22810;&#26679;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in digitizing tissue slides and the fast-paced progress in artificial intelligence, including deep learning, have boosted the field of computational pathology. This field holds tremendous potential to automate clinical diagnosis, predict patient prognosis and response to therapy, and discover new morphological biomarkers from tissue images. Some of these artificial intelligence-based systems are now getting approved to assist clinical diagnosis; however, technical barriers remain for their widespread clinical adoption and integration as a research tool. This Review consolidates recent methodological advances in computational pathology for predicting clinical end points in whole-slide images and highlights how these developments enable the automation of clinical practice and the discovery of new biomarkers. We then provide future perspectives as the field expands into a broader range of clinical and research tasks with increasingly diverse modalities of clinical data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23567;&#22411;&#26080;&#20154;&#26426;&#37197;&#22791;360&#24230;&#25668;&#20687;&#22836;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#25216;&#26415;&#65292;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32463;&#21382;&#20005;&#37325;&#30772;&#22351;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28779;&#28798;&#22330;&#26223;&#19979;&#32463;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;NeRFs&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06143</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#20390;&#23519;&#65306;&#21033;&#29992;&#26080;&#20154;&#26426;&#65292;360&#24230;&#25668;&#20687;&#22836;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#24357;&#21512;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and Neural Radiance Fields. (arXiv:2401.06143v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23567;&#22411;&#26080;&#20154;&#26426;&#37197;&#22791;360&#24230;&#25668;&#20687;&#22836;&#21644;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#25216;&#26415;&#65292;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#29983;&#25104;&#31934;&#30830;&#30340;3D&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#32463;&#21382;&#20005;&#37325;&#30772;&#22351;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28779;&#28798;&#22330;&#26223;&#19979;&#32463;&#36807;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;NeRFs&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28798;&#23475;&#24773;&#20917;&#19979;&#30340;&#25968;&#23383;&#24577;&#21183;&#24863;&#30693;&#39046;&#22495;&#20013;&#65292;&#20934;&#30830;&#30340;&#25968;&#23383;&#34920;&#31034;&#65292;&#22914;3D&#27169;&#22411;&#65292;&#21457;&#25381;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#25937;&#25588;&#38431;&#30340;&#23433;&#20840;&#65292;&#36890;&#24120;&#20250;&#37096;&#32626;&#26426;&#22120;&#20154;&#24179;&#21488;&#26469;&#29983;&#25104;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#23567;&#20110;30&#21400;&#31859;&#30340;&#32039;&#20945;&#22411;&#26080;&#20154;&#26426;&#37197;&#22791;360&#24230;&#25668;&#20687;&#22836;&#30340;&#33021;&#21147;&#19982;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#12290;NeRF&#26159;&#19968;&#31181;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21033;&#29992;2D&#22270;&#20687;&#25512;&#26029;&#20219;&#20309;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#28982;&#21518;&#26681;&#25454;&#38656;&#35201;&#20174;&#21508;&#20010;&#35282;&#24230;&#21512;&#25104;&#23427;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#32463;&#21382;&#20102;&#24040;&#22823;&#30772;&#22351;&#30340;&#22478;&#24066;&#29615;&#22659;&#65292;&#20854;&#20013;&#24314;&#31569;&#29289;&#30340;&#32467;&#26500;&#23436;&#25972;&#24615;&#24050;&#32463;&#21463;&#25439;&#21040;&#26080;&#27861;&#36827;&#20837;&#30340;&#31243;&#24230;-&#36890;&#24120;&#22312;&#22320;&#38663;&#21518;&#21644;&#20005;&#37325;&#28779;&#28798;&#21518;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#36817;&#30340;&#28779;&#28798;&#22330;&#26223;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;NeRFs&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#20005;&#37325;&#30772;&#22351;&#30340;&#29615;&#22659;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of digital situational awareness during disaster situations, accurate digital representations, like 3D models, play an indispensable role. To ensure the safety of rescue teams, robotic platforms are often deployed to generate these models. In this paper, we introduce an innovative approach that synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller than 30 cm, equipped with 360 degree cameras and the advances of Neural Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D representation of any scene using 2D images and then synthesize it from various angles upon request. This method is especially tailored for urban environments which have experienced significant destruction, where the structural integrity of buildings is compromised to the point of barring entry-commonly observed post-earthquakes and after severe fires. We have tested our approach through recent post-fire scenario, underlining the efficacy of NeRFs even in ch
&lt;/p&gt;</description></item><item><title>QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06137</link><description>&lt;p&gt;
QuasiNet: &#19968;&#31181;&#20855;&#26377;&#21487;&#35757;&#32451;&#20056;&#31215;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06137
&lt;/p&gt;
&lt;p&gt;
QuasiNet&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20056;&#31215;&#23618;&#35299;&#20915;&#20102;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#25910;&#25947;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#20284;XOR&#25110;&#22855;&#20598;&#26657;&#39564;&#31561;&#38590;&#39064;&#30340;&#23567;&#35268;&#27169;&#38544;&#34255;&#31070;&#32463;&#20803;&#19979;&#21482;&#33021;&#23454;&#29616;&#26377;&#38480;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#25104;&#21151;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21463;&#29616;&#26377;&#20855;&#26377;&#25152;&#35859;&#20056;&#31215;&#31070;&#32463;&#20803;&#21644;&#30001;&#32463;&#20856;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#35268;&#21017;&#21551;&#21457;&#65292;&#20248;&#38597;&#22320;&#35299;&#20915;&#20102;&#20114;&#26021;&#24773;&#20917;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#20855;&#26377;&#39044;&#35774;&#19988;&#19981;&#21487;&#35843;&#33410;&#26435;&#37325;&#30340;&#20056;&#31215;&#31070;&#32463;&#20803;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#20803;&#20056;&#31215;&#23618;&#20063;&#33021;&#22815;&#23398;&#20064;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#25104;&#21151;&#29575;&#19982;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#22312;&#21069;&#36848;&#38382;&#39064;&#21644;&#20854;&#20182;&#38590;&#39064;&#65288;&#22914;&#20004;&#20010;&#34746;&#26059;&#65289;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#26356;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#22312; RLHF &#20013;&#35299;&#20915;&#22870;&#21169;&#24314;&#27169;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#24182;&#35299;&#20915;&#22312;&#29305;&#23450;&#20998;&#24067;&#25968;&#25454;&#19978;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06080</link><description>&lt;p&gt;
RLHF&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31192;&#23494; Part II: &#22870;&#21169;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#22312; RLHF &#20013;&#35299;&#20915;&#22870;&#21169;&#24314;&#27169;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#24182;&#35299;&#20915;&#22312;&#29305;&#23450;&#20998;&#24067;&#25968;&#25454;&#19978;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#26356;&#26377;&#24110;&#21161;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#22870;&#21169;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20154;&#31867;&#20559;&#22909;&#30340;&#20195;&#29702;&#65292;&#20197;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#20013;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#22952;&#30861;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#65288;2&#65289;&#22312;&#29305;&#23450;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#36845;&#20195;RLHF&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20174;&#25968;&#25454;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#22522;&#20110;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;DINO&#27169;&#22411;&#20013;&#26500;&#24314;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#25163;&#26415;&#29305;&#23450;&#30340;&#28145;&#24230;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06013</link><description>&lt;p&gt;
&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#28145;&#24230;&#20272;&#35745;&#30340;&#25163;&#26415;DINO&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;DINO&#27169;&#22411;&#20013;&#26500;&#24314;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#25163;&#26415;&#29305;&#23450;&#30340;&#28145;&#24230;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#23545;&#20110;&#19977;&#32500;&#37325;&#24314;&#12289;&#25163;&#26415;&#23548;&#33322;&#21644;&#22686;&#24378;&#29616;&#23454;&#21487;&#35270;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#65288;&#22914;DINOv2&#65289;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20854;&#22312;&#21307;&#23398;&#21644;&#25163;&#26415;&#29305;&#23450;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#28145;&#24230;&#20272;&#35745;&#30340;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Surgical-DINO&#65292;&#36825;&#26159;DINOv2&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#20302;&#31209;&#36866;&#37197;&#12290;&#25105;&#20204;&#22312;DINO&#20013;&#26500;&#24314;&#20102;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;DINO&#20013;&#65292;&#20197;&#36866;&#24212;&#25163;&#26415;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20923;&#32467;&#20102;DINO&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#35270;&#35273;&#34920;&#31034;&#33021;&#21147;&#65292;&#21482;&#20248;&#21270;&#20102;LoRA&#23618;&#21644;&#28145;&#24230;&#35299;&#30721;&#22120;&#65292;&#20197;&#25972;&#21512;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;ex...
&lt;/p&gt;
&lt;p&gt;
Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.05437</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#25345;&#32493;&#25910;&#38598;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#29992;&#20110;&#25512;&#26029;&#20010;&#20307;&#30340;&#34892;&#20026;&#65292;&#22914;&#30561;&#30496;&#12289;&#20307;&#21147;&#27963;&#21160;&#21644;&#24773;&#32490;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#20852;&#36259;&#21644;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#27880;&#37322;&#26377;&#38480;&#65292;&#24314;&#27169;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#21487;&#31359;&#25140;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;10&#20010;&#29983;&#29702;&#21644;&#34892;&#20026;&#20449;&#21495;&#30340;&#21464;&#21270;&#29575;&#19981;&#21516;&#30340;&#25513;&#30721;&#27604;&#29575;&#65292;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21333;&#35843;&#20449;&#21495;&#21017;&#19981;&#28982;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22635;&#20805;&#31574;&#30053;&#21644;&#25513;&#30721;&#27604;&#29575;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04531</link><description>&lt;p&gt;
MERA: &#20420;&#35821;LLM&#32508;&#21512;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23835;&#36215;&#24341;&#20154;&#27880;&#30446;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;LM&#22312;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#23637;&#31034;&#20102;&#25552;&#21319;&#65292;&#24182;&#19988;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#23450;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#21644;LM&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;LM&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#30456;&#20851;&#39118;&#38505;&#20173;&#38656;&#26356;&#22909;&#22320;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;&#20420;&#35821;&#22810;&#27169;&#24577;&#26550;&#26500;&#35780;&#20272;&#65288;MERA&#65289;&#25351;&#23548;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20197;&#20420;&#35821;&#20026;&#23548;&#21521;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#25490;&#38500;&#25968;&#25454;&#27844;&#28431;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2401.04003</link><description>&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#22312;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#19979;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2401.04003v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;LTL&#35268;&#33539;&#20013;&#65292;&#35813;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23558;&#25628;&#32034;&#31354;&#38388;&#25286;&#20998;&#20026;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#26426;&#22120;&#20154;&#35268;&#21010;&#19982;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#20027;&#35201;&#26159;&#22522;&#20110;&#38024;&#23545;&#20010;&#20307;&#25110;&#32676;&#20307;&#26426;&#22120;&#20154;&#30340;&#21333;&#19968;&#20844;&#24335;&#12290;&#20294;&#38543;&#30528;&#20219;&#21153;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;LTL&#20844;&#24335;&#19981;&#21487;&#36991;&#20813;&#22320;&#21464;&#24471;&#20887;&#38271;&#65292;&#20351;&#35299;&#37322;&#21644;&#35268;&#33539;&#29983;&#25104;&#21464;&#24471;&#22797;&#26434;&#65292;&#21516;&#26102;&#36824;&#23545;&#35268;&#21010;&#22120;&#30340;&#35745;&#31639;&#33021;&#21147;&#36896;&#25104;&#21387;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#32467;&#26500;&#21040;&#20855;&#26377;&#35821;&#27861;&#21644;&#35821;&#20041;&#38656;&#27714;&#30340;LTL&#35268;&#33539;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#27604;&#25153;&#24179;&#35268;&#33539;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#35745;&#21010;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#35268;&#21010;&#12290;&#25628;&#32034;&#31354;&#38388;&#30001;&#26494;&#25955;&#30456;&#20114;&#36830;&#25509;&#30340;&#23376;&#31354;&#38388;&#36817;&#20284;&#34920;&#31034;&#65292;&#27599;&#20010;&#23376;&#31354;&#38388;&#23545;&#24212;&#19968;&#20010;LTL&#35268;&#33539;&#12290;&#25628;&#32034;&#20027;&#35201;&#21463;&#38480;&#20110;&#21333;&#20010;&#23376;&#31354;&#38388;&#65292;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research into robotic planning with temporal logic specifications, notably Linear Temporal Logic (LTL), was largely based on singular formulas for individual or groups of robots. But with increasing task complexity, LTL formulas unavoidably grow lengthy, complicating interpretation and specification generation, and straining the computational capacities of the planners. By leveraging the intrinsic structure of tasks, we introduced a hierarchical structure to LTL specifications with requirements on syntax and semantics, and proved that they are more expressive than their flat counterparts. Second, we employ a search-based approach to synthesize plans for a multi-robot system, accomplishing simultaneous task allocation and planning. The search space is approximated by loosely interconnected sub-spaces, with each sub-space corresponding to one LTL specification. The search is predominantly confined to a single sub-space, transitioning to another sub-space under certain conditions, de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01055</link><description>&lt;p&gt;
LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20027;&#27969;&#30340;LLM&#65288;&#22914;LLaMA&#65289;&#26159;&#22522;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#24635;&#35745;&#32791;&#36153;&#20102;1440&#20010;GPU&#23567;&#26102;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35832;&#22914;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#65306;C-Eval&#12289;MMLU&#12289;AGI-Eval&#21644;GAOKAO-Bench&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#35832;&#22914;...
&lt;/p&gt;
&lt;p&gt;
In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#25351;&#26631;&#12289;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#12289;&#39044;&#27979;&#25351;&#26631;&#12289;&#25490;&#24207;&#25351;&#26631;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2312.16015</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Evaluation Techniques for Recommendation Systems. (arXiv:2312.16015v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#32508;&#21512;&#30340;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#25351;&#26631;&#12289;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#12289;&#39044;&#27979;&#25351;&#26631;&#12289;&#25490;&#24207;&#25351;&#26631;&#21644;&#19994;&#21153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#23545;&#20110;&#29992;&#25143;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#21644;&#28385;&#24847;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#36825;&#20123;&#25512;&#33616;&#31995;&#32479;&#36234;&#26469;&#36234;&#24433;&#21709;&#29992;&#25143;&#30340;&#36873;&#25321;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25216;&#26415;&#24615;&#33021;&#65292;&#32780;&#21464;&#24471;&#23545;&#20110;&#19994;&#21153;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#26631;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#29305;&#24615;&#65292;&#27599;&#20010;&#25351;&#26631;&#19987;&#38376;&#25429;&#25417;&#31995;&#32479;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#30340;&#25351;&#26631;&#65306;&#30456;&#20284;&#24615;&#25351;&#26631;&#65306;&#29992;&#20110;&#37327;&#21270;&#22522;&#20110;&#20869;&#23481;&#30340;&#36807;&#28388;&#26426;&#21046;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#35780;&#20272;&#21327;&#21516;&#36807;&#28388;&#25216;&#26415;&#30340;&#20934;&#30830;&#24615;&#65307;&#20505;&#36873;&#29983;&#25104;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#31995;&#32479;&#26377;&#25928;&#22320;&#35782;&#21035;&#24191;&#27867;&#20294;&#30456;&#20851;&#30340;&#39033;&#30446;&#30340;&#33021;&#21147;&#65307;&#39044;&#27979;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#30340;&#29992;&#25143;&#20559;&#22909;&#30340;&#20934;&#30830;&#24615;&#65307;&#25490;&#24207;&#25351;&#26631;&#65306;&#29992;&#20110;&#35780;&#20272;&#25512;&#33616;&#39034;&#24207;&#30340;&#26377;&#25928;&#24615;&#65307;&#19994;&#21153;&#25351;&#26631;&#65306;&#29992;&#20110;&#23545;&#40784;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms. As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success. This paper addresses the multifaceted nature of recommendations system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance. We discuss  * Similarity Metrics: to quantify the precision of content-based filtering mechanisms and assess the accuracy of collaborative filtering techniques.  * Candidate Generation Metrics: to evaluate how effectively the system identifies a broad yet relevant range of items.  * Predictive Metrics: to assess the accuracy of forecasted user preferences.  * Ranking Metrics: to evaluate the effectiveness of the order in which recommendations are presented.  * Business Metrics: to align the performance of the recommendat
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#27973;&#23618;PQC&#21644;&#21487;&#35757;&#32451;&#30340;&#38376;&#23618;&#26469;&#21019;&#24314;&#37327;&#23376;&#24577;&#21644;&#23398;&#20064;&#32416;&#32544;&#12290;&#19982;QMeL&#30456;&#27604;&#65292;QPMeL&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.01655</link><description>&lt;p&gt;
&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;: &#39640;&#25928;&#32463;&#20856;&#23398;&#20064;&#30340;&#37327;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Quantum Polar Metric Learning: Efficient Classically Learned Quantum Embeddings. (arXiv:2312.01655v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#27973;&#23618;PQC&#21644;&#21487;&#35757;&#32451;&#30340;&#38376;&#23618;&#26469;&#21019;&#24314;&#37327;&#23376;&#24577;&#21644;&#23398;&#20064;&#32416;&#32544;&#12290;&#19982;QMeL&#30456;&#27604;&#65292;QPMeL&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#22312;&#32463;&#20856;&#25968;&#25454;&#33539;&#30068;&#20013;&#34920;&#29616;&#20986;&#26497;&#26377;&#28508;&#21147;&#30340;&#32467;&#26524;&#65292;&#21019;&#24314;&#20102;&#20998;&#31163;&#26126;&#26174;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#36825;&#20010;&#24819;&#27861;&#20063;&#34987;&#24212;&#29992;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#65292;&#36890;&#36807;&#37327;&#23376;&#24230;&#37327;&#23398;&#20064;(QMeL)&#12290;QMeL&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#39318;&#20808;&#20351;&#29992;&#32463;&#20856;&#27169;&#22411;&#23558;&#25968;&#25454;&#21387;&#32553;&#20197;&#36866;&#24212;&#26377;&#38480;&#25968;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#65292;&#28982;&#21518;&#20351;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;(PQC)&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21019;&#24314;&#26356;&#22909;&#30340;&#20998;&#31163;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;(NISQ)&#35774;&#22791;&#19978;&#65292;QMeL&#35299;&#20915;&#26041;&#26696;&#23548;&#33268;&#30005;&#36335;&#23485;&#24230;&#21644;&#28145;&#24230;&#36739;&#22823;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20165;&#21253;&#21547;$R_y$&#21644;$R_z$&#38376;&#30340;&#27973;&#23618;PQC&#21019;&#24314;&#37327;&#23376;&#24577;&#65292;&#24182;&#21033;&#29992;&#21487;&#35757;&#32451;&#30340;$ZZ(\theta)$&#38376;&#23618;&#23398;&#20064;&#32416;&#32544;&#12290;&#30005;&#36335;&#36824;&#36890;&#36807;SWAP&#27979;&#35797;&#35745;&#31639;&#20445;&#30495;&#24230;&#65292;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#20445;&#30495;&#24230;&#19977;&#20803;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#65292;&#29992;&#20110;&#21516;&#26102;&#35757;&#32451;&#32463;&#20856;&#21644;&#37327;&#23376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning has recently shown extremely promising results in the classical data domain, creating well-separated feature spaces. This idea was also adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL consists of a 2 step process with a classical model to compress the data to fit into the limited number of qubits, then train a Parameterized Quantum Circuit(PQC) to create better separation in Hilbert Space. However, on Noisy Intermediate Scale Quantum (NISQ) devices. QMeL solutions result in high circuit width and depth, both of which limit scalability. We propose Quantum Polar Metric Learning (QPMeL) that uses a classical model to learn the parameters of the polar form of a qubit. We then utilize a shallow PQC with $R_y$ and $R_z$ gates to create the state and a trainable layer of $ZZ(\theta)$-gates to learn entanglement. The circuit also computes fidelity via a SWAP Test for our proposed Fidelity Triplet Loss function, used to train both classical and quantum 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.17431</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25509;&#22320;&#65306;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#23558;&#22522;&#30784;&#27169;&#22411;&#25509;&#22320;&#65292;&#20197;&#35299;&#20915;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#37322;&#25918;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#34892;&#19994;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24191;&#27867;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32534;&#30721;&#30340;Foundation Models&#65288;FMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36890;&#36807;&#23558;FMs&#36866;&#24212;&#20110;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#25110;&#22686;&#21152;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#23545;&#20854;&#36827;&#34892;&#25509;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#20805;&#20998;&#21457;&#25381;FMs&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25509;&#22320;FMs&#38754;&#20020;&#30528;&#22810;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12289;&#25968;&#25454;&#38544;&#31169;&#12289;&#27169;&#22411;&#24322;&#26500;&#24615;&#21644;&#27169;&#22411;&#25152;&#26377;&#26435;&#12290;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65288;FTL&#65289;&#65292;&#21363;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#23545;&#36890;&#36807;FTL-FM&#21033;&#29992;FMs&#36827;&#34892;&#25509;&#22320;&#30340;&#38656;&#27714;&#24378;&#28872;&#22686;&#38271;&#12290;&#21463;&#21040;FTL-FM&#30740;&#31350;&#30340;&#24378;&#21170;&#22686;&#38271;&#21644;FTL-FM&#23545;&#24037;&#19994;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;FTL-FM&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24314;&#31435;FMs&#30340;&#25509;&#22320;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.07946</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#26032;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#20063;&#36234;&#26469;&#36234;&#26222;&#21450;&#12290;&#36825;&#20123;&#26694;&#26550;&#21033;&#29992;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#33410;&#33021;&#30340;&#35774;&#22791;&#38388;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#20063;&#21152;&#22823;&#20102;&#23545;&#24378;&#22823;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;FL&#23433;&#20840;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20294;&#26159;&#23545;&#20110;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#23545;&#25239;&#33410;&#28857;&#37096;&#32626;&#30340;&#20316;&#29992;&#20173;&#28982;&#36739;&#23569;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#23545;&#25239;&#37096;&#32626;&#31574;&#30053;&#19979;&#20998;&#24067;&#24335;FL&#30340;&#24615;&#33021;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#22522;&#32447;&#31574;&#30053;&#26469;&#37096;&#32626;&#23545;&#25239;&#33410;&#28857;&#65306;&#38543;&#26426;&#37096;&#32626;&#21644;&#22522;&#20110;&#32593;&#32476;&#20013;&#24515;&#24615;&#30340;&#37096;&#32626;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#25239;&#33410;&#28857;&#20043;&#38388;&#30340;&#24179;&#22343;&#32593;&#32476;&#36317;&#31163;&#65292;&#20248;&#20808;&#32771;&#34385;&#23545;&#25239;&#30340;&#20998;&#25955;&#24615;&#32780;&#19981;&#26159;&#20013;&#24515;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Federated Learning (FL) grows in popularity, new decentralized frameworks are becoming widespread. These frameworks leverage the benefits of decentralized environments to enable fast and energy-efficient inter-device communication. However, this growing popularity also intensifies the need for robust security measures. While existing research has explored various aspects of FL security, the role of adversarial node placement in decentralized networks remains largely unexplored. This paper addresses this gap by analyzing the performance of decentralized FL for various adversarial placement strategies when adversaries can jointly coordinate their placement within a network. We establish two baseline strategies for placing adversarial node: random placement and network centrality-based placement. Building on this foundation, we propose a novel attack algorithm that prioritizes adversarial spread over adversarial centrality by maximizing the average network distance between adversaries.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2311.03865</link><description>&lt;p&gt;
&#24403;&#20844;&#24179;&#24615;&#36935;&#35265;&#38544;&#31169;&#65306;&#36890;&#36807;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#25506;&#32034;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20844;&#24179;&#20108;&#20998;&#31867;&#22120;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#25581;&#31034;&#20102;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26080;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20844;&#24179;&#24615;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#38024;&#23545;&#20855;&#26377;&#27495;&#35270;&#34892;&#20026;&#30340;&#26377;&#20559;&#27169;&#22411;&#30340;&#20844;&#24179;&#26041;&#27861;&#65292;&#20197;&#36798;&#21040;&#20844;&#24179;&#39044;&#27979;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#20123;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#25968;&#25512;&#26029;&#20986;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#38024;&#23545;&#20844;&#24179;&#22686;&#24378;&#27169;&#22411;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#26080;&#25928;&#30340;&#12290;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#35757;&#32451;&#30340;&#27169;&#22411;&#36864;&#21270;&#20026;&#31616;&#21333;&#30340;&#38408;&#20540;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#25915;&#20987;&#24615;&#33021;&#38477;&#20302;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20844;&#24179;&#24615;&#26041;&#27861;&#24448;&#24448;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22823;&#22810;&#25968;&#23376;&#32676;&#20307;&#30340;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#25552;&#39640;&#20102;&#25104;&#21151;&#25915;&#20987;&#30340;&#38590;&#24230;&#65292;&#21516;&#26102;&#25193;&#22823;&#20102;&#25104;&#21592;&#21644;&#38750;&#25104;&#21592;&#25968;&#25454;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon th
&lt;/p&gt;</description></item><item><title>Inner-IoU&#25439;&#22833;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#36741;&#21161;&#36793;&#30028;&#26694;&#35745;&#31639;&#25439;&#22833;&#65292;&#26377;&#25928;&#21152;&#36895;&#20102;&#36793;&#30028;&#26694;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#20132;&#24182;&#27604;&#25439;&#22833;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.02877</link><description>&lt;p&gt;
Inner-IoU: &#20511;&#21161;&#36741;&#21161;&#36793;&#30028;&#26694;&#26356;&#26377;&#25928;&#30340;&#20132;&#24182;&#27604;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v4 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02877
&lt;/p&gt;
&lt;p&gt;
Inner-IoU&#25439;&#22833;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#36741;&#21161;&#36793;&#30028;&#26694;&#35745;&#31639;&#25439;&#22833;&#65292;&#26377;&#25928;&#21152;&#36895;&#20102;&#36793;&#30028;&#26694;&#22238;&#24402;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#20132;&#24182;&#27604;&#25439;&#22833;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26816;&#27979;&#22120;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36793;&#30028;&#26694;&#22238;&#24402;&#65288;BBR&#65289;&#25439;&#22833;&#20989;&#25968;&#19981;&#26029;&#26356;&#26032;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;IoU&#30340;BBR&#20173;&#28982;&#30528;&#37325;&#20110;&#36890;&#36807;&#28155;&#21152;&#26032;&#30340;&#25439;&#22833;&#39033;&#26469;&#21152;&#36895;&#25910;&#25947;&#65292;&#24573;&#35270;&#20102;IoU&#25439;&#22833;&#39033;&#26412;&#36523;&#30340;&#23616;&#38480;&#24615;&#12290;&#23613;&#31649;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;IoU&#25439;&#22833;&#21487;&#20197;&#26377;&#25928;&#25551;&#36848;&#36793;&#30028;&#26694;&#22238;&#24402;&#30340;&#29366;&#24577;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#19981;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#26816;&#27979;&#22120;&#21644;&#26816;&#27979;&#20219;&#21153;&#33258;&#25105;&#35843;&#25972;&#65292;&#24182;&#19988;&#27809;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#24615;&#12290;&#22522;&#20110;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;BBR&#27169;&#22411;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65306;&#21306;&#20998;&#19981;&#21516;&#30340;&#22238;&#24402;&#26679;&#26412;&#65292;&#24182;&#21033;&#29992;&#19981;&#21516;&#23610;&#24230;&#30340;&#36741;&#21161;&#36793;&#30028;&#26694;&#35745;&#31639;&#25439;&#22833;&#65292;&#21487;&#20197;&#26377;&#25928;&#21152;&#36895;&#36793;&#30028;&#26694;&#22238;&#24402;&#36807;&#31243;&#12290;&#23545;&#20110;&#39640;IoU&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#36739;&#23567;&#30340;&#36741;&#21161;&#36793;&#30028;&#26694;&#35745;&#31639;&#25439;&#22833;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#32780;&#23545;&#20110;&#20302;IoU&#30340;&#26679;&#26412;&#65292;&#21017;&#36866;&#21512;&#20351;&#29992;&#36739;&#22823;&#30340;&#36741;&#21161;&#36793;&#30028;&#26694;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Inner-IoU&#25439;&#22833;&#65292;&#23427;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
With the rapid development of detectors, Bounding Box Regression (BBR) loss function has constantly updated and optimized. However, the existing IoU-based BBR still focus on accelerating convergence by adding new loss terms, ignoring the limitations of IoU loss term itself. Although theoretically IoU loss can effectively describe the state of bounding box regression,in practical applications, it cannot adjust itself according to different detectors and detection tasks, and does not have strong generalization. Based on the above, we first analyzed the BBR model and concluded that distinguishing different regression samples and using different scales of auxiliary bounding boxes to calculate losses can effectively accelerate the bounding box regression process. For high IoU samples, using smaller auxiliary bounding boxes to calculate losses can accelerate convergence, while larger auxiliary bounding boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which calculates 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffDub&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#65292;&#22312;&#20445;&#30041;&#20854;&#20182;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#32534;&#30721;&#21644;&#38754;&#37096;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#37197;&#38899;&#30340;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01811</link><description>&lt;p&gt;
DiffDub: &#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#30340;&#36890;&#29992;&#28436;&#21592;&#35270;&#35273;&#37197;&#38899;
&lt;/p&gt;
&lt;p&gt;
DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffDub&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#20462;&#22797;&#28210;&#26579;&#22120;&#65292;&#22312;&#20445;&#30041;&#20854;&#20182;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#32534;&#30721;&#21644;&#38754;&#37096;&#23450;&#20301;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#37197;&#38899;&#30340;&#36136;&#37327;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#36890;&#29992;&#30340;&#35270;&#35273;&#37197;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#21019;&#26032;&#30475;&#21040;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#33539;&#20363;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#20013;&#38388;&#34920;&#31034;&#35299;&#32806;&#28210;&#26579;&#21644;&#21475;&#22411;&#21516;&#27493;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31895;&#31961;&#30340;&#26631;&#35760;&#28857;&#25110;&#23616;&#38480;&#20110;&#21333;&#19968;&#30340;&#35828;&#35805;&#20154;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffDub&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#37197;&#38899;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20462;&#22797;&#28210;&#26579;&#22120;&#21644;&#19968;&#20010;&#33945;&#29256;&#26469;&#32472;&#21046;Diffusion&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#30028;&#23450;&#21487;&#32534;&#36753;&#21306;&#22495;&#21644;&#26410;&#26356;&#25913;&#21306;&#22495;&#12290;&#36825;&#26679;&#21487;&#20197;&#22312;&#20445;&#30041;&#20854;&#20313;&#37096;&#20998;&#30340;&#21516;&#26102;&#26080;&#32541;&#22635;&#20805;&#19979;&#21322;&#33080;&#21306;&#22495;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#35821;&#20041;&#32534;&#30721;&#22120;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#25429;&#25417;&#39640;&#32423;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#24573;&#30053;&#20102;&#38754;&#37096;&#23450;&#20301;&#65292;&#23548;&#33268;&#24103;&#38388;&#21475;&#25110;&#40763;&#23376;&#25238;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#21151;&#33021;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Generating high-quality and person-generic visual dubbing remains a challenge. Recent innovation has seen the advent of a two-stage paradigm, decoupling the rendering and lip synchronization process facilitated by intermediate representation as a conduit. Still, previous methodologies rely on rough landmarks or are confined to a single speaker, thus limiting their performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We first craft the Diffusion auto-encoder by an inpainting renderer incorporating a mask to delineate editable zones and unaltered regions. This allows for seamless filling of the lower-face region while preserving the remaining parts. Throughout our experiments, we encountered several challenges. Primarily, the semantic encoder lacks robustness, constricting its ability to capture high-level features. Besides, the modeling ignored facial positioning, causing mouth or nose jitters across frames. To tackle these issues, we employ versatile strategies, inc
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02772</link><description>&lt;p&gt;
&#29992;&#20110;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#12290;SAF&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#21069;&#21521;&#36807;&#31243;&#20013;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#19982;Spike Representation&#21644;OTTT&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;SNNs&#35757;&#32451;&#20013;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#33033;&#20914;&#32047;&#31215;&#36716;&#21457;&#65288;SAF&#65289;&#12290;&#24050;&#30693;SNNs&#20855;&#26377;&#39640;&#33021;&#25928;&#20294;&#38590;&#20197;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;&#35768;&#22810;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#38388;&#19978;&#30340;&#22312;&#32447;&#35757;&#32451;&#65288;OTTT&#65289;&#26159;&#19968;&#31181;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25233;&#21046;&#20869;&#23384;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;GPU&#19978;&#39640;&#25928;&#35745;&#31639;&#65292;OTTT&#38656;&#35201;&#36827;&#34892;&#33033;&#20914;&#24207;&#21015;&#25805;&#20316;&#21644;&#33033;&#20914;&#24207;&#21015;&#21152;&#26435;&#27714;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;OTTT&#19982;Spike Representation&#65288;&#21478;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65289;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#65292;&#20294;&#19982;Spike Representation&#30340;&#29702;&#35770;&#19968;&#33268;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21363;SAF&#21487;&#20197;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20943;&#23569;&#19968;&#21322;&#30340;&#25805;&#20316;&#27425;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;SAF&#20998;&#21035;&#19982;Spike Representation&#21644;OTTT&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#35748;&#20102;......
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a new paradigm for training spiking neural networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are energy-efficient but difficult to train. Consequently, many researchers have proposed various methods to solve this problem, among which online training through time (OTTT) is a method that allows inferring at each time step while suppressing the memory cost. However, to compute efficiently on GPUs, OTTT requires operations with spike trains and weighted summation of spike trains during forwarding. In addition, OTTT has shown a relationship with the Spike Representation, an alternative training method, though theoretical agreement with Spike Representation has yet to be proven. Our proposed method can solve these problems; namely, SAF can halve the number of operations during the forward process, and it can be theoretically proven that SAF is consistent with the Spike Representation and OTTT, respectively. Furthermore, we confirmed the a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#35013;&#37197;&#36136;&#37327;&#24182;&#20248;&#21270;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.13745</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Computer Vision Technology for Robotized Wire Harness Assembly. (arXiv:2309.13745v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#20154;&#21270;&#32447;&#26463;&#35013;&#37197;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#35013;&#37197;&#36136;&#37327;&#24182;&#20248;&#21270;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#26463;&#22312;&#29616;&#20195;&#27773;&#36710;&#20013;&#26159;&#30005;&#23376;&#31995;&#32479;&#30340;&#37325;&#35201;&#30828;&#20214;&#12290;&#38543;&#30528;&#27773;&#36710;&#34892;&#19994;&#21521;&#30005;&#21160;&#21270;&#21644;&#33258;&#21160;&#39550;&#39542;&#30340;&#36716;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#27773;&#36710;&#30005;&#23376;&#20135;&#21697;&#36127;&#36131;&#33021;&#37327;&#20256;&#36755;&#21644;&#23433;&#20840;&#20851;&#38190;&#21151;&#33021;&#65292;&#22914;&#25805;&#32437;&#12289;&#39550;&#39542;&#36741;&#21161;&#21644;&#23433;&#20840;&#31995;&#32479;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#20174;&#23433;&#20840;&#35282;&#24230;&#23545;&#27773;&#36710;&#32447;&#26463;&#25552;&#20986;&#26356;&#39640;&#30340;&#35201;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;&#22312;&#36710;&#36742;&#20013;&#30340;&#39640;&#36136;&#37327;&#32447;&#26463;&#35013;&#37197;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#32447;&#26463;&#35013;&#37197;&#25805;&#20316;&#20173;&#30001;&#29087;&#32451;&#24037;&#20154;&#25163;&#21160;&#23436;&#25104;&#65292;&#20854;&#20013;&#19968;&#20123;&#25163;&#21160;&#24037;&#24207;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#36136;&#37327;&#25511;&#21046;&#21644;&#20154;&#20307;&#24037;&#31243;&#23398;&#12290;&#34892;&#19994;&#20013;&#20063;&#25345;&#32493;&#38656;&#27714;&#25552;&#39640;&#31454;&#20105;&#21147;&#24182;&#33719;&#24471;&#24066;&#22330;&#20221;&#39069;&#12290;&#22240;&#27492;&#65292;&#24076;&#26395;&#33021;&#22312;&#25552;&#39640;&#20154;&#20307;&#24037;&#31243;&#23398;&#21644;&#20248;&#21270;&#21171;&#21160;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#35777;&#35013;&#37197;&#36136;&#37327;&#12290;&#26426;&#22120;&#20154;&#21270;&#35013;&#37197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wire harnesses are essential hardware for electronic systems in modern automotive vehicles. With a shift in the automotive industry towards electrification and autonomous driving, more and more automotive electronics are responsible for energy transmission and safety-critical functions such as maneuvering, driver assistance, and safety system. This paradigm shift places more demand on automotive wiring harnesses from the safety perspective and stresses the greater importance of high-quality wire harness assembly in vehicles. However, most of the current operations of wire harness assembly are still performed manually by skilled workers, and some of the manual processes are problematic from different perspectives, such as quality control and ergonomics. There is also a persistent demand in the industry to increase competitiveness and gain market share. Hence, assuring assembly quality while improving ergonomics and optimizing labor costs is desired. Robotized assembly, accomplished by r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Whisper&#27169;&#22411;&#20174;&#26410;&#26631;&#27880;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#19982;&#20154;&#24037;&#26631;&#27880;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08535</link><description>&lt;p&gt;
&#29992;Whisper&#27169;&#22411;&#20174;&#33258;&#21160;&#26631;&#27880;&#20013;&#33719;&#24471;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Visual Speech Recognition for Low-resource Languages with Automatic Labels From Whisper Model. (arXiv:2309.08535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Whisper&#27169;&#22411;&#20174;&#26410;&#26631;&#27880;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#33258;&#21160;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#19982;&#20154;&#24037;&#26631;&#27880;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;(VSR)&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#26631;&#27880;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#19982;&#20043;&#21069;&#35797;&#22270;&#36890;&#36807;&#20174;&#20854;&#20182;&#35821;&#35328;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#35821;&#35328;&#30340;VSR&#24615;&#33021;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#19981;&#21516;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;Whisper&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#35821;&#35328;&#35782;&#21035;&#21644;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;&#23427;&#29992;&#20110;&#36807;&#28388;&#25152;&#38656;&#35821;&#35328;&#30340;&#25968;&#25454;&#65292;&#24182;&#20174;&#26410;&#27880;&#37322;&#30340;&#22810;&#35821;&#35328;&#35270;&#21548;&#25968;&#25454;&#27744;&#20013;&#36716;&#24405;&#26631;&#31614;&#12290;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#33258;&#21160;&#26631;&#31614;&#21644;&#20154;&#24037;&#26631;&#27880;&#26631;&#31614;&#35757;&#32451;&#30340;VSR&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#19981;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#36798;&#21040;&#19982;&#20154;&#24037;&#27880;&#37322;&#26631;&#31614;&#30456;&#20284;&#30340;VSR&#24615;&#33021;&#12290;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#30340;&#36807;&#31243;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#22823;&#35268;&#27169;&#26410;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11086</link><description>&lt;p&gt;
&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;&#21464;&#20998;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#26469;&#20248;&#21270;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#19978;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#21457;&#23637;&#23545;&#20110; NISQ &#35745;&#31639;&#26426;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#31639;&#27861;&#38656;&#35201;&#30701;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#36825;&#31181;&#30005;&#36335;&#26356;&#26131;&#20110;&#22312;&#36817;&#26399;&#30828;&#20214;&#19978;&#23454;&#29616;&#65292;&#20063;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#26377;&#36259;&#30340;&#31639;&#27861;&#26159;&#25152;&#35859;&#30340;&#21464;&#20998;&#23545;&#35282;&#21270;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#31639;&#27861;&#23376;&#20363;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#22788;&#29702;&#20197;&#37327;&#23376;&#29366;&#24577;&#32534;&#30721;&#30340;&#25968;&#25454;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#36776;&#37327;&#23376;&#24577;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#31995;&#32479;&#30340;&#32416;&#32544;&#24615;&#36136;&#65292;&#25110;&#32773;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22312;&#37327;&#23376;&#29366;&#24577;&#23545;&#35282;&#21270;&#20219;&#21153;&#20013;&#25152;&#38656;&#30005;&#36335;&#38750;&#24120;&#27973;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#30005;&#36335;&#28145;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21487;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#20174;&#32780;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.09910</link><description>&lt;p&gt;
LabelBench&#65306;&#22522;&#20110;&#32508;&#21512;&#26694;&#26550;&#30340;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32508;&#21512;&#24615;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;LabelBench&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#25968;&#25454;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#65292;&#20294;&#33719;&#21462;&#26631;&#35760;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#20943;&#32531;&#36825;&#19968;&#25104;&#26412;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65289;&#26088;&#22312;&#23454;&#29616;&#26631;&#31614;&#39640;&#25928;&#24615;&#65306;&#20174;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#31034;&#20363;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#26368;&#20339;&#30340;&#26631;&#31614;&#25928;&#29575;&#36890;&#24120;&#38656;&#35201;&#36825;&#20123;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#24182;&#27809;&#26377;&#25429;&#25417;&#21040;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LabelBench&#35299;&#20915;&#20102;&#36825;&#20010;&#32570;&#38519;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#32508;&#21512;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#35780;&#20272;&#22810;&#20010;&#26631;&#31614;&#39640;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#20316;&#20026;LabelBench&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#19968;&#36215;&#20351;&#29992;&#30340;&#26368;&#26032;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35777;&#26126;&#20102;&#27604;&#20808;&#21069;&#25253;&#21578;&#30340;&#26356;&#22909;&#30340;&#26631;&#31614;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.06251</link><description>&lt;p&gt;
&#36890;&#20449;&#31995;&#32479;&#20013;AI&#36890;&#29992;&#24615;&#19982;&#21487;&#25193;&#23637;&#24615;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#20449;&#31995;&#32479;&#20013;&#35299;&#20915;&#22797;&#26434;&#21644;&#21160;&#24577;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#32988;&#20219;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#20219;&#21153;&#30340;AI&#24212;&#29992;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#26465;&#20214;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#65292;&#20351;&#24471;&#31639;&#27861;&#26080;&#27861;&#36866;&#24212;&#20110;&#24120;&#35265;&#30340;&#32593;&#32476;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#21487;&#20197;&#22312;&#32593;&#32476;&#29615;&#22659;&#12289;&#24847;&#22270;&#21644;&#25511;&#21046;&#20219;&#21153;&#19978;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#21644;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;AI&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26550;&#26500;&#23558;&#20013;&#22830;&#21270;&#23398;&#20064;&#21151;&#33021;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#25968;&#25454;&#37319;&#38598;&#21151;&#33021;&#20998;&#31163;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#19979;&#65292;&#25512;&#24191;&#20102;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#35821;&#35328;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#30340;&#36716;&#21464;&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05614</link><description>&lt;p&gt;
&#21452;&#35821;&#31867;&#27604;&#27604;&#20363;
&lt;/p&gt;
&lt;p&gt;
Bilingual analogical proportions. (arXiv:2305.05614v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#19979;&#65292;&#25512;&#24191;&#20102;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20174;&#21333;&#35821;&#35328;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#30340;&#36716;&#21464;&#65292;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#27604;&#20363;&#26159;&#8220;$a$&#21040;$b$&#23601;&#22914;&#21516;$c$&#21040;$d$&#8221;&#36825;&#31181;&#34920;&#36798;&#24335;&#65292;&#23427;&#22788;&#20110;&#31867;&#27604;&#25512;&#29702;&#30340;&#26680;&#24515;&#65292;&#21518;&#32773;&#21448;&#22788;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#26680;&#24515;&#12290;&#20316;&#32773;&#26368;&#36817;&#22522;&#20110;&#36890;&#29992;&#20195;&#25968;&#21644;&#19968;&#38454;&#36923;&#36753;&#30340;&#19968;&#33324;&#35774;&#23450;&#65292;&#20174;&#31532;&#19968;&#21407;&#29702;&#24320;&#22987;&#20171;&#32461;&#20102;&#19968;&#31181;&#25277;&#35937;&#30340;&#31867;&#27604;&#27604;&#20363;&#20195;&#25968;&#36923;&#36753;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#28304;&#20195;&#25968;&#21644;&#30446;&#26631;&#20195;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#23558;&#20854;&#21333;&#35821;&#35328;&#26694;&#26550;&#25512;&#24191;&#21040;&#21452;&#35821;&#35328;&#26694;&#26550;&#65292;&#20854;&#20013;&#22522;&#30784;&#35821;&#35328;&#21487;&#33021;&#19981;&#21516;&#12290;&#36890;&#36807;&#22312;&#27604;&#20363;&#35777;&#26126;&#20013;&#20351;&#29992;&#38480;&#23450;&#35821;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#30340;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25512;&#24191;&#65292;&#24191;&#27867;&#22320;&#25193;&#23637;&#20102;&#22522;&#30784;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#20174;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#35828;&#65292;&#26412;&#25991;&#26159;&#36808;&#21521;&#31867;&#27604;&#25512;&#29702;&#25968;&#23398;&#29702;&#35770;&#30340;&#36827;&#19968;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is to $d$'' at the core of analogical reasoning which itself is at the core of human and artificial intelligence. The author has recently introduced {\em from first principles} an abstract algebro-logical framework of analogical proportions within the general setting of universal algebra and first-order logic. In that framework, the source and target algebras have the {\em same} underlying language. The purpose of this paper is to generalize his unilingual framework to a bilingual one where the underlying languages may differ. This is achieved by using hedges in justifications of proportions. The outcome is a major generalization vastly extending the applicability of the underlying framework. In a broader sense, this paper is a further step towards a mathematical theory of analogical reasoning.
&lt;/p&gt;</description></item><item><title>TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.15954</link><description>&lt;p&gt;
TraffNet&#65306;&#23398;&#20064;&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#20132;&#36890;&#29983;&#25104;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15954
&lt;/p&gt;
&lt;p&gt;
TraffNet&#26159;&#19968;&#20010;&#23398;&#20064;&#20132;&#36890;&#37327;&#29983;&#25104;&#21407;&#22240;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#34920;&#31034;&#20026;&#24322;&#26500;&#22270;&#65292;&#21033;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#29983;&#25104;&#21407;&#22240;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#36335;&#32593;&#32476;&#25968;&#23383;&#23402;&#29983;&#65288;RNDT&#65289;&#22312;&#24320;&#21457;&#19979;&#19968;&#20195;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#26102;&#20915;&#31574;&#65292;RNDT&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#20174;&#22312;&#32447;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#21160;&#24577;&#23398;&#20064;&#20132;&#36890;&#27169;&#24335;&#24182;&#29983;&#25104;&#39640;&#20445;&#30495;&#27169;&#25311;&#32467;&#26524;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24403;&#21069;&#20132;&#36890;&#39044;&#27979;&#25216;&#26415;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25216;&#26415;&#20165;&#36890;&#36807;&#25366;&#25496;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#26469;&#39044;&#27979;&#26410;&#26469;&#20132;&#36890;&#65292;&#32780;&#24573;&#30053;&#20102;&#20132;&#36890;&#29983;&#25104;&#30340;&#21407;&#22240;&#65292;&#20363;&#22914;&#20132;&#36890;&#38656;&#27714;&#21644;&#36335;&#24452;&#36873;&#25321;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#23545;&#20110;&#23454;&#26102;&#20915;&#31574;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; TraffNet&#65292;&#35813;&#26694;&#26550;&#20174;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#20013;&#23398;&#20064;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#34920;&#31034;&#36947;&#36335;&#32593;&#32476;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24182;&#20837;&#39044;&#27979;&#25152;&#38656;&#30340;&#20854;&#20182;&#25968;&#25454;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#33021;&#22815;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#22240;&#26524;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.07946</link><description>&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#32479;&#19968;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19979;&#36827;&#34892;&#32479;&#19968;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#33258;&#30001;&#33021;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#23436;&#20840;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24320;&#21457;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#20197;&#26368;&#22823;&#21270;&#30001;&#22806;&#37096;&#30417;&#30563;&#21592;&#25351;&#23450;&#30340;&#22870;&#21169;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#28041;&#21450;&#37096;&#20998;&#35266;&#27979;&#65292;&#24418;&#24335;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#34892;&#21160;&#21644;&#35266;&#27979;&#35760;&#24518;&#25110;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#29615;&#22659;&#30340;&#30495;&#23454;&#29366;&#24577;&#26469;&#35299;&#20915;POMDP&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#38543;&#26102;&#38388;&#32858;&#21512;&#35266;&#27979;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25512;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#35768;&#22810;&#26679;&#26412;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20851;&#27880;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#24573;&#35270;&#20102;&#25512;&#26029;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;&#22312;POMDP&#20013;&#21046;&#23450;&#30340;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31216;&#20026;&#26399;&#26395;&#33258;&#30001;&#33021;&#65288;EFE&#65289;&#30340;&#20989;&#25968;&#25351;&#23548;&#20195;&#29702;&#36873;&#25321;&#21160;&#20316;&#12290;&#36825;&#25552;&#20379;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#65288;&#23500;&#26377;&#24320;&#21457;&#24615;&#65289;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2210.03050</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#26368;&#26032;&#27867;&#21270;&#30740;&#31350;&#65306;&#20998;&#31867;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20160;&#20040;&#26159;&#8220;&#33391;&#22909;&#30340;&#27867;&#21270;&#8221;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20173;&#19981;&#26126;&#30830;&#65292;&#20063;&#27809;&#26377;&#27867;&#21270;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34920;&#24449;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#35813;&#20998;&#31867;&#27861;&#22522;&#20110;&#23545;&#27867;&#21270;&#30740;&#31350;&#30340;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#21253;&#21547;&#20102;&#20116;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#65306;&#20027;&#35201;&#21160;&#26426;&#12289;&#30740;&#31350;&#30340;&#27867;&#21270;&#31867;&#22411;&#12289;&#32771;&#34385;&#30340;&#25968;&#25454;&#36716;&#31227;&#31867;&#22411;&#12289;&#25968;&#25454;&#36716;&#31227;&#30340;&#26469;&#28304;&#20197;&#21450;&#36716;&#31227;&#22312;&#24314;&#27169;&#27969;&#31243;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#23545;&#27979;&#35797;&#27867;&#21270;&#30340;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20849;&#36827;&#34892;&#20102;600&#22810;&#20010;&#23454;&#39564;&#12290;&#36890;&#36807;&#32508;&#36848;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffRes&#65292;&#21487;&#20197;&#23454;&#29616;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#24494;&#20998;&#26102;&#38388;&#20998;&#36776;&#29575;&#24314;&#27169;&#12290;&#32473;&#23450;&#19968;&#20010;&#29992;&#22266;&#23450;&#36339;&#27493;&#22823;&#23567;&#35745;&#31639;&#30340;&#22768;&#35889;&#22270;&#65292;DiffRes&#23558;&#38750;&#37325;&#35201;&#30340;&#26102;&#38388;&#24103;&#21512;&#24182;&#65292;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#24103;&#12290;</title><link>http://arxiv.org/abs/2210.01719</link><description>&lt;p&gt;
&#23398;&#20064;&#38899;&#39057;&#20998;&#31867;&#20013;&#22768;&#35889;&#22270;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning Temporal Resolution in Spectrogram for Audio Classification. (arXiv:2210.01719v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffRes&#65292;&#21487;&#20197;&#23454;&#29616;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#24494;&#20998;&#26102;&#38388;&#20998;&#36776;&#29575;&#24314;&#27169;&#12290;&#32473;&#23450;&#19968;&#20010;&#29992;&#22266;&#23450;&#36339;&#27493;&#22823;&#23567;&#35745;&#31639;&#30340;&#22768;&#35889;&#22270;&#65292;DiffRes&#23558;&#38750;&#37325;&#35201;&#30340;&#26102;&#38388;&#24103;&#21512;&#24182;&#65292;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#22768;&#35889;&#22270;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#30340;&#26102;&#39057;&#34920;&#31034;&#26041;&#27861;&#12290;&#22768;&#35889;&#22270;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#26159;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#35813;&#20998;&#36776;&#29575;&#21462;&#20915;&#20110;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#20013;&#20351;&#29992;&#30340;&#36339;&#27493;&#22823;&#23567;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#36339;&#27493;&#22823;&#23567;&#24212;&#20026;&#24120;&#37327;&#20540;&#65288;&#20363;&#22914;10&#27627;&#31186;&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#22768;&#38899;&#65292;&#22266;&#23450;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#12290;&#26102;&#38388;&#20998;&#36776;&#29575;&#19981;&#20165;&#24433;&#21709;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#36824;&#24433;&#21709;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffRes&#65292;&#21487;&#20197;&#23454;&#29616;&#38899;&#39057;&#20998;&#31867;&#20013;&#30340;&#21487;&#24494;&#20998;&#26102;&#38388;&#20998;&#36776;&#29575;&#24314;&#27169;&#12290;&#32473;&#23450;&#19968;&#20010;&#29992;&#22266;&#23450;&#36339;&#27493;&#22823;&#23567;&#35745;&#31639;&#30340;&#22768;&#35889;&#22270;&#65292;DiffRes&#23558;&#38750;&#37325;&#35201;&#30340;&#26102;&#38388;&#24103;&#21512;&#24182;&#65292;&#21516;&#26102;&#20445;&#30041;&#37325;&#35201;&#30340;&#24103;&#12290;DiffRes&#20316;&#20026;&#38899;&#39057;&#22768;&#35889;&#22270;&#21644;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#8220;&#25554;&#20837;&#8221;&#27169;&#22359;&#65292;&#21487;&#20197;&#19982;&#20998;&#31867;&#20219;&#21153;&#20849;&#21516;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;DiffRes&#65292;&#20351;&#29992;mel
&lt;/p&gt;
&lt;p&gt;
The audio spectrogram is a time-frequency representation that has been widely used for audio classification. One of the key attributes of the audio spectrogram is the temporal resolution, which depends on the hop size used in the Short-Time Fourier Transform (STFT). Previous works generally assume the hop size should be a constant value (e.g., 10 ms). However, a fixed temporal resolution is not always optimal for different types of sound. The temporal resolution affects not only classification accuracy but also computational cost. This paper proposes a novel method, DiffRes, that enables differentiable temporal resolution modeling for audio classification. Given a spectrogram calculated with a fixed hop size, DiffRes merges non-essential time frames while preserving important frames. DiffRes acts as a "drop-in" module between an audio spectrogram and a classifier and can be jointly optimized with the classification task. We evaluate DiffRes on five audio classification tasks, using mel
&lt;/p&gt;</description></item></channel></rss>