<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing for Large Language Models: A Survey. (arXiv:2310.16218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16218
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#20197;&#32435;&#20837;&#26032;&#30693;&#35782;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#20197;&#20854;&#20986;&#33394;&#30340;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26681;&#25454;&#20854;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25913;&#21464;&#20102;&#23398;&#26415;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#20026;&#20854;&#21442;&#25968;&#25968;&#37327;&#21069;&#25152;&#26410;&#26377;&#12290;&#24403;&#38656;&#35201;&#39057;&#32321;&#24341;&#20837;&#26032;&#30693;&#35782;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26102;&#65292;&#36825;&#20010;&#32570;&#28857;&#26356;&#21152;&#26174;&#33879;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#26356;&#26032;&#39044;&#35757;&#32451;LLMs&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#36890;&#36807;&#30452;&#25509;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;LLMs&#20013;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#35757;&#32451;LLMs&#21487;&#33021;&#35745;&#31639;&#36164;&#28304;&#23494;&#38598;&#65292;&#24182;&#19988;&#23384;&#22312;&#23558;&#19982;&#27169;&#22411;&#26356;&#26032;&#26080;&#20851;&#30340;&#26377;&#20215;&#20540;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36864;&#21270;&#30340;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#27169;&#22411;&#32534;&#36753;(KME)&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#31934;&#30830;&#20462;&#25913;LLMs&#20197;&#32435;&#20837;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, wit
&lt;/p&gt;</description></item><item><title>SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.15539</link><description>&lt;p&gt;
SteloCoder:&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#29992;&#20110;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15539
&lt;/p&gt;
&lt;p&gt;
SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;StarCoder&#21644;Code Llama&#20998;&#21035;&#23637;&#31034;&#20102;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#32763;&#35793;&#21151;&#33021;&#19978;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#21644;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SteloCoder&#65292;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#19987;&#20026;&#22810;&#32534;&#31243;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#32780;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SteloCoder&#23454;&#29616;&#20102;C ++&#65292;C&#65283;&#65292;JavaScript&#65292;Java&#25110;PHP&#21040;Python&#20195;&#30721;&#32763;&#35793;&#65292;&#32780;&#26080;&#38656;&#25351;&#23450;&#36755;&#20837;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#32452;&#28151;&#21512;&#65288;Mixture-of-Experts&#65292;MoE&#65289;&#25216;&#26415;&#21644;&#19968;&#20010;&#25511;&#21046;&#22810;&#20219;&#21153;&#30340;&#38376;&#25511;&#32593;&#32476;&#26469;&#20462;&#25913;StarCoder&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#26469;&#33719;&#24471;&#19987;&#23478;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;Low-Rank Adaptive Method&#65292;LoRA&#65289;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#38480;&#21046;&#20026;StarCoder&#21442;&#25968;&#25968;&#37327;&#30340;&#20165;0.06&#65285;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;tr
&lt;/p&gt;
&lt;p&gt;
With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.11236</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#33394;&#24425;&#20256;&#36882;&#65306;&#24102;&#26377;&#20132;&#25442;&#22240;&#23376;&#30340;&#25260;&#21319;&#27169;&#22411;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Colour Passing Revisited: Lifted Model Construction with Commutative Factors. (arXiv:2309.11236v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#20132;&#25442;&#22240;&#23376;&#30340;&#25260;&#21319;&#27169;&#22411;&#26500;&#24314;&#21487;&#20197;&#21033;&#29992;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23454;&#29616;&#23545;&#39046;&#22495;&#22823;&#23567;&#30340;&#21487;&#34892;&#30340;&#27010;&#29575;&#25512;&#29702;&#12290;&#20026;&#20102;&#24212;&#29992;&#25260;&#21319;&#25512;&#29702;&#65292;&#24517;&#39035;&#33719;&#24471;&#19968;&#20010;&#25260;&#21319;&#34920;&#31034;&#65292;&#24182;&#19988;&#20026;&#20102;&#36825;&#26679;&#20570;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#12290;&#19981;&#36807;&#65292;&#25105;&#20204;&#21457;&#29616;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#22312;&#26500;&#24314;&#25260;&#21319;&#34920;&#31034;&#26102;&#24573;&#30053;&#20102;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#33394;&#24425;&#20256;&#36882;&#31639;&#27861;&#65292;&#21033;&#29992;&#36923;&#36753;&#21464;&#37327;&#26500;&#24314;&#19968;&#20010;&#19982;&#29305;&#23450;&#25512;&#29702;&#31639;&#27861;&#26080;&#20851;&#30340;&#25260;&#21319;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;&#31163;&#32447;&#38454;&#27573;&#21033;&#29992;&#22240;&#23376;&#30340;&#20132;&#25442;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#26816;&#27979;&#20986;&#26356;&#22810;&#30340;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#21387;&#32553;&#29575;&#65292;&#20351;&#24471;&#24212;&#29992;&#35813;&#27169;&#22411;&#26102;&#30340;&#27010;&#29575;&#25512;&#29702;&#26597;&#35810;&#36895;&#24230;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
&lt;/p&gt;</description></item><item><title>C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2309.07597</link><description>&lt;p&gt;
C-Pack: &#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#25171;&#21253;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07597
&lt;/p&gt;
&lt;p&gt;
C-Pack&#26159;&#19968;&#22871;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12289;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#35813;&#36164;&#28304;&#38598;&#22312;C-MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;+10%&#30340;&#34920;&#29616;&#65292;&#24182;&#36890;&#36807;&#25972;&#21512;&#21644;&#20248;&#21270;&#19968;&#22871;&#35757;&#32451;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;C-Pack&#36824;&#21457;&#24067;&#20102;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#35813;&#36164;&#28304;&#38598;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;C-Pack&#65292;&#36825;&#26159;&#19968;&#22871;&#26174;&#33879;&#25512;&#36827;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#39046;&#22495;&#30340;&#36164;&#28304;&#12290;C-Pack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#36164;&#28304;&#12290;1&#65289;C-MTEB&#26159;&#19968;&#20010;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;35&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#12290;2&#65289;C-MTP&#26159;&#19968;&#20010;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#27721;&#35821;&#35821;&#26009;&#24211;&#20013;&#31574;&#21010;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#12290;3&#65289;C-TEM&#26159;&#19968;&#20010;&#28085;&#30422;&#22810;&#20010;&#23610;&#23544;&#30340;&#23884;&#20837;&#27169;&#22411;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;C-MTEB&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#30340;&#25152;&#26377;&#27721;&#35821;&#25991;&#26412;&#23884;&#20837;&#36798;&#21040;&#20102;&#21457;&#24067;&#26102;&#30340;&#26368;&#39640;+10%&#12290;&#25105;&#20204;&#36824;&#25972;&#21512;&#21644;&#20248;&#21270;&#20102;C-TEM&#30340;&#25972;&#22871;&#35757;&#32451;&#26041;&#27861;&#12290;&#38500;&#20102;&#25105;&#20204;&#20851;&#20110;&#26222;&#36890;&#27721;&#35821;&#23884;&#20837;&#30340;&#36164;&#28304;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#33521;&#35821;&#25991;&#26412;&#23884;&#20837;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#36825;&#20123;&#33521;&#35821;&#27169;&#22411;&#22312;MTEB&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#24067;&#30340;&#33521;&#35821;&#25968;&#25454;&#27604;&#27721;&#35821;&#25968;&#25454;&#22823;2&#20493;&#12290;&#25152;&#26377;&#36825;&#20123;&#36164;&#28304;&#37117;&#21487;&#20197;&#22312;https://github.com/FlagOpen/FlagEmbedding&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve state-of-the-art performance on MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;</description></item><item><title>Where2Explore&#26159;&#19968;&#31181;&#38024;&#23545;&#26410;&#35265;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#26368;&#23569;&#25968;&#37327;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26032;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.07473</link><description>&lt;p&gt;
Where2Explore: &#20026;&#26410;&#35265;&#36807;&#30340;&#20851;&#33410;&#29289;&#20307;&#36827;&#34892;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects. (arXiv:2309.07473v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07473
&lt;/p&gt;
&lt;p&gt;
Where2Explore&#26159;&#19968;&#31181;&#38024;&#23545;&#26410;&#35265;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#26368;&#23569;&#25968;&#37327;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20855;&#26377;&#31867;&#20284;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#29289;&#20307;&#30340;&#25805;&#20316;&#26159;&#26426;&#22120;&#20154;&#20013;&#19968;&#39033;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#29289;&#20307;&#31867;&#21035;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#20960;&#20309;&#21644;&#35821;&#20041;&#24046;&#24322;&#65292;&#20197;&#24448;&#30340;&#25805;&#32437;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#31867;&#21035;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#20801;&#35768;&#26426;&#22120;&#20154;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#36827;&#34892;&#23569;&#37327;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19982;&#27599;&#20010;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#26114;&#36149;&#19988;&#20302;&#25928;&#30340;&#27979;&#35797;&#20132;&#20114;&#12290;&#37492;&#20110;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#19981;&#21516;&#30340;&#31867;&#21035;&#36890;&#24120;&#20849;&#20139;&#31867;&#20284;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;&#25805;&#20316;&#26159;&#24517;&#35201;&#30340;&#65292;&#27604;&#22914;&#21487;&#25289;&#21160;&#30340;&#25163;&#26564;&#21644;&#21487;&#25235;&#21462;&#30340;&#36793;&#32536;-&#36825;&#20010;&#22240;&#32032;&#22312;&#20197;&#21069;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#20849;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Where2Explore&#8221;&#65292;&#19968;&#31181;&#25506;&#32034;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#26368;&#23569;&#20132;&#20114;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Articulated object manipulation is a fundamental yet challenging task in robotics. Due to significant geometric and semantic variations across object categories, previous manipulation models struggle to generalize to novel categories. Few-shot learning is a promising solution for alleviating this issue by allowing robots to perform a few interactions with unseen objects. However, extant approaches often necessitate costly and inefficient test-time interactions with each unseen instance. Recognizing this limitation, we observe that despite their distinct shapes, different categories often share similar local geometries essential for manipulation, such as pullable handles and graspable edges - a factor typically underutilized in previous few-shot learning works. To harness this commonality, we introduce 'Where2Explore', an affordance learning framework that effectively explores novel categories with minimal interactions on a limited number of instances. Our framework explicitly estimates
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.02730</link><description>&lt;p&gt;
Stylebook: &#22312;&#21482;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#20013;&#36827;&#34892;&#20381;&#36182;&#20869;&#23481;&#30340;&#35828;&#35805;&#39118;&#26684;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stylebook: Content-Dependent Speaking Style Modeling for Any-to-Any Voice Conversion using Only Speech Data. (arXiv:2309.02730v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363; Stylebook&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#26679;&#24335;&#25163;&#20876;&#65292;&#21487;&#20197;&#23454;&#29616;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#24544;&#23454;&#22797;&#21046;&#21644;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#20219;&#24847;-&#20219;&#24847;&#35821;&#38899;&#36716;&#25442;&#27169;&#22411;&#25104;&#21151;&#22320;&#23558;&#19968;&#20123;&#30446;&#26631;&#35821;&#38899;&#30340;&#39118;&#26684;&#20449;&#24687;&#36716;&#31227;&#21040;&#36716;&#25442;&#30340;&#35821;&#38899;&#20013;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#24544;&#23454;&#22320;&#22797;&#21046;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#30446;&#26631;&#35821;&#38899;&#20013;&#25552;&#21462;&#20016;&#23500;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#39640;&#25928;&#22320;&#36716;&#31227;&#21040;&#28304;&#35821;&#38899;&#20869;&#23481;&#19978;&#65292;&#32780;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#25110;&#35828;&#35805;&#32773;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27169;&#22411;&#25910;&#38598;&#19982;&#19981;&#21516;&#38899;&#32032;&#20869;&#23481;&#30456;&#23545;&#24212;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#35828;&#35805;&#39118;&#26684;&#12290;&#36825;&#20123;&#39118;&#26684;&#29992;&#19968;&#32452;&#31216;&#20026;&#26679;&#24335;&#25163;&#20876;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#19979;&#19968;&#27493;&#20013;&#65292;&#26679;&#24335;&#25163;&#20876;&#19982;&#28304;&#35821;&#38899;&#30340;&#38899;&#32032;&#20869;&#23481;&#19968;&#36215;&#21442;&#19982;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28304;&#20869;&#23481;&#30340;&#26368;&#32456;&#30446;&#26631;&#39118;&#26684;&#12290;&#26368;&#21518;&#65292;&#20174;&#28304;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#20869;&#23481;&#20449;&#24687;&#21644;&#20381;&#36182;&#20869;&#23481;&#30340;&#30446;&#26631;&#39118;&#26684;&#23884;&#20837;&#34987;&#36755;&#20837;&#21040;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
While many recent any-to-any voice conversion models succeed in transferring some target speech's style information to the converted speech, they still lack the ability to faithfully reproduce the speaking style of the target speaker. In this work, we propose a novel method to extract rich style information from target utterances and to efficiently transfer it to source speech content without requiring text transcriptions or speaker labeling. Our proposed approach introduces an attention mechanism utilizing a self-supervised learning (SSL) model to collect the speaking styles of a target speaker each corresponding to the different phonetic content. The styles are represented with a set of embeddings called stylebook. In the next step, the stylebook is attended with the source speech's phonetic content to determine the final target style for each source content. Finally, content information extracted from the source speech and content-dependent target style embeddings are fed into a dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12367</link><description>&lt;p&gt;
SafeAR: &#36890;&#36807;&#39118;&#38505;&#24863;&#30693;&#31574;&#30053;&#23454;&#29616;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
SafeAR: Towards Safer Algorithmic Recourse by Risk-Aware Policies. (arXiv:2308.12367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65288;SafeAR&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#39118;&#38505;&#22240;&#32032;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#31561;&#20851;&#38190;&#39046;&#22495;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20026;&#37027;&#20123;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20915;&#31574;&#19981;&#21033;&#24433;&#21709;&#30340;&#20010;&#20307;&#25552;&#20379;&#34917;&#25937;&#25514;&#26045;&#30340;&#38656;&#27714;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65307;&#20010;&#20307;&#24212;&#35813;&#33719;&#24471;&#25913;&#21892;&#33258;&#36523;&#24773;&#20917;&#21644;&#33719;&#24471;&#26377;&#21033;&#20915;&#31574;&#30340;&#24314;&#35758;&#12290;&#20043;&#21069;&#20851;&#20110;&#39034;&#24207;&#31639;&#27861;&#34917;&#25937;&#30340;&#24037;&#20316;&#8212;&#8212;&#25512;&#33616;&#19968;&#31995;&#21015;&#21464;&#21270;&#8212;&#8212;&#20027;&#35201;&#20851;&#27880;&#34892;&#21160;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#21464;&#21270;&#30340;&#25509;&#36817;&#31243;&#24230;&#30830;&#23450;&#34892;&#21160;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#26410;&#32771;&#34385;&#29305;&#24449;&#21464;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#34917;&#25937;&#20013;&#39640;&#20110;&#24179;&#22343;&#25104;&#26412;&#30340;&#39118;&#38505;&#12290;&#22914;&#26524;&#34917;&#25937;&#25514;&#26045;&#21487;&#33021;&#65288;&#20197;&#19968;&#23450;&#27010;&#29575;&#65289;&#23548;&#33268;&#26356;&#31967;&#31957;&#30340;&#24773;&#20917;&#65292;&#32780;&#24674;&#22797;&#38656;&#35201;&#20184;&#20986;&#38750;&#24120;&#39640;&#30340;&#20195;&#20215;&#65292;&#37027;&#23558;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#35745;&#31639;&#21644;&#35780;&#20272;&#34917;&#25937;&#25514;&#26045;&#26102;&#65292;&#24517;&#39035;&#32771;&#34385;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#32771;&#34385;&#20102;&#36825;&#31181;&#39118;&#38505;&#22240;&#32032;&#35745;&#31639;&#20986;&#30340;&#34917;&#25937;&#25514;&#26045;&#31216;&#20026;&#26356;&#23433;&#20840;&#30340;&#31639;&#27861;&#34917;&#25937;&#65288;SafeAR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of machine learning (ML) models in critical domains such as finance and healthcare, the need to offer recourse for those adversely affected by the decisions of ML models has become more important; individuals ought to be provided with recommendations on actions to take for improving their situation and thus receive a favorable decision. Prior work on sequential algorithmic recourse -- which recommends a series of changes -- focuses on action feasibility and uses the proximity of feature changes to determine action costs. However, the uncertainties of feature changes and the risk of higher than average costs in recourse have not been considered. It is undesirable if a recourse could (with some probability) result in a worse situation from which recovery requires an extremely high cost. It is essential to incorporate risks when computing and evaluating recourse. We call the recourse computed with such risk considerations as Safer Algorithmic Recourse (SafeAR). The ob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04237</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21512;&#39044;&#27979;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction. (arXiv:2308.04237v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#30340;&#26080;&#32447;&#36890;&#36947;&#32852;&#21512;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20849;&#20139;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26381;&#21153;&#22120;&#24076;&#26395;&#26681;&#25454;&#27169;&#22411;&#23545;&#26032;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#12290;&#35774;&#22791;&#21487;&#20197;&#36890;&#36807;&#20849;&#21516;&#30340;&#26080;&#32447;&#20449;&#36947;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#24182;&#19988;&#21487;&#20197;&#35775;&#38382;&#20197;&#21069;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22914;&#26524;&#35774;&#22791;&#26080;&#27861;&#35775;&#38382;&#26032;&#36755;&#20837;&#65292;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#26381;&#21153;&#22120;&#30340;&#25512;&#29702;&#20915;&#31574;&#36136;&#37327;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#32852;&#21512;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65292;&#21033;&#29992;&#35774;&#22791;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26469;&#25552;&#39640;&#26381;&#21153;&#22120;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#32852;&#21512;CP&#20013;&#65292;&#35774;&#22791;&#21521;&#26381;&#21153;&#22120;&#20256;&#36882;&#20851;&#20110;&#26412;&#22320;&#25968;&#25454;&#19978;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#25439;&#22833;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#22120;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#26657;&#20934;&#19968;&#20010;&#20915;&#31574;&#21306;&#38388;&#65292;&#20197;&#20415;&#20854;&#22312;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#21487;&#38752;&#24615;&#27700;&#24179;&#19979;&#20445;&#35777;&#21253;&#21547;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a setting in which devices and a server share a pre-trained model. The server wishes to make an inference on a new input given the model. Devices have access to data, previously not used for training, and can communicate to the server over a common wireless channel. If the devices have no access to the new input, can communication from devices to the server enhance the quality of the inference decision at the server? Recent work has introduced federated conformal prediction (CP), which leverages devices-to-server communication to improve the reliability of the server's decision. With federated CP, devices communicate to the server information about the loss accrued by the shared pre-trained model on the local data, and the server leverages this information to calibrate a decision interval, or set, so that it is guaranteed to contain the correct answer with a pre-defined target reliability level. Previous work assumed noise-free communication, whereby devices can communicate a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.16387</link><description>&lt;p&gt;
Relation-Oriented: &#36808;&#21521;&#19982;&#30693;&#35782;&#23545;&#20934;&#30340;&#22240;&#26524;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Relation-Oriented: Toward Knowledge-Aligned Causal AI. (arXiv:2307.16387v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#35270;&#35282;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#30340;&#35266;&#23519;&#27169;&#22411;&#19982;&#23454;&#38469;&#29702;&#35299;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#23454;&#29616;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#23454;&#36341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24212;&#29992;&#19968;&#20010;&#35266;&#23519;&#23548;&#21521;&#30340;&#21407;&#21017;&#65292;&#20854;&#20013;&#35266;&#23519;&#21464;&#37327;&#20808;&#23384;&#22312;&#24182;&#20026;&#26500;&#24314;&#20851;&#31995;&#22880;&#23450;&#22522;&#30784;&#12290;&#34429;&#28982;&#23545;&#20110;&#20256;&#32479;&#27169;&#22411;&#26469;&#35828;&#36275;&#22815;&#20102;&#65292;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#19982;&#22823;&#25968;&#25454;&#30340;&#25972;&#21512;&#26292;&#38706;&#20102;&#35266;&#23519;&#27169;&#22411;&#19982;&#25105;&#20204;&#30340;&#23454;&#38469;&#29702;&#35299;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22609;&#36896;&#20102;&#30001;&#20851;&#31995;&#23450;&#20041;&#30340;&#35748;&#30693;&#23454;&#20307;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36328;&#36234;&#26102;&#38388;&#21644;&#36229;&#32500;&#24230;&#31354;&#38388;&#21046;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#34987;&#38480;&#21046;&#22312;&#35266;&#23519;&#26500;&#24314;&#20013;&#12290;&#20174;&#19968;&#31181;&#21019;&#26032;&#30340;&#20851;&#31995;&#23548;&#21521;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26469;&#33258;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20581;&#24247;&#20449;&#24687;&#23398;&#30340;&#30452;&#35266;&#20363;&#23376;&#65292;&#20998;&#26512;&#20102;&#22312;&#25105;&#20204;&#24403;&#21069;&#30340;&#24314;&#27169;&#33539;&#24335;&#20013;&#36825;&#31181;&#19981;&#23545;&#40784;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20851;&#31995;&#23450;&#20041;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20316;&#20026;&#20851;&#31995;&#23548;&#21521;&#24314;&#27169;&#30340;&#19968;&#31181;&#23454;&#38469;&#23454;&#26045;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning, we naturally apply an Observation-Oriented principle, in which observational variables preexist and set the stage for constructing relationships. While sufficient for traditional models, the integration of AI with big data exposes the misalignment between the observational models and our actual comprehension. Contrarily, humans shape cognitive entities defined by relationships, enabling us to formulate knowledge across temporal and hyper-dimensional spaces, rather than being confined to observational constructs. From an innovative Relation-Oriented perspective, this study examines the roots of this misalignment within our current modeling paradigm, illuminated by intuitive examples from computer vision and health informatics. We also introduce the relation-defined representation learning methodology as a practical implementation of Relation-Oriented modeling, supported by extensive experimental validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13332</link><description>&lt;p&gt;
&#22312;&#38169;&#35823;&#25351;&#23450;&#30340;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#22240;&#23376;
&lt;/p&gt;
&lt;p&gt;
The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation. (arXiv:2307.13332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#24182;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65292;&#36825;&#20123;&#22240;&#23376;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#30693;&#36947;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;&#22312;&#20989;&#25968;&#36924;&#36817;&#30340;&#38169;&#35823;&#25351;&#23450;&#20013;&#20250;&#20986;&#29616;&#20056;&#27861;&#25918;&#22823;&#22240;&#23376;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;\emph{&#36924;&#36817;&#22240;&#23376;}&#30340;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#30340;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#26368;&#20339;&#24418;&#24335;&#65292;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#32447;&#24615;&#31163;&#31574;&#30053;&#20540;&#20989;&#25968;&#20272;&#35745;&#20013;&#30340;&#24191;&#27867;&#35774;&#32622;&#20013;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20854;&#20013;&#20173;&#26377;&#35768;&#22810;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31181;&#35774;&#32622;&#19979;&#30340;&#36924;&#36817;&#22240;&#23376;&#65292;&#20363;&#22914;&#21152;&#26435;$L_2$&#33539;&#25968;&#65288;&#20854;&#20013;&#21152;&#26435;&#26159;&#31163;&#32447;&#29366;&#24577;&#20998;&#24067;&#65289;&#65292;$L_\infty$&#33539;&#25968;&#65292;&#29366;&#24577;&#21035;&#21517;&#30340;&#23384;&#22312;&#19982;&#21542;&#20197;&#21450;&#23545;&#29366;&#24577;&#31354;&#38388;&#30340;&#20840;&#38754;&#19982;&#37096;&#20998;&#35206;&#30422;&#12290;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26368;&#20248;&#30340;&#28176;&#36817;&#36924;&#36817;&#22240;&#23376;&#65288;&#33267;&#22810;&#24120;&#25968;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#30830;&#23450;&#20102;$L_2(\mu)$&#33539;&#25968;&#30340;&#20004;&#20010;&#20381;&#36182;&#20110;&#23454;&#20363;&#30340;&#22240;&#23376;&#21644;$L_\infty$&#33539;&#25968;&#30340;&#19968;&#20010;&#22240;&#23376;&#65292;&#23427;&#20204;&#34987;&#35777;&#26126;&#20915;&#23450;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#30340;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.04308</link><description>&lt;p&gt;
GPT-3&#30340;&#20154;&#26684;&#27979;&#35797;&#65306;&#26102;&#38388;&#21487;&#38752;&#24615;&#26377;&#38480;&#65292;&#20294;&#20984;&#26174;&#20102;&#31038;&#20132;&#28212;&#26395;&#30340;&#20154;&#26684;&#24037;&#20855;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results. (arXiv:2306.04308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-3 Davinci-003&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#29305;&#36136;&#65292;&#20294;&#22312;&#19981;&#21516;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;GPT-3 Davinci-003&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#21450;&#20854;&#20010;&#24615;&#21270;&#36164;&#26009;&#30340;&#20154;&#26684;&#38382;&#21367;&#30340;&#26102;&#38388;&#21487;&#38752;&#24615;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22330;&#21512;&#65292;&#24515;&#29702;&#38382;&#21367;&#34987;&#24212;&#29992;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#28982;&#21518;&#23558;&#22238;&#31572;&#19982;&#20154;&#31867;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#20123;&#37327;&#34920;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#26377;&#20123;&#21017;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#19968;&#33268;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Davinci-003&#26174;&#31034;&#20986;&#19968;&#20010;&#31038;&#20132;&#28212;&#26395;&#21644;&#20146;&#31038;&#20250;&#30340;&#20154;&#26684;&#29305;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#20146;&#21644;&#21147;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#31572;&#30340;&#22522;&#30784;&#65292;&#26080;&#35770;&#26159;&#30001;&#20027;&#35266;&#33258;&#25105;&#21453;&#24605;&#36824;&#26159;&#39044;&#23450;&#31639;&#27861;&#39537;&#21160;&#65292;&#23578;&#19981;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
To assess the potential applications and limitations of chatbot GPT-3 Davinci-003, this study explored the temporal reliability of personality questionnaires applied to the chatbot and its personality profile. Psychological questionnaires were administered to the chatbot on two separate occasions, followed by a comparison of the responses to human normative data. The findings revealed varying levels of agreement in the chatbot's responses over time, with some scales displaying excellent while others demonstrated poor agreement. Overall, Davinci-003 displayed a socially desirable and pro-social personality profile, particularly in the domain of communion. However, the underlying basis of the chatbot's responses, whether driven by conscious self-reflection or predetermined algorithms, remains uncertain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15835</link><description>&lt;p&gt;
PDE+&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion. (arXiv:2305.15835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PDE+&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#19981;&#20165;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#25110;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#36824;&#26377;&#26356;&#23569;&#30340;&#35757;&#32451;&#36718;&#27425;&#21644;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#20998;&#24067;&#26102;&#30340;&#24615;&#33021;&#12290;&#30446;&#21069;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#33539;&#24335;&#30340;&#26041;&#24335;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#22122;&#22768;&#27880;&#20837;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#30001;&#20110;&#27169;&#22411;&#30340;&#38750;&#24179;&#28369;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;&#27867;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#30452;&#25509;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#20989;&#25968;&#26469;&#22686;&#24378;&#23427;&#65292;&#32780;&#19981;&#26159;&#32858;&#28966;&#20110;&#35843;&#25972;&#36755;&#20837;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#19982;&#29305;&#23450;PDE&#35299;&#30340;&#24179;&#28369;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21363;&#8220;&#36755;&#36816;&#26041;&#31243;&#8221;&#12290;&#36825;&#26679;&#24314;&#31435;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24635;&#20307;&#26694;&#26550;&#65292;&#23558;&#33258;&#36866;&#24212;&#20998;&#24067;&#25193;&#25955;&#24341;&#20837;&#36755;&#36816;&#26041;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#35299;&#30340;&#24179;&#28369;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#25554;&#20214;&#27169;&#22359;&#20351;&#29992;&#65292;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#35757;&#32451;&#31639;&#27861;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#32593;&#32476;&#21442;&#25968;&#21644;&#35757;&#32451;&#36718;&#27425;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization of neural networks is a central challenge in machine learning, especially concerning the performance under distributions that differ from training ones. Current methods, mainly based on the data-driven paradigm such as data augmentation, adversarial training, and noise injection, may encounter limited generalization due to model non-smoothness. In this paper, we propose to investigate generalization from a Partial Differential Equation (PDE) perspective, aiming to enhance it directly through the underlying function of neural networks, rather than focusing on adjusting input data. Specifically, we first establish the connection between neural network generalization and the smoothness of the solution to a specific PDE, namely ``transport equation''. Building upon this, we propose a general framework that introduces adaptive distributional diffusion into transport equation to enhance the smoothness of its solution, thereby improving generalization. In the context of neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13030</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations. (arXiv:2305.13030v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#23545;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#22312;&#32593;&#32476;&#31354;&#38388;&#20013;&#29983;&#25104;&#28789;&#27963;&#21644;&#22810;&#26679;&#21270;&#34892;&#20026;&#30340;&#24378;&#22823;&#26694;&#26550;&#65307;&#28982;&#32780;&#65292;&#22312;&#24314;&#27169;&#30495;&#23454;&#19990;&#30028;&#29983;&#29289;&#22810;&#26234;&#33021;&#20307;&#26102;&#65292;&#22312;&#28304;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#65288;&#21363; RL &#30340;&#32593;&#32476;&#31354;&#38388;&#65289;&#20043;&#38388;&#23384;&#22312;&#22495;&#24046;&#24322;&#65292;&#24182;&#19988;&#28304;&#29615;&#22659;&#21442;&#25968;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#30340;&#30495;&#23454;&#19990;&#30028;&#23637;&#31034;&#20013;&#36827;&#34892; RL &#30340;&#33258;&#36866;&#24212;&#34892;&#21160;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#32467;&#21512; RL &#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22522;&#20110;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#30340;&#28436;&#31034;&#21160;&#20316;&#26469;&#22312; RL &#20013;&#21033;&#29992;&#26410;&#30693;&#28304;&#21160;&#24577;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20026;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#22312;&#22797;&#21046;&#21644;&#25512;&#24191;&#20043;&#38388;&#24179;&#34913;&#30340; RL &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling of real-world biological multi-agents is a fundamental problem in various scientific and engineering fields. Reinforcement learning (RL) is a powerful framework to generate flexible and diverse behaviors in cyberspace; however, when modeling real-world biological multi-agents, there is a domain gap between behaviors in the source (i.e., real-world data) and the target (i.e., cyberspace for RL), and the source environment parameters are usually unknown. In this paper, we propose a method for adaptive action supervision in RL from real-world demonstrations in multi-agent scenarios. We adopt an approach that combines RL and supervised learning by selecting actions of demonstrations in RL based on the minimum distance of dynamic time warping for utilizing the information of the unknown source dynamics. This approach can be easily applied to many existing neural network architectures and provide us with an RL model balanced between reproducibility as imitation and generalization ab
&lt;/p&gt;</description></item><item><title>RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11476</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#22810;&#26679;&#39118;&#38505;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11476
&lt;/p&gt;
&lt;p&gt;
RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#20013;&#65292;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#35299;&#20915;&#31454;&#20105;&#24615;&#28216;&#25103;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#24403;&#21069;&#30340;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#20248;&#21270;&#20195;&#29702;&#31243;&#24207;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#32988;&#29575;&#26102;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#24182;&#20135;&#29983;&#21333;&#19968;&#21516;&#36136;&#21270;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25171;&#30772;&#20725;&#23616;&#24182;&#22686;&#24378;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#26041;&#27861;&#21487;&#33021;&#22312;&#20110;&#22686;&#21152;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#22686;&#21152;&#22810;&#26679;&#24615;&#24182;&#19981;&#26159;&#26131;&#22914;&#21453;&#25484;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#21487;&#20197;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#36825;&#19968;&#35270;&#35282;&#20986;&#21457;&#22686;&#21152;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#39118;&#38505;&#25935;&#24863;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(RPPO)&#65292;&#23427;&#22312;&#26368;&#22351;&#21644;&#26368;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#20043;&#38388;&#24179;&#28369;&#22320;&#25554;&#20540;&#65292;&#20801;&#35768;&#20855;&#26377;&#25152;&#38656;&#39118;&#38505;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the great successes of Reinforcement Learning (RL), self-play algorithms play an essential role in solving competitive games. Current self-play algorithms optimize the agent to maximize expected win-rates against its current or historical copies, making it often stuck in the local optimum and its strategy style simple and homogeneous. A possible solution is to improve the diversity of policies, which helps the agent break the stalemate and enhances its robustness when facing different opponents. However, enhancing diversity in the self-play algorithms is not trivial. In this paper, we aim to introduce diversity from the perspective that agents could have diverse risk preferences in the face of uncertainty. Specifically, we design a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning and allows for policy learning with desired risk preferences. Seamlessly inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.08754</link><description>&lt;p&gt;
W-MAE&#65306;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; W-MAE &#30340;&#39044;&#35757;&#32451;&#22825;&#27668;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#20855;&#26377;&#30452;&#25509;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#30340;&#38271;&#26399;&#35745;&#31639;&#25361;&#25112;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#30340;&#36830;&#32493;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#23558;&#39044;&#35757;&#32451;&#25216;&#26415;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#30340;&#20855;&#26377;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#30340;&#22825;&#27668;&#27169;&#22411;W-MAE&#12290;W-MAE&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#37325;&#24314;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#22312;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;W-MAE&#20197;&#39044;&#27979;&#27668;&#35937;&#21464;&#37327;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#32780;&#23545;&#22825;&#27668;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#27599;&#20845;&#23567;&#26102;&#36873;&#25321;&#19968;&#27425;&#26679;&#26412;&#65292;&#20165;&#20351;&#29992;&#20004;&#24180;&#30340;ERA5&#25968;&#25454;&#65292;&#23545;W-MAE&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23558;W-MAE&#19982;FourCastNet&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05836</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Game-theoretic Framework for Federated Learning. (arXiv:2304.05836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21516;&#26102;&#32771;&#34385;&#21040;&#32852;&#37030;&#23398;&#20064;&#30340;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#25910;&#30410;&#65292;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#31639;&#27861;&#23545;&#20110;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#25915;&#20987;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#33391;&#24615;&#21442;&#19982;&#32773;&#26088;&#22312;&#21327;&#21516;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#21322;&#35802;&#23454;&#30340;&#23545;&#25163;&#26102;&#65292;\textit{&#38544;&#31169;&#27844;&#28431;}&#30340;&#39118;&#38505;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#35774;&#35745;&#20445;&#25252;&#26426;&#21046;&#65292;&#35201;&#20040;&#19987;&#27880;&#20110;&#21457;&#26126;&#25915;&#20987;&#26426;&#21046;&#12290;&#34429;&#28982;&#20445;&#25252;&#32773;&#19982;&#25915;&#20987;&#32773;&#20043;&#38388;&#30340;&#26007;&#20105;&#20284;&#20046;&#27704;&#26080;&#27490;&#22659;&#65292;&#20294;&#25105;&#20204;&#20851;&#24515;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#20107;&#20808;&#39044;&#38450;&#28508;&#22312;&#30340;&#25915;&#20987;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;FL&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#30340;&#30456;&#24212;&#25910;&#30410;&#65292;&#20854;&#20013;&#21253;&#25324;&#35745;&#31639;&#25104;&#26412;&#12289;FL&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#27844;&#28431;&#39118;&#38505;&#12290;&#25105;&#20204;&#23558;&#27492;&#28216;&#25103;&#31216;&#20026;&#32852;&#37030;&#23398;&#20064;&#23433;&#20840;&#21338;&#24328;&#65288;FLSG&#65289;&#65292;&#22312;&#20854;&#20013;&#20445;&#25252;&#32773;&#21644;&#25915;&#20987;&#32773;&#37117;&#19981;&#30693;&#36947;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#22266;&#26377;&#30340;\textit{&#19981;&#23436;&#20840;&#20449;&#24687;}&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;FLSG&#19982;&#19968;&#20010;\textit{oracle}&#30456;&#20851;&#32852;&#65292;&#35813;oracle&#20855;&#26377;&#25152;&#26377;&#21442;&#19982;&#32773;&#30340;&#25910;&#30410;&#30693;&#35782;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#21508;&#31181;&#25928;&#29992;&#20989;&#25968;&#21644;&#25915;&#20987;&#27169;&#22411;&#32452;&#21512;&#19979;FLSG&#30340;&#32435;&#20160;&#22343;&#34913;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#26469;&#36817;&#20284;oracle&#24182;&#20445;&#25345;&#38544;&#31169;&#12290;&#23454;&#39564;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#39044;&#38450;&#21644;&#26816;&#27979;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;FL&#22330;&#26223;&#20013;&#30340;&#25915;&#20987;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, benign participants aim to optimize a global model collaboratively. However, the risk of \textit{privacy leakage} cannot be ignored in the presence of \textit{semi-honest} adversaries. Existing research has focused either on designing protection mechanisms or on inventing attacking mechanisms. While the battle between defenders and attackers seems never-ending, we are concerned with one critical question: is it possible to prevent potential attacks in advance? To address this, we propose the first game-theoretic framework that considers both FL defenders and attackers in terms of their respective payoffs, which include computational costs, FL model utilities, and privacy leakage risks. We name this game the Federated Learning Security Game (FLSG), in which neither defenders nor attackers are aware of all participants' payoffs.  To handle the \textit{incomplete information} inherent in this situation, we propose associating the FLSG with an \textit{oracle} that ha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17505</link><description>&lt;p&gt;
&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Space-fluid Adaptive Sampling by Self-Organisation. (arXiv:2210.17505v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32452;&#32455;&#30340;&#31354;&#38388;&#27969;&#20307;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#26469;&#20272;&#35745;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#65292;&#20854;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#35843;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#31649;&#29702;&#65288;&#20272;&#35745;&#12289;&#39044;&#27979;&#25110;&#25511;&#21046;&#65289;&#38543;&#31354;&#38388;&#21464;&#21270;&#30340;&#20449;&#21495;&#65292;&#20363;&#22914;&#20998;&#24067;&#24335;&#20256;&#24863;&#25968;&#25454;&#25110;&#35745;&#31639;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31454;&#20105;&#21644;&#29983;&#38271;/&#32553;&#23567;&#30340;&#21160;&#24577;&#21010;&#20998;&#31354;&#38388;&#26041;&#27861;&#65292;&#21327;&#21516;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#20272;&#35745;&#31354;&#38388;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22330;&#30340;&#21327;&#35843;&#26694;&#26550;&#20013;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#33258;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20272;&#35745;&#22797;&#26434;&#20989;&#25968;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#36319;&#36394;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#26102;&#31354;&#29616;&#35937;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recurrent task in coordinated systems is managing (estimating, predicting, or controlling) signals that vary in space, such as distributed sensed data or computation outcomes. Especially in large-scale settings, the problem can be addressed through decentralised and situated computing systems: nodes can locally sense, process, and act upon signals, and coordinate with neighbours to implement collective strategies. Accordingly, in this work we devise distributed coordination strategies for the estimation of a spatial phenomenon through collaborative adaptive sampling. Our design is based on the idea of dynamically partitioning space into regions that compete and grow/shrink to provide accurate aggregate sampling. Such regions hence define a sort of virtualised space that is "fluid", since its structure adapts in response to pressure forces exerted by the underlying phenomenon. We provide an adaptive sampling algorithm in the field-based coordination framework, and prove it is self-sta
&lt;/p&gt;</description></item></channel></rss>