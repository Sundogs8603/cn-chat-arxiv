<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#24213;&#23618;&#24120;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;3D&#20132;&#20114;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12288</link><description>&lt;p&gt;
CHORUS: &#20174;&#26080;&#38480;&#21512;&#25104;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images. (arXiv:2308.12288v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12288
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;3D&#20154;&#20307;-&#29289;&#20307;&#31354;&#38388;&#20851;&#31995;&#30340;&#24213;&#23618;&#24120;&#35782;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;3D&#20132;&#20114;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#21644;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#25945;&#26426;&#22120;&#29702;&#35299;&#21644;&#24314;&#27169;&#22810;&#26679;&#21270;&#30340;3D&#20154;&#20307;-&#29289;&#20307;&#20132;&#20114;&#30340;&#24213;&#23618;&#31354;&#38388;&#24120;&#35782;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23384;&#22312;&#21487;&#20197;&#34987;&#35270;&#20026;&#31867;&#20284;&#20154;&#31867;&#21644;&#33258;&#28982;&#30340;&#20132;&#20114;&#29305;&#23450;&#27969;&#24418;&#65292;&#20294;&#26159;&#21363;&#20351;&#26159;&#30456;&#20284;&#30340;&#20132;&#20114;&#65292;&#20154;&#20307;&#23039;&#21183;&#21644;&#29289;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#20063;&#21487;&#20197;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;&#24471;&#27880;&#37322;3D&#20132;&#20114;&#30340;&#20219;&#21153;&#22256;&#38590;&#19988;&#38590;&#20197;&#25193;&#23637;&#65292;&#38480;&#21046;&#20102;&#29992;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#28508;&#21147;&#12290;&#23398;&#20064;&#20154;&#31867;&#21644;&#29289;&#20307;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;3D&#31354;&#38388;&#20851;&#31995;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#36890;&#36807;&#23637;&#31034;&#22810;&#20010;&#20174;&#19981;&#21516;&#35270;&#35282;&#25429;&#33719;&#30340;2D&#22270;&#20687;&#65292;&#24403;&#20154;&#31867;&#19982;&#30456;&#21516;&#31867;&#22411;&#30340;&#29289;&#20307;&#20114;&#21160;&#26102;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#24847;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#20316;&#20026;"&#26080;&#38480;"&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#21487;&#25511;&#24615;&#21644;&#35270;&#35282;&#22810;&#26679;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#20854;&#19981;&#23436;&#32654;&#20043;&#22788;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a method for teaching machines to understand and model the underlying spatial common sense of diverse human-object interactions in 3D in a self-supervised way. This is a challenging task, as there exist specific manifolds of the interactions that can be considered human-like and natural, but the human pose and the geometry of objects can vary even for similar interactions. Such diversity makes the annotating task of 3D interactions difficult and hard to scale, which limits the potential to reason about that in a supervised way. One way of learning the 3D spatial relationship between humans and objects during interaction is by showing multiple 2D images captured from different viewpoints when humans interact with the same type of objects. The core idea of our method is to leverage a generative model that produces high-quality 2D images from an arbitrary text prompt input as an "unbounded" data generator with effective controllability and view diversity. Despite its imperfecti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.12284</link><description>&lt;p&gt;
D4&#65306;&#36890;&#36807;&#25991;&#26723;&#21435;&#37325;&#19982;&#22810;&#26679;&#21270;&#25913;&#36827;LLM&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
D4: Improving LLM Pretraining via Document De-Duplication and Diversification. (arXiv:2308.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#21644;&#25968;&#25454;&#37325;&#22797;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;D4&#31639;&#27861;&#21487;&#20197;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#21152;&#36895;&#35757;&#32451;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#34987;&#29992;&#20110;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#23545;&#26469;&#33258;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;&#23613;&#21487;&#33021;&#22810;&#30340;&#26631;&#35760;&#36827;&#34892;&#19968;&#27425;&#23398;&#20064;&#12290;&#34429;&#28982;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#20114;&#32852;&#32593;&#23616;&#37096;&#19978;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#19981;&#26029;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#25913;&#36827;&#30340;&#35268;&#27169;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#20943;&#23567;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#25968;&#25454;&#36873;&#25321;&#23545;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#38500;&#20102;MinHash&#31561;&#31616;&#21333;&#30340;&#21435;&#37325;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#23637;&#31034;&#20102;&#36890;&#36807;&#35880;&#24910;&#30340;&#25968;&#25454;&#36873;&#25321;(&#22312;&#21435;&#37325;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;)&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;(&#25552;&#39640;&#20102;20%&#30340;&#25928;&#29575;)&#24182;&#19988;&#22312;16&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24179;&#22343;&#19979;&#28216;&#20934;&#30830;&#29575;&#19978;&#26377;&#25152;&#25552;&#21319;(&#39640;&#36798;2%)&#65292;&#22312;6.7B&#27169;&#22411;&#35268;&#27169;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26234;&#33021;&#37325;&#22797;&#25968;&#25454;&#30340;&#34920;&#29616;&#24635;&#26159;&#20248;&#20110;&#22522;&#32447;&#35757;&#32451;(&#32780;&#37325;&#22797;&#38543;&#26426;&#25968;&#25454;&#30340;&#34920;&#29616;&#27604;&#22522;&#32447;&#35757;&#32451;&#26356;&#24046;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32874;&#26126;&#30340;&#25968;&#25454;&#22788;&#29702;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;LLM&#30340;&#35757;&#32451;&#25928;&#26524;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26816;&#39564;&#20102;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#21450;&#20854;&#38598;&#25104;&#22312;&#22522;&#20934;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;FLMs&#30340;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#23637;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.12272</link><description>&lt;p&gt;
&#31616;&#21333;&#21363;&#26159;&#26356;&#22909;&#65292;&#22823;&#24182;&#19981;&#36275;&#22815;&#65306;&#36208;&#21521;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple is Better and Large is Not Enough: Towards Ensembling of Foundational Language Models. (arXiv:2308.12272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26816;&#39564;&#20102;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#21450;&#20854;&#38598;&#25104;&#22312;&#22522;&#20934;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;FLMs&#30340;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#23637;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;(FLMs)&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#32773;&#27491;&#22312;&#24320;&#21457;&#26356;&#22823;&#30340;FLMs&#65288;&#20363;&#22914;&#65292;XLNet&#12289;T5&#65289;&#20197;&#23454;&#29616;&#19978;&#19979;&#25991;&#21270;&#30340;&#35821;&#35328;&#34920;&#31034;&#12289;&#20998;&#31867;&#21644;&#29983;&#25104;&#12290;&#34429;&#28982;&#24320;&#21457;&#26356;&#22823;&#30340;FLMs&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#20063;&#23384;&#22312;&#34394;&#26500;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#39118;&#38505;&#12290;&#20174;&#26681;&#26412;&#19978;&#35828;&#65292;&#26356;&#22823;&#30340;FLMs&#24314;&#31435;&#22312;&#36739;&#23567;&#30340;FLMs&#65288;&#20363;&#22914;&#65292;BERT&#65289;&#30340;&#22522;&#30784;&#20043;&#19978;&#65307;&#22240;&#27492;&#65292;&#20154;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#36739;&#23567;&#30340;FLMs&#30340;&#28508;&#21147;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#38598;&#25104;&#26469;&#23454;&#29616;&#12290;&#22312;&#24403;&#21069;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;FLMs&#21450;&#20854;&#38598;&#25104;&#36827;&#34892;&#20102;&#23454;&#38469;&#26816;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;FLMs&#30340;&#38598;&#25104;&#21487;&#20197;&#24433;&#21709;&#20854;&#20010;&#20307;&#20851;&#27880;&#65292;&#24182;&#25581;&#31034;&#19981;&#21516;FLMs&#20043;&#38388;&#30340;&#21327;&#35843;&#21644;&#21512;&#20316;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#21033;&#29992;BERT&#24182;&#23450;&#20041;&#20102;&#19977;&#31181;&#20854;&#20182;&#30340;&#38598;&#25104;&#25216;&#26415;&#65306;{&#27973;&#23618;&#12289;&#21322;&#28145;&#23618;&#21644;&#28145;&#23618;}&#65292;&#20854;&#20013;&#28145;&#23618;&#38598;&#25104;&#24341;&#20837;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundational Language Models (FLMs) have advanced natural language processing (NLP) research. Current researchers are developing larger FLMs (e.g., XLNet, T5) to enable contextualized language representation, classification, and generation. While developing larger FLMs has been of significant advantage, it is also a liability concerning hallucination and predictive uncertainty. Fundamentally, larger FLMs are built on the same foundations as smaller FLMs (e.g., BERT); hence, one must recognize the potential of smaller FLMs which can be realized through an ensemble. In the current research, we perform a reality check on FLMs and their ensemble on benchmark and real-world datasets. We hypothesize that the ensembling of FLMs can influence the individualistic attention of FLMs and unravel the strength of coordination and cooperation of different FLMs. We utilize BERT and define three other ensemble techniques: {Shallow, Semi, and Deep}, wherein the Deep-Ensemble introduces a knowledge-guide
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#20195;&#26367;&#20256;&#32479;&#30340;&#20219;&#21153;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.12270</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22870;&#21169;&#35843;&#21046;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language Reward Modulation for Pretraining Reinforcement Learning. (arXiv:2308.12270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAMP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#22870;&#21169;&#20989;&#25968;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#20195;&#26367;&#20256;&#32479;&#30340;&#20219;&#21153;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#65288;LRF&#65289;&#26469;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#31283;&#23450;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#24403;&#20170;&#30340;LRF&#26159;&#21542;&#26368;&#36866;&#21512;&#20316;&#20026;&#20219;&#21153;&#22870;&#21169;&#30340;&#30452;&#25509;&#26367;&#20195;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LRF&#20316;&#20026;RL&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Language Reward Modulated Pretraining&#65288;LAMP&#65289;&#65292;&#23427;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#38646;&#23556;&#33021;&#21147;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#25928;&#29992;&#26469;&#20195;&#26367;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#12290;LAMP&#20351;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#12289;&#39044;&#35757;&#32451;&#30340;VLM&#26469;&#21487;&#25193;&#23637;&#22320;&#29983;&#25104;&#22122;&#22768;&#30340;&#25506;&#32034;&#22870;&#21169;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#20010;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25351;&#20196;&#38598;&#21644;&#19968;&#20010;&#20195;&#29702;&#22312;&#39044;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#22270;&#20687;&#35266;&#27979;&#20043;&#38388;&#30340;&#23545;&#27604;&#23545;&#40784;&#12290;LAMP&#19982;&#26631;&#20934;&#30340;&#25506;&#32034;&#22870;&#21169;&#19968;&#36215;&#20248;&#21270;&#36825;&#20123;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using learned reward functions (LRFs) as a means to solve sparse-reward reinforcement learning (RL) tasks has yielded some steady progress in task-complexity through the years. In this work, we question whether today's LRFs are best-suited as a direct replacement for task rewards. Instead, we propose leveraging the capabilities of LRFs as a pretraining signal for RL. Concretely, we propose $\textbf{LA}$nguage Reward $\textbf{M}$odulated $\textbf{P}$retraining (LAMP) which leverages the zero-shot capabilities of Vision-Language Models (VLMs) as a $\textit{pretraining}$ utility for RL as opposed to a downstream task reward. LAMP uses a frozen, pretrained VLM to scalably generate noisy, albeit shaped exploration rewards by computing the contrastive alignment between a highly diverse collection of language instructions and the image observations of an agent in its pretraining environment. LAMP optimizes these rewards in conjunction with standard novelty-seeking exploration rewards with rei
&lt;/p&gt;</description></item><item><title>FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.12264</link><description>&lt;p&gt;
FECoM: &#26397;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
FECoM: A Step towards Fine-Grained Energy Measurement for Deep Learning. (arXiv:2308.12264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12264
&lt;/p&gt;
&lt;p&gt;
FECoM&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#22240;&#32032;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#12289;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20854;&#33021;&#28304;&#28040;&#32791;&#36805;&#36895;&#22686;&#38271;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20419;&#36827;&#32511;&#33394;&#21457;&#23637;&#21644;&#19981;&#21516;&#31890;&#24230;&#30340;&#33021;&#28304;&#24847;&#35782;&#65292;&#20197;&#38480;&#21046;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#30899;&#25490;&#25918;&#26159;&#24403;&#21153;&#20043;&#24613;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20934;&#30830;&#27979;&#37327;&#21644;&#20248;&#21270;&#32454;&#31890;&#24230;&#65288;&#20363;&#22914;&#26041;&#27861;&#32423;&#21035;&#65289;&#33021;&#32791;&#30340;&#26631;&#20934;&#21644;&#21487;&#37325;&#22797;&#24037;&#20855;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FECoM&#65288;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#20202;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#32454;&#31890;&#24230;&#28145;&#24230;&#23398;&#20064;&#33021;&#32791;&#27979;&#37327;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FECoM&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28145;&#24230;&#23398;&#20064;API&#36827;&#34892;&#27010;&#35201;&#20998;&#26512;&#30340;&#26426;&#21046;&#12290;FECoM&#36890;&#36807;&#20351;&#29992;&#38745;&#24577;&#20202;&#22120;&#20998;&#26512;&#21644;&#32771;&#34385;&#35745;&#31639;&#36127;&#36733;&#21644;&#28201;&#24230;&#31283;&#23450;&#24615;&#31561;&#21508;&#31181;&#22240;&#32032;&#26469;&#35299;&#20915;&#32454;&#31890;&#24230;&#33021;&#32791;&#27979;&#37327;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;FECoM&#22312;&#26368;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#19978;&#27979;&#37327;&#32454;&#31890;&#24230;&#33021;&#32791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing usage, scale, and complexity of Deep Learning (DL) models, their rapidly growing energy consumption has become a critical concern. Promoting green development and energy awareness at different granularities is the need of the hour to limit carbon emissions of DL systems. However, the lack of standard and repeatable tools to accurately measure and optimize energy consumption at a fine granularity (e.g., at method level) hinders progress in this area. In this paper, we introduce FECoM (Fine-grained Energy Consumption Meter), a framework for fine-grained DL energy consumption measurement. Specifically, FECoM provides researchers and developers a mechanism to profile DL APIs. FECoM addresses the challenges of measuring energy consumption at fine-grained level by using static instrumentation and considering various factors, including computational load and temperature stability. We assess FECoM's capability to measure fine-grained energy consumption for one of the most p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>LLMRec&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#23545;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#31561;&#20934;&#30830;&#24615;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#29087;&#32451;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12241</link><description>&lt;p&gt;
LLMRec: &#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLMRec: Benchmarking Large Language Models on Recommendation Task. (arXiv:2308.12241v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12241
&lt;/p&gt;
&lt;p&gt;
LLMRec&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#29992;&#20110;&#23545;LLMs&#22312;&#25512;&#33616;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#31561;&#20934;&#30830;&#24615;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20013;&#31561;&#29087;&#32451;&#31243;&#24230;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25512;&#33616;&#39046;&#22495;&#30340;&#24212;&#29992;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMRec&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#23545;&#22810;&#31181;&#25512;&#33616;&#20219;&#21153;&#36827;&#34892;LLMs&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#20960;&#31181;&#28909;&#38376;&#30340;&#29616;&#25104;LLMs&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;ChatGLM&#65292;&#28085;&#30422;&#20102;&#35780;&#20998;&#39044;&#27979;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#30452;&#25509;&#25512;&#33616;&#12289;&#35299;&#37322;&#29983;&#25104;&#21644;&#35780;&#35770;&#25688;&#35201;&#31561;&#20116;&#20010;&#25512;&#33616;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25552;&#39640;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#22522;&#20110;&#20934;&#30830;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;&#39034;&#24207;&#25512;&#33616;&#21644;&#30452;&#25509;&#25512;&#33616;&#65289;&#19978;&#21482;&#34920;&#29616;&#20986;&#20013;&#31561;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25511;&#21046;&#33021;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art me
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38041;&#32452;&#23398;&#29305;&#24449;&#30340;AI&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#24515;&#34880;&#31649;&#39118;&#38505;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#39640;&#39118;&#38505;&#29305;&#24449;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.12224</link><description>&lt;p&gt;
AI-&#22686;&#24378;&#30340;&#38041;&#32452;&#23398;&#22312;&#24515;&#34880;&#31649;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing cardiovascular risk prediction through AI-enabled calcium-omics. (arXiv:2308.12224v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38041;&#32452;&#23398;&#29305;&#24449;&#30340;AI&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;&#24515;&#34880;&#31649;&#39118;&#38505;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#39640;&#39118;&#38505;&#29305;&#24449;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20896;&#29366;&#21160;&#33033;&#38041;&#21270;&#26159;&#37325;&#35201;&#30340;&#24515;&#34880;&#31649;&#19981;&#33391;&#20107;&#20214;&#65288;MACE&#65289;&#30340;&#39044;&#27979;&#22240;&#23376;&#12290;&#20256;&#32479;&#30340;&#38463;&#21152;&#29305;&#26031;&#39039;&#20998;&#25968;&#20165;&#20165;&#26159;&#31616;&#21333;&#22320;&#23545;&#38041;&#36827;&#34892;&#27714;&#21644;&#65292;&#34429;&#28982;&#20197;&#38750;&#32447;&#24615;&#30340;&#26041;&#24335;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#30142;&#30149;&#30340;&#31243;&#24230;&#12290;&#30446;&#26631;&#65306;&#30830;&#23450;&#26159;&#21542;&#20351;&#29992;&#35814;&#32454;&#30340;&#38041;&#21270;&#29305;&#24449;&#65288;&#21363;&#38041;&#32452;&#23398;&#65289;&#36890;&#36807;AI&#26041;&#27861;&#21487;&#20197;&#25913;&#36827;MACE&#30340;&#39044;&#27979;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30740;&#31350;&#20102;&#38041;&#21270;&#30340;&#20854;&#20182;&#29305;&#24449;&#65292;&#21253;&#25324;&#36136;&#37327;&#12289;&#20307;&#31215;&#12289;&#23494;&#24230;&#12289;&#31354;&#38388;&#20998;&#24067;&#12289;&#21306;&#22495;&#31561;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#24377;&#24615;&#32593;&#27491;&#21017;&#21270;&#30340;Cox&#27169;&#22411;&#23545;2457&#20363;CT&#38041;&#20998;&#25968;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#35813;&#25968;&#25454;&#26159;&#30001;&#22823;&#22411;&#20813;&#36153;&#30340;CLARIFY&#39033;&#30446;&#65288;ClinicalTri-als.gov&#26631;&#35782;&#31526;&#65306;NCT04075162&#65289;&#24471;&#21040;&#30340;&#65292;&#35813;&#39033;&#30446;&#30528;&#37325;&#20110;MACE&#20107;&#20214;&#12290;&#25105;&#20204;&#37319;&#29992;&#37319;&#26679;&#25216;&#26415;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#36873;&#23450;&#29305;&#24449;&#30340;Cox&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#21487;&#20197;&#35299;&#37322;&#39640;&#39118;&#38505;&#29305;&#24449;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#25552;&#20986;&#30340;&#38041;&#32452;&#23398;&#27169;&#22411;&#32463;&#36807;&#20462;&#25913;&#30340;&#21516;&#27493;&#27169;&#22411;&#20351;&#24471;&#24515;&#34880;&#31649;&#39118;&#38505;&#39044;&#27979;&#24471;&#21040;&#20102;&#26126;&#26174;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background. Coronary artery calcium (CAC) is a powerful predictor of major adverse cardiovascular events (MACE). Traditional Agatston score simply sums the calcium, albeit in a non-linear way, leaving room for improved calcification assessments that will more fully capture the extent of disease.  Objective. To determine if AI methods using detailed calcification features (i.e., calcium-omics) can improve MACE prediction.  Methods. We investigated additional features of calcification including assessment of mass, volume, density, spatial distribution, territory, etc. We used a Cox model with elastic-net regularization on 2457 CT calcium score (CTCS) enriched for MACE events obtained from a large no-cost CLARIFY program (ClinicalTri-als.gov Identifier: NCT04075162). We employed sampling techniques to enhance model training. We also investigated Cox models with selected features to identify explainable high-risk characteristics.  Results. Our proposed calcium-omics model with modified syn
&lt;/p&gt;</description></item><item><title>&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.12221</link><description>&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;
&lt;/p&gt;
&lt;p&gt;
Critical Learning Periods Emerge Even in Deep Linear Networks. (arXiv:2308.12221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12221
&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#20851;&#38190;&#23398;&#20064;&#26399;&#65292;&#36825;&#20123;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#25351;&#22312;&#21457;&#32946;&#26089;&#26399;&#65292;&#26242;&#26102;&#30340;&#24863;&#30693;&#32570;&#38519;&#20250;&#23545;&#34892;&#20026;&#21644;&#23398;&#20064;&#34920;&#31034;&#20135;&#29983;&#27704;&#20037;&#24433;&#21709;&#30340;&#26102;&#38388;&#27573;&#12290;&#23613;&#31649;&#29983;&#29289;&#32593;&#32476;&#21644;&#20154;&#24037;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#24046;&#24322;&#65292;&#20294;&#20851;&#38190;&#23398;&#20064;&#26399;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#37117;&#26377;&#32463;&#39564;&#35266;&#23519;&#21040;&#12290;&#36825;&#34920;&#26126;&#20851;&#38190;&#23398;&#20064;&#26399;&#21487;&#33021;&#26159;&#23398;&#20064;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#32780;&#19981;&#26159;&#29983;&#29289;&#23398;&#19978;&#30340;&#20598;&#28982;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20026;&#20160;&#20040;&#20851;&#38190;&#23398;&#20064;&#26399;&#20250;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#23588;&#20854;&#26159;&#19981;&#28165;&#26970;&#22312;&#20004;&#20010;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#23398;&#20064;&#26399;&#26159;&#21542;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26550;&#26500;&#25110;&#20248;&#21270;&#32454;&#33410;&#12290;&#20026;&#20102;&#30830;&#23450;&#20851;&#38190;&#30340;&#22522;&#26412;&#22240;&#32032;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#32593;&#32476;&#20063;&#26174;&#31034;&#20986;&#29983;&#29289;&#23398;&#21644;&#20154;&#24037;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#36827;&#34892;&#20998;&#26512;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#38190;&#23398;&#20064;&#26399;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#36171;&#20104;&#20102;&#21306;&#20998;OOD&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12213</link><description>&lt;p&gt;
CLIPN&#29992;&#20110;&#38646;&#26679;&#26412;OOD&#26816;&#27979;&#65306;&#25945;CLIP&#35828;&#8220;&#19981;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No. (arXiv:2308.12213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLIPN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#36171;&#20104;&#20102;&#21306;&#20998;OOD&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OOD&#26816;&#27979;&#26159;&#25351;&#22312;&#20869;&#37096;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#36755;&#20837;&#22270;&#20687;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#12290;&#35774;&#35745;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#30340;&#21508;&#31181;OOD&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#20165;&#38656;&#35201;ID&#30340;&#31867;&#21517;&#30340;CLIP&#39537;&#21160;&#30340;&#38646;&#26679;&#26412;OOD&#26816;&#27979;&#26041;&#27861;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;CLIP&#35828;&#8220;&#19981;&#8221;&#65288;CLIPN&#65289;&#65292;&#23427;&#36171;&#20104;&#20102;CLIP&#22312;&#36923;&#36753;&#19978;&#35828;&#8220;&#19981;&#8221;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#36890;&#36807;&#31215;&#26497;&#30340;&#35821;&#20041;&#25552;&#31034;&#21644;&#21542;&#23450;&#30340;&#35821;&#20041;&#25552;&#31034;&#65292;&#20026;CLIP&#25552;&#20379;&#21306;&#20998;OOD&#26679;&#26412;&#21644;ID&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#23398;&#20064;&#30340;&#8220;&#19981;&#8221;&#25552;&#31034;&#31526;&#21644;&#19968;&#20010;&#8220;&#19981;&#8221;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#25429;&#33719;&#22270;&#20687;&#20013;&#30340;&#21542;&#23450;&#35821;&#20041;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#65306;&#22270;&#20687;-&#25991;&#26412;&#20108;&#20803;&#30456;&#21453;&#25439;&#22833;&#21644;&#25991;&#26412;&#35821;&#20041;&#30456;&#21453;&#25439;&#22833;&#65292;&#29992;&#20110;&#25945;&#25480;CLIPN&#20851;&#32852;OOD&#21644;ID&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection refers to training the model on an in-distribution (ID) dataset to classify whether the input images come from unknown classes. Considerable effort has been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, zero-shot OOD detection methods driven by CLIP, which only require class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying "no" (\textbf{CLIPN}), which empowers the logic of saying "no" within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples using positive-semantic prompts and negation-semantic prompts. Specifically, we design a novel learnable "no" prompt and a "no" text encoder to capture negation semantics within images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to assoc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;L2GMOM&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#26114;&#36149;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.12212</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#20197;&#20248;&#21270;&#21160;&#21147;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn Financial Networks for Optimising Momentum Strategies. (arXiv:2308.12212v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;L2GMOM&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#26114;&#36149;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21160;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#39118;&#38505;&#28322;&#20215;&#65292;&#23427;&#21033;&#29992;&#37329;&#34701;&#32593;&#32476;&#20013;&#36164;&#20135;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26500;&#24314;&#37329;&#34701;&#32593;&#32476;&#30340;&#36807;&#31243;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#25968;&#25454;&#24211;&#21644;&#37329;&#34701;&#19987;&#19994;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23567;&#22411;&#21644;&#23398;&#26415;&#26426;&#26500;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#26041;&#27861;&#23558;&#32593;&#32476;&#26500;&#24314;&#21644;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#35270;&#20026;&#21333;&#29420;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#25237;&#36164;&#32452;&#21512;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2GMOM&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#21516;&#26102;&#23398;&#20064;&#37329;&#34701;&#32593;&#32476;&#21644;&#20248;&#21270;&#32593;&#32476;&#21160;&#37327;&#31574;&#30053;&#30340;&#20132;&#26131;&#20449;&#21495;&#12290;L2GMOM&#27169;&#22411;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#21069;&#21521;&#20256;&#25773;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#26159;&#20174;&#31639;&#27861;&#23637;&#24320;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;L2GMOM&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#25237;&#36164;&#32452;&#21512;&#32489;&#25928;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20363;&#22914;&#36127;&#22799;&#26222;&#27604;&#29575;&#12290;&#22312;&#22238;&#27979;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;50&#20010;&#30495;&#23454;&#21830;&#19994;&#27969;&#31243;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21457;&#29616;&#21644;&#20998;&#26512;&#29992;&#25143;&#23436;&#25104;&#21830;&#19994;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#23545;&#20110;&#20219;&#21153;&#25366;&#25496;&#21644;&#27969;&#31243;&#33258;&#21160;&#21270;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12211</link><description>&lt;p&gt;
&#35760;&#24405;&#20102;50&#20010;&#21830;&#19994;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Recording of 50 Business Assignments. (arXiv:2308.12211v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;50&#20010;&#30495;&#23454;&#21830;&#19994;&#27969;&#31243;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21457;&#29616;&#21644;&#20998;&#26512;&#29992;&#25143;&#23436;&#25104;&#21830;&#19994;&#20219;&#21153;&#30340;&#36807;&#31243;&#65292;&#23545;&#20110;&#20219;&#21153;&#25366;&#25496;&#21644;&#27969;&#31243;&#33258;&#21160;&#21270;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#25366;&#25496;&#30340;&#20027;&#35201;&#29992;&#36884;&#20043;&#19968;&#26159;&#21457;&#29616;&#21644;&#20998;&#26512;&#29992;&#25143;&#22914;&#20309;&#23436;&#25104;&#21830;&#19994;&#20219;&#21153;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#20851;&#36807;&#31243;&#25928;&#29575;&#21644;&#20248;&#21270;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;50&#20010;&#30495;&#23454;&#21830;&#19994;&#27969;&#31243;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20219;&#21153;&#25366;&#25496;&#21644;&#27969;&#31243;&#33258;&#21160;&#21270;&#31561;&#21508;&#31181;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#28508;&#21147;&#65292;&#26159;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main use cases of process mining is to discover and analyze how users follow business assignments, providing valuable insights into process efficiency and optimization. In this paper, we present a comprehensive dataset consisting of 50 real business processes. The dataset holds significant potential for research in various applications, including task mining and process automation which is a valuable resource for researchers and practitioners.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#22320;&#37327;&#21270;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#26500;&#36896;&#20102;&#32039;&#33268;&#30340;ReachTube&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#20445;&#35777;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20013;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.12192</link><description>&lt;p&gt;
&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques. (arXiv:2308.12192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12192
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#22320;&#37327;&#21270;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#22522;&#20110;Lagrangian&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#26500;&#36896;&#20102;&#32039;&#33268;&#30340;ReachTube&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#20445;&#35777;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#20013;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#30340;&#20351;&#29992;&#65292;&#23454;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20171;&#32461;&#20102;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;Lagrangian&#39564;&#35777;&#25216;&#26415;&#12290;&#23427;&#20204;&#24418;&#24335;&#21270;&#22320;&#37327;&#21270;&#20102;&#20219;&#20309;&#20197;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#24418;&#24335;&#21576;&#29616;&#30340;&#26102;&#38388;&#36830;&#32493;&#36807;&#31243;&#30340;&#34892;&#20026;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;LRT-NG&#65292;SLR&#21644;GoTube&#31639;&#27861;&#65292;&#29992;&#20110;&#26500;&#36896;&#32039;&#33268;ReachTube&#65292;&#21363;&#22312;&#32473;&#23450;&#26102;&#38388;&#33539;&#22260;&#20869;&#21040;&#36798;&#30340;&#29366;&#24577;&#38598;&#30340;&#36807;&#24230;&#20272;&#35745;&#65292;&#24182;&#20026;ReachTube&#36793;&#30028;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#19982;&#31995;&#32479;&#26041;&#31243;&#30456;&#20851;&#30340;&#21464;&#20998;&#26041;&#31243;&#12289;&#22343;&#20540;&#23450;&#29702;&#21644;Lipschitz&#24120;&#25968;&#22312;&#23454;&#29616;&#30830;&#23450;&#24615;&#21644;&#32479;&#35745;&#24615;&#20445;&#35777;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#22312;LRT-NG&#20013;&#65292;Lipschitz&#24120;&#25968;&#34987;&#29992;&#20316;&#21021;&#22987;&#25200;&#21160;&#30340;&#33192;&#32960;&#22240;&#23376;&#65292;&#20197;&#35745;&#31639;&#26925;&#22278;&#20307;&#20013;&#30340;&#21322;&#24452;&#65292;&#35813;&#26925;&#22278;&#20307;&#20197;&#26368;&#20339;&#24230;&#37327;&#26041;&#24335;&#36807;&#24230;&#20272;&#35745;&#20102;&#21487;&#36798;&#29366;&#24577;&#38598;&#12290;&#22312;SLR&#21644;GoTube&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;Lipschitz&#24120;&#25968;&#22312;&#26679;&#26412;&#21608;&#22260;&#35745;&#31639;&#23616;&#37096;&#29699;&#26469;&#33719;&#24471;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12175</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomalies detection in IIoT edge devices networks using federated learning. (arXiv:2308.12175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#36793;&#32536;&#35774;&#22791;&#32593;&#32476;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#35768;&#22810;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#36825;&#35201;&#27714;&#20005;&#26684;&#30340;&#38544;&#31169;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23433;&#20840;&#38382;&#39064;&#65292;&#19968;&#20123;&#19994;&#20027;&#19981;&#24895;&#23558;&#20854;&#25968;&#25454;&#25552;&#20379;&#32473;&#20844;&#21496;&#20043;&#22806;&#30340;&#20154;&#12290;&#32852;&#21512;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25910;&#38598;&#25968;&#25454;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#19981;&#20250;&#22312;&#32593;&#32476;&#19978;&#20849;&#20139;&#29992;&#20110;&#35757;&#32451;&#12290;Fedavg&#20316;&#20026;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#20043;&#19968;&#65292;&#20801;&#35768;&#22312;&#35757;&#32451;&#20250;&#35805;&#26399;&#38388;&#23558;&#27169;&#22411;&#22797;&#21046;&#21040;&#21442;&#19982;&#35774;&#22791;&#19978;&#12290;&#35774;&#22791;&#21487;&#20197;&#38543;&#26426;&#36873;&#25321;&#65292;&#20063;&#21487;&#20197;&#20013;&#26029;&#12290;&#29983;&#25104;&#30340;&#27169;&#22411;&#23558;&#21457;&#36865;&#21040;&#21327;&#35843;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#35745;&#31639;&#26469;&#33258;&#23436;&#25104;&#35757;&#32451;&#30340;&#35774;&#22791;&#30340;&#24179;&#22343;&#27169;&#22411;&#12290;&#36825;&#20010;&#36807;&#31243;&#24490;&#29615;&#37325;&#22797;&#65292;&#30452;&#21040;&#36798;&#21040;&#25152;&#38656;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;/&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#35777;&#26126;&#20102;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#31181;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#24544;&#23454;&#24615;&#35780;&#20272;&#19978;&#30456;&#36739;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#26377;18%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12157</link><description>&lt;p&gt;
&#29992;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#35780;&#20272;&#24544;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Faithfulness Using the Longest Supported Subsequence. (arXiv:2308.12157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#35777;&#26126;&#20102;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#31181;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#22312;&#24544;&#23454;&#24615;&#35780;&#20272;&#19978;&#30456;&#36739;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#24230;&#37327;&#26377;18%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26085;&#30410;&#22797;&#26434;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#24635;&#32467;&#21644;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#12290;&#30001;&#20110;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#21487;&#33021;&#30340;&#31572;&#26696;&#31181;&#31867;&#32321;&#22810;&#65292;&#30830;&#20445;&#23427;&#20204;&#30340;&#21709;&#24212;&#22312;&#35821;&#22659;&#20013;&#26377;&#26681;&#25454;&#12289;&#24544;&#23454;&#21487;&#20449;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#30001;&#35821;&#22659;&#25903;&#25345;&#30340;&#20027;&#24352;&#20013;&#26368;&#38271;&#30340;&#38750;&#36830;&#32493;&#23376;&#23383;&#31526;&#20018;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#26368;&#38271;&#25903;&#25345;&#23376;&#24207;&#21015;&#65288;LSS&#65289;&#12290;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24494;&#35843;&#20102;&#19968;&#20010;&#27169;&#22411;&#20197;&#29983;&#25104;LSS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#20351;&#29992;LSS&#26102;&#65292;&#36825;&#20123;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20998;&#26356;&#30456;&#20851;&#65292;&#30456;&#23545;&#20110;&#19981;&#20351;&#29992;LSS&#26102;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#24230;&#37327;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#24544;&#23454;&#24615;&#30340;&#26368;&#20808;&#36827;&#24230;&#37327;&#30340;&#25913;&#36827;&#36798;&#21040;&#20102;18%&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#24635;&#32467;&#20219;&#21153;&#19978;&#19968;&#30452;&#20248;&#20110;&#20854;&#20182;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As increasingly sophisticated language models emerge, their trustworthiness becomes a pivotal issue, especially in tasks such as summarization and question-answering. Ensuring their responses are contextually grounded and faithful is challenging due to the linguistic diversity and the myriad of possible answers. In this paper, we introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, which we refer to as the Longest Supported Subsequence (LSS). Using a new human-annotated dataset, we finetune a model to generate LSS. We introduce a new method of evaluation and demonstrate that these metrics correlate better with human ratings when LSS is employed, as opposed to when it is not. Our proposed metric demonstrates an 18% enhancement over the prevailing state-of-the-art metric for faithfulness on our dataset. Our metric consistently outperforms other metrics on a summarizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23558;&#24494;&#34920;&#24773;&#21644;&#29983;&#29702;&#20449;&#21495;&#32435;&#20837;&#22810;&#27169;&#24577;&#28508;&#22312;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12156</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#28508;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#24494;&#34920;&#24773;&#21644;&#29983;&#29702;&#20449;&#21495;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multimodal Latent Emotion Recognition from Micro-expression and Physiological Signals. (arXiv:2308.12156v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#23558;&#24494;&#34920;&#24773;&#21644;&#29983;&#29702;&#20449;&#21495;&#32435;&#20837;&#22810;&#27169;&#24577;&#28508;&#22312;&#24773;&#32490;&#35782;&#21035;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#24341;&#20837;&#28508;&#22312;&#24773;&#32490;&#35782;&#21035;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#30410;&#22788;&#65292;&#37325;&#28857;&#20851;&#27880;&#24494;&#34920;&#24773;&#65288;ME&#65289;&#21644;&#29983;&#29702;&#20449;&#21495;&#65288;PS&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;ME&#21644;PS&#65292;&#21253;&#25324;&#19968;&#32500;&#21487;&#20998;&#31163;&#12289;&#21487;&#28151;&#21512;&#30340;&#28145;&#23618;&#32593;&#32476;&#65292;&#26631;&#20934;&#21270;&#30340;&#27491;&#24577;&#20998;&#24067;&#21152;&#26435;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#21450;&#28145;&#24230;/&#29983;&#29702;&#23398;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#27880;&#24847;&#27169;&#22359;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#21152;&#26435;&#34701;&#21512;&#26041;&#27861;&#21644;&#27880;&#24847;&#27169;&#22359;&#22343;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the benefits of incorporating multimodal data for improving latent emotion recognition accuracy, focusing on micro-expression (ME) and physiological signals (PS). The proposed approach presents a novel multimodal learning framework that combines ME and PS, including a 1D separable and mixable depthwise inception network, a standardised normal distribution weighted feature fusion method, and depth/physiology guided attention modules for multimodal learning. Experimental results show that the proposed approach outperforms the benchmark method, with the weighted fusion method and guided attention modules both contributing to enhanced performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#25968;&#25454;&#38598;&#25214;&#20986;&#20102;&#24433;&#21709;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.12131</link><description>&lt;p&gt;
&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semantic Change Detection for the Romanian Language. (arXiv:2308.12131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#32599;&#39532;&#23612;&#20122;&#35821;&#30340;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#25968;&#25454;&#38598;&#25214;&#20986;&#20102;&#24433;&#21709;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#20041;&#21464;&#21270;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#26102;&#26399;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#29992;&#27861;&#26469;&#35782;&#21035;&#21333;&#35789;&#24847;&#20041;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#30495;&#23454;&#30340;&#33521;&#35821;&#21644;&#32599;&#39532;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#19978;&#20998;&#26512;&#20102;&#21019;&#24314;&#38745;&#24577;&#21644;&#19978;&#19979;&#25991;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#30340;&#19981;&#21516;&#31574;&#30053;&#65292;&#21363;Word2Vec&#21644;ELMo&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27969;&#31243;&#24182;&#30830;&#23450;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;&#33521;&#35821;&#25968;&#25454;&#38598;(SEMEVAL-CCOHA)&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#21333;&#35789;&#23884;&#20837;&#27169;&#22411;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23558;&#23454;&#39564;&#37325;&#28857;&#25918;&#22312;&#32599;&#39532;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#19978;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#31181;&#36164;&#28304;&#26377;&#38480;&#35821;&#35328;&#20013;&#35821;&#20041;&#21464;&#21270;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#24847;&#20041;&#30340;&#33719;&#21462;&#21644;&#20002;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;&#35821;&#26009;&#24211;&#30340;&#19981;&#21516;&#65292;&#36873;&#25321;&#27169;&#22411;&#21644;&#35745;&#31639;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20998;&#25968;&#30340;&#36317;&#31163;&#26159;&#32771;&#34385;&#30340;&#26368;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic semantic change methods try to identify the changes that appear over time in the meaning of words by analyzing their usage in diachronic corpora. In this paper, we analyze different strategies to create static and contextual word embedding models, i.e., Word2Vec and ELMo, on real-world English and Romanian datasets. To test our pipeline and determine the performance of our models, we first evaluate both word embedding models on an English dataset (SEMEVAL-CCOHA). Afterward, we focus our experiments on a Romanian dataset, and we underline different aspects of semantic changes in this low-resource language, such as meaning acquisition and loss. The experimental results show that, depending on the corpus, the most important factors to consider are the choice of model and the distance to calculate a score for detecting semantic change.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#25513;&#34109;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#35813;&#20559;&#24046;&#12290;&#36890;&#36807;&#35780;&#20272;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#65288;CNN&#21644;ViT&#65289;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12127</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#21435;&#38500;&#32972;&#26223;&#20559;&#24046;&#30340;&#25513;&#34109;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Masking Strategies for Background Bias Removal in Computer Vision Models. (arXiv:2308.12127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#25513;&#34109;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#35813;&#20559;&#24046;&#12290;&#36890;&#36807;&#35780;&#20272;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#65288;CNN&#21644;ViT&#65289;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#30001;&#20110;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#26497;&#20026;&#24494;&#22937;&#19988;&#27599;&#31867;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#65292;&#24456;&#23481;&#26131;&#21463;&#21040;&#32972;&#26223;&#30456;&#20851;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#20351;&#29992;&#31283;&#20581;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#28508;&#22312;&#26679;&#26412;&#12290;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#35780;&#20272;&#20102;&#35832;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViT&#65289;&#31561;&#26631;&#20934;&#39592;&#24178;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25513;&#34109;&#31574;&#30053;&#26469;&#20943;&#36731;&#32972;&#26223;&#24341;&#36215;&#30340;&#20559;&#24046;&#65306;&#26089;&#26399;&#25513;&#34109;&#65292;&#21363;&#22312;&#65288;&#36755;&#20837;&#65289;&#22270;&#20687;&#32423;&#21035;&#19978;&#21435;&#38500;&#32972;&#26223;&#20449;&#24687;&#65292;&#20197;&#21450;&#26202;&#26399;&#25513;&#34109;&#65292;&#21363;&#36873;&#25321;&#24615;&#22320;&#25513;&#34109;&#19982;&#32972;&#26223;&#30456;&#23545;&#24212;&#30340;&#39640;&#23618;&#31354;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#20102;CNN&#21644;ViT&#27169;&#22411;&#22312;&#19981;&#21516;&#25513;&#34109;&#31574;&#30053;&#19979;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#23545;&#38750;&#20998;&#24067;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.12108</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31995;&#25968;&#37327;&#21270;&#22855;&#24322;&#27169;&#22411;&#20013;&#30340;&#36864;&#21270;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Quantifying degeneracy in singular models via the learning coefficient. (arXiv:2308.12108v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#29992;&#20110;&#31934;&#30830;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36864;&#21270;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#21442;&#25968;&#21306;&#22495;&#30340;&#36864;&#21270;&#39034;&#24207;&#65292;&#24182;&#25581;&#31034;&#20102;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26159;&#20855;&#26377;&#22797;&#26434;&#36864;&#21270;&#30340;&#22855;&#24322;&#32479;&#35745;&#27169;&#22411;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#31995;&#25968;&#30340;&#37327;&#65292;&#23427;&#22312;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;&#20013;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36864;&#21270;&#31243;&#24230;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35777;&#26126;DNN&#20013;&#30340;&#36864;&#21270;&#19981;&#33021;&#20165;&#36890;&#36807;&#35745;&#31639;&#8220;&#24179;&#22374;&#8221;&#26041;&#21521;&#30340;&#25968;&#37327;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#35745;&#31639;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;&#29702;&#35770;&#20540;&#30340;&#20302;&#32500;&#27169;&#22411;&#19978;&#28436;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#33021;&#22815;&#27491;&#30830;&#24674;&#22797;&#24863;&#20852;&#36259;&#21442;&#25968;&#21306;&#22495;&#20043;&#38388;&#36864;&#21270;&#30340;&#39034;&#24207;&#12290;&#23545;MNIST&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#21487;&#20197;&#25581;&#31034;&#38543;&#26426;&#20248;&#21270;&#22120;&#23545;&#26356;&#36864;&#21270;&#25110;&#19981;&#22826;&#36864;&#21270;&#30340;&#20020;&#30028;&#28857;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2308.12075</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#31283;&#23450;RNN&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stabilizing RNN Gradients through Pre-training. (arXiv:2308.12075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#65292;&#25299;&#23637;&#20102;&#24050;&#26377;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22797;&#26434;&#32467;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#20247;&#22810;&#29702;&#35770;&#37117;&#24314;&#35758;&#36890;&#36807;&#38450;&#27490;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#25351;&#25968;&#24418;&#24335;&#38543;&#28145;&#24230;&#25110;&#26102;&#38388;&#22686;&#38271;&#26469;&#31283;&#23450;&#21644;&#25913;&#21892;&#35757;&#32451;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20998;&#26512;&#26159;&#22312;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25110;&#21333;&#23618;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#23398;&#30340;&#21487;&#35299;&#24615;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20307;&#31995;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#20197;&#33267;&#20110;&#26080;&#27861;&#36827;&#34892;&#35299;&#26512;&#21021;&#22987;&#21270;&#26102;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#32593;&#32476;&#23454;&#29616;&#23616;&#37096;&#31283;&#23450;&#24615;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#30693;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#65292;&#28085;&#30422;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#23478;&#26063;&#65292;&#23545;&#25968;&#25454;&#21644;&#21442;&#25968;&#20998;&#24067;&#30340;&#35201;&#27714;&#36739;&#23569;&#65292;&#36825;&#20010;&#29702;&#35770;&#34987;&#31216;&#20026;&#23616;&#37096;&#31283;&#23450;&#24615;&#26465;&#20214;&#65288;LSC&#65289;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#32463;&#20856;&#30340;Glorot&#12289;He&#21644;&#27491;&#20132;&#21021;&#22987;&#21270;&#26041;&#26696;&#22312;&#24212;&#29992;&#20110;&#21069;&#39304;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26102;&#21487;&#20197;&#28385;&#36275;LSC&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#28145;&#23618;&#24490;&#29615;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;&#12290;</title><link>http://arxiv.org/abs/2308.12069</link><description>&lt;p&gt;
&#38752;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning. (arXiv:2308.12069v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#36710;&#36742;&#30340;&#21453;&#24212;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#39550;&#39542;&#39118;&#26684;&#25351;&#30340;&#26159;&#20854;&#19982;&#20854;&#20182;AV&#30340;&#34892;&#20026;&#21644;&#30456;&#20114;&#20316;&#29992;&#26041;&#24335;&#12290;&#22312;&#22810;&#36710;&#36742;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#33021;&#22815;&#35782;&#21035;&#38468;&#36817;AV&#30340;&#39550;&#39542;&#39118;&#26684;&#30340;AV&#21487;&#20197;&#21487;&#38752;&#35780;&#20272;&#30896;&#25758;&#39118;&#38505;&#24182;&#20570;&#20986;&#26356;&#21512;&#29702;&#30340;&#39550;&#39542;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;AV&#39550;&#39542;&#39118;&#26684;&#30340;&#23450;&#20041;&#24182;&#19981;&#19968;&#33268;&#65292;&#23613;&#31649;&#36890;&#24120;&#35748;&#20026;&#39550;&#39542;&#39118;&#26684;&#36890;&#36807;AV&#30340;&#36712;&#36857;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#29109;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;ME-IRL&#65289;&#26041;&#27861;&#20316;&#20026;&#25104;&#26412;&#20989;&#25968;&#26469;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#39550;&#39542;&#39118;&#26684;&#30340;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21363;AV&#23545;&#38468;&#36817;AV&#30340;&#21453;&#24212;&#26041;&#24335;&#65292;&#22312;&#20808;&#21069;&#30340;ME-IRL&#26041;&#27861;&#30340;&#29305;&#24449;&#35774;&#35745;&#20013;&#24182;&#26410;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#23558;&#39550;&#39542;&#39118;&#26684;&#25551;&#36848;&#20026;&#19968;&#31995;&#21015;&#21152;&#26435;&#29305;&#24449;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39069;&#22806;&#30340;&#26032;&#29305;&#24449;&#26469;&#25429;&#25417;AV&#30340;&#21453;&#24212;&#24863;&#30693;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#39550;&#39542;&#39118;&#26684;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles 
&lt;/p&gt;</description></item><item><title>InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;</title><link>http://arxiv.org/abs/2308.12067</link><description>&lt;p&gt;
InstructionGPT-4: &#19968;&#20010;200&#25351;&#20196;&#33539;&#24335;&#29992;&#20110;&#24494;&#35843;MiniGPT-4
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4. (arXiv:2308.12067v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12067
&lt;/p&gt;
&lt;p&gt;
InstructionGPT-4&#36890;&#36807;&#20165;&#20351;&#29992;200&#20010;&#20363;&#23376;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#24230;&#37327;&#21644;&#36873;&#25321;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;&#22312;&#21508;&#31181;&#35780;&#20272;&#20219;&#21153;&#20013;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#36807;&#31243;&#33719;&#21462;&#20854;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65306;&#22312;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30417;&#30563;&#24335;&#35270;&#35273;-&#35821;&#35328;&#25351;&#20196;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#37327;&#30340;&#39640;&#36136;&#37327;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InstructionGPT-4&#65292;&#23427;&#32463;&#36807;&#24494;&#35843;&#30340;&#25968;&#25454;&#38598;&#21482;&#21253;&#21547;200&#20010;&#20363;&#23376;&#65292;&#32422;&#21344;MiniGPT-4&#23545;&#40784;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#30340;&#36981;&#24490;&#25351;&#20196;&#25968;&#25454;&#30340;6%&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20960;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#36136;&#37327;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#22522;&#20110;&#36825;&#20123;&#24230;&#37327;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#36873;&#25321;&#22120;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#36807;&#28388;&#20302;&#36136;&#37327;&#30340;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;InstructionGPT-4&#22312;&#21508;&#31181;&#35780;&#20272;&#65288;&#22914;&#35270;&#35273;&#38382;&#31572;&#12289;GPT-4&#20559;&#22909;&#65289;&#19978;&#20248;&#20110;&#21407;&#22987;&#30340;MiniGPT-4&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findi
&lt;/p&gt;</description></item><item><title>&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12066</link><description>&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#65306;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#25512;&#29702;&#30340;&#31639;&#27861;&#21644;&#31995;&#32479;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference. (arXiv:2308.12066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12066
&lt;/p&gt;
&lt;p&gt;
&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20854;&#25104;&#21151;&#28304;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#24456;&#39640;&#65292;&#20294;LLMs&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;LLMs&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#19981;&#25104;&#27604;&#20363;&#22320;&#25193;&#22823;&#35745;&#31639;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#39640;&#23384;&#20648;&#38656;&#27714;&#21644;&#31232;&#30095;&#19987;&#23478;&#30340;&#21160;&#24577;&#28608;&#27963;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;MoE&#30340;&#20869;&#23384;&#21344;&#29992;&#39640;&#30340;&#19987;&#23478;&#21442;&#25968;&#36716;&#31227;&#21040;CPU&#20869;&#23384;&#19978;&#65292;&#20294;&#26159;&#20174;CPU&#36801;&#31227;&#24050;&#28608;&#27963;&#30340;&#19987;&#23478;&#21040;GPU&#30340;&#24310;&#36831;&#23548;&#33268;&#20102;&#39640;&#24615;&#33021;&#24320;&#38144;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#38376;&#25511;MoE&#31995;&#32479;&#36890;&#36807;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;MoE&#26550;&#26500;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPROUT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#21487;&#20197;&#35782;&#21035;&#20986;&#32477;&#22823;&#37096;&#20998;&#30340;&#38169;&#20998;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.12065</link><description>&lt;p&gt;
&#23558;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#38598;&#21512;&#36215;&#26469;&#20197;&#25552;&#39640;&#40657;&#30418;&#20998;&#31867;&#22120;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers. (arXiv:2308.12065v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPROUT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#21487;&#20197;&#35782;&#21035;&#20986;&#32477;&#22823;&#37096;&#20998;&#30340;&#38169;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#39044;&#27979;&#38169;&#35823;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#20986;&#29616;&#38169;&#20998;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#38169;&#20998;&#21487;&#33021;&#23545;&#31995;&#32479;&#20135;&#29983;&#36830;&#38145;&#25928;&#24212;&#65292;&#21487;&#33021;&#23548;&#33268;&#20851;&#38190;&#25925;&#38556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SPROUT&#65292;&#19968;&#31181;&#36890;&#36807;&#23545;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26469;&#24576;&#30097;&#38169;&#20998;&#30340;&#23433;&#20840;&#24615;&#23553;&#35013;&#22120;&#12290;&#22914;&#26524;&#26816;&#27979;&#21040;&#38169;&#20998;&#65292;SPROUT&#20250;&#38459;&#27490;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20256;&#25773;&#21040;&#31995;&#32479;&#20013;&#12290;&#23545;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#26159;&#65292;SPROUT&#23558;&#19981;&#31283;&#23450;&#30340;&#36755;&#20986;&#65288;&#38169;&#20998;&#65289;&#36716;&#21270;&#20026;&#25968;&#25454;&#36951;&#28431;&#25925;&#38556;&#65292;&#22312;&#31995;&#32479;&#23618;&#38754;&#19978;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36827;&#34892;&#31649;&#29702;&#12290;SPROUT&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36866;&#29992;&#20110;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SPROUT&#24635;&#26159;&#33021;&#22815;&#35782;&#21035;&#20986;&#22823;&#37096;&#20998;&#36229;&#32423;&#38169;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of super
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12053</link><description>&lt;p&gt;
&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Layer-wise Feedback Propagation. (arXiv:2308.12053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#32454;&#21270;&#19982;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#20219;&#21153;&#30340;&#36129;&#29486;&#20998;&#37197;&#22870;&#21169;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;LFP&#21462;&#24471;&#20102;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23618;&#32423;&#21453;&#39304;&#20256;&#25773;&#65288;LFP&#65289;&#8221;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#23618;&#32423;&#30456;&#20851;&#24615;&#20256;&#25773;&#65288;LRP&#65289;&#65292;&#26681;&#25454;&#27599;&#20010;&#36830;&#25509;&#23545;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#36129;&#29486;&#29420;&#31435;&#20998;&#37197;&#22870;&#21169;&#12290;&#36825;&#19982;&#20256;&#32479;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#19981;&#21516;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#26159;&#26397;&#21521;&#20272;&#35745;&#30340;&#25439;&#22833;&#26368;&#23567;&#20540;&#26356;&#26032;&#21442;&#25968;&#12290;LFP&#22312;&#27169;&#22411;&#20013;&#20256;&#25773;&#22870;&#21169;&#20449;&#21495;&#65292;&#32780;&#26080;&#38656;&#26799;&#24230;&#35745;&#31639;&#12290;&#23427;&#22686;&#24378;&#25509;&#25910;&#21040;&#27491;&#21453;&#39304;&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#38477;&#20302;&#25509;&#25910;&#21040;&#36127;&#21453;&#39304;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#35777;&#26126;&#20102;LFP&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LFP&#20811;&#26381;&#20102;&#26799;&#24230;&#26041;&#27861;&#30340;&#26576;&#20123;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#23545;&#26377;&#24847;&#20041;&#30340;&#23548;&#25968;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;LFP&#22914;&#20309;&#35299;&#20915;&#26799;&#24230;&#26041;&#27861;&#30456;&#20851;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2308.12050</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#26377;&#25928;&#22320;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65292;&#36825;&#20123;&#25216;&#26415;&#34987;&#35777;&#26126;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;PPO&#38656;&#35201;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#23454;&#29616;&#65292;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#19982;RL&#29615;&#22659;&#20132;&#20114;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#19982;&#36807;&#28388;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#65288;RWR&#65289;&#20197;&#21450;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38544;&#31169;&#30340;&#36300;&#20498;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#28145;&#24230;&#26080;&#30417;&#30563;&#30340;RGB2Depth&#36866;&#24212;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#39046;&#22495;&#20013;&#21033;&#29992;RGB&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36300;&#20498;&#26816;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#26631;&#31614;&#30340;RGB&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#28145;&#24230;&#25968;&#25454;&#36827;&#34892;&#36328;&#27169;&#24577;&#39046;&#22495;&#36866;&#24212;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.12049</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#26080;&#30417;&#30563;&#30340;RGB2Depth&#36866;&#24212;&#23454;&#29616;&#25903;&#25345;&#38544;&#31169;&#30340;&#36300;&#20498;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Privacy-Supporting Fall Detection via Deep Unsupervised RGB2Depth Adaptation. (arXiv:2308.12049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38544;&#31169;&#30340;&#36300;&#20498;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#28145;&#24230;&#26080;&#30417;&#30563;&#30340;RGB2Depth&#36866;&#24212;&#23454;&#29616;&#20102;&#22312;&#28145;&#24230;&#39046;&#22495;&#20013;&#21033;&#29992;RGB&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36300;&#20498;&#26816;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#26631;&#31614;&#30340;RGB&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#28145;&#24230;&#25968;&#25454;&#36827;&#34892;&#36328;&#27169;&#24577;&#39046;&#22495;&#36866;&#24212;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36300;&#20498;&#26816;&#27979;&#26159;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#35302;&#21457;&#35686;&#25253;&#24182;&#22312;&#20154;&#21592;&#36300;&#20498;&#26102;&#23454;&#29616;&#26356;&#24555;&#30340;&#24178;&#39044;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#26631;&#20934;&#30340;RGB&#35270;&#39057;&#25968;&#25454;&#65292;&#20294;&#36825;&#31181;&#35814;&#32454;&#30340;&#22806;&#35266;&#24863;&#30693;&#30417;&#27979;&#23384;&#22312;&#26174;&#30528;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#20256;&#24863;&#22120;&#22312;&#20445;&#25252;&#38544;&#31169;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#25429;&#25417;&#29289;&#20307;&#19982;&#20256;&#24863;&#22120;&#25110;&#30456;&#26426;&#30340;&#36317;&#31163;&#65292;&#30465;&#30053;&#20102;&#39068;&#33394;&#21644;&#32441;&#29702;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25903;&#25345;&#38544;&#31169;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#20351;&#24471;RGB&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#28145;&#24230;&#39046;&#22495;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#28145;&#24230;&#25968;&#25454;&#36827;&#34892;&#36300;&#20498;&#26816;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#36300;&#20498;&#26816;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;RGB&#21040;&#28145;&#24230;&#65288;RGB2Depth&#65289;&#36328;&#27169;&#24577;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#26631;&#31614;&#30340;RGB&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#28145;&#24230;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#27969;&#31243;&#21253;&#25324;&#19968;&#20010;&#20013;&#38388;&#39046;&#22495;&#27169;&#22359;&#29992;&#20110;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#21450;&#27169;&#24577;&#23545;&#25239;&#24615;&#25439;&#22833;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fall detection is a vital task in health monitoring, as it allows the system to trigger an alert and therefore enabling faster interventions when a person experiences a fall. Although most previous approaches rely on standard RGB video data, such detailed appearance-aware monitoring poses significant privacy concerns. Depth sensors, on the other hand, are better at preserving privacy as they merely capture the distance of objects from the sensor or camera, omitting color and texture information. In this paper, we introduce a privacy-supporting solution that makes the RGB-trained model applicable in depth domain and utilizes depth data at test time for fall detection. To achieve cross-modal fall detection, we present an unsupervised RGB to Depth (RGB2Depth) cross-modal domain adaptation approach that leverages labelled RGB data and unlabelled depth data during training. Our proposed pipeline incorporates an intermediate domain module for feature bridging, modality adversarial loss for m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgT-GAN&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#20026;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#20449;&#24687;&#65292;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22270;&#20687;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12045</link><description>&lt;p&gt;
CgT-GAN: CLIP&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
CgT-GAN: CLIP-guided Text GAN for Image Captioning. (arXiv:2308.12045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgT-GAN&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;&#20026;&#27169;&#22411;&#25552;&#20379;&#35270;&#35273;&#20449;&#24687;&#65292;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22270;&#20687;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#22312;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#22270;&#20687;&#25551;&#36848;&#22330;&#26223;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#20687;&#25551;&#36848;&#30340;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;CLIP-based&#22270;&#20687;&#25551;&#36848;&#26041;&#27861;&#37319;&#29992;&#20102;&#32431;&#25991;&#26412;&#35757;&#32451;&#33539;&#24335;&#65292;&#21363;&#20174;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#37325;&#24314;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#24046;&#36317;&#25110;&#32773;&#25991;&#26412;&#23884;&#20837;&#30340;&#23384;&#20648;&#38656;&#27714;&#19978;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#32771;&#34385;&#21040;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#33719;&#21462;&#22270;&#20687;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#30340;&#25991;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;CgT-GAN&#65289;&#65292;&#23558;&#22270;&#20687;&#21152;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#8220;&#30475;&#21040;&#8221;&#30495;&#23454;&#30340;&#35270;&#35273;&#27169;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;CgT-GAN&#33021;&#22815;&#27169;&#20223;&#22806;&#37096;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#30701;&#35821;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;&#22870;&#21169;&#25552;&#20379;&#35821;&#20041;&#24341;&#23548;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22120;&#36890;&#36807;GAN&#30340;&#37492;&#21035;&#22120;&#35745;&#31639;&#30340;&#33258;&#28982;&#24230;&#21644;&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#30340;&#35821;&#20041;&#20174;&#32780;&#33719;&#24471;&#32852;&#21512;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to "see" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the sema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2308.12043</link><description>&lt;p&gt;
IncreLoRA: &#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning. (arXiv:2308.12043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12043
&lt;/p&gt;
&lt;p&gt;
IncreLoRA&#26159;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#24182;&#19981;&#39640;&#25928;&#65292;&#29305;&#21035;&#26159;&#24403;&#23384;&#22312;&#22823;&#37327;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#35757;&#32451;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#24050;&#26377;&#35768;&#22810;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#65288;PEFT&#65289;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#65292;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#31209;&#20998;&#35299;&#30697;&#38453;&#27880;&#20837;&#27599;&#20010;&#30446;&#26631;&#27169;&#22359;&#30340;&#20856;&#22411;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LoRA&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22359;&#20013;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21098;&#26525;LoRA&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26465;&#20214;&#19979;&#65292;&#20462;&#21098;&#21442;&#25968;&#30697;&#38453;&#30340;&#31209;&#30340;&#19978;&#30028;&#20173;&#28982;&#21463;&#21040;&#39044;&#35774;&#20540;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IncreLoRA&#65292;&#19968;&#31181;&#22686;&#37327;&#21442;&#25968;&#20998;&#37197;&#26041;&#27861;&#65292;&#26681;&#25454;&#27599;&#20010;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20462;&#21098;&#26041;&#27861;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20174;&#22686;&#21152;&#21442;&#25968;&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;&#35843;&#20248;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PREFER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#21644;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12033</link><description>&lt;p&gt;
PREFER: &#36890;&#36807;&#21453;&#39304;-&#21453;&#24605;-&#20248;&#21270;&#30340;&#25552;&#31034;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine. (arXiv:2308.12033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PREFER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#21644;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24378;&#22823;&#33021;&#21147;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25552;&#31034;&#26368;&#36817;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#31034;&#38598;&#25104;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20852;&#36259;&#65292;&#20197;&#35299;&#20915; LLMs &#30340;&#24187;&#35273;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#33539;&#24335;&#65292;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#20934;&#22791;&#30340;&#25552;&#31034;&#38598;&#21512;&#65292;&#24182;&#19988;&#26080;&#27861;&#38024;&#23545;&#19981;&#21516;&#30340;&#24369;&#23398;&#20064;&#22120;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#12289;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026; PREFER (&#36890;&#36807;&#21453;&#39304;-&#21453;&#24605;-&#20248;&#21270;&#30340;&#25552;&#31034;&#38598;&#25104;&#23398;&#20064;)&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32771;&#34385;&#21040;&#24369;&#23398;&#20064;&#22120;&#24212;&#35813;&#20851;&#27880;&#25552;&#21319;&#36807;&#31243;&#20013;&#30340;&#22256;&#38590;&#26679;&#26412;&#65292;PREFER &#26500;&#24314;&#20102;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#29992;&#20110;&#21453;&#24605;&#29616;&#26377;&#24369;&#23398;&#20064;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22522;&#20110;&#27492;&#65292;LLM &#38656;&#35201;&#33258;&#21160;&#21512;&#25104;&#26032;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Pompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinemen
&lt;/p&gt;</description></item><item><title>CACTUS&#26159;&#19968;&#20010;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;&#30340;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#65292;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12031</link><description>&lt;p&gt;
CACTUS: &#19968;&#20010;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures. (arXiv:2308.12031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12031
&lt;/p&gt;
&lt;p&gt;
CACTUS&#26159;&#19968;&#20010;&#29992;&#20110;&#21457;&#29616;&#32467;&#26500;&#30340;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#65292;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#25512;&#21160;&#20102;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#37096;&#32626;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#23545;&#20110;&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#24320;&#21457;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#25361;&#25112;&#12290;CACTUS&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#25277;&#35937;&#21644;&#20998;&#31867;&#24037;&#20855;&#65292;&#29992;&#20110;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26377;&#25928;&#22320;&#25903;&#25345;&#23433;&#20840;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#23545;&#20998;&#31867;&#23646;&#24615;&#30340;&#39069;&#22806;&#25903;&#25345;&#65292;&#20445;&#25345;&#20854;&#21407;&#22987;&#21547;&#20041;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#20248;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#23427;&#21521;&#29992;&#25143;&#26174;&#31034;&#27599;&#20010;&#31867;&#21035;&#30340;&#23646;&#24615;&#39057;&#29575;&#24182;&#26681;&#25454;&#20854;&#21306;&#20998;&#33021;&#21147;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#23041;&#26031;&#24247;&#26143;&#35786;&#26029;&#20083;&#33146;&#30284;&#21644;&#30002;&#29366;&#33146;0387&#25968;&#25454;&#38598;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.12030</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#19982;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Length Controlled Generation with Reinforcement Learning. (arXiv:2308.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12030
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22870;&#21169;&#27169;&#22411;&#26469;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#24182;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT-4&#22240;&#20854;&#24778;&#20154;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#25104;&#20026;LLM&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35805;&#39064;&#65292;&#23427;&#36824;&#20351;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#26356;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#29983;&#25104;&#25152;&#38656;&#38271;&#24230;&#30340;&#21512;&#36866;&#31572;&#26696;&#25110;&#25991;&#31456;&#12290;&#27492;&#22806;&#65292;LLM&#20013;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#38750;&#24120;&#32791;&#26102;&#65292;&#32780;&#25511;&#21046;&#29983;&#25104;&#38271;&#24230;&#30340;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#38271;&#24230;&#20219;&#24847;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#65292;&#20174;&#32780;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38271;&#24230;&#25511;&#21046;&#26041;&#27861;&#26469;&#23454;&#29616;&#38271;&#24230;&#21463;&#25511;&#29983;&#25104;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#31867;&#20284;GPT&#30340;LLM&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#25110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22870;&#21169;&#27169;&#22411;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#65292;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#39044;&#23450;&#20041;&#30446;&#26631;&#38271;&#24230;&#36827;&#34892;&#22870;&#21169;&#26469;&#24433;&#21709;LLM&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12029</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#23610;&#24230;&#19981;&#21464;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12029
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65288;SI-MTL&#65289;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#23545;&#25968;&#21464;&#25442;&#21644;&#23545;&#20219;&#21153;&#26799;&#24230;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26159;&#19968;&#31181;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#24179;&#34913;&#20173;&#28982;&#26159;MTL&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25439;&#22833;/&#26799;&#24230;&#23610;&#24230;&#30340;&#19981;&#24179;&#34913;&#32463;&#24120;&#23548;&#33268;&#24615;&#33021;&#25240;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23610;&#24230;&#19981;&#21464;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;SI-MTL&#65289;&#26041;&#27861;&#65292;&#20174;&#25439;&#22833;&#21644;&#26799;&#24230;&#35282;&#24230;&#32531;&#35299;&#20102;&#20219;&#21153;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SI-MTL&#21253;&#21547;&#23545;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#36827;&#34892;&#30340;&#23545;&#25968;&#21464;&#25442;&#65292;&#20197;&#30830;&#20445;&#22312;&#25439;&#22833;&#27700;&#24179;&#19978;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#19968;&#31181;&#26799;&#24230;&#24179;&#34913;&#26041;&#27861;SI-G&#65292;&#23427;&#23558;&#25152;&#26377;&#20219;&#21153;&#30340;&#26799;&#24230;&#24402;&#19968;&#21270;&#20026;&#19982;&#26368;&#22823;&#26799;&#24230;&#33539;&#25968;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#19968;&#33268;&#35777;&#26126;&#20102;SI-G&#30340;&#26377;&#25928;&#24615;&#21644;SI-MTL&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25512;&#33616;&#32467;&#26524;&#21644;&#38750;&#27963;&#36291;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.12028</link><description>&lt;p&gt;
LKPNR: &#22522;&#20110;LLM&#21644;KG&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LKPNR: LLM and KG for Personalized News Recommendation Framework. (arXiv:2308.12028v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#25512;&#33616;&#32467;&#26524;&#21644;&#38750;&#27963;&#36291;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#24456;&#38590;&#29702;&#35299;&#26032;&#38395;&#25991;&#26412;&#20013;&#22797;&#26434;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#23613;&#20154;&#24847;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#26041;&#27861;&#26356;&#36866;&#29992;&#20110;&#20855;&#26377;&#20016;&#23500;&#21382;&#21490;&#34892;&#20026;&#30340;&#27963;&#36291;&#29992;&#25143;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#35299;&#20915;&#38750;&#27963;&#36291;&#29992;&#25143;&#30340;&#8220;&#38271;&#23614;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32467;&#21512;&#36827;&#20256;&#32479;&#26041;&#27861;&#30340;&#35821;&#20041;&#34920;&#31034;&#30340;&#26032;&#22411;&#36890;&#29992;&#26694;&#26550;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#22797;&#26434;&#26032;&#38395;&#25991;&#26412;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#21253;&#21547;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#26032;&#38395;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#26032;&#38395;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;KG&#20013;&#30340;&#22810;&#20010;&#36339;&#25968;&#25366;&#25496;&#39640;&#38454;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the "long tail problem" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex news texts, we use LLMs' powerful text understanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities and mines high-order structural information through multiple hops in KG, thus allevia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.12014</link><description>&lt;p&gt;
&#20174;&#25351;&#20196;&#21040;&#20869;&#22312;&#20154;&#31867;&#20215;&#20540; - &#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models. (arXiv:2308.12014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#30340;&#19981;&#21516;&#35266;&#28857;&#65292;&#24182;&#36861;&#36394;&#20854;&#28436;&#21270;&#36335;&#24452;&#65292;&#26088;&#22312;&#24110;&#21161;&#30830;&#23450;&#26368;&#37325;&#35201;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#24120;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#30001;&#22823;&#37327;&#21442;&#25968;&#32452;&#25104;&#65292;&#19981;&#20165;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33719;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#36824;&#21576;&#29616;&#20986;&#36739;&#23567;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#27169;&#22411;&#19982;&#26085;&#24120;&#29983;&#27963;&#30340;&#26085;&#30410;&#20132;&#32455;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#30340;&#31038;&#20250;&#21361;&#23475;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#36827;&#34892;&#20102;&#65292;&#20197;&#20351;LLM&#19982;&#20154;&#31867;&#23545;&#40784;&#65292;&#20197;&#20351;&#23427;&#20204;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#28385;&#36275;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#8220;&#19982;&#20309;&#23545;&#40784;&#8221;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35752;&#35770;&#65292;&#19981;&#24403;&#30340;&#23545;&#40784;&#30446;&#26631;&#29978;&#33267;&#21487;&#33021;&#36866;&#24471;&#20854;&#21453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#19981;&#21516;&#23545;&#40784;&#30446;&#26631;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#36861;&#36394;&#23427;&#20204;&#30340;&#28436;&#21270;&#36335;&#24452;&#65292;&#20197;&#24110;&#21161;&#30830;&#23450;&#26368;&#22522;&#26412;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#40784;&#30446;&#26631;&#30340;&#23450;&#20041;&#21644;&#23545;&#40784;&#35780;&#20272;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#30456;&#20851;&#24037;&#20316;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12013</link><description>&lt;p&gt;
&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Quantum-Noise-driven Generative Diffusion Models. (arXiv:2308.12013v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12013
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#37327;&#23376;&#29305;&#24615;&#20197;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#20027;&#35201;&#35745;&#31639;&#22256;&#38590;&#65292;&#24182;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#35270;&#20026;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#32780;&#38750;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23454;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#26029;&#20986;&#22797;&#26434;&#21644;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#24182;&#20135;&#29983;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26694;&#26550;&#65292;&#26368;&#36817;&#22312;&#21019;&#24314;&#21512;&#25104;&#25991;&#26412;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#24050;&#32463;&#36229;&#36234;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#23376;&#25512;generalization&#65292;&#21363;&#19977;&#31181;&#21487;&#33021;&#22312;&#23454;&#38469;&#37327;&#23376;&#31995;&#32479;&#19978;&#36827;&#34892;&#23454;&#39564;&#30340;&#37327;&#23376;&#22122;&#22768;&#39537;&#21160;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#29420;&#29305;&#30340;&#37327;&#23376;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26377;&#22122;&#22768;&#37327;&#23376;&#22788;&#29702;&#22120;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#21040;&#30340;&#30456;&#24178;&#24615;&#12289;&#32416;&#32544;&#24615;&#21644;&#22122;&#22768;&#20043;&#38388;&#30340;&#38750;&#24179;&#20961;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20027;&#35201;&#35745;&#31639;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#37327;&#23376;&#22122;&#22768;&#19981;&#20316;&#20026;&#38656;&#35201;&#26816;&#27979;&#21644;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#32780;&#26159;&#20316;&#20026;&#19968;&#31181;&#21487;&#21033;&#29992;&#30340;&#29305;&#24615;&#65292;&#20351;&#24471;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a ver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Topical-Chat&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#21160;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.11995</link><description>&lt;p&gt;
Topical-Chat: &#36827;&#23637;&#20013;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. (arXiv:2308.11995v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;Topical-Chat&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#21160;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#30740;&#31350;&#32773;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31038;&#20132;&#26426;&#22120;&#20154;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#28145;&#20837;&#12289;&#26377;&#36259;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#22312;&#19982;&#25317;&#26377;&#33258;&#24049;&#19990;&#30028;&#30693;&#35782;&#30340;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#26102;&#65292;&#38656;&#35201;&#26377;&#25928;&#22320;&#21033;&#29992;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#20855;&#26377;&#26126;&#30830;&#35282;&#33394;&#30340;&#26679;&#24335;&#21270;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36824;&#27809;&#26377;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#20027;&#39064;&#28085;&#30422;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Topical-Chat&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#20154;-&#20154;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#30693;&#35782;&#28085;&#30422;&#20102;8&#20010;&#24191;&#27867;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#23545;&#35805;&#21442;&#19982;&#32773;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#35282;&#33394;&#65292;&#20197;&#36827;&#19968;&#27493;&#20419;&#36827;&#24320;&#25918;&#22495;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#22312;Topical-Chat&#19978;&#35757;&#32451;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#20197;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building socialbots that can have deep, engaging open-domain conversations with humans is one of the grand challenges of artificial intelligence (AI). To this end, bots need to be able to leverage world knowledge spanning several domains effectively when conversing with humans who have their own world knowledge. Existing knowledge-grounded conversation datasets are primarily stylized with explicit roles for conversation partners. These datasets also do not explore depth or breadth of topical coverage with transitions in conversations. We introduce Topical-Chat, a knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don't have explicitly defined roles, to help further research in open-domain conversational AI. We also train several state-of-the-art encoder-decoder conversational models on Topical-Chat and perform automated and human evaluation for benchmarking.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#20316;&#20026;&#30149;&#29702;&#23398;&#23478;&#25968;&#23383;&#23402;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21069;&#21015;&#33146;&#30284;&#30149;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#24182;&#33021;&#22815;&#36739;&#20934;&#30830;&#22320;&#26816;&#27979;&#21069;&#21015;&#33146;&#30284;&#21644;&#20272;&#35745;&#32959;&#30244;&#20307;&#31215;&#12290;&#28982;&#32780;&#65292;&#22312;&#32959;&#30244;&#20998;&#32423;&#26041;&#38754;&#65292;&#24403;&#24212;&#29992;&#20110;&#21069;&#21015;&#33146;&#20999;&#38500;&#26631;&#26412;&#26102;&#65292;&#20854;&#19968;&#33268;&#24615;&#26377;&#25152;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2308.11992</link><description>&lt;p&gt;
&#20316;&#20026;&#21069;&#21015;&#33146;&#30284;&#30149;&#29702;&#23398;&#25968;&#23383;&#23402;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Critical Evaluation of Artificial Intelligence as Digital Twin of Pathologist for Prostate Cancer Pathology. (arXiv:2308.11992v1 [q-bio.TO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#20316;&#20026;&#30149;&#29702;&#23398;&#23478;&#25968;&#23383;&#23402;&#29983;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21069;&#21015;&#33146;&#30284;&#30149;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#20855;&#26377;&#21487;&#27604;&#24615;&#65292;&#24182;&#33021;&#22815;&#36739;&#20934;&#30830;&#22320;&#26816;&#27979;&#21069;&#21015;&#33146;&#30284;&#21644;&#20272;&#35745;&#32959;&#30244;&#20307;&#31215;&#12290;&#28982;&#32780;&#65292;&#22312;&#32959;&#30244;&#20998;&#32423;&#26041;&#38754;&#65292;&#24403;&#24212;&#29992;&#20110;&#21069;&#21015;&#33146;&#20999;&#38500;&#26631;&#26412;&#26102;&#65292;&#20854;&#19968;&#33268;&#24615;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#30149;&#29702;&#23398;&#22312;&#20020;&#24202;&#27835;&#30103;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#26102;&#38388;&#28040;&#32791;&#36739;&#22823;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#26816;&#27979;&#21069;&#21015;&#33146;&#30284;&#21644;&#20998;&#32423;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;2,603&#20010;&#29992;&#33487;&#26408;&#31934;&#20234;&#32418;&#26579;&#33394;&#30340;&#21069;&#21015;&#33146;&#32452;&#32455;&#32452;&#32455;&#23398;&#22270;&#20687;&#19978;&#27979;&#35797;&#20102;&#22522;&#20110;AI&#30340;&#30149;&#29702;&#23398;&#25968;&#23383;&#23402;&#29983;vPatho&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24433;&#21709;vPatho&#21644;&#20845;&#21517;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#20043;&#38388;&#32959;&#30244;&#20998;&#32423;&#19981;&#19968;&#33268;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;vPatho&#22312;&#21069;&#21015;&#33146;&#30284;&#26816;&#27979;&#21644;&#32959;&#30244;&#20307;&#31215;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#25991;&#29486;&#25253;&#36947;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;vPatho&#21644;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#20043;&#38388;&#30340;&#21327;&#35843;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35782;&#21035;&#34917;&#20805;&#30340;&#32452;&#32455;&#23398;&#29305;&#24449;&#65288;&#22914;&#23548;&#31649;&#12289;&#31579;&#29366;&#12289;&#31070;&#32463;&#12289;&#34880;&#31649;&#21644;&#28107;&#24052;&#32454;&#32990;&#28024;&#28070;&#65289;&#26041;&#38754;&#65292;&#20013;&#31561;&#21040;&#26174;&#33879;&#19968;&#33268;&#24615;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;&#20294;&#26159;&#65292;&#24403;&#24212;&#29992;&#20110;&#21069;&#21015;&#33146;&#20999;&#38500;&#26631;&#26412;&#65288;kappa = 0.44&#65289;&#26102;&#65292;&#32959;&#30244;&#20998;&#32423;&#30340;&#19968;&#33268;&#24615;&#26174;&#31034;&#20986;&#19979;&#38477;&#65292;&#19982;&#27963;&#26816;&#26680;&#24515;&#65288;kappa = 0.&#65289;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prostate cancer pathology plays a crucial role in clinical management but is time-consuming. Artificial intelligence (AI) shows promise in detecting prostate cancer and grading patterns. We tested an AI-based digital twin of a pathologist, vPatho, on 2,603 histology images of prostate tissue stained with hematoxylin and eosin. We analyzed various factors influencing tumor-grade disagreement between vPatho and six human pathologists. Our results demonstrated that vPatho achieved comparable performance in prostate cancer detection and tumor volume estimation, as reported in the literature. Concordance levels between vPatho and human pathologists were examined. Notably, moderate to substantial agreement was observed in identifying complementary histological features such as ductal, cribriform, nerve, blood vessels, and lymph cell infiltrations. However, concordance in tumor grading showed a decline when applied to prostatectomy specimens (kappa = 0.44) compared to biopsy cores (kappa = 0.
&lt;/p&gt;</description></item><item><title>&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#26159;&#19968;&#31181;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#65292;&#29992;&#20110;&#22312;&#20851;&#31995;&#39046;&#22495;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#30456;&#27604;&#38750;&#20851;&#31995;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12289;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#19979;&#20063;&#33021;&#26377;&#25928;&#24212;&#23545;&#12290;</title><link>http://arxiv.org/abs/2308.11991</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Relational Concept Based Models. (arXiv:2308.11991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11991
&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#26159;&#19968;&#31181;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#65292;&#29992;&#20110;&#22312;&#20851;&#31995;&#39046;&#22495;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#20219;&#21153;&#39044;&#27979;&#65292;&#30456;&#27604;&#38750;&#20851;&#31995;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#24182;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#26102;&#24178;&#39044;&#12289;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#19979;&#20063;&#33021;&#26377;&#25928;&#24212;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#31995;&#39046;&#22495;&#20013;&#35774;&#35745;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#65306;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#27010;&#24565;&#30340;&#27169;&#22411;&#65288;CBMs&#65289;&#65292;&#24182;&#27809;&#26377;&#35774;&#35745;&#26469;&#35299;&#20915;&#20851;&#31995;&#38382;&#39064;&#65292;&#32780;&#20851;&#31995;&#27169;&#22411;&#20063;&#27809;&#26377;&#20687;CBMs&#37027;&#26679;&#21487;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#27010;&#24565;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#20379;&#21487;&#35299;&#37322;&#20219;&#21153;&#39044;&#27979;&#30340;&#20851;&#31995;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23478;&#26063;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#22270;&#20687;&#20998;&#31867;&#21040;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#34920;&#26126;&#20851;&#31995;CBMs&#65306;&#65288;i&#65289;&#19982;&#29616;&#26377;&#30340;&#20851;&#31995;&#40657;&#30418;&#30340;&#27867;&#21270;&#24615;&#33021;&#30456;&#21305;&#37197;&#65288;&#19981;&#21516;&#20110;&#38750;&#20851;&#31995;&#30340;CBMs&#65289;&#65292;&#65288;ii&#65289;&#25903;&#25345;&#29983;&#25104;&#37327;&#21270;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#65288;iii&#65289;&#26377;&#25928;&#24212;&#23545;&#27979;&#35797;&#26102;&#30340;&#24178;&#39044;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#32463;&#21463;&#20303;&#21253;&#25324;&#36229;&#20986;&#20998;&#24067;&#24773;&#26223;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#21644;&#31232;&#32570;&#30340;&#27010;&#24565;&#30417;&#30563;&#31561;&#33499;&#21051;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11978</link><description>&lt;p&gt;
&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26469;&#36827;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#32473;&#23450;&#30340;&#26631;&#31614;&#39044;&#27979;&#19968;&#20010;&#23436;&#25972;&#30340;&#20855;&#26377;&#22810;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#36825;&#20010;&#20219;&#21153;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#20998;&#23376;&#35774;&#35745;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#20986;&#29616;&#20102;&#20960;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;(1) &#36825;&#20123;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#24448;&#24448;&#26410;&#32463;&#28145;&#20837;&#25506;&#32034;&#65307;(2) &#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#21482;&#22312;&#26377;&#38480;&#30340;&#25351;&#26631;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;GNN&#26367;&#25442;&#20026;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;GNN&#65292;&#30740;&#31350;&#20102;GNN&#22312;&#20998;&#23376;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#31181;&#19981;&#21516;&#29983;&#25104;&#26694;&#26550;&#65288;GCPN&#21644;GraphAF&#65289;&#20013;&#20845;&#31181;GNN&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#20998;&#23376;&#29983;&#25104;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11975</link><description>&lt;p&gt;
&#20351;&#29992;&#31526;&#21512;&#22238;&#24402;&#26041;&#27861;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximating Score-based Explanation Techniques Using Conformal Regression. (arXiv:2308.11975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65292;&#24182;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#25552;&#21319;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32463;&#24120;&#34987;&#29992;&#26469;&#29702;&#35299;&#40657;&#30418;&#27169;&#22411;&#32972;&#21518;&#30340;&#36923;&#36753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#37322;&#25216;&#26415;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26102;&#38388;&#20851;&#38190;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#22238;&#24402;&#27169;&#22411;&#26469;&#36817;&#20284;&#35780;&#20998;&#35299;&#37322;&#25216;&#26415;&#65288;&#22914;SHAP&#65289;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24402;&#32435;&#24335;&#31526;&#21512;&#39044;&#27979;&#26694;&#26550;&#25552;&#20379;&#20102;&#36817;&#20284;&#20540;&#30340;&#26377;&#25928;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#38750;&#31526;&#21512;&#24230;&#24230;&#37327;&#65292;&#26088;&#22312;&#21516;&#26102;&#32771;&#34385;&#36817;&#20284;&#35299;&#37322;&#30340;&#38590;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#20302;&#24265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#36817;&#20284;&#35299;&#37322;&#30340;&#25928;&#29575;&#65288;&#21306;&#38388;&#22823;&#23567;&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time com
&lt;/p&gt;</description></item><item><title>Blending-NeRF&#26159;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#33021;&#22815;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#21407;&#22987;&#23545;&#35937;&#21644;&#30446;&#26631;&#23545;&#35937;&#24182;&#28155;&#21152;&#39118;&#26684;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#36827;&#34892;&#25351;&#23548;&#65292;&#24182;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#28155;&#21152;&#26032;&#23545;&#35937;&#12289;&#20462;&#25913;&#32441;&#29702;&#21644;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#37096;&#20998;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11974</link><description>&lt;p&gt;
Blending-NeRF&#65306;&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#23616;&#37096;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields. (arXiv:2308.11974v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11974
&lt;/p&gt;
&lt;p&gt;
Blending-NeRF&#26159;&#19968;&#31181;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#23454;&#29616;&#23616;&#37096;&#32534;&#36753;&#65292;&#33021;&#22815;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#28151;&#21512;&#21407;&#22987;&#23545;&#35937;&#21644;&#30446;&#26631;&#23545;&#35937;&#24182;&#28155;&#21152;&#39118;&#26684;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#36827;&#34892;&#25351;&#23548;&#65292;&#24182;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#28155;&#21152;&#26032;&#23545;&#35937;&#12289;&#20462;&#25913;&#32441;&#29702;&#21644;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#37096;&#20998;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;3D&#23545;&#35937;&#30340;&#23616;&#37096;&#32534;&#36753;&#26159;&#19968;&#39033;&#29305;&#21035;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#19981;&#25197;&#26354;&#23545;&#35937;&#24418;&#29366;&#30340;&#24773;&#20917;&#19979;&#65292;&#23616;&#37096;&#28151;&#21512;&#21407;&#22987;3D&#23545;&#35937;&#19982;&#30446;&#26631;&#26032;&#23545;&#35937;&#21644;&#39118;&#26684;&#25928;&#26524;&#24182;&#19981;&#26159;&#19968;&#31181;&#30452;&#25509;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;NeRF&#30340;&#27169;&#22411;&#8212;&#8212;Blending-NeRF&#65292;&#23427;&#30001;&#20004;&#20010;NeRF&#32593;&#32476;&#32452;&#25104;&#65306;&#39044;&#35757;&#32451;&#30340;NeRF&#21644;&#21487;&#32534;&#36753;&#30340;NeRF&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20351;Blending-NeRF&#33021;&#22815;&#27491;&#30830;&#22320;&#32534;&#36753;&#30001;&#25991;&#26412;&#23450;&#20301;&#30340;&#30446;&#26631;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;CLIP&#65292;&#25105;&#20204;&#24341;&#23548;Blending-NeRF&#28155;&#21152;&#20855;&#26377;&#19981;&#21516;&#39068;&#33394;&#21644;&#23494;&#24230;&#30340;&#26032;&#23545;&#35937;&#65292;&#20462;&#25913;&#32441;&#29702;&#65292;&#24182;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Blending-NeRF&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#20135;&#29983;&#33258;&#28982;&#19988;&#23616;&#37096;&#32534;&#36753;&#30340;3D&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-driven localized editing of 3D objects is particularly difficult as locally mixing the original 3D object with the intended new object and style effects without distorting the object's form is not a straightforward process. To address this issue, we propose a novel NeRF-based model, Blending-NeRF, which consists of two NeRF networks: pretrained NeRF and editable NeRF. Additionally, we introduce new blending operations that allow Blending-NeRF to properly edit target regions which are localized by text. By using a pretrained vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects with varying colors and densities, modify textures, and remove parts of the original object. Our extensive experiments demonstrate that Blending-NeRF produces naturally and locally edited 3D objects from various text prompts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36741;&#21161;&#20215;&#20540;&#65288;VOA&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34920;&#31034;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#25552;&#20379;&#36741;&#21161;&#25152;&#20135;&#29983;&#30340;&#39044;&#26399;&#25104;&#26412;&#20943;&#23569;&#65292;&#20197;&#21450;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#21521;&#21738;&#20010;&#20195;&#29702;&#25552;&#20379;&#36741;&#21161;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11961</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#36741;&#21161;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Value of Assistance for Mobile Agents. (arXiv:2308.11961v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36741;&#21161;&#20215;&#20540;&#65288;VOA&#65289;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34920;&#31034;&#22312;&#31227;&#21160;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#25552;&#20379;&#36741;&#21161;&#25152;&#20135;&#29983;&#30340;&#39044;&#26399;&#25104;&#26412;&#20943;&#23569;&#65292;&#20197;&#21450;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#21521;&#21738;&#20010;&#20195;&#29702;&#25552;&#20379;&#36741;&#21161;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20195;&#29702;&#32463;&#24120;&#21463;&#21040;&#26102;&#38388;&#21644;&#26426;&#22120;&#20154;&#31227;&#21160;&#23548;&#33268;&#30340;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#22256;&#25200;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#25191;&#34892;&#36741;&#21161;&#25805;&#20316;&#26469;&#20943;&#23569;&#26426;&#22120;&#20154;&#20301;&#32622;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#21327;&#20316;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;&#36718;&#24335;&#26426;&#22120;&#20154;&#21487;&#20197;&#35831;&#27714;&#19968;&#26550;&#26080;&#20154;&#26426;&#30340;&#24110;&#21161;&#65292;&#26080;&#20154;&#26426;&#21487;&#20197;&#39134;&#21040;&#20854;&#39044;&#20272;&#20301;&#32622;&#19978;&#65292;&#24182;&#22312;&#22320;&#22270;&#19978;&#26174;&#31034;&#20854;&#30830;&#20999;&#20301;&#32622;&#65292;&#25110;&#32773;&#38506;&#21516;&#20854;&#21040;&#36798;&#30446;&#26631;&#20301;&#32622;&#12290;&#30001;&#20110;&#36741;&#21161;&#21487;&#33021;&#26114;&#36149;&#19988;&#26377;&#38480;&#65292;&#24182;&#19988;&#21487;&#33021;&#30001;&#22242;&#38431;&#30340;&#19981;&#21516;&#25104;&#21592;&#25552;&#20986;&#35831;&#27714;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25903;&#25345;&#20309;&#26102;&#20197;&#21450;&#21521;&#20195;&#29702;&#25552;&#20379;&#21738;&#31181;&#36741;&#21161;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#22312;&#22242;&#38431;&#20013;&#20915;&#23450;&#24110;&#21161;&#21738;&#20010;&#20195;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36741;&#21161;&#20215;&#20540;&#65288;Value of Assistance&#65292;VOA&#65289;&#26469;&#34920;&#31034;&#22312;&#25191;&#34892;&#30340;&#29305;&#23450;&#28857;&#36741;&#21161;&#23558;&#20135;&#29983;&#30340;&#39044;&#26399;&#25104;&#26412;&#20943;&#23569;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#20154;&#20301;&#32622;&#20272;&#35745;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;VOA&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile robotic agents often suffer from localization uncertainty which grows with time and with the agents' movement. This can hinder their ability to accomplish their task. In some settings, it may be possible to perform assistive actions that reduce uncertainty about a robot's location. For example, in a collaborative multi-robot system, a wheeled robot can request assistance from a drone that can fly to its estimated location and reveal its exact location on the map or accompany it to its intended location. Since assistance may be costly and limited, and may be requested by different members of a team, there is a need for principled ways to support the decision of which assistance to provide to an agent and when, as well as to decide which agent to help within a team. For this purpose, we propose Value of Assistance (VOA) to represent the expected cost reduction that assistance will yield at a given point of execution. We offer ways to compute VOA based on estimations of the robot's
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2308.11958</link><description>&lt;p&gt;
&#36890;&#36807;&#20877;&#29983;&#24615;&#27491;&#21017;&#21270;&#32500;&#25345;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Maintaining Plasticity via Regenerative Regularization. (arXiv:2308.11958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#30340;&#21487;&#22609;&#24615;&#19988;&#26131;&#20110;&#23454;&#26045;&#12290;&#35813;&#26041;&#27861;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#24182;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#21487;&#22609;&#24615;&#25351;&#30340;&#26159;&#20195;&#29702;&#24555;&#36895;&#36866;&#24212;&#26032;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#27969;&#26102;&#20250;&#22833;&#21435;&#21487;&#22609;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;L2 Init&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;L2&#27491;&#21017;&#21270;&#24212;&#29992;&#20110;&#21021;&#22987;&#21442;&#25968;&#65292;&#26469;&#32500;&#25345;&#21487;&#22609;&#24615;&#12290;&#36825;&#19982;&#26631;&#20934;&#30340;L2&#27491;&#21017;&#21270;&#38750;&#24120;&#30456;&#20284;&#65292;&#21807;&#19968;&#30340;&#21306;&#21035;&#22312;&#20110;L2 Init&#27491;&#21017;&#21270;&#26397;&#21521;&#21407;&#28857;&#12290;L2 Init&#26131;&#20110;&#23454;&#26045;&#65292;&#21482;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#36229;&#21442;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#21160;&#26426;&#19982;&#37325;&#32622;&#31070;&#32463;&#20803;&#25110;&#21442;&#25968;&#20540;&#30340;&#26041;&#27861;&#30456;&#21516;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#24403;&#26368;&#36817;&#30340;&#25439;&#22833;&#23545;&#29305;&#23450;&#21442;&#25968;&#19981;&#25935;&#24863;&#26102;&#65292;&#36825;&#20123;&#21442;&#25968;&#20250;&#21521;&#23427;&#20204;&#30340;&#21021;&#22987;&#20540;&#28418;&#31227;&#12290;&#36825;&#20351;&#24471;&#21442;&#25968;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#22312;&#20195;&#34920;&#36830;&#32493;&#23398;&#20064;&#20013;&#19981;&#21516;&#31867;&#22411;&#38750;&#24179;&#31283;&#24615;&#30340;&#31616;&#21333;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;L2 Init&#33021;&#22815;&#19968;&#33268;&#22320;&#20943;&#36731;&#21487;&#22609;&#24615;&#30340;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates 
&lt;/p&gt;</description></item><item><title>&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;MiniBatch-SFL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;MiniBatch SGD&#21040;SplitFed Learning&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#22343;&#34913;&#25968;&#25454;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11953</link><description>&lt;p&gt;
&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning: &#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation. (arXiv:2308.11953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11953
&lt;/p&gt;
&lt;p&gt;
&#24403;MiniBatch SGD&#36935;&#19978;SplitFed Learning&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;MiniBatch-SFL&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;MiniBatch SGD&#21040;SplitFed Learning&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#22343;&#34913;&#25968;&#25454;&#23548;&#33268;&#30340;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#65288;&#20363;&#22914;&#36793;&#32536;&#35774;&#22791;&#65289;&#33021;&#22815;&#21327;&#21516;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;FL&#22312;&#35745;&#31639;&#19978;&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#38656;&#35201;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#22810;&#27425;&#12290;SplitFed&#23398;&#20064;&#65288;SFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#19968;&#20010;&#20999;&#21106;&#23618;&#23558;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#26469;&#20943;&#36731;&#23458;&#25143;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#36127;&#36733;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21482;&#38656;&#35201;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#39640;&#24230;&#19981;&#22343;&#34913;&#26102;&#65292;SFL&#20173;&#28982;&#38754;&#20020;&#30528;&#8220;&#23458;&#25143;&#31471;&#28418;&#31227;&#8221;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MiniBatch-SFL&#12290;&#35813;&#31639;&#27861;&#23558;MiniBatch SGD&#24341;&#20837;SFL&#20013;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;FL&#26041;&#24335;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#32780;&#26381;&#21153;&#22120;&#21017;&#31867;&#20284;&#20110;MiniBatch SGD&#35757;&#32451;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;MiniBatch-SFL&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#39044;&#26399;&#30340;&#26381;&#21153;&#22120;&#31471;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#26356;&#26032;&#26469;&#24471;&#21040;&#26399;&#26395;&#25439;&#22833;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11951</link><description>&lt;p&gt;
&#20174;&#35270;&#39057;&#20013;&#29983;&#25104;&#23039;&#21183;&#35843;&#33410;&#30340;&#34394;&#25311;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Pose Modulated Avatars from Video. (arXiv:2308.11951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#26469;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#65292;&#24182;&#36890;&#36807;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#24314;&#27169;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30001;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#21644;&#31232;&#30095;&#30340;&#25668;&#20687;&#26426;&#32452;&#21487;&#20197;&#37325;&#24314;&#21160;&#24577;&#20154;&#20307;&#36816;&#21160;&#21644;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#34915;&#29289;&#21644;&#30382;&#32932;&#30340;&#21464;&#24418;&#19982;&#39592;&#26550;&#23039;&#21183;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#19982;&#38544;&#24335;&#23398;&#20064;&#25110;&#20381;&#36182;&#20195;&#29702;&#34920;&#38754;&#30340;&#29616;&#26377;&#35282;&#33394;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#23039;&#21183;&#38656;&#35201;&#19981;&#21516;&#30340;&#39057;&#29575;&#20998;&#37197;&#12290;&#24573;&#35270;&#36825;&#31181;&#21306;&#21035;&#20250;&#22312;&#24179;&#28369;&#21306;&#22495;&#20135;&#29983;&#22122;&#28857;&#20266;&#24433;&#65292;&#25110;&#20351;&#38160;&#21033;&#21306;&#22495;&#30340;&#32454;&#31890;&#24230;&#32441;&#29702;&#21644;&#24418;&#29366;&#32454;&#33410;&#27169;&#31946;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#39057;&#22495;&#20013;&#26159;&#33258;&#36866;&#24212;&#21644;&#26174;&#24335;&#30340;&#21452;&#20998;&#25903;&#31070;&#32463;&#32593;&#32476;&#12290;&#31532;&#19968;&#20010;&#20998;&#25903;&#26159;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#22320;&#24314;&#27169;&#36523;&#20307;&#37096;&#20301;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#39592;&#26550;&#23039;&#21183;&#20316;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#20010;&#20998;&#25903;&#23558;&#36825;&#20123;&#30456;&#20851;&#29305;&#24449;&#32452;&#21512;&#21040;&#19968;&#32452;&#20840;&#23616;&#39057;&#29575;&#20013;&#65292;&#28982;&#21518;&#35843;&#33410;&#29305;&#24449;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose. Unlike existing avatar models that are learned implicitly or rely on a proxy surface, our approach is motivated by the observation that different poses necessitate unique frequency assignments. Neglecting this distinction yields noisy artifacts in smooth areas or blurs fine-grained texture and shape details in sharp regions. We develop a two-branch neural network that is adaptive and explicit in the frequency domain. The first branch is a graph neural network that models correlations among body parts locally, taking skeleton pose as input. The second branch combines these correlation features to a set of global frequencies and then modulates the feature encoding. Our experiments demonstrate that our network outperforms state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21435;&#38654;&#12290;</title><link>http://arxiv.org/abs/2308.11949</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#22270;&#20687;&#21435;&#38654;&#27169;&#22411;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-quality Image Dehazing with Diffusion Model. (arXiv:2308.11949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21435;&#38654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23494;&#38598;&#38654;&#38718;&#22330;&#26223;&#19979;&#65292;&#22270;&#20687;&#21435;&#38654;&#38754;&#20020;&#30528;&#24456;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#38654;&#38718;&#22270;&#20687;&#20013;&#24456;&#23569;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#23494;&#38598;&#38654;&#38718;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#20869;&#23481;&#21644;&#39068;&#33394;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;DDPM&#27809;&#26377;&#32771;&#34385;&#21040;&#21435;&#38654;&#20219;&#21153;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#20449;&#24687;&#34917;&#20840;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DehazeDDPM&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#21069;&#19968;&#38454;&#27573;&#29992;&#22823;&#27668;&#25955;&#23556;&#27169;&#22411;&#65288;ASM&#65289;&#23545;&#21435;&#38654;&#20219;&#21153;&#36827;&#34892;&#29289;&#29702;&#24314;&#27169;&#65292;&#23558;&#20998;&#24067;&#25289;&#36817;&#28165;&#26224;&#25968;&#25454;&#65292;&#24182;&#36171;&#20104;DehazeDDPM&#20855;&#26377;&#38654;&#38718;&#24863;&#30693;&#33021;&#21147;&#12290;&#21518;&#19968;&#38454;&#27573;&#21033;&#29992;DDPM&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#34917;&#20607;&#38654;&#38718;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-ind
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;LongDanceDiff&#65292;&#29992;&#20110;&#38271;&#26102;&#33310;&#36424;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#26102;&#38388;&#36830;&#36143;&#24615;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#37096;&#20998;&#21152;&#22122;&#31574;&#30053;&#21644;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#30446;&#26631;&#26469;&#22686;&#21152;&#21160;&#20316;&#22810;&#26679;&#24615;&#21644;&#20943;&#36731;&#20923;&#32467;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11945</link><description>&lt;p&gt;
&#38271;&#26102;&#33310;&#36424;&#29983;&#25104;&#19982;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;(LongDanceDiff)
&lt;/p&gt;
&lt;p&gt;
LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model. (arXiv:2308.11945v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;LongDanceDiff&#65292;&#29992;&#20110;&#38271;&#26102;&#33310;&#36424;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#26102;&#38388;&#36830;&#36143;&#24615;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#37096;&#20998;&#21152;&#22122;&#31574;&#30053;&#21644;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#30446;&#26631;&#26469;&#22686;&#21152;&#21160;&#20316;&#22810;&#26679;&#24615;&#21644;&#20943;&#36731;&#20923;&#32467;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20276;&#38543;&#38899;&#20048;&#36339;&#33310;&#19968;&#30452;&#26159;&#34920;&#36798;&#24773;&#24863;&#30340;&#37325;&#35201;&#33402;&#26415;&#24418;&#24335;&#12290;&#30001;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#32467;&#26500;&#65292;&#19982;&#38899;&#20048;&#21516;&#27493;&#30340;&#38271;&#26102;3D&#36924;&#30495;&#33310;&#36424;&#29983;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#38271;&#26102;&#38388;&#33310;&#36424;&#26102;&#23384;&#22312;&#31215;&#32047;&#35823;&#24046;&#21644;&#35757;&#32451;-&#25512;&#29702;&#20559;&#24046;&#24341;&#36215;&#30340;&#20923;&#32467;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;LongDanceDiff&#65292;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38271;&#26399;&#33310;&#36424;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#36830;&#36143;&#24615;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#25361;&#25112;&#12290;LongDanceDiff&#21253;&#21547;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36755;&#20837;&#26159;&#38899;&#20048;&#12289;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#22122;&#22768;&#26410;&#26469;&#30340;&#21160;&#20316;&#30340;&#20018;&#32852;&#12290;&#36825;&#31181;&#37096;&#20998;&#21152;&#22122;&#30340;&#31574;&#30053;&#21033;&#29992;&#20102;&#20840;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#23398;&#20064;&#20102;&#38899;&#20048;&#21644;&#36807;&#21435;&#21160;&#20316;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#22686;&#21152;&#29983;&#25104;&#33310;&#36424;&#21160;&#20316;&#30340;&#22810;&#26679;&#24615;&#21644;&#20943;&#36731;&#20923;&#32467;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#30446;&#26631;&#65292;&#29992;&#20110;&#35268;&#33539;&#21270;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dancing with music is always an essential human art form to express emotion. Due to the high temporal-spacial complexity, long-term 3D realist dance generation synchronized with music is challenging. Existing methods suffer from the freezing problem when generating long-term dances due to error accumulation and training-inference discrepancy. To address this, we design a conditional diffusion model, LongDanceDiff, for this sequence-to-sequence long-term dance generation, addressing the challenges of temporal coherency and spatial constraint. LongDanceDiff contains a transformer-based diffusion model, where the input is a concatenation of music, past motions, and noised future motions. This partial noising strategy leverages the full-attention mechanism and learns the dependencies among music and past motions. To enhance the diversity of generated dance motions and mitigate the freezing problem, we introduce a mutual information minimization objective that regularizes the dependency bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11943</link><description>&lt;p&gt;
RamseyRL:&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching. (arXiv:2308.11943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#21270;&#30340;Ramsey&#25968;&#21453;&#20363;&#25628;&#32034;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Ramsey&#25968;&#26159;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#25968;&#20026;$n$&#30340;&#31616;&#21333;&#26080;&#21521;&#22270;&#21253;&#21547;&#19968;&#20010;&#39034;&#24207;&#20026;$s$&#30340;&#22242;&#25110;&#32773;&#19968;&#20010;&#39034;&#24207;&#20026;$t$&#30340;&#29420;&#31435;&#38598;&#30340;&#26368;&#23567;&#25968;$n = R(s, t)$&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26368;&#20339;&#20248;&#20808;&#25628;&#32034;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#20197;&#23547;&#25214;&#29305;&#23450;Ramsey&#25968;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22270;&#21521;&#37327;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#25628;&#32034;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#25628;&#32034;&#65289;&#65292;&#36890;&#36807;&#34913;&#37327;&#22270;&#24418;&#25104;&#20026;&#21453;&#20363;&#30340;&#21487;&#33021;&#24615;&#26469;&#27979;&#31639;&#12290;&#25991;&#31456;&#36824;&#25552;&#20986;&#20102;&#31639;&#27861;&#20248;&#21270;&#20197;&#38480;&#21046;&#22810;&#39033;&#24335;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#19981;&#26159;&#25552;&#20379;&#26032;&#30340;&#21453;&#20363;&#65292;&#32780;&#26159;&#20171;&#32461;&#21644;&#35780;&#20272;&#25903;&#25345;Ramsey&#21453;&#20363;&#25506;&#32034;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#20854;&#20182;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20195;&#30721;&#21644;&#26041;&#27861;&#36890;&#36807;PyPI&#36719;&#20214;&#21253;&#21644;GitHub&#23384;&#20648;&#24211;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.11939</link><description>&lt;p&gt;
&#38646;&#21806;&#38656;&#27714;&#39044;&#27979;&#65306;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Retail Demand Forecasting: A Comparative Study for Multivariate Time Series. (arXiv:2308.11939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#30340;&#38656;&#27714;&#39044;&#27979;&#26159;&#36130;&#21153;&#32489;&#25928;&#21644;&#20379;&#24212;&#38142;&#25928;&#29575;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#38543;&#30528;&#20840;&#29699;&#24066;&#22330;&#26085;&#30410;&#20114;&#32852;&#20114;&#36890;&#65292;&#20225;&#19994;&#24320;&#22987;&#37319;&#29992;&#20808;&#36827;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#33719;&#21462;&#31454;&#20105;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#21382;&#21490;&#38144;&#21806;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#23439;&#35266;&#32463;&#27982;&#26465;&#20214;&#23545;&#28040;&#36153;&#32773;&#25903;&#20986;&#34892;&#20026;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23458;&#25143;&#38656;&#27714;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#28040;&#36153;&#32773;&#29289;&#20215;&#25351;&#25968;&#65288;CPI&#65289;&#12289;&#28040;&#36153;&#32773;&#20449;&#24515;&#25351;&#25968;&#65288;ICS&#65289;&#21644;&#22833;&#19994;&#29575;&#31561;&#23439;&#35266;&#32463;&#27982;&#21464;&#37327;&#30456;&#32467;&#21512;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#21033;&#29992;&#36825;&#20010;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#21508;&#31181;&#22238;&#24402;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#38646;&#21806;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#29942;&#39048;Transformer&#30340;&#21452;&#27969;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#20214;&#22270;&#20687;&#21644;&#20307;&#32032;&#29305;&#24449;&#34701;&#21512;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11937</link><description>&lt;p&gt;
&#23398;&#20064;&#29942;&#39048;Transformer&#36827;&#34892;&#22522;&#20110;&#20107;&#20214;&#22270;&#20687;-&#20307;&#32032;&#29305;&#24449;&#34701;&#21512;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification. (arXiv:2308.11937v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#29942;&#39048;Transformer&#30340;&#21452;&#27969;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#20214;&#22270;&#20687;&#21644;&#20307;&#32032;&#29305;&#24449;&#34701;&#21512;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20351;&#29992;&#20107;&#20214;&#30456;&#26426;&#35782;&#21035;&#30446;&#26631;&#29289;&#20307;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#20107;&#20214;&#27969;&#34920;&#31034;&#20026;&#28857;&#20113;&#12289;&#20307;&#32032;&#12289;&#22270;&#20687;&#31561;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#33021;&#21463;&#21040;&#20197;&#19979;&#22240;&#32032;&#30340;&#38480;&#21046;&#65306;&#21333;&#35843;&#30340;&#27169;&#24577;&#34920;&#36798;&#21644;&#32593;&#32476;&#32467;&#26500;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21452;&#27969;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#20214;&#34920;&#31034;&#12289;&#25552;&#21462;&#21644;&#34701;&#21512;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#27169;&#25311;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#34920;&#31034;&#24418;&#24335;&#65306;&#20107;&#20214;&#22270;&#20687;&#21644;&#20107;&#20214;&#20307;&#32032;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#21644;&#32467;&#26500;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#21487;&#20197;&#20998;&#21035;&#23398;&#20064;&#31354;&#38388;&#20449;&#24687;&#21644;&#19977;&#32500;&#31435;&#20307;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#29942;&#39048;Transformer&#26469;&#20419;&#36827;&#21452;&#27969;&#20449;&#24687;&#30340;&#34701;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing target objects using an event-based camera draws more and more attention in recent years. Existing works usually represent the event streams into point-cloud, voxel, image, etc, and learn the feature representations using various deep neural networks. Their final results may be limited by the following factors: monotonous modal expressions and the design of the network structure. To address the aforementioned challenges, this paper proposes a novel dual-stream framework for event representation, extraction, and fusion. This framework simultaneously models two common representations: event images and event voxels. By utilizing Transformer and Structured Graph Neural Network (GNN) architectures, spatial information and three-dimensional stereo information can be learned separately. Additionally, a bottleneck Transformer is introduced to facilitate the fusion of the dual-stream information. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11924</link><description>&lt;p&gt;
&#19981;&#21516;&#25919;&#31574;&#22312;&#26080;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diverse Policies Converge in Reward-free Markov Decision Processe. (arXiv:2308.11924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#20915;&#31574;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#20026;&#20102;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#21457;&#23637;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#24050;&#25104;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#22810;&#31181;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#20013;&#27809;&#26377;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#22238;&#31572;&#20102;&#31639;&#27861;&#22914;&#20309;&#25910;&#25947;&#20197;&#21450;&#31639;&#27861;&#30340;&#25928;&#29575;&#22914;&#20309;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#22810;&#26679;&#21270;&#31574;&#30053;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#36825;&#26679;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#39640;&#25928;&#24615;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#28608;&#27963;&#24471;&#20998;&#30340;&#27010;&#24565;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#27010;&#24565;&#26159;&#21542;&#21253;&#21547;&#35270;&#35273;&#32447;&#32034;&#26469;&#31579;&#36873;&#20986;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.11920</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#35270;&#35273;&#27010;&#24565;&#36807;&#28388;&#19979;&#30340;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification. (arXiv:2308.11920v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#28608;&#27963;&#24471;&#20998;&#30340;&#27010;&#24565;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#27010;&#24565;&#26159;&#21542;&#21253;&#21547;&#35270;&#35273;&#32447;&#32034;&#26469;&#31579;&#36873;&#20986;&#26377;&#24847;&#20041;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#26159;&#26500;&#24314;&#21487;&#38752;&#27169;&#22411;&#29992;&#20110;&#21508;&#31181;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;(CBMs)&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20316;&#20026;&#20013;&#38388;&#30446;&#26631;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#24037;&#21171;&#21160;&#26469;&#26500;&#24314;&#27010;&#24565;&#38598;&#19981;&#21516;&#65292;&#26368;&#36817;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#27010;&#24565;&#30340;&#24037;&#20316;&#20351;&#33258;&#21160;&#29983;&#25104;&#27010;&#24565;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#27010;&#24565;&#26159;&#21542;&#19982;&#35270;&#35273;&#30456;&#20851;&#65292;&#32780;&#36825;&#26159;&#35745;&#31639;&#26377;&#24847;&#20041;&#27010;&#24565;&#24471;&#20998;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#28608;&#27963;&#24471;&#20998;&#65292;&#29992;&#26469;&#34913;&#37327;&#27010;&#24565;&#26159;&#21542;&#21253;&#21547;&#35270;&#35273;&#32447;&#32034;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#25968;&#25454;&#36731;&#26494;&#35745;&#31639;&#12290;&#35745;&#31639;&#24471;&#21040;&#30340;&#35270;&#35273;&#28608;&#27963;&#24471;&#20998;&#28982;&#21518;&#29992;&#20110;&#36807;&#28388;&#20986;&#19981;&#22826;&#21487;&#35265;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#35270;&#35273;&#24847;&#20041;&#30340;&#26368;&#32456;&#27010;&#24565;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#35270;&#35273;&#28608;&#27963;&#24471;&#20998;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is a crucial factor in building reliable models for various medical applications. Concept Bottleneck Models (CBMs) enable interpretable image classification by utilizing human-understandable concepts as intermediate targets. Unlike conventional methods that require extensive human labor to construct the concept set, recent works leveraging Large Language Models (LLMs) for generating concepts made automatic concept generation possible. However, those methods do not consider whether a concept is visually relevant or not, which is an important factor in computing meaningful concept scores. Therefore, we propose a visual activation score that measures whether the concept contains visual cues or not, which can be easily computed with unlabeled image data. Computed visual activation scores are then used to filter out the less visible concepts, thus resulting in a final concept set with visually meaningful concepts. Our experimental results show that adopting the proposed vis
&lt;/p&gt;</description></item><item><title>LFS-GAN&#26159;&#19968;&#20010;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.11917</link><description>&lt;p&gt;
LFS-GAN: &#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LFS-GAN: Lifelong Few-Shot Image Generation. (arXiv:2308.11917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11917
&lt;/p&gt;
&lt;p&gt;
LFS-GAN&#26159;&#19968;&#20010;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#20316;&#20026;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#21516;&#26102;&#36935;&#21040;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#32456;&#36523;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#35843;&#21046;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#32780;&#19988;&#19981;&#33021;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#26102;&#23384;&#22312;&#20005;&#37325;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#32456;&#36523;&#23569;&#26679;&#26412;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;LFS-GAN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32456;&#36523;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;&#20219;&#21153;&#29305;&#23450;&#35843;&#21046;&#22120; - &#21487;&#23398;&#20064;&#22240;&#23376;&#21270;&#24352;&#37327;&#65288;LeFT&#65289;&#26469;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#12290;LeFT&#20855;&#26377;&#31209;&#32422;&#26463;&#24182;&#19988;&#20855;&#26377;&#20016;&#23500;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a challenging lifelong few-shot image generation task for the first time. In this situation, a generative model learns a sequence of tasks using only a few samples per task. Consequently, the learned model encounters both catastrophic forgetting and overfitting problems at a time. Existing studies on lifelong GANs have proposed modulation-based methods to prevent catastrophic forgetting. However, they require considerable additional parameters and cannot generate high-fidelity and diverse images from limited data. On the other hand, the existing few-shot GANs suffer from severe catastrophic forgetting when learning multiple tasks. To alleviate these issues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that can generate high-quality and diverse images in lifelong few-shot image generation task. Our proposed framework learns each task using an efficient task-specific modulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained and has a rich repres
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;</title><link>http://arxiv.org/abs/2308.11914</link><description>&lt;p&gt;
&#36808;&#21521;&#22240;&#26524;GPT&#65306;&#36890;&#36807;&#20419;&#36827;LLMs&#20013;&#30340;&#22240;&#26524;&#19968;&#33268;&#24615;&#65292;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#23454;&#29616;&#24544;&#23454;&#30340;&#30693;&#35782;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs. (arXiv:2308.11914v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#65292;&#36890;&#36807;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#30340;&#21512;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#35884;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#30693;&#35782;&#22238;&#24518;&#21644;&#25512;&#29702;&#30340;&#33030;&#24369;&#24615;&#24341;&#36215;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#40723;&#21169;LLMs&#33258;&#20027;&#35745;&#21010;&#21644;&#35299;&#20915;&#38382;&#39064;&#25110;&#24191;&#27867;&#37319;&#26679;&#25512;&#29702;&#38142;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26410;&#33021;&#35299;&#20915;&#27010;&#24565;&#21644;&#25512;&#29702;&#35884;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#25512;&#29702;&#35884;&#35823;&#65292;&#25105;&#20204;&#20174;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#20013;&#24471;&#21040;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#22686;&#21152;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#21644;&#22240;&#26524;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#22810;&#20010;&#26234;&#33021;&#20307;&#65288;&#21363;&#25512;&#29702;&#22120;&#21644;&#22240;&#26524;&#35780;&#20272;&#22120;&#65289;&#22312;&#25512;&#29702;&#21644;&#19968;&#33268;&#24615;&#33539;&#24335;&#20013;&#21327;&#20316;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#25512;&#29702;&#22120;&#19987;&#27880;&#20110;&#25552;&#20379;&#20855;&#26377;&#20154;&#31867;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22240;&#26524;&#35780;&#20272;&#22120;&#20195;&#29702;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#31572;&#26696;&#26159;&#21542;&#20174;&#38382;&#39064;&#20013;&#22240;&#26524;&#25512;&#23548;&#20986;&#26469;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#24182;&#29992;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#31572;&#26696;&#26469;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoner and causal evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the causal evaluator agent scrutinizes if the answer in a solution is causally deducible from the question and vice versa, with a counterfactual answer replacin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11905</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#25509;&#21463;&#36793;&#30028;&#36827;&#34892;&#21551;&#21457;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Utilizing Admissible Bounds for Heuristic Learning. (arXiv:2308.11905v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#21069;&#21521;&#25628;&#32034;&#31639;&#27861;&#30340;&#21551;&#21457;&#24335;&#20989;&#25968;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#24212;&#35813;&#23398;&#20064;&#30340;&#20869;&#23481;&#12289;&#22914;&#20309;&#35757;&#32451;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#26679;&#20570;&#30340;&#29702;&#35770;&#35748;&#35782;&#36824;&#24456;&#23569;&#12290;&#36825;&#31181;&#29702;&#35299;&#30340;&#19981;&#36275;&#23548;&#33268;&#25991;&#29486;&#20013;&#36827;&#34892;&#25968;&#25454;&#38598;&#36873;&#25321;&#65288;&#27425;&#20248;&#25104;&#26412;&#23545;&#26368;&#20248;&#25104;&#26412;&#25110;&#21487;&#25509;&#21463;&#23545;&#19981;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#65289;&#21644;&#20248;&#21270;&#25351;&#26631;&#65288;&#20363;&#22914;&#24179;&#26041;&#35823;&#24046;&#21644;&#32477;&#23545;&#35823;&#24046;&#65289;&#26102;&#36827;&#34892;&#20102;&#20020;&#26102;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#35757;&#32451;&#21551;&#21457;&#24335;&#20989;&#25968;&#32570;&#20047;&#21487;&#25509;&#21463;&#24615;&#65292;&#23545;&#20110;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25509;&#21463;&#24615;&#30340;&#37325;&#35201;&#24615;&#20063;&#32570;&#20047;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#20316;&#20026;&#25130;&#26029;&#39640;&#26031;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#26126;&#30830;&#20102;&#22312;&#30417;&#30563;&#21551;&#21457;&#24335;&#23398;&#20064;&#20013;&#21487;&#25509;&#21463;&#21551;&#21457;&#24335;&#30340;&#20316;&#29992;&#65292;&#30456;&#27604;&#26222;&#36890;&#39640;&#26031;&#20998;&#24067;&#65292;&#32039;&#32553;&#20102;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#25968;&#23398;&#27169;&#22411;&#24544;&#23454;&#22320;&#36981;&#24490;&#20102;&#26368;&#22823;&#29109;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2308.11898</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#19968;&#31867;&#20998;&#31867;&#30340;&#20248;&#21270;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Exploring the Optimization Objective of One-Class Classification for Anomaly Detection. (arXiv:2308.11898v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#19968;&#31181;&#38271;&#26399;&#20197;&#26469;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#20511;&#21161;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#30340;&#24378;&#22823;&#34920;&#31034;&#33021;&#21147;&#65292;OCC&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#36890;&#24120;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;OCC&#26041;&#27861;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#39592;&#24178;&#32593;&#32476;&#29305;&#24449;&#30340;&#21306;&#20998;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#24403;&#21069;&#22823;&#22810;&#25968;&#26041;&#27861;&#24378;&#35843;&#29305;&#24449;&#36801;&#31227;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;OCC&#26041;&#27861;&#20013;&#30340;&#20248;&#21270;&#30446;&#26631;&#31354;&#38388;&#20063;&#21487;&#33021;&#26159;&#24433;&#21709;&#24615;&#33021;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;OCC&#30340;&#20248;&#21270;&#30446;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#25512;&#23548;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#27934;&#35265;&#65306;&#22312;&#36866;&#24403;&#30340;&#33539;&#25968;&#31354;&#38388;&#20013;&#65292;&#20219;&#20309;&#31354;&#38388;&#37117;&#21487;&#20197;&#20316;&#20026;&#36229;&#29699;&#24515;&#30340;&#31561;&#25928;&#26367;&#20195;&#65292;&#26080;&#38656;&#20381;&#36182;&#20110;&#35757;&#32451;&#26679;&#26412;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#23450;&#21487;&#34892;&#22495;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is a longstanding method for anomaly detection. With the powerful representation capability of the pre-trained backbone, OCC methods have witnessed significant performance improvements. Typically, most of these OCC methods employ transfer learning to enhance the discriminative nature of the pre-trained backbone's features, thus achieving remarkable efficacy. While most current approaches emphasize feature transfer strategies, we argue that the optimization objective space within OCC methods could also be an underlying critical factor influencing performance. In this work, we conducted a thorough investigation into the optimization objective of OCC. Through rigorous theoretical analysis and derivation, we unveil a key insights: any space with the suitable norm can serve as an equivalent substitute for the hypersphere center, without relying on the distribution assumption of training samples. Further, we provide guidelines for determining the feasible domai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.11891</link><description>&lt;p&gt;
&#32447;&#24615;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20998;&#26512;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Deciphering Tabular Data Using Large Language Model. (arXiv:2308.11891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#34920;&#26684;&#24207;&#21015;&#21270;&#27169;&#22359;&#21644;&#32416;&#27491;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#19968;&#30452;&#26159;&#23398;&#26415;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#38543;&#30528;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#22788;&#29702;&#19982;&#34920;&#26684;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#34920;&#26684;&#32467;&#26500;&#21644;&#20869;&#23481;&#19978;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#23558;&#34920;&#26684;&#24207;&#21015;&#21270;&#30340;&#27169;&#22359;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#32416;&#27491;&#26426;&#21046;&#26469;&#20462;&#27491;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#20173;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of natural language processing, the understanding of tabular data has perpetually stood as a focal point of scholarly inquiry. The emergence of expansive language models, exemplified by the likes of ChatGPT, has ushered in a wave of endeavors wherein researchers aim to harness these models for tasks related to table-based question answering. Central to our investigative pursuits is the elucidation of methodologies that amplify the aptitude of such large language models in discerning both the structural intricacies and inherent content of tables, ultimately facilitating their capacity to provide informed responses to pertinent queries. To this end, we have architected a distinctive module dedicated to the serialization of tables for seamless integration with expansive language models. Additionally, we've instituted a corrective mechanism within the model to rectify potential inaccuracies. Experimental results indicate that, although our proposed method trails the SOTA by ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11884</link><description>&lt;p&gt;
&#23558;Wikidata&#20998;&#31867;&#20307;&#31995;&#38598;&#25104;&#21040;YAGO&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating the Wikidata Taxonomy into YAGO. (arXiv:2308.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#24182;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikidata&#26159;&#26368;&#22823;&#30340;&#20844;&#20849;&#36890;&#29992;&#30693;&#35782;&#24211;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#30340;&#21512;&#20316;&#24615;&#36136;&#65292;&#20854;&#27169;&#24335;&#21644;&#20998;&#31867;&#20307;&#31995;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;YAGO 4&#30693;&#35782;&#24211;&#20013;&#65292;&#25105;&#20204;&#23558;Wikidata&#19982;Schema.org&#30340;&#26412;&#20307;&#35770;&#32467;&#21512;&#36215;&#26469;&#65292;&#20943;&#23569;&#21644;&#28165;&#29702;&#20998;&#31867;&#20307;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#25968;&#25454;&#19978;&#36816;&#34892;&#33258;&#21160;&#25512;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#33293;&#24323;&#20102;&#22823;&#37096;&#20998;&#30340;Wikidata&#20998;&#31867;&#20307;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25972;&#20010;Wikidata&#20998;&#31867;&#20307;&#31995;&#23613;&#21487;&#33021;&#22320;&#21512;&#24182;&#21040;YAGO&#30693;&#35782;&#24211;&#20013;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36923;&#36753;&#32422;&#26463;&#21644;&#31867;&#19982;&#23454;&#20363;&#30340;&#32454;&#33268;&#21306;&#20998;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21019;&#24314;&#20102;YAGO 4.5&#65292;&#20026;YAGO&#28155;&#21152;&#20102;&#20016;&#23500;&#30340;&#20449;&#24687;&#31867;&#21035;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30693;&#35782;&#24211;&#30340;&#36923;&#36753;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wikidata is one of the largest public general-purpose Knowledge Bases (KBs). Yet, due to its collaborative nature, its schema and taxonomy have become convoluted. For the YAGO 4 KB, we combined Wikidata with the ontology from Schema.org, which reduced and cleaned up the taxonomy and constraints and made it possible to run automated reasoners on the data. However, it also cut away large parts of the Wikidata taxonomy. In this paper, we present our effort to merge the entire Wikidata taxonomy into the YAGO KB as much as possible. We pay particular attention to logical constraints and a careful distinction of classes and instances. Our work creates YAGO 4.5, which adds a rich layer of informative classes to YAGO, while at the same time keeping the KB logically consistent.
&lt;/p&gt;</description></item><item><title>Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.11878</link><description>&lt;p&gt;
Cabrita: &#24357;&#21512;&#22806;&#35821;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Cabrita: closing the gap for foreign languages. (arXiv:2308.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11878
&lt;/p&gt;
&lt;p&gt;
Cabrita&#26159;&#19968;&#31181;&#35299;&#20915;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21487;&#25215;&#21463;&#30340;&#25104;&#26412;&#35299;&#20915;&#20102;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#20013;&#26377;&#20004;&#20010;&#37325;&#35201;&#30446;&#30340;&#65306;i)&#22686;&#24378;&#22312;&#29305;&#23450;&#35821;&#35328;&#25110;&#39046;&#22495;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;ii)&#30830;&#20445;&#26377;&#25928;&#30340;&#26631;&#35760;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#30456;&#20851;&#25104;&#26412;&#65292;&#36825;&#20123;&#25104;&#26412;&#21487;&#33021;&#36798;&#21040;&#20845;&#20301;&#25968;&#29978;&#33267;&#19971;&#20301;&#25968;&#30340;&#32654;&#20803;&#37329;&#39069;&#65292;&#36825;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#21644;&#28041;&#21450;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25104;&#26412;&#25361;&#25112;&#65292;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#26159;&#20381;&#36182;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23613;&#31649;&#26368;&#36817;&#20986;&#29616;&#20102;&#20687;LLaMA&#21644;LLaMA-2&#27169;&#22411;&#36825;&#26679;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#26576;&#20123;&#29305;&#23450;&#39046;&#22495;&#38382;&#39064;&#20173;&#28982;&#34920;&#29616;&#20302;&#25928;&#65292;&#25110;&#32773;&#22312;&#28041;&#21450;&#23545;&#35805;&#24335;&#35760;&#24518;&#36164;&#28304;&#30340;&#22330;&#26223;&#20013;&#26080;&#25928;&#65292;&#22240;&#20026;&#34920;&#31034;&#25991;&#26412;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cabrita&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;&#23427;&#25104;&#21151;&#35299;&#20915;&#20102;&#24615;&#33021;&#21644;&#39640;&#25928;&#26631;&#35760;&#21270;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#25104;&#26412;&#21487;&#25215;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.  The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.  To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#32467;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#21019;&#20260;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#24182;&#22312;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#22810;&#20010;&#27169;&#22411;&#20197;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11877</link><description>&lt;p&gt;
&#32508;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20998;&#26512;&#29992;&#20110;&#21019;&#20260;&#20998;&#31867;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrated Image and Location Analysis for Wound Classification: A Deep Learning Approach. (arXiv:2308.11877v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#32467;&#21512;&#22270;&#20687;&#21644;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#21019;&#20260;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#25552;&#20379;&#31934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#24182;&#22312;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#22810;&#20010;&#27169;&#22411;&#20197;&#25552;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#21644;&#24930;&#24615;&#21019;&#20260;&#30340;&#20840;&#29699;&#36127;&#25285;&#20026;&#22686;&#24378;&#21019;&#20260;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#26696;&#20363;&#65292;&#36825;&#26159;&#35786;&#26029;&#21644;&#30830;&#23450;&#26368;&#20339;&#27835;&#30103;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#21019;&#20260;&#20998;&#31867;&#20026;&#22235;&#31867;&#65306;&#31958;&#23615;&#30149;&#24615;&#12289;&#21387;&#21147;&#24615;&#12289;&#22806;&#31185;&#24615;&#21644;&#38745;&#33033;&#28291;&#30113;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20351;&#29992;&#21019;&#20260;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#26356;&#31934;&#30830;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#20010;&#20307;&#37096;&#22270;&#35889;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21019;&#20260;&#20301;&#32622;&#26631;&#35760;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#21019;&#20260;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#26550;&#26500;&#20013;&#25972;&#21512;&#20102;VGG16&#12289;ResNet152&#21644;EfficientNet&#31561;&#27169;&#22411;&#12290;&#35813;&#26550;&#26500;&#21253;&#25324;&#20102;&#31354;&#38388;&#21644;&#36890;&#36947;&#32423;&#30340;Squeeze-and-Excitation&#27169;&#22359;&#12289;Axial Attention&#21644;&#33258;&#36866;&#24212;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#31561;&#20803;&#32032;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The global burden of acute and chronic wounds presents a compelling case for enhancing wound classification methods, a vital step in diagnosing and determining optimal treatments. Recognizing this need, we introduce an innovative multi-modal network based on a deep convolutional neural network for categorizing wounds into four categories: diabetic, pressure, surgical, and venous ulcers. Our multi-modal network uses wound images and their corresponding body locations for more precise classification. A unique aspect of our methodology is incorporating a body map system that facilitates accurate wound location tagging, improving upon traditional wound image classification techniques. A distinctive feature of our approach is the integration of models such as VGG16, ResNet152, and EfficientNet within a novel architecture. This architecture includes elements like spatial and channel-wise Squeeze-and-Excitation modules, Axial Attention, and an Adaptive Gated Multi-Layer Perceptron, providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;ClimateBench&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#22312;&#27169;&#25311;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11854</link><description>&lt;p&gt;
&#23547;&#25214;&#23436;&#32654;&#25311;&#21512;: &#24212;&#29992;&#22238;&#24402;&#27169;&#22411;&#21040;ClimateBench v1.0
&lt;/p&gt;
&lt;p&gt;
Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0. (arXiv:2308.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;ClimateBench&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27668;&#20505;&#27169;&#25311;&#30340;&#33021;&#21147;&#65292;&#37325;&#28857;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#22312;&#27169;&#25311;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#27169;&#25311;&#22120;&#30340;&#27668;&#20505;&#39044;&#27979;&#26159;&#30740;&#31350;&#30340;&#37325;&#28857;&#20043;&#19968;&#65292;&#20197;&#20351;&#20915;&#31574;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;&#22120;&#20316;&#20026;&#35745;&#31639;&#22797;&#26434;&#30340;GCM&#27169;&#25311;&#22120;&#30340;&#26367;&#20195;&#65292;&#21487;&#20197;&#20943;&#23569;&#26102;&#38388;&#21644;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;ClimateBench&#26159;&#19968;&#20010;&#26368;&#36817;&#20026;&#35780;&#20272;&#27668;&#20505;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;&#22120;&#30340;&#24615;&#33021;&#32780;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;&#34987;&#35748;&#20026;&#26159;&#22522;&#30784;&#65292;&#22238;&#24402;&#27169;&#22411;&#22312;&#27668;&#20505;&#27169;&#25311;&#20013;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#21033;&#29992;&#26680;&#25216;&#24039;&#65292;&#22238;&#24402;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#20851;&#31995;&#24182;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#35780;&#20272;&#22312;&#19978;&#36848;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#38750;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#25311;&#33021;&#21147;&#12290;&#20854;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#22120;&#35777;&#26126;&#20102;&#23427;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;</title><link>http://arxiv.org/abs/2308.11849</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data. (arXiv:2308.11849v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31227;&#21160;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#38656;&#27714;&#21709;&#24212;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36710;&#31449;&#25317;&#25380;&#30340;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#23454;&#29616;&#23454;&#26102;&#35843;&#24230;&#65292;&#24182;&#30528;&#37325;&#32771;&#34385;&#39640;&#38656;&#27714;&#36710;&#31449;&#30340;&#37325;&#26032;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38081;&#36335;&#37325;&#26032;&#35843;&#24230;&#26159;&#19968;&#31181;&#21450;&#26102;&#28789;&#27963;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#26681;&#25454;&#26102;&#21464;&#26465;&#20214;&#33258;&#21160;&#25913;&#21464;&#36816;&#33829;&#35745;&#21010;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#38081;&#36335;&#20013;&#20056;&#23458;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#27969;&#21160;&#24615;&#65292;&#20027;&#35201;&#20381;&#36182;&#22522;&#20110;OD&#30340;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21015;&#36710;&#30340;&#38656;&#27714;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#38271;&#26399;&#32039;&#24613;&#24773;&#20917;&#19979;&#30340;&#35843;&#24230;&#26356;&#26032;&#21407;&#21017;&#24573;&#35270;&#20102;&#38656;&#27714;&#22312;&#26102;&#38388;&#19978;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38656;&#27714;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31227;&#21160;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#30495;&#23454;&#19990;&#30028;&#30340;&#20056;&#23458;&#27969;&#21160;&#24615;&#26469;&#20419;&#36827;&#23454;&#26102;&#35843;&#24230;&#12290;&#19982;&#32593;&#32476;&#23618;&#38754;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#30528;&#37325;&#20851;&#27880;&#32039;&#24613;&#21306;&#22495;&#19978;&#28216;&#30340;&#39640;&#38656;&#27714;&#36710;&#31449;&#12290;&#30446;&#26631;&#26159;&#37325;&#26032;&#23433;&#25490;&#36890;&#36807;&#35813;&#30446;&#26631;&#31449;&#30340;&#22810;&#26465;&#32447;&#36335;&#19978;&#21463;&#21040;&#20005;&#37325;&#31361;&#21457;&#20107;&#20214;&#65288;&#22914;&#33258;&#28982;&#28798;&#23475;&#65289;&#24433;&#21709;&#30340;&#25152;&#26377;&#21015;&#36710;&#12290;&#38656;&#35201;&#29305;&#21035;&#27880;&#24847;&#36991;&#20813;&#31215;&#32047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.11842</link><description>&lt;p&gt;
&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2308.11842v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;${\rm E}(3)$&#31561;&#21464;&#24615;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#30028;&#20013;&#35782;&#21035;&#21644;&#20998;&#26512;&#23545;&#31216;&#27169;&#24335;&#24050;&#32463;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#29289;&#29702;&#23398;&#20013;&#30340;&#24341;&#21147;&#23450;&#24459;&#30340;&#21046;&#23450;&#21644;&#21270;&#23398;&#32467;&#26500;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#22312;&#26576;&#20123;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#20013;&#22266;&#26377;&#30340;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#24615;&#65292;&#20197;&#21450;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24418;&#24335;&#21270;&#22320;&#25551;&#36848;&#19968;&#31867;&#20855;&#26377;&#19968;&#33324;&#23545;&#31216;&#24615;&#27010;&#24565;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#35813;&#27010;&#24565;&#20801;&#35768;&#23384;&#22312;&#23545;&#31216;&#30340;&#26368;&#20248;&#20540;&#21644;&#31574;&#30053;&#12290;&#21463;&#21040;&#36825;&#20123;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20855;&#26377;&#23545;&#31216;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20316;&#20026;&#22810;&#26234;&#20307;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#22312;&#21508;&#31181;&#21327;&#20316;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#22312;&#20855;&#26377;&#37325;&#22797;&#23545;&#31216;&#27169;&#24335;&#30340;&#26410;&#35265;&#22330;&#26223;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#31561;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11838</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Study on Calibration. (arXiv:2308.11838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11838
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31354;&#38388;&#25506;&#32034;&#20102;&#27169;&#22411;&#26657;&#20934;&#23646;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#27169;&#22411;&#26657;&#20934;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65292;&#24182;&#21487;&#20197;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#26657;&#20934;&#38382;&#39064;&#65292;&#23613;&#31649;&#39044;&#27979;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#20351;&#29992;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#26694;&#26550;&#26469;&#25913;&#21892;&#26657;&#20934;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#26657;&#20934;&#23646;&#24615;&#30340;&#30740;&#31350;&#26377;&#28857;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#20840;&#38754;&#25506;&#32034;&#26657;&#20934;&#23646;&#24615;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#23613;&#30340;&#27169;&#22411;&#26550;&#26500;&#31354;&#38388;&#12290;&#25105;&#20204;&#29305;&#21035;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26657;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;NATS-Bench&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#20102;90&#20010;&#22522;&#20110;&#21306;&#38388;&#30340;&#26657;&#20934;&#24230;&#37327;&#21644;12&#20010;&#20854;&#20182;&#26657;&#20934;&#24230;&#37327;&#65292;&#28085;&#30422;&#20102;117,702&#20010;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#22238;&#31572;&#35813;&#39046;&#22495;&#19968;&#20123;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27169;&#22411;&#26657;&#20934;&#33021;&#21542;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#27867;&#21270;&#65311;&#65288;ii&#65289;&#33021;&#21542;&#21516;&#26102;&#20860;&#39038;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can rob
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#24179;&#22343;&#26041;&#27861;&#26469;&#30740;&#31350;&#22260;&#20135;&#26399;&#20154;&#31867;&#22823;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.11836</link><description>&lt;p&gt;
&#25551;&#36848;&#20154;&#31867;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#22260;&#20135;&#26399;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Characterizing normal perinatal development of the human brain structural connectivity. (arXiv:2308.11836v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#24179;&#22343;&#26041;&#27861;&#26469;&#30740;&#31350;&#22260;&#20135;&#26399;&#20154;&#31867;&#22823;&#33041;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#22823;&#33041;&#21457;&#32946;&#29305;&#28857;&#26159;&#39640;&#24230;&#26377;&#24207;&#30340;&#32467;&#26500;&#36830;&#25509;&#24418;&#25104;&#12290;&#36825;&#31181;&#36830;&#25509;&#30340;&#20114;&#30456;&#20851;&#24615;&#26159;&#22823;&#33041;&#35748;&#30693;&#33021;&#21147;&#30340;&#22522;&#30784;&#65292;&#24433;&#21709;&#20854;&#23545;&#30142;&#30149;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#22312;&#22260;&#20135;&#26399;&#38454;&#27573;&#37327;&#21270;&#35780;&#20272;&#32467;&#26500;&#36830;&#25509;&#23545;&#30740;&#31350;&#27491;&#24120;&#21644;&#24322;&#24120;&#31070;&#32463;&#21457;&#32946;&#24456;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#20013;&#20272;&#35745;&#36830;&#25509;&#32452;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#12290;&#23545;&#20110;&#22260;&#20135;&#26399;&#65292;&#36825;&#20123;&#35745;&#31639;&#38754;&#20020;&#24555;&#36895;&#33041;&#21457;&#23637;&#21644;&#25104;&#20687;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#21152;&#19978;&#39640;&#20010;&#20307;&#38388;&#21464;&#24322;&#24615;&#65292;&#36825;&#20123;&#22240;&#32032;&#20351;&#24471;&#25551;&#36848;&#32467;&#26500;&#36830;&#25509;&#30340;&#27491;&#24120;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#32570;&#20047;&#22312;&#36825;&#19968;&#20851;&#38190;&#33041;&#21457;&#32946;&#38454;&#27573;&#30340;&#32467;&#26500;&#36830;&#25509;&#24230;&#37327;&#30340;&#21487;&#38752;&#22522;&#32447;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#31354;&#24179;&#22343;&#30340;&#35745;&#31639;&#26694;&#26550;&#26469;&#30830;&#23450;&#32467;&#26500;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determi
&lt;/p&gt;</description></item><item><title>&#31639;&#27861;&#21457;&#29616;&#20102;&#22823;&#37327;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#65292;&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#24182;&#23548;&#33268;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.11829</link><description>&lt;p&gt;
&#31639;&#27861;&#36741;&#21161;&#19979;&#23545;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#20869;&#22312;&#39034;&#24207;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Algorithm-assisted discovery of an intrinsic order among mathematical constants. (arXiv:2308.11829v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11829
&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#21457;&#29616;&#20102;&#22823;&#37327;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#25581;&#31034;&#20102;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#65292;&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#24182;&#23548;&#33268;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31639;&#27861;&#23545;&#25968;&#23398;&#39046;&#22495;&#30340;&#21457;&#29616;&#21457;&#25381;&#20102;&#36234;&#26469;&#36234;&#22823;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25506;&#32034;&#22823;&#21442;&#25968;&#31354;&#38388;&#26041;&#38754;&#65292;&#20154;&#31867;&#21487;&#33021;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#25165;&#33021;&#23436;&#25104;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#21644;&#31639;&#27861;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20154;&#31867;&#30452;&#35273;&#19982;&#35745;&#31639;&#26426;&#31639;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#20250;&#23548;&#33268;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#30340;&#21457;&#29616;&#65292;&#21542;&#21017;&#36825;&#20123;&#27010;&#24565;&#23558;&#38590;&#20197;&#23547;&#25214;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;&#35745;&#31639;&#26426;&#31639;&#27861;&#65292;&#21457;&#29616;&#20102;&#22823;&#37327;&#30340;&#36830;&#20998;&#25968;&#20844;&#24335;&#65292;&#29992;&#20110;&#22522;&#26412;&#30340;&#25968;&#23398;&#24120;&#25968;&#12290;&#31639;&#27861;&#21457;&#29616;&#30340;&#20844;&#24335;&#25968;&#37327;&#20043;&#22810;&#25581;&#31034;&#20102;&#19968;&#20010;&#31216;&#20026;&#20445;&#23432;&#30697;&#38453;&#22330;&#30340;&#26032;&#39062;&#25968;&#23398;&#32467;&#26500;&#12290;&#36825;&#20123;&#30697;&#38453;&#22330;(1)&#32479;&#19968;&#20102;&#25968;&#21315;&#20010;&#24050;&#30693;&#30340;&#20844;&#24335;&#65292;(2)&#29983;&#25104;&#20102;&#26080;&#38480;&#22810;&#30340;&#26032;&#20844;&#24335;&#65292;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;(3)&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#23398;&#24120;&#25968;&#20043;&#38388;&#24847;&#24819;&#19981;&#21040;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In recent decades, a growing number of discoveries in fields of mathematics have been assisted by computer algorithms, primarily for exploring large parameter spaces that humans would take too long to investigate. As computers and algorithms become more powerful, an intriguing possibility arises - the interplay between human intuition and computer algorithms can lead to discoveries of novel mathematical concepts that would otherwise remain elusive. To realize this perspective, we have developed a massively parallel computer algorithm that discovers an unprecedented number of continued fraction formulas for fundamental mathematical constants. The sheer number of formulas discovered by the algorithm unveils a novel mathematical structure that we call the conservative matrix field. Such matrix fields (1) unify thousands of existing formulas, (2) generate infinitely many new formulas, and most importantly, (3) lead to unexpected relations between different mathematical constants, including
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11827</link><description>&lt;p&gt;
&#25506;&#32034;GPT&#27169;&#22411;&#22312;&#32771;&#35797;&#20013;&#30340;&#25928;&#26524;&#65306;&#39550;&#39542;&#25191;&#29031;&#30693;&#35782;&#27979;&#35797;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver's License Knowledge Test. (arXiv:2308.11827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#65292;&#35753;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#32771;&#35797;&#39064;&#30446;&#12290;&#22312;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#30340;&#27979;&#35797;&#20013;&#65292;GPT-3&#27169;&#22411;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;Open AI&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25797;&#38271;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#20854;&#30693;&#35782;&#20165;&#38480;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#38480;&#21046;&#20351;&#24471;&#24403;&#38754;&#20020;&#26377;&#20851;&#26368;&#26032;&#21457;&#23637;&#25110;&#38750;&#20844;&#24320;&#25991;&#20214;&#30340;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26080;&#25928;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20043;&#21069;&#26410;&#21253;&#21547;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20449;&#24687;&#28304;&#30340;&#19978;&#19979;&#25991;&#26469;&#20351;GPT&#27169;&#22411;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#39044;&#22788;&#29702;&#12289;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#30340;&#23884;&#20837;&#12289;&#36890;&#36807;&#25972;&#21512;&#19978;&#19979;&#25991;&#23884;&#20837;&#26500;&#24314;&#25552;&#31034;&#20197;&#21450;&#20351;&#29992;GPT&#27169;&#22411;&#29983;&#25104;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#21463;&#25511;&#27979;&#35797;&#22330;&#26223;&#65292;&#20351;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#39550;&#39542;&#25163;&#20876;&#20316;&#20026;&#20449;&#24687;&#28304;&#12290;GPT-3&#27169;&#22411;&#22312;&#19968;&#22871;50&#36947;&#26679;&#26412;&#39550;&#39542;&#30693;&#35782;&#27979;&#35797;&#39064;&#19978;&#21462;&#24471;&#20102;96%&#30340;&#21450;&#26684;&#20998;&#25968;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#21450;&#26684;&#20998;&#25968;&#19979;&#38477;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.11809</link><description>&lt;p&gt;
&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#24615;&#27010;&#29575;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Expressive probabilistic sampling in recurrent neural networks. (arXiv:2308.11809v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11809
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#22914;&#20309;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#24102;&#26377;&#21333;&#29420;&#36755;&#20986;&#21333;&#20803;&#30340;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#37319;&#26679;&#30340;&#22823;&#33041;&#21151;&#33021;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#20551;&#35774;&#31070;&#32463;&#27963;&#21160;&#26159;&#26469;&#33258;&#22823;&#33041;&#29992;&#20110;&#27010;&#29575;&#35745;&#31639;&#30340;&#27010;&#29575;&#20998;&#24067;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31070;&#32463;&#21160;&#21147;&#23398;&#26426;&#21046;&#27169;&#22411;&#22914;&#20309;&#20174;&#20219;&#24847;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#20989;&#25968;&#20998;&#26512;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24037;&#20855;&#26469;&#25506;&#32034;$\textit{&#24490;&#29615;}$&#31070;&#32463;&#30005;&#36335;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#26368;&#23567;&#26550;&#26500;&#35201;&#27714;&#12290;&#39318;&#20808;&#25105;&#20204;&#32771;&#34385;&#20256;&#32479;&#30340;&#37319;&#26679;&#27169;&#22411;&#65292;&#23427;&#30001;&#19968;&#20010;&#31070;&#32463;&#20803;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#36755;&#20986;&#30452;&#25509;&#34920;&#31034;&#26679;&#26412;&#65288;&#20165;&#37319;&#26679;&#22120;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#35748;&#20026;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#31361;&#35302;&#30005;&#27969;&#21644;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#33021;&#22815;&#20174;&#22797;&#26434;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#29420;&#30340;&#36755;&#20986;&#21333;&#20803;&#38598;&#30340;&#24490;&#29615;&#31070;&#32463;&#30005;&#36335;&#30340;&#21457;&#25918;&#29575;&#21160;&#21147;&#23398;&#21487;&#20197;&#20174;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In sampling-based Bayesian models of brain function, neural activities are assumed to be samples from probability distributions that the brain uses for probabilistic computation. However, a comprehensive understanding of how mechanistic models of neural dynamics can sample from arbitrary distributions is still lacking. We use tools from functional analysis and stochastic differential equations to explore the minimum architectural requirements for $\textit{recurrent}$ neural circuits to sample from complex distributions. We first consider the traditional sampling model consisting of a network of neurons whose outputs directly represent the samples (sampler-only network). We argue that synaptic current and firing-rate dynamics in the traditional model have limited capacity to sample from a complex probability distribution. We show that the firing rate dynamics of a recurrent neural circuit with a separate set of output units can sample from an arbitrary probability distribution. We call 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11776</link><description>&lt;p&gt;
WS-SfMLearner: &#33258;&#21161;&#30417;&#30563;&#24335;&#22312;&#26410;&#30693;&#30456;&#26426;&#21442;&#25968;&#24773;&#20917;&#19979;&#36827;&#34892;&#25163;&#26415;&#35270;&#39057;&#30340;&#21333;&#30446;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters. (arXiv:2308.11776v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#25163;&#26415;&#35270;&#39057;&#20013;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#35270;&#39057;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#22312;&#35768;&#22810;&#22270;&#20687;&#24341;&#23548;&#25163;&#26415;&#31243;&#24207;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#26415;&#22330;&#26223;&#20013;&#30340;&#20142;&#24230;&#21644;&#22122;&#22768;&#19981;&#19968;&#33268;&#65292;&#21019;&#24314;&#28145;&#24230;&#22320;&#22270;&#30495;&#23454;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#19968;&#20010;&#20934;&#30830;&#19988;&#40065;&#26834;&#30340;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#30456;&#26426;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#24341;&#36215;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#30340;&#26356;&#22810;&#20851;&#27880;&#12290;&#23613;&#31649;&#19968;&#20123;&#33258;&#21161;&#30417;&#30563;&#26041;&#27861;&#20943;&#36731;&#20102;&#23545;&#30495;&#23454;&#28145;&#24230;&#22320;&#22270;&#21644;&#20301;&#23039;&#30340;&#38656;&#27714;&#65292;&#20294;&#23427;&#20204;&#20173;&#38656;&#35201;&#24050;&#30693;&#30340;&#30456;&#26426;&#20869;&#21442;&#25968;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#36890;&#24120;&#32570;&#22833;&#25110;&#26410;&#35760;&#24405;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#20013;&#30456;&#26426;&#20869;&#21442;&#25968;&#39044;&#27979;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33258;&#21161;&#30417;&#30563;&#28145;&#24230;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31995;&#32479;&#65292;&#21487;&#20197;&#39044;&#27979;&#31934;&#30830;&#30340;&#28145;&#24230;&#22320;&#22270;&#12289;&#30456;&#26426;&#20301;&#23039;&#21644;&#30456;&#26426;&#20869;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25104;&#26412;&#20307;&#31215;&#30340;&#30417;&#30563;&#26041;&#24335;&#26469;&#25552;&#20379;&#31995;&#32479;&#30340;&#33258;&#25105;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the sy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;3ET&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#65292;&#22312;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#21644;&#20934;&#30830;&#30340;&#30643;&#23380;&#36861;&#36394;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2308.11771</link><description>&lt;p&gt;
3ET: &#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network. (arXiv:2308.11771v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;3ET&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#21464;&#21270;&#30340;ConvLSTM&#32593;&#32476;&#30340;&#39640;&#25928;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#65292;&#22312;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26102;&#31354;&#29305;&#24449;&#25552;&#21462;&#21644;&#20934;&#30830;&#30340;&#30643;&#23380;&#36861;&#36394;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#30340;&#22522;&#20110;&#21464;&#21270;&#30340;Convolutional Long Short-Term Memory (CB-ConvLSTM) &#27169;&#22411;&#65292;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#30524;&#29699;&#36861;&#36394;&#65292;&#36825;&#26159;&#19979;&#19968;&#20195;&#21487;&#31359;&#25140;&#21307;&#30103;&#25216;&#26415;&#65288;&#22914;AR/VR&#22836;&#30420;&#65289;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#32593;&#33180;&#28789;&#24863;&#30340;&#20107;&#20214;&#30456;&#26426;&#30340;&#20302;&#24310;&#36831;&#21709;&#24212;&#21644;&#31232;&#30095;&#36755;&#20986;&#20107;&#20214;&#27969;&#30340;&#20248;&#21183;&#65292;&#36229;&#36807;&#20256;&#32479;&#30340;&#22522;&#20110;&#24103;&#30340;&#30456;&#26426;&#12290;&#25105;&#20204;&#30340;CB-ConvLSTM&#26550;&#26500;&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#20107;&#20214;&#27969;&#20013;&#25552;&#21462;&#26102;&#31354;&#29305;&#24449;&#65292;&#29992;&#20110;&#30643;&#23380;&#36861;&#36394;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;CNN&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#22686;&#24378;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#22686;&#37327;&#32534;&#30721;&#24490;&#29615;&#36335;&#24452;&#65292;CB-ConvLSTM&#22312;&#32463;&#36807;&#26631;&#35760;&#30340;&#30643;&#23380;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#20013;&#65292;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#31639;&#26415;&#25805;&#20316;&#20943;&#23569;&#20102;&#32422;4.7&#20493;&#12290;&#36825;&#31181;&#25552;&#39640;&#25928;&#29575;&#30340;&#22686;&#21152;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#30524;&#29699;&#36861;&#36394;&#12290;&#39033;&#30446;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;\url{https://github.com/qinche106/cb-convlstm-eyetracking}&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>KnowledGPT&#26159;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KnowledGPT&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11761</link><description>&lt;p&gt;
KnowledGPT&#65306;&#21033;&#29992;&#26816;&#32034;&#21644;&#23384;&#20648;&#35775;&#38382;&#30693;&#35782;&#24211;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. (arXiv:2308.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11761
&lt;/p&gt;
&lt;p&gt;
KnowledGPT&#26159;&#19968;&#20010;&#20840;&#38754;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#24211;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;KnowledGPT&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#22320;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24433;&#21709;&#21147;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#23436;&#25972;&#24615;&#12289;&#21450;&#26102;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#23558;LLMs&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#36830;&#25509;&#36215;&#26469;&#65292;&#20294;&#26159;&#30693;&#35782;&#24211;(KBs)&#30340;&#25972;&#21512;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#19988;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowledGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;LLMs&#19982;&#21508;&#31181;&#30693;&#35782;&#24211;&#36830;&#25509;&#36215;&#26469;&#65292;&#26041;&#20415;&#30693;&#35782;&#30340;&#26816;&#32034;&#21644;&#23384;&#20648;&#12290;&#26816;&#32034;&#36807;&#31243;&#37319;&#29992;&#24605;&#32500;&#21551;&#21457;&#31243;&#24207;&#65292;&#29983;&#25104;&#29992;&#20110;KB&#30340;&#25628;&#32034;&#35821;&#35328;&#30340;&#20195;&#30721;&#65292;&#20854;&#20013;&#39044;&#23450;&#20041;&#20102;KB&#25805;&#20316;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#26816;&#32034;&#22806;&#65292;KnowledGPT&#36824;&#25552;&#20379;&#20102;&#23558;&#30693;&#35782;&#23384;&#20648;&#22312;&#20010;&#24615;&#21270;&#30693;&#35782;&#24211;&#20013;&#30340;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#20010;&#20307;&#29992;&#25143;&#38656;&#27714;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#23558;LLMs&#19982;KBs&#38598;&#25104;&#65292;KnowledGPT&#33021;&#22815;&#27491;&#30830;&#22238;&#31572;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions 
&lt;/p&gt;</description></item><item><title>VBMO&#26159;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#26368;&#20248;&#35268;&#21010;&#24182;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#36873;&#25321;&#26368;&#20339;&#35268;&#21010;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2308.11755</link><description>&lt;p&gt;
VBMO: &#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
VBMO: Voting-Based Multi-Objective Path Planning. (arXiv:2308.11755v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11755
&lt;/p&gt;
&lt;p&gt;
VBMO&#26159;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#26368;&#20248;&#35268;&#21010;&#24182;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#36873;&#25321;&#26368;&#20339;&#35268;&#21010;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VBMO&#65292;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#30340;&#22810;&#30446;&#26631;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#29983;&#25104;&#26368;&#20248;&#21333;&#30446;&#26631;&#35268;&#21010;&#65292;&#36890;&#36807;&#35780;&#20272;&#27599;&#20010;&#35268;&#21010;&#30456;&#23545;&#20110;&#20854;&#20182;&#30446;&#26631;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#36873;&#25321;&#26368;&#20339;&#35268;&#21010;&#12290;VBMO&#19981;&#20351;&#29992;&#25163;&#21160;&#35843;&#33410;&#30340;&#26435;&#37325;&#65292;&#22312;&#25628;&#32034;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#32771;&#34385;&#22810;&#20010;&#30446;&#26631;&#65292;&#20063;&#19981;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#12290;&#30456;&#21453;&#65292;&#23427;&#32771;&#34385;&#21040;&#22312;&#19968;&#20010;&#30446;&#26631;&#19978;&#26368;&#20248;&#30340;&#35268;&#21010;&#21487;&#33021;&#22312;&#20854;&#20182;&#30446;&#26631;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;VBMO&#20351;&#29992;&#19977;&#31181;&#25237;&#31080;&#26426;&#21046;&#65306;&#33539;&#22260;&#25237;&#31080;&#12289;Borda&#25237;&#31080;&#21644;&#32508;&#21512;&#36190;&#25104;&#25237;&#31080;&#12290;&#23545;&#20110;&#22810;&#26679;&#21644;&#22797;&#26434;&#29615;&#22659;&#30340;&#24191;&#27867;&#35780;&#20272;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#29983;&#25104;&#28385;&#36275;&#22810;&#20010;&#30446;&#26631;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents VBMO, the Voting-Based Multi-Objective path planning algorithm, that generates optimal single-objective plans, evaluates each of them with respect to the other objectives, and selects one with a voting mechanism. VBMO does not use hand-tuned weights, consider the multiple objectives at every step of search, or use an evolutionary algorithm. Instead, it considers how a plan that is optimal in one objective may perform well with respect to others. VBMO incorporates three voting mechanisms: range, Borda, and combined approval. Extensive evaluation in diverse and complex environments demonstrates the algorithm's ability to efficiently produce plans that satisfy multiple objectives.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24694;&#24847;&#22495;&#21517;&#26816;&#27979;&#20013;&#23384;&#22312;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#21363;&#22810;&#23454;&#20363;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#25915;&#20987;&#32773;&#21516;&#26102;&#25805;&#32437;&#24694;&#24847;&#22270;&#20013;&#30340;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11754</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24694;&#24847;&#22495;&#21517;&#26816;&#27979;&#20013;&#30340;&#22810;&#23454;&#20363;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Multi-Instance Adversarial Attack on GNN-Based Malicious Domain Detection. (arXiv:2308.11754v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11754
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24694;&#24847;&#22495;&#21517;&#26816;&#27979;&#20013;&#23384;&#22312;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#21363;&#22810;&#23454;&#20363;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#25915;&#20987;&#32773;&#21516;&#26102;&#25805;&#32437;&#24694;&#24847;&#22270;&#20013;&#30340;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#22495;&#21517;&#26816;&#27979;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#26088;&#22312;&#26816;&#27979;&#20114;&#32852;&#32593;&#22495;&#21517;&#26159;&#21542;&#19982;&#32593;&#32476;&#25915;&#20987;&#30456;&#20851;&#32852;&#12290;&#22312;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#35748;&#20026;&#38750;&#24120;&#26377;&#25928;&#12290;&#22522;&#20110;GNN&#30340;&#24694;&#24847;&#22495;&#21517;&#26816;&#27979;&#20351;&#29992;DNS&#26085;&#24535;&#23558;&#20114;&#32852;&#32593;&#22495;&#21517;&#34920;&#31034;&#20026;&#24694;&#24847;&#24615;&#22270;&#65288;DMG&#65289;&#20013;&#30340;&#33410;&#28857;&#65292;&#24182;&#35757;&#32451;GNN&#36890;&#36807;&#21033;&#29992;&#24050;&#32463;&#35782;&#21035;&#30340;&#24694;&#24847;&#22495;&#21517;&#25512;&#26029;&#20854;&#24694;&#24847;&#24615;&#12290;&#30001;&#20110;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#21487;&#35775;&#38382;&#30340;DNS&#26085;&#24535;&#26469;&#26500;&#24314;DMGs&#65292;&#22240;&#27492;&#23427;&#20026;&#25915;&#20987;&#32773;&#25805;&#32437;&#20854;&#22495;&#21517;&#33410;&#28857;&#22312;DMGs&#20013;&#30340;&#29305;&#24449;&#21644;&#36830;&#25509;&#26292;&#38706;&#20102;&#19968;&#20010;&#28431;&#27934;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25805;&#32437;&#21333;&#20010;&#25915;&#20987;&#32773;&#33410;&#28857;&#30340;&#23041;&#32961;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#36890;&#24120;&#29983;&#25104;&#22810;&#20010;&#22495;&#21517;&#20197;&#32463;&#27982;&#26041;&#24335;&#23454;&#29616;&#20854;&#30446;&#26631;&#24182;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23613;&#21487;&#33021;&#22810;&#30340;&#22495;&#21517;&#20013;&#36867;&#36991;&#21457;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31216;&#21516;&#26102;&#25805;&#32437;DMG&#20013;&#30340;&#22810;&#20010;&#33410;&#28857;&#30340;&#25915;&#20987;&#20026;&#22810;&#23454;&#20363;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malicious domain detection (MDD) is an open security challenge that aims to detect if an Internet domain is associated with cyber-attacks. Among many approaches to this problem, graph neural networks (GNNs) are deemed highly effective. GNN-based MDD uses DNS logs to represent Internet domains as nodes in a maliciousness graph (DMG) and trains a GNN to infer their maliciousness by leveraging identified malicious domains. Since this method relies on accessible DNS logs to construct DMGs, it exposes a vulnerability for adversaries to manipulate their domain nodes' features and connections within DMGs. Existing research mainly concentrates on threat models that manipulate individual attacker nodes. However, adversaries commonly generate multiple domains to achieve their goals economically and avoid detection. Their objective is to evade discovery across as many domains as feasible. In this work, we call the attack that manipulates several nodes in the DMG concurrently a multi-instance evas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11748</link><description>&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#20020;&#24202;&#21644;&#25968;&#23383;&#25968;&#25454;&#36827;&#34892;&#24739;&#32773;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Patient Clustering via Integrated Profiling of Clinical and Digital Data. (arXiv:2308.11748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#65292;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#20010;&#20154;&#37197;&#32622;&#30340;&#24739;&#32773;&#32858;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#35774;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32422;&#26463;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#24739;&#32773;&#30340;&#20020;&#24202;&#25968;&#25454;&#21644;&#25968;&#23383;&#20132;&#20114;&#25968;&#25454;&#65288;&#21253;&#25324;&#27983;&#35272;&#21644;&#25628;&#32034;&#65289;&#26469;&#26500;&#24314;&#24739;&#32773;&#37197;&#32622;&#12290;&#20316;&#20026;&#35813;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#29983;&#25104;&#20102;&#38750;&#36127;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#20316;&#20026;&#24739;&#32773;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21307;&#30103;&#20445;&#20581;&#32593;&#31449;&#30340;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#37319;&#29992;&#20102;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#32858;&#31867;&#21644;&#25512;&#33616;&#33021;&#21147;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#19968;&#33268;&#24615;&#21644;&#25512;&#33616;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11738</link><description>&lt;p&gt;
&#36229;&#20986;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Lifted Inference beyond First-Order Logic. (arXiv:2308.11738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#36229;&#36234;&#19968;&#38454;&#36923;&#36753;&#30340;&#25552;&#21319;&#25512;&#29702;&#38382;&#39064;&#65292;&#25193;&#23637;&#20102;&#35745;&#25968;&#37327;&#35789;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#24182;&#22312;&#38480;&#23450;&#20102;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21152;&#26435;&#19968;&#38454;&#27169;&#22411;&#35745;&#25968;(WFOMC)&#26159;&#27010;&#29575;&#25512;&#29702;&#30340;&#22522;&#30784;&#12290;&#30001;&#20110;WFOMC&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65288;$\#$P&#23436;&#20840;&#65289;&#65292;&#22240;&#27492;&#33021;&#22815;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36827;&#34892;WFOMC&#30340;&#36923;&#36753;&#30862;&#29255;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;&#36825;&#26679;&#30340;&#30862;&#29255;&#34987;&#31216;&#20026;&#22495;&#21487;&#25552;&#21319;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35745;&#25968;&#37327;&#35789;&#65288;$\mathrm{C^2}$&#65289;&#25193;&#23637;&#30340;&#20004;&#20010;&#21464;&#37327;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#20013;&#65292;&#21487;&#20197;&#36827;&#34892;&#22495;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#38750;&#24490;&#29615;&#24615;&#21644;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#36830;&#36890;&#24615;&#65292;&#19981;&#33021;&#22312;$\mathrm{C^2}$&#25110;&#19968;&#38454;&#36923;&#36753;&#20013;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;$\mathrm{C^2}$&#30340;&#22495;&#21487;&#25552;&#21319;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#36825;&#26679;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23558;$\mathrm{C^2}$&#21477;&#23376;&#30340;&#19968;&#20010;&#20851;&#31995;&#38480;&#23450;&#20026;&#34920;&#31034;&#26377;&#21521;&#26080;&#29615;&#22270;&#12289;&#36830;&#36890;&#22270;&#12289;&#26641;&#65288;&#25110;&#26377;&#21521;&#26641;&#65289;&#25110;&#26862;&#26519;&#65288;&#25110;&#26377;&#21521;&#26862;&#26519;&#65289;&#26102;&#65292;&#23427;&#20173;&#28982;&#20445;&#25345;&#20102;&#22495;&#21487;&#25552;&#21319;&#24615;&#12290;&#25152;&#26377;&#25105;&#20204;&#30340;&#32467;&#26524;&#37117;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\mathrm{C^2}$ with multiple such properties. We show that any $\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results r
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.11730</link><description>&lt;p&gt;
&#22810;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#20010;&#25991;&#26723;&#19978;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#39044;&#35757;&#32451;&#12289;&#25552;&#31034;&#12289;&#39044;&#27979;&#8221;&#33539;&#24335;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;OD-QA&#65289;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MD-QA&#65289;&#22330;&#26223;&#19979;&#25506;&#32034;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#35201;&#27714;&#23545;&#19981;&#21516;&#25991;&#26723;&#30340;&#20869;&#23481;&#21644;&#32467;&#26500;&#20043;&#38388;&#30340;&#36923;&#36753;&#20851;&#32852;&#26377;&#28145;&#20837;&#29702;&#35299;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;&#65288;KGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;MD-QA&#20013;&#20026;LLMs&#25552;&#31034;&#27491;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#22270;&#26500;&#24314;&#27169;&#22359;&#21644;&#22270;&#36941;&#21382;&#27169;&#22359;&#12290;&#23545;&#20110;&#22270;&#26500;&#24314;&#65292;&#25105;&#20204;&#20351;&#29992;&#33410;&#28857;&#26469;&#34920;&#31034;&#25991;&#27573;&#25110;&#25991;&#26723;&#32467;&#26500;&#65288;&#20363;&#22914;&#65292;&#39029;&#38754;/&#34920;&#26684;&#65289;&#65292;&#32780;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#25991;&#27573;&#20043;&#38388;&#30340;&#35821;&#20041;/&#35789;&#27719;&#30456;&#20284;&#24615;&#25110;&#32773;&#25991;&#26723;&#20869;&#30340;&#32467;&#26500;&#20851;&#31995;&#12290;&#23545;&#20110;&#22270;&#36941;&#21382;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LM&#30340;&#22270;&#36941;&#21382;&#22120;&#65292;&#23427;&#22312;&#33410;&#28857;&#20043;&#38388;&#23548;&#33322;&#24182;&#25910;&#38598;&#25903;&#25345;&#24615;&#30340;&#25991;&#27573;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;MD-QA&#20013;&#36827;&#34892;&#31572;&#26696;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11720</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#38598;&#21512;&#25193;&#23637;&#30340;&#31034;&#20363;&#36827;&#34892;&#35821;&#35328;&#25506;&#27979;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Advancing Relation Extraction through Language Probing with Exemplars from Set Co-Expansion. (arXiv:2308.11720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#21512;&#25193;&#23637;&#21644;&#20195;&#34920;&#24615;&#31034;&#20363;&#30340;&#35821;&#35328;&#25506;&#27979;&#26041;&#27861;&#26469;&#25512;&#36827;&#20851;&#31995;&#25552;&#21462;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#31867;&#21035;&#25490;&#24207;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#32463;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20195;&#34920;&#24615;&#31034;&#20363;&#21644;&#38598;&#21512;&#25193;&#23637;&#26469;&#25552;&#39640;&#20851;&#31995;&#20998;&#31867;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#23545;&#27604;&#31867;&#20043;&#38388;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36890;&#36807;&#20195;&#34920;&#24615;&#31034;&#20363;&#20026;&#27599;&#20010;&#20851;&#31995;&#31867;&#25552;&#20379;&#31181;&#23376;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#31639;&#27861;&#36890;&#36807;&#23558;&#30446;&#26631;&#23545;&#21644;&#30446;&#26631;&#31867;&#30340;&#20195;&#34920;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#32435;&#20837;&#35757;&#32451;&#30446;&#26631;&#26469;&#20016;&#23500;&#35757;&#32451;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#38598;&#21512;&#25193;&#23637;&#36807;&#31243;&#36824;&#28041;&#21450;&#19968;&#20010;&#32771;&#34385;&#23545;&#27604;&#31867;&#31034;&#20363;&#30340;&#31867;&#21035;&#25490;&#24207;&#36807;&#31243;&#12290;&#21033;&#29992;&#26080;&#19978;&#19979;&#25991;&#30340;Hearst&#27169;&#24335;&#21033;&#29992;&#20851;&#31995;&#25552;&#21450;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#26469;&#30830;&#23450;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#12290;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#38598;&#21512;&#25193;&#23637;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a pivotal task in automatically extracting structured information from unstructured text. In this paper, we present a multi-faceted approach that integrates representative examples and through co-set expansion. The primary goal of our method is to enhance relation classification accuracy and mitigating confusion between contrastive classes.  Our approach begins by seeding each relationship class with representative examples. Subsequently, our co-set expansion algorithm enriches training objectives by incorporating similarity measures between target pairs and representative pairs from the target class. Moreover, the co-set expansion process involves a class ranking procedure that takes into account exemplars from contrastive classes. Contextual details encompassing relation mentions are harnessed via context-free Hearst patterns to ascertain contextual similarity.  Empirical evaluation demonstrates the efficacy of our co-set expansion approach, resulting in a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#23384;&#20648;&#38480;&#21046;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#20174;&#32780;&#20026;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.11691</link><description>&lt;p&gt;
&#36793;&#32536;&#19978;&#23545;&#26032;&#30340;&#20154;&#20307;&#27963;&#21160;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#30340;&#23454;&#29992;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Practical Insights on Incremental Learning of New Human Physical Activity on the Edge. (arXiv:2308.11691v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#23384;&#20648;&#38480;&#21046;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#20174;&#32780;&#20026;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#65288;Edge ML&#65289;&#23558;&#35745;&#31639;&#26234;&#33021;&#20174;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#36716;&#31227;&#21040;&#36793;&#32536;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#22914;&#38477;&#20302;&#24310;&#36831;&#12289;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#21644;&#20943;&#23569;&#36830;&#25509;&#20381;&#36182;&#65292;&#21560;&#24341;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#36825;&#20123;&#20248;&#21183;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#23427;&#20204;&#24341;&#20837;&#20102;&#20256;&#32479;&#22522;&#20110;&#20113;&#30340;&#26041;&#27861;&#20013;&#32570;&#22833;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#21463;&#38480;&#25968;&#25454;&#23384;&#20648;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#35745;&#31639;&#33021;&#21147;&#20197;&#21450;&#23398;&#20064;&#31867;&#21035;&#25968;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;MAGNETO&#31995;&#32479;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#36890;&#36807;&#31227;&#21160;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#20154;&#20307;&#27963;&#21160;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#31038;&#20132;&#20114;&#21160;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#25581;&#31034;&#21516;&#19968;&#33258;&#28982;&#20154;&#30340;&#34394;&#25311;&#36523;&#20221;&#20851;&#32852;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;/&#38750;&#27861;&#27963;&#21160;&#30340;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2308.11684</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#20351;&#29992;&#35821;&#35328;&#21644;&#31038;&#20132;&#20114;&#21160;&#29305;&#24449;&#36827;&#34892;&#29992;&#25143;&#36523;&#20221;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
User Identity Linkage in Social Media Using Linguistic and Social Interaction Features. (arXiv:2308.11684v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21644;&#31038;&#20132;&#20114;&#21160;&#29305;&#24449;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#25581;&#31034;&#21516;&#19968;&#33258;&#28982;&#20154;&#30340;&#34394;&#25311;&#36523;&#20221;&#20851;&#32852;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;/&#38750;&#27861;&#27963;&#21160;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36890;&#24120;&#20250;&#22312;&#20854;&#21162;&#21147;&#25193;&#22823;&#20182;&#20204;&#30340;&#24605;&#24819;&#12289;&#35266;&#28857;&#21644;&#35266;&#28857;&#20256;&#25773;&#33539;&#22260;&#26102;&#25317;&#26377;&#22810;&#20010;&#36134;&#25143;&#12290;&#22312;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#20542;&#21521;&#20110;&#21019;&#24314;&#22810;&#20010;&#36134;&#25143;&#65292;&#20197;&#32469;&#36807;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23454;&#26045;&#30340;&#25171;&#20987;&#25514;&#26045;&#65292;&#21363;&#20351;&#20854;&#20013;&#19968;&#20123;&#36134;&#25143;&#34987;&#26242;&#20572;&#65292;&#20063;&#21487;&#20197;&#20445;&#30041;&#20182;&#20204;&#30340;&#22312;&#32447;&#36523;&#20221;&#12290;&#29992;&#25143;&#36523;&#20221;&#20851;&#32852;&#26088;&#22312;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#36134;&#25143;&#21487;&#33021;&#23646;&#20110;&#21516;&#19968;&#20010;&#33258;&#28982;&#20154;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;/&#38750;&#27861;&#27963;&#21160;&#30340;&#20256;&#25773;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#29992;&#25143;&#22312;&#32447;&#27963;&#21160;&#30340;&#22810;&#20010;&#23646;&#24615;&#65292;&#20197;&#30830;&#23450;&#20004;&#20010;&#25110;&#22810;&#20010;&#34394;&#25311;&#36523;&#20221;&#26159;&#21542;&#23646;&#20110;&#21516;&#19968;&#20010;&#30495;&#23454;&#33258;&#28982;&#20154;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#28389;&#29992;&#21644;&#19982;&#24656;&#24598;&#20027;&#20041;&#26377;&#20851;&#30340;Twitter&#20869;&#23481;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media users often hold several accounts in their effort to multiply the spread of their thoughts, ideas, and viewpoints. In the particular case of objectionable content, users tend to create multiple accounts to bypass the combating measures enforced by social media platforms and thus retain their online identity even if some of their accounts are suspended. User identity linkage aims to reveal social media accounts likely to belong to the same natural person so as to prevent the spread of abusive/illegal activities. To this end, this work proposes a machine learning-based detection model, which uses multiple attributes of users' online activity in order to identify whether two or more virtual identities belong to the same real natural person. The models efficacy is demonstrated on two cases on abusive and terrorism-related Twitter content.
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"</title><link>http://arxiv.org/abs/2308.11676</link><description>&lt;p&gt;
"&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#30740;&#31350;"
&lt;/p&gt;
&lt;p&gt;
A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework. (arXiv:2308.11676v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11676
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#28151;&#28102;&#21327;&#21464;&#37327;&#23545;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#25512;&#26029;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#26469;&#22686;&#24378;&#23545;&#36825;&#20123;&#27169;&#22411;&#22522;&#26412;&#21407;&#29702;&#30340;&#29702;&#35299;&#65292;&#20026;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#24102;&#26469;&#20102;&#28508;&#22312;&#20215;&#20540;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#65288;POF&#65289;&#22312;&#22240;&#26524;&#25512;&#26029;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;POF&#30340;&#22240;&#26524;&#25512;&#26029;&#27169;&#22411;&#65288;CIMs-B-POF&#65289;&#26088;&#22312;&#28040;&#38500;&#28151;&#28102;&#20559;&#24046;&#65292;&#24182;&#40664;&#35748;&#23384;&#22312;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#36825;&#19968;&#20551;&#35774;&#35748;&#20026;&#21327;&#21464;&#37327;&#20165;&#30001;&#28151;&#28102;&#21464;&#37327;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#20445;&#25345;&#28151;&#28102;&#21327;&#21464;&#37327;&#30340;&#20551;&#35774;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#39640;&#32500;&#21327;&#21464;&#37327;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#22312;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#20043;&#21069;&#21306;&#20998;&#21327;&#21464;&#37327;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23558;&#38750;&#28151;&#28102;&#30340;&#21327;&#21464;&#37327;&#35270;&#20026;&#28151;&#28102;&#21464;&#37327;&#30340;&#21518;&#26524;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;CIMs-B-POF&#26102;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22270;&#24418;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;CIMs-B-POF&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#21033;&#29992;&#36825;&#20010;&#22270;&#24418;&#26694;&#26550;&#65292;&#25105;&#20204;&#23545;CIMs-B-POF&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#37327;&#21270;&#20998;&#26512;&#12290;"
&lt;/p&gt;
&lt;p&gt;
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quant
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22312;&#32447;&#27169;&#24335;&#30340;BCI&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#22312;&#32447;&#22788;&#29702;&#30340;&#29305;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27835;&#30103;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.11656</link><description>&lt;p&gt;
BCI&#35780;&#20272;&#30340;&#20266;&#22312;&#32447;&#26694;&#26550;&#65306;MOABB&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Pseudo-online framework for BCI evaluation: A MOABB perspective. (arXiv:2308.11656v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#22312;&#32447;&#27169;&#24335;&#30340;BCI&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#22312;&#32447;&#22788;&#29702;&#30340;&#29305;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#27835;&#30103;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BCI&#65288;&#33041;&#26426;&#25509;&#21475;&#65289;&#25216;&#26415;&#26377;&#19977;&#31181;&#25805;&#20316;&#27169;&#24335;&#65306;&#22312;&#32447;&#12289;&#31163;&#32447;&#21644;&#20266;&#22312;&#32447;&#12290;&#22312;&#32447;&#27169;&#24335;&#19979;&#65292;&#23454;&#26102;&#30340;&#33041;&#30005;&#25968;&#25454;&#34987;&#25345;&#32493;&#20998;&#26512;&#12290;&#31163;&#32447;&#27169;&#24335;&#19979;&#65292;&#20449;&#21495;&#22312;&#37319;&#38598;&#21518;&#36827;&#34892;&#22788;&#29702;&#12290;&#20266;&#22312;&#32447;&#27169;&#24335;&#19979;&#65292;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#34987;&#22788;&#29702;&#25104;&#20223;&#30495;&#23454;&#26102;&#25509;&#25910;&#30340;&#24418;&#24335;&#12290;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#31163;&#32447;&#27169;&#24335;&#32463;&#24120;&#20998;&#26512;&#25972;&#20010;&#25968;&#25454;&#65292;&#32780;&#22312;&#32447;&#21644;&#20266;&#22312;&#32447;&#27169;&#24335;&#21482;&#20998;&#26512;&#30701;&#26102;&#38388;&#31383;&#21475;&#30340;&#25968;&#25454;&#12290;&#31163;&#32447;&#20998;&#26512;&#36890;&#24120;&#19982;&#24322;&#27493;BCI&#19968;&#36215;&#20351;&#29992;&#65292;&#36825;&#38480;&#21046;&#20102;&#20998;&#26512;&#21040;&#39044;&#23450;&#20041;&#30340;&#26102;&#38388;&#31383;&#21475;&#12290;&#19982;&#22312;&#32447;&#21644;&#20266;&#22312;&#32447;&#27169;&#24335;&#20860;&#23481;&#30340;&#24322;&#27493;BCI&#20801;&#35768;&#28789;&#27963;&#30340;&#24605;&#32500;&#27963;&#21160;&#25345;&#32493;&#26102;&#38388;&#12290;&#31163;&#32447;&#22788;&#29702;&#24448;&#24448;&#26356;&#20934;&#30830;&#65292;&#32780;&#22312;&#32447;&#20998;&#26512;&#21017;&#26356;&#36866;&#29992;&#20110;&#27835;&#30103;&#24212;&#29992;&#12290;&#20266;&#22312;&#32447;&#23454;&#29616;&#36817;&#20284;&#20110;&#26080;&#23454;&#26102;&#32422;&#26463;&#30340;&#22312;&#32447;&#22788;&#29702;&#12290;&#35768;&#22810;&#31163;&#32447;&#30340;BCI&#30740;&#31350;&#30456;&#23545;&#20110;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#24773;&#26223;&#24341;&#20837;&#20102;&#20559;&#24046;&#65292;&#24433;&#21709;&#20102;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: BCI (Brain-Computer Interface) technology operates in three modes: online, offline, and pseudo-online. In the online mode, real-time EEG data is constantly analyzed. In offline mode, the signal is acquired and processed afterwards. The pseudo-online mode processes collected data as if they were received in real-time. The main difference is that the offline mode often analyzes the whole data, while the online and pseudo-online modes only analyze data in short time windows. Offline analysis is usually done with asynchronous BCIs, which restricts analysis to predefined time windows. Asynchronous BCI, compatible with online and pseudo-online modes, allows flexible mental activity duration. Offline processing tends to be more accurate, while online analysis is better for therapeutic applications. Pseudo-online implementation approximates online processing without real-time constraints. Many BCI studies being offline introduce biases compared to real-life scenarios, impacting clas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11654</link><description>&lt;p&gt;
&#22823;&#22411;&#21464;&#21387;&#22120;&#26159;&#26356;&#22909;&#30340;&#33041;&#30005;&#22270;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Transformers are Better EEG Learners. (arXiv:2308.11654v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#26631;&#35760;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35268;&#27169;&#36828;&#36828;&#20302;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#22240;&#27492;&#24456;&#38590;&#23558;&#20174;EEG&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#24320;&#21457;&#21040;&#20687;GPT-4 100T&#36825;&#26679;&#30340;&#35268;&#27169;&#65292;&#20174;&#32780;&#23436;&#20840;&#21457;&#25381;&#35813;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;AdaCE&#65292;&#21363;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#24418;&#24335;&#30340;&#25554;&#25300;&#24335;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21464;&#21387;&#22120;&#12290;&#25552;&#20986;&#30340;AdaCE&#27169;&#22359;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#22312;&#22810;&#31181;&#22522;&#20110;EEG&#30340;&#39044;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#30340;Swin-Transformer&#19978;&#30340;AdaCE&#36798;&#21040;&#20102;99.6&#65285;&#30340;&#31934;&#24230;&#65292;&#32477;&#23545;&#25913;&#21892;&#20102;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-deco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#33041;&#30005;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11651</link><description>&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#36328;&#20027;&#39064;&#33041;&#30005;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Cross Subject EEG Decoding. (arXiv:2308.11651v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#33041;&#30005;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#20110;&#33041;&#30005;&#35299;&#30721;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#28040;&#26497;&#24433;&#21709;&#65306;1&#65289;&#20449;&#21495;&#20013;&#22266;&#26377;&#30340;&#39640;&#26041;&#24046;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#27745;&#26579;&#65292;2&#65289;&#33041;&#30005;&#25968;&#25454;&#38598;&#36890;&#24120;&#30456;&#23545;&#36739;&#23567;&#65292;&#32473;&#23450;&#20102;&#37319;&#38598;&#25104;&#26412;&#12289;&#27880;&#37322;&#25104;&#26412;&#21644;&#25152;&#38656;&#30340;&#21162;&#21147;&#37327;&#12290;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24050;&#32463;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#22312;&#31354;&#38388;&#22495;&#12289;&#26102;&#38388;&#22495;&#25110;&#39057;&#29575;&#22495;&#19978;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25805;&#20316;&#65292;&#25163;&#24037;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#19987;&#23478;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#21160;&#24577;&#28436;&#21270;&#26469;&#25552;&#39640;&#35299;&#30721;&#30340;&#31283;&#20581;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#19968;&#26063;&#28436;&#21270;&#25968;&#25454;&#20998;&#24067;&#26469;&#23454;&#29616;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#22522;&#20110;Wasserstein&#26799;&#24230;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25968;&#25454;&#28436;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has shown to be effective for Electroencephalography (EEG) decoding tasks. Yet, its performance can be negatively influenced by two key factors: 1) the high variance and different types of corruption that are inherent in the signal, 2) the EEG datasets are usually relatively small given the acquisition cost, annotation cost and amount of effort needed. Data augmentation approaches for alleviation of this problem have been empirically studied, with augmentation operations on spatial domain, time domain or frequency domain handcrafted based on expertise of domain knowledge. In this work, we propose a principled approach to perform dynamic evolution on the data for improvement of decoding robustness. The approach is based on distributionally robust optimization and achieves robustness by optimizing on a family of evolved data distributions instead of the single training data distribution. We derived a general data evolution framework based on Wasserstein gradient f
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#25506;&#35752;&#20102;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335; Web &#32534;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#12289;&#38480;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040; Web &#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#36890;&#36807;&#36825;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#20026;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#22312;&#26410;&#26469;&#30340; Web &#32534;&#31243;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;</title><link>http://arxiv.org/abs/2308.11649</link><description>&lt;p&gt;
&#25506;&#32034;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#23545;&#20110;&#20132;&#20114;&#24335; Web &#32534;&#31243;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Power of Creative AI Tools and Game-Based Methodologies for Interactive Web-Based Programming. (arXiv:2308.11649v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#25506;&#35752;&#20102;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335; Web &#32534;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#12289;&#38480;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040; Web &#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#36890;&#36807;&#36825;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#20026;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#22312;&#26410;&#26469;&#30340; Web &#32534;&#31243;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#22522;&#20110; Web &#30340;&#32534;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#21019;&#24314;&#21160;&#24577;&#21644;&#20132;&#20114;&#24335;&#30340;&#32593;&#31449;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#36825;&#20123;&#36827;&#27493;&#30340;&#21069;&#27839;&#65292;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#25215;&#35834;&#25552;&#20379;&#22686;&#24378;&#30340;&#29992;&#25143;&#20307;&#39564;&#21644;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#25552;&#39640;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#26412;&#31456;&#25506;&#35752;&#20102;&#36825;&#20123;&#24037;&#20855;&#21644;&#26041;&#27861;&#22312;&#20132;&#20114;&#24335; Web &#32534;&#31243;&#20013;&#30340;&#28508;&#21147;&#65292;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#21183;&#12289;&#38480;&#21046;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040; Web &#24320;&#21457;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#22914;&#38544;&#31169;&#38382;&#39064;&#21644; AI &#29983;&#25104;&#20869;&#23481;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#36825;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#21019;&#36896;&#24615; AI &#24037;&#20855;&#21644;&#22522;&#20110;&#28216;&#25103;&#30340;&#26041;&#27861;&#23545;&#20110; Web &#32534;&#31243;&#26410;&#26469;&#30340;&#28608;&#21160;&#20154;&#24515;&#21487;&#33021;&#24615;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the fields of artificial intelligence and web-based programming have seen tremendous advancements, enabling developers to create dynamic and interactive websites and applications. At the forefront of these advancements, creative AI tools and game-based methodologies have emerged as potent instruments, promising enhanced user experiences and increased engagement in educational environments. This chapter explores the potential of these tools and methodologies for interactive web-based programming, examining their benefits, limitations, and real-world applications. We examine the challenges and ethical considerations that arise when integrating these technologies into web development, such as privacy concerns and the potential for bias in AI-generated content. Through this exploration, we aim to provide insights into the exciting possibilities that creative AI tools and game-based methodologies offer for the future of web-based programming.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;FedRANE&#26041;&#27861;&#32467;&#21512;&#20102;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65292;&#21487;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11646</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#21512;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Joint Local Relational Augmentation and Global Nash Equilibrium for Federated Learning with Non-IID Data. (arXiv:2308.11646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;FedRANE&#26041;&#27861;&#32467;&#21512;&#20102;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;&#65292;&#21487;&#20197;&#35299;&#20915;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38750;&#29420;&#31435;&#38750;&#21516;&#20998;&#24067;&#25968;&#25454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#38656;&#35201;&#26381;&#21153;&#22120;&#21644;&#19968;&#31995;&#21015;&#20855;&#26377;&#20998;&#25955;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20043;&#38388;&#21512;&#20316;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#12290;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26377;&#25928;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#36827;&#38750;&#29420;&#31435;&#30456;&#21516;&#20998;&#24067;(non-IID)&#30340;&#20998;&#25955;&#25968;&#25454;&#30340;&#24314;&#27169;&#12290;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#65292;&#22312;&#25968;&#25454;&#24314;&#27169;&#20013;&#23384;&#22312;&#26469;&#33258;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20869;&#19968;&#33268;&#24615;&#21644;&#24322;&#26500;&#23458;&#25143;&#31471;&#20998;&#24067;&#20043;&#38388;&#30340;&#23458;&#25143;&#31471;&#38388;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#19981;&#20165;&#38459;&#30861;&#20102;&#23569;&#25968;&#25968;&#25454;&#30340;&#20805;&#20998;&#34920;&#31034;&#65292;&#36824;&#24102;&#26469;&#20102;&#19981;&#19968;&#33268;&#30340;&#27169;&#22411;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#21516;&#26102;&#22788;&#29702;&#19978;&#36848;&#20004;&#31181;&#32806;&#21512;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedRANE&#65292;&#23427;&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65292;&#21363;&#23616;&#37096;&#20851;&#31995;&#22686;&#24378;(LRA)&#21644;&#20840;&#23616;&#32435;&#20160;&#22343;&#34913;(GNE)&#65292;&#20197;&#21516;&#26102;&#35299;&#20915;&#23458;&#25143;&#31471;&#20869;&#21644;&#23458;&#25143;&#31471;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning paradigm that needs collaboration between a server and a series of clients with decentralized data. To make FL effective in real-world applications, existing work devotes to improving the modeling of decentralized data with non-independent and identical distributions (non-IID). In non-IID settings, there are intra-client inconsistency that comes from the imbalanced data modeling, and inter-client inconsistency among heterogeneous client distributions, which not only hinders sufficient representation of the minority data, but also brings discrepant model deviations. However, previous work overlooks to tackle the above two coupling inconsistencies together. In this work, we propose FedRANE, which consists of two main modules, i.e., local relational augmentation (LRA) and global Nash equilibrium (GNE), to resolve intra- and inter-client inconsistency simultaneously. Specifically, in each client, LRA mines the similarity relations a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.11639</link><description>&lt;p&gt;
&#36890;&#36807;&#22788;&#29702;S&#21442;&#25968;&#27169;&#24335;&#23545;&#27687;&#21270;&#38111;&#38177;&#30005;&#26497;&#36827;&#34892;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
In situ Fault Diagnosis of Indium Tin Oxide Electrodes by Processing S-Parameter Patterns. (arXiv:2308.11639v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#23545;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#30005;&#23376;&#39046;&#22495;&#65292;&#27687;&#21270;&#38111;&#38177;&#65288;ITO&#65289;&#30005;&#26497;&#22312;&#26174;&#31034;&#22120;&#12289;&#20256;&#24863;&#22120;&#21644;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#25928;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#26159;&#30830;&#20445;&#35774;&#22791;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35270;&#35273;&#26816;&#26597;&#23545;&#20110;&#36879;&#26126;&#30340;ITO&#30005;&#26497;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#29616;&#26377;&#30340;&#25925;&#38556;&#26816;&#27979;&#26041;&#27861;&#22312;&#30830;&#23450;&#32570;&#38519;&#26681;&#26412;&#21407;&#22240;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#30772;&#22351;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25955;&#23556;&#21442;&#25968;&#65288;S&#21442;&#25968;&#65289;&#20449;&#21495;&#22788;&#29702;&#30340;&#29616;&#22330;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#31934;&#24230;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26681;&#25454;&#32570;&#38519;&#29366;&#24577;&#33719;&#21462;&#20102;&#20840;&#38754;&#30340;S&#21442;&#25968;&#27169;&#24335;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#65292;&#21516;&#26102;&#20998;&#26512;&#25925;&#38556;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optoelectronics, indium tin oxide (ITO) electrodes play a crucial role in various applications, such as displays, sensors, and solar cells. Effective fault detection and diagnosis of the ITO electrodes are essential to ensure the performance and reliability of the devices. However, traditional visual inspection is challenging with transparent ITO electrodes, and existing fault detection methods have limitations in determining the root causes of the defects, often requiring destructive evaluations. In this study, an in situ fault diagnosis method is proposed using scattering parameter (S-parameter) signal processing, offering early detection, high diagnostic accuracy, noise robustness, and root cause analysis. A comprehensive S-parameter pattern database is obtained according to defect states. Deep learning (DL) approaches, including multilayer perceptron (MLP), convolutional neural network (CNN), and transformer, are then used to simultaneously analyze the cause and sev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#26041;&#27861;&#21512;&#25104;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#30340;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11638</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
IoT Data Trust Evaluation via Machine Learning. (arXiv:2308.11638v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11638
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#26041;&#27861;&#21512;&#25104;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#30340;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#29992;&#20110;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35780;&#20272;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#24456;&#38590;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#31216;&#20026;&#38543;&#26426;&#28216;&#36208;&#34917;&#20840;&#65288;RWI&#65289;&#65292;&#36890;&#36807;&#20174;&#29616;&#26377;&#30340;&#21487;&#20449;&#25968;&#25454;&#21512;&#25104;&#19981;&#21487;&#20449;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20174;&#29289;&#32852;&#32593;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#26032;&#29305;&#24449;&#65292;&#26377;&#25928;&#25429;&#25417;&#20854;&#33258;&#30456;&#20851;&#24615;&#20197;&#21450;&#19982;&#37051;&#36817;&#65288;&#23545;&#31561;&#65289;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35782;&#21035;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#21512;&#25104;&#30340;&#22522;&#20934;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#24320;&#21457;&#21644;&#39564;&#35777;&#29289;&#32852;&#32593;&#25968;&#25454;&#20449;&#20219;&#35780;&#20272;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various approaches based on supervised or unsupervised machine learning (ML) have been proposed for evaluating IoT data trust. However, assessing their real-world efficacy is hard mainly due to the lack of related publicly-available datasets that can be used for benchmarking. Since obtaining such datasets is challenging, we propose a data synthesis method, called random walk infilling (RWI), to augment IoT time-series datasets by synthesizing untrustworthy data from existing trustworthy data. Thus, RWI enables us to create labeled datasets that can be used to develop and validate ML models for IoT data trust evaluation. We also extract new features from IoT time-series sensor data that effectively capture its auto-correlation as well as its cross-correlation with the data of the neighboring (peer) sensors. These features can be used to learn ML models for recognizing the trustworthiness of IoT sensor data. Equipped with our synthesized ground-truth-labeled datasets and informative corr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#20316;&#65292;&#20811;&#26381;&#20102;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11636</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#20869;&#22312;&#20449;&#24687;&#25552;&#21319;BCI&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Aggregating Intrinsic Information to Enhance BCI Performance through Federated Learning. (arXiv:2308.11636v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#20316;&#65292;&#20811;&#26381;&#20102;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26500;&#24314;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25968;&#25454;&#19981;&#36275;&#26159;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#38754;&#20020;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#22242;&#38431;&#21644;&#26426;&#26500;&#20026;&#21516;&#19968;&#20010;BCI&#20219;&#21153;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20294;&#30001;&#20110;&#35774;&#22791;&#30340;&#24322;&#36136;&#24615;&#65292;&#20849;&#20139;&#26469;&#33258;&#22810;&#20010;&#31449;&#28857;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25968;&#25454;&#22810;&#26679;&#24615;&#22312;&#20419;&#36827;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#27492;&#36825;&#20010;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#19981;&#21487;&#20302;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#23569;&#35752;&#35770;&#36825;&#20010;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#25968;&#25454;&#38598;&#20869;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#24120;&#26159;&#22312;&#19981;&#21516;&#21463;&#35797;&#32773;&#25110;&#19981;&#21516;&#20250;&#35805;&#35774;&#32622;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#33041;&#30005;&#22270;&#35299;&#30721;&#26694;&#26550;&#65288;FLEEG&#65289;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#20026;BCI&#24102;&#26469;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21512;&#20316;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#34987;&#20998;&#37197;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;tr
&lt;/p&gt;
&lt;p&gt;
Insufficient data is a long-standing challenge for Brain-Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and tr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27969;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#38750;&#20405;&#20837;&#24335;&#30005;&#36127;&#33655;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#23558;&#30005;&#27969;&#20449;&#21495;&#26144;&#23556;&#21040;&#20108;&#32500;&#39068;&#33394;&#29305;&#24449;&#22270;&#20687;&#19978;&#65292;&#24182;&#20351;&#29992;U&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#20986;&#25152;&#26377;&#30005;&#36127;&#33655;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.11627</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#27969;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#38750;&#20405;&#20837;&#24335;&#30005;&#36127;&#33655;&#30417;&#27979;&#26041;&#27861;&#29992;&#20110;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Electric Load Monitoring Approach Based on Current Feature Visualization for Smart Energy Management. (arXiv:2308.11627v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11627
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30005;&#27969;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#38750;&#20405;&#20837;&#24335;&#30005;&#36127;&#33655;&#30417;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#23558;&#30005;&#27969;&#20449;&#21495;&#26144;&#23556;&#21040;&#20108;&#32500;&#39068;&#33394;&#29305;&#24449;&#22270;&#20687;&#19978;&#65292;&#24182;&#20351;&#29992;U&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#20986;&#25152;&#26377;&#30005;&#36127;&#33655;&#65292;&#20197;&#23454;&#29616;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#27969;&#34892;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#26234;&#33021;&#30005;&#21147;&#33021;&#28304;&#31649;&#29702;&#30340;&#38750;&#20405;&#20837;&#24335;&#36127;&#33655;&#30417;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20449;&#21495;&#21464;&#25442;&#21644;Gramian Angular Field&#65288;GAF&#65289;&#26041;&#27861;&#23558;&#19968;&#32500;&#30005;&#27969;&#20449;&#21495;&#26144;&#23556;&#21040;&#20108;&#32500;&#39068;&#33394;&#29305;&#24449;&#22270;&#20687;&#19978;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;U&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#39068;&#33394;&#29305;&#24449;&#22270;&#20687;&#20013;&#35782;&#21035;&#20986;&#25152;&#26377;&#30340;&#30005;&#36127;&#33655;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#20026;&#22522;&#20110;&#20113;&#30340;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#33410;&#32422;&#30005;&#21147;&#31995;&#32479;&#25511;&#21046;&#20013;&#30340;&#33021;&#28304;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art smart city has been calling for an economic but efficient energy management over large-scale network, especially for the electric power system. It is a critical issue to monitor, analyze and control electric loads of all users in system. In this paper, we employ the popular computer vision techniques of AI to design a non-invasive load monitoring method for smart electric energy management. First of all, we utilize both signal transforms (including wavelet transform and discrete Fourier transform) and Gramian Angular Field (GAF) methods to map one-dimensional current signals onto two-dimensional color feature images. Second, we propose to recognize all electric loads from color feature images using a U-shape deep neural network with multi-scale feature extraction and attention mechanism. Third, we design our method as a cloud-based, non-invasive monitoring of all users, thereby saving energy cost during electric power system control. Experimental results on both pu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11624</link><description>&lt;p&gt;
&#29992;&#36890;&#29992;&#35774;&#22791;&#32534;&#30721;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#38761;&#26032;TCAD&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#35774;&#22791;&#32423;&#19978;&#24212;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#34920;&#31034;&#25216;&#26415;&#26469;&#23545;TCAD&#22120;&#20214;&#27169;&#25311;&#20013;&#30340;&#21322;&#23548;&#20307;&#22120;&#20214;&#36827;&#34892;&#32534;&#30721;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#36890;&#29992;&#32534;&#30721;&#26041;&#26696;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#26448;&#26009;&#32423;&#21644;&#22120;&#20214;&#32423;&#23884;&#20837;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31354;&#38388;&#20851;&#31995;&#30340;&#23884;&#20837;&#65292;&#21463;&#26377;&#38480;&#20803;&#32593;&#26684;&#20013;&#24120;&#29992;&#30340;&#25554;&#20540;&#25805;&#20316;&#21551;&#21457;&#32780;&#26469;&#12290;&#21033;&#29992;&#22120;&#20214;&#27169;&#25311;&#30340;&#36890;&#29992;&#29289;&#29702;&#23450;&#24459;&#36827;&#34892;&#20840;&#38754;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65292;&#21253;&#25324;&#22522;&#20110;&#27850;&#26494;&#20223;&#30495;&#30340;&#26367;&#20195;&#21644;&#22522;&#20110;&#28418;&#31227;&#25193;&#25955;&#27169;&#22411;&#30340;&#30005;&#27969;-&#30005;&#21387;&#65288;IV&#65289;&#39044;&#27979;&#12290;&#36825;&#20004;&#32773;&#37117;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#31216;&#20026;RelGAT&#65289;&#23454;&#29616;&#30340;&#12290;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;Sentaurus TCAD&#22120;&#20214;&#27169;&#25311;&#22120;&#30340;&#35814;&#32454;&#25216;&#26415;&#32454;&#33410;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#35774;&#22791;&#32423;&#19978;&#37319;&#29992;&#25552;&#20986;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
An innovative methodology that leverages artificial intelligence (AI) and graph representation for semiconductor device encoding in TCAD device simulation is proposed. A graph-based universal encoding scheme is presented that not only considers material-level and device-level embeddings, but also introduces a novel spatial relationship embedding inspired by interpolation operations typically used in finite element meshing. Universal physical laws from device simulations are leveraged for comprehensive data-driven modeling, which encompasses surrogate Poisson emulation and current-voltage (IV) prediction based on drift-diffusion model. Both are achieved using a novel graph attention network, referred to as RelGAT. Comprehensive technical details based on the device simulator Sentaurus TCAD are presented, empowering researchers to adopt the proposed AI-driven Electronic Design Automation (EDA) solution at the device level.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;DASH&#33258;&#36866;&#24212;&#21644;&#35843;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#32593;&#32476;&#26465;&#20214;&#36873;&#25321;&#36866;&#24403;&#30340;&#36136;&#37327;&#32423;&#21035;&#21644;&#36827;&#34892;&#22359;&#35843;&#24230;&#65292;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;(QoE)&#12290;</title><link>http://arxiv.org/abs/2308.11621</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;DASH&#33258;&#36866;&#24212;&#21644;&#35843;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning -based Adaptation and Scheduling Methods for Multi-source DASH. (arXiv:2308.11621v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#28304;DASH&#33258;&#36866;&#24212;&#21644;&#35843;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#26681;&#25454;&#32593;&#32476;&#26465;&#20214;&#36873;&#25321;&#36866;&#24403;&#30340;&#36136;&#37327;&#32423;&#21035;&#21644;&#36827;&#34892;&#22359;&#35843;&#24230;&#65292;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;(QoE)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21160;&#24577;&#33258;&#36866;&#24212;&#27969;&#23186;&#20307;&#20256;&#36755;(DASH)&#22312;&#35270;&#39057;&#27969;&#23186;&#20307;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;DASH&#20013;&#65292;&#23458;&#25143;&#31471;&#25353;&#29031;&#39034;&#24207;&#20174;&#26381;&#21153;&#22120;&#19979;&#36733;&#35270;&#39057;&#22359;&#12290;&#35270;&#39057;&#23458;&#25143;&#31471;&#30340;&#36895;&#29575;&#33258;&#36866;&#24212;&#21151;&#33021;&#36890;&#36807;&#26681;&#25454;&#32593;&#32476;&#26465;&#20214;&#36873;&#25321;&#36866;&#24403;&#30340;&#36136;&#37327;&#32423;&#21035;&#26469;&#25552;&#39640;&#29992;&#25143;&#30340;&#20307;&#39564;&#36136;&#37327;(QoE)&#12290;&#22914;&#20170;&#65292;&#20869;&#23481;&#20998;&#21457;&#32593;&#32476;&#12289;&#36793;&#32536;&#32531;&#23384;&#32593;&#32476;&#12289;&#20869;&#23481;&#20013;&#24515;&#32593;&#32476;&#31561;&#32593;&#32476;&#36890;&#24120;&#22312;&#22810;&#20010;&#32531;&#23384;&#33410;&#28857;&#19978;&#22797;&#21046;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#30740;&#31350;&#20102;&#22810;&#28304;&#35270;&#39057;&#27969;&#23186;&#20307;&#12290;&#22312;&#22810;&#28304;&#27969;&#23186;&#20307;&#20013;&#65292;&#35270;&#39057;&#22359;&#21487;&#33021;&#22240;&#32593;&#32476;&#36335;&#24452;&#30340;&#19981;&#21516;&#26465;&#20214;&#32780;&#20197;&#20081;&#24207;&#26041;&#24335;&#21040;&#36798;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20445;&#35777;&#39640;QoE&#65292;&#35270;&#39057;&#23458;&#25143;&#31471;&#19981;&#20165;&#38656;&#35201;&#36827;&#34892;&#36895;&#29575;&#33258;&#36866;&#24212;&#65292;&#36824;&#38656;&#35201;&#36827;&#34892;&#22359;&#35843;&#24230;&#12290;&#24378;&#21270;&#23398;&#20064;(RL)&#24050;&#32463;&#25104;&#20026;&#36817;&#24180;&#26469;&#21508;&#20010;&#39046;&#22495;&#26368;&#20808;&#36827;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;RL&#30340;&#22810;&#28304;&#35270;&#39057;&#27969;&#23186;&#20307;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic adaptive streaming over HTTP (DASH) has been widely used in video streaming recently. In DASH, the client downloads video chunks in order from a server. The rate adaptation function at the video client enhances the user's quality-of-experience (QoE) by choosing a suitable quality level for each video chunk to download based on the network condition. Today networks such as content delivery networks, edge caching networks, content-centric networks,... usually replicate video contents on multiple cache nodes. We study video streaming from multiple sources in this work. In multi-source streaming, video chunks may arrive out of order due to different conditions of the network paths. Hence, to guarantee a high QoE, the video client needs not only rate adaptation but also chunk scheduling. Reinforcement learning (RL) has emerged as the state-of-the-art control method in various fields in recent years. This paper proposes two algorithms for streaming from multiple sources: RL-based ada
&lt;/p&gt;</description></item><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11241</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wav2vec2&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;Transformer&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#36824;&#29992;&#20110;&#25972;&#20010;&#35821;&#38899;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#26377;&#25928;&#31471;&#21040;&#31471;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21442;&#25968;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#26377;&#25928;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#38376;&#27744;&#21270;(Temporal Gate Pooling)&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;Conformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;BEST-RQ&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;VoxCeleb1&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#20165;&#26377;28.5M&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20855;&#26377;317.7M&#20010;&#21442;&#25968;&#30340;wav2vec2&#30456;&#24403;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/HarunoriKawano/speaker-identification-with-tgp&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#65292;&#21517;&#20026;Prompting Robotic Modalities&#65288;PRM&#65289;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#24182;&#19988;&#22312;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#35774;&#35745;&#27169;&#24335;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;LLM&#25552;&#31034;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11236</link><description>&lt;p&gt;
ROSGPT_Vision: &#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts. (arXiv:2308.11236v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#65292;&#21517;&#20026;Prompting Robotic Modalities&#65288;PRM&#65289;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#24182;&#19988;&#22312;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#19978;&#24212;&#29992;&#20102;&#36825;&#31181;&#35774;&#35745;&#27169;&#24335;&#12290;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;&#21644;LLM&#25552;&#31034;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#65292;&#19979;&#19968;&#20195;&#26426;&#22120;&#20154;&#21487;&#20197;&#20165;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26469;&#36827;&#34892;&#21629;&#20196;&#12290;&#27599;&#20010;&#25552;&#31034;&#36890;&#36807;&#20854;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21333;&#29420;&#26597;&#35810;&#29305;&#23450;&#30340;&#26426;&#22120;&#20154;&#27169;&#24577;&#12290;&#20013;&#22830;&#20219;&#21153;&#27169;&#24577;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#33410;&#25972;&#20010;&#36890;&#20449;&#20197;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#27169;&#24335;&#21629;&#21517;&#20026;&#65306;Prompting Robotic Modalities&#65288;PRM&#65289;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#36825;&#20010;PRM&#35774;&#35745;&#27169;&#24335;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21517;&#20026;ROSGPT_Vision&#30340;&#26032;&#30340;&#26426;&#22120;&#20154;&#26694;&#26550;&#12290;ROSGPT_Vision&#21482;&#38656;&#35201;&#20004;&#20010;&#25552;&#31034;&#21363;&#21487;&#25191;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#65306;&#19968;&#20010;&#26159;&#35270;&#35273;&#25552;&#31034;&#65292;&#19968;&#20010;&#26159;LLM&#25552;&#31034;&#12290;&#35270;&#35273;&#25552;&#31034;&#20197;&#33258;&#28982;&#35821;&#35328;&#25552;&#21462;&#19982;&#25152;&#32771;&#34385;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#35821;&#20041;&#29305;&#24449;&#65288;&#35270;&#35273;&#26426;&#22120;&#20154;&#27169;&#24577;&#65289;&#12290;&#21516;&#26102;&#65292;LLM&#25552;&#31034;&#35843;&#33410;&#26426;&#22120;&#20154;&#23545;&#35270;&#35273;&#25551;&#36848;&#30340;&#21453;&#24212;&#65288;&#20219;&#21153;&#27169;&#24577;&#65289;&#12290;&#35813;&#26694;&#26550;&#33258;&#21160;&#21270;&#20102;&#36825;&#20004;&#20010;&#25552;&#31034;&#32972;&#21518;&#30340;&#25152;&#26377;&#26426;&#21046;&#12290;&#35813;&#26694;&#26550;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.11234</link><description>&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#20248;&#21270;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding. (arXiv:2308.11234v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32456;&#36523;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#26234;&#33021;&#20307;&#36991;&#24320;&#25317;&#22581;&#36335;&#24452;&#26469;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#24635;&#20307;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#35201;&#27714;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#26234;&#33021;&#20307;&#35745;&#31639;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#25152;&#26377;&#26234;&#33021;&#20307;&#37117;&#22312;&#20849;&#20139;&#22320;&#22270;&#19978;&#31227;&#21160;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#24403;&#21069;&#30340;&#31639;&#27861;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#37117;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35268;&#21010;&#33258;&#30001;&#27969;&#21160;&#30340;&#26368;&#20248;&#36335;&#24452;&#65292;&#36825;&#20250;&#23548;&#33268;&#25317;&#22581;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MAPF&#26041;&#27861;&#65292;&#36890;&#36807;&#36319;&#38543;&#36991;&#20813;&#25317;&#22581;&#30340;&#36335;&#24452;&#26469;&#24341;&#23548;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#30340;&#22320;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#36825;&#20010;&#24819;&#27861;&#65306;&#19968;&#27425;&#24615;MAPF&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#26377;&#19968;&#20010;&#30446;&#30340;&#22320;&#65292;&#20197;&#21450;&#32456;&#36523;MAPF&#65292;&#26234;&#33021;&#20307;&#19981;&#26029;&#34987;&#20998;&#37197;&#26032;&#20219;&#21153;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;MAPF&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#32456;&#36523;MAPF&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#24635;&#20307;&#21534;&#21520;&#37327;&#30340;&#22823;&#24133;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11217</link><description>&lt;p&gt;
&#22823;&#27169;&#22411;&#26102;&#20195;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#22312;&#22823;&#27169;&#22411;&#26102;&#20195;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#28608;&#21169;&#26426;&#21046;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#33021;&#22815;&#20840;&#38754;&#24863;&#30693;&#21644;&#35782;&#21035;&#29289;&#29702;&#19990;&#30028;&#65292;&#24050;&#25104;&#20026;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36335;&#24452;&#12290;&#28982;&#32780;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#22823;&#27169;&#22411;&#22312;&#29305;&#23450;&#24037;&#19994;&#39046;&#22495;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#29702;&#24819;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#22810;&#20010;&#20225;&#19994;&#21033;&#29992;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#21327;&#21516;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36328;&#22330;&#26223;&#30340;&#26234;&#33021;&#26381;&#21153;&#12290;&#20316;&#32773;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#27169;&#22411;&#26102;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26234;&#33021;&#22522;&#30784;&#21644;&#30446;&#26631;&#30340;&#25112;&#30053;&#36716;&#21464;&#65292;&#20197;&#21450;&#22312;&#24322;&#26500;&#25968;&#25454;&#12289;&#27169;&#22411;&#32858;&#21512;&#12289;&#24615;&#33021;&#21644;&#25104;&#26412;&#26435;&#34913;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#28608;&#21169;&#26426;&#21046;&#26041;&#38754;&#38754;&#20020;&#30340;&#26032;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#39046;&#20808;&#20225;&#19994;&#22312;&#22478;&#24066;&#23433;&#20840;&#36816;&#33829;&#31649;&#29702;&#26041;&#38754;&#36129;&#29486;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#37096;&#32626;&#21644;&#39640;&#25928;&#24615;&#33021;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#23545;&#35805;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#26041;&#27861;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.10380</link><description>&lt;p&gt;
&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;&#20248;&#21270;&#33258;&#21160;&#24418;&#24335;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability. (arXiv:2308.10380v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#23545;&#35805;&#26041;&#27861;&#26469;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#19968;&#26041;&#27861;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#33258;&#28982;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#20010;&#24615;&#21270;&#33021;&#28304;&#30456;&#20851;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#23450;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#19978;&#26377;&#36731;&#24494;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#26159;&#29992;&#25143;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#21046;&#23450;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#29992;&#25143;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#23558;&#20248;&#21270;&#27714;&#35299;&#22120;&#19982;LLM&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20854;&#29702;&#35299;&#21644;&#21709;&#24212;&#29992;&#25143;&#35268;&#33539;&#21644;&#20559;&#22909;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#20379;&#38750;&#32447;&#24615;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24320;&#21019;&#20102;&#20154;&#23548;&#21521;&#30340;&#20248;&#21270;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#26032;&#27010;&#24565;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#35268;&#33539;&#33258;&#21160;&#32763;&#35793;&#20026;&#20248;&#21270;&#23454;&#20363;&#12290;&#36825;&#20351;&#24471;LLM&#33021;&#22815;&#20998;&#26512;&#12289;&#35299;&#37322;&#21644;&#35299;&#20915;&#21508;&#31181;&#19982;&#23454;&#20363;&#30456;&#20851;&#30340;&#33021;&#28304;&#38382;&#39064;&#65292;&#31361;&#30772;&#20102;&#24403;&#21069;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#33021;&#28304;&#39046;&#22495;&#30340;&#21508;&#31181;&#24120;&#35265;&#20219;&#21153;&#65292;&#20174;&#30005;&#21147;&#21040;...
&lt;/p&gt;
&lt;p&gt;
This paper outlines a natural conversational approach to solving personalized energy-related problems using large language models (LLMs). We focus on customizable optimization problems that necessitate repeated solving with slight variations in modeling and are user-specific, hence posing a challenge to devising a one-size-fits-all model. We put forward a strategy that augments an LLM with an optimization solver, enhancing its proficiency in understanding and responding to user specifications and preferences while providing nonlinear reasoning capabilities. Our approach pioneers the novel concept of human-guided optimization autoformalism, translating a natural language task specification automatically into an optimization instance. This enables LLMs to analyze, explain, and tackle a variety of instance-specific energy-related problems, pushing beyond the limits of current prompt-based techniques.  Our research encompasses various commonplace tasks in the energy sector, from electric v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.09917</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#23610;&#24230;&#19968;&#33268;&#24615;&#30340;&#33258;&#30417;&#30563;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learning Multiscale Consistency for Self-supervised Electron Microscopy Instance Segmentation. (arXiv:2308.09917v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#35270;&#35273;&#34920;&#31034;&#25429;&#33719;&#30005;&#23376;&#26174;&#24494;&#38236;&#23454;&#20363;&#20998;&#21106;&#20013;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#20363;&#30340;&#22797;&#26434;&#24418;&#24577;&#21644;&#19981;&#20805;&#36275;&#30340;&#27880;&#37322;&#65292;&#30005;&#23376;&#26174;&#24494;&#38236;&#65288;EM&#65289;&#20307;&#31215;&#20013;&#30340;&#23454;&#20363;&#20998;&#21106;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#26368;&#36817;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#33719;&#21462;&#23545;&#20110;EM&#23454;&#20363;&#20998;&#21106;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#32990;&#32452;&#32455;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#25429;&#25417;&#22797;&#26434;&#30340;&#35270;&#35273;&#27169;&#24335;&#21644;&#20307;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25152;&#33719;&#21462;&#30340;&#20808;&#39564;&#30693;&#35782;&#19981;&#36275;&#20197;&#24212;&#29992;&#20110;&#19979;&#28216;&#30340;EM&#20998;&#26512;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#30340;&#35270;&#35273;&#34920;&#31034;&#26469;&#25429;&#25417;EM&#20307;&#31215;&#20013;&#30340;&#20307;&#32032;&#32423;&#21644;&#29305;&#24449;&#32423;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#37325;&#26500;&#20989;&#25968;&#24378;&#21046;&#23454;&#26045;Siamese&#32593;&#32476;&#36755;&#20986;&#20043;&#38388;&#30340;&#20307;&#32032;&#32423;&#19968;&#33268;&#24615;&#65292;&#24182;&#32467;&#21512;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#36719;&#29305;&#24449;&#21305;&#37197;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#29305;&#24449;&#32423;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation in electron microscopy (EM) volumes poses a significant challenge due to the complex morphology of instances and insufficient annotations. Self-supervised learning has recently emerged as a promising solution, enabling the acquisition of prior knowledge of cellular tissue structures that are essential for EM instance segmentation. However, existing pretraining methods often lack the ability to capture complex visual patterns and relationships between voxels, which results in the acquired prior knowledge being insufficient for downstream EM analysis tasks. In this paper, we propose a novel pretraining framework that leverages multiscale visual representations to capture both voxel-level and feature-level consistency in EM volumes. Specifically, our framework enforces voxel-level consistency between the outputs of a Siamese network by a reconstruction function, and incorporates a cross-attention mechanism for soft feature matching to achieve fine-grained feature-lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;</title><link>http://arxiv.org/abs/2308.06644</link><description>&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#33976;&#39311;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation. (arXiv:2308.06644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#22522;&#20110;&#25193;&#25955;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;16&#20493;&#30340;&#25512;&#29702;&#36895;&#24230;&#25552;&#21319;&#65292;&#20165;&#26377;0.019%&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;NP&#23436;&#20840;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#36845;&#20195;&#35780;&#20272;&#29305;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25512;&#29702;&#19978;&#24120;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36880;&#27493;&#33976;&#39311;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#20165;&#22312;&#21333;&#27493;&#20869;&#39044;&#27979;&#20004;&#20010;&#27493;&#39588;&#20043;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#36880;&#27493;&#33976;&#39311;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;TSP-50&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#36895;&#24230;&#24555;&#20102;16&#20493;&#65292;&#24615;&#33021;&#20165;&#26377;0.019%&#30340;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.16680</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26223;&#35266;&#65306;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#36131;&#20219;&#31561;&#22810;&#20010;&#32500;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39046;&#20808;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23545;&#20154;&#31867;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#20063;&#26292;&#38706;&#20986;&#22266;&#26377;&#30340;&#39118;&#38505;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#30340;&#21452;&#37325;&#24615;&#36136;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#21487;&#20449;&#24230;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#25991;&#29486;&#65292;&#20294;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#28041;&#21450;&#36825;&#20123;&#27169;&#22411;&#30340;&#38271;&#26399;&#21644;&#26032;&#20852;&#23041;&#32961;&#65292;&#28085;&#30422;&#20102;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36131;&#20219;&#36825;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#24352;&#35814;&#23613;&#30340;&#22320;&#22270;&#65292;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#38469;&#24314;&#35758;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#36825;&#20123;&#21162;&#21147;&#23545;&#20110;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models and large language models have emerged as leading-edge generative models and have sparked a revolutionary impact on various aspects of human life. However, the practical implementation of these models has also exposed inherent risks, highlighting their dual nature and raising concerns regarding their trustworthiness. Despite the abundance of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, This paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: privacy, security, fairness, and responsibility. In this way, we construct an extensive map outlining the trustworthiness of these models, while also providing practical recommendations and identifying future directions. These efforts are crucial for promoting the trustworthy deployment of these models, ulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12907</link><description>&lt;p&gt;
GridMM:&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;
&lt;/p&gt;
&lt;p&gt;
GridMM: Grid Memory Map for Vision-and-Language Navigation. (arXiv:2307.12907v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GridMM&#65292;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#33258;&#39030;&#21521;&#19979;&#32593;&#26684;&#35760;&#24518;&#22270;&#65292;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#32467;&#26500;&#21270;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;3D&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#36828;&#31243;&#20301;&#32622;&#12290;&#20026;&#20102;&#34920;&#31034;&#20808;&#21069;&#35775;&#38382;&#30340;&#29615;&#22659;&#65292;VLN&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20351;&#29992;&#32463;&#24120;&#24615;&#29366;&#24577;&#12289;&#25299;&#25169;&#22320;&#22270;&#25110;&#33258;&#39030;&#21521;&#19979;&#30340;&#35821;&#20041;&#22320;&#22270;&#26469;&#23454;&#29616;&#35760;&#24518;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#39030;&#21521;&#19979;&#30340;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#24182;&#21160;&#24577;&#22686;&#38271;&#30340;&#32593;&#26684;&#35760;&#24518;&#22270;&#65288;&#21363;GridMM&#65289;&#26469;&#32467;&#26500;&#21270;&#35775;&#38382;&#30340;&#29615;&#22659;&#12290;&#20174;&#20840;&#23616;&#35270;&#35282;&#26469;&#30475;&#65292;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#22312;&#33258;&#19978;&#32780;&#19979;&#30340;&#35270;&#22270;&#20013;&#34987;&#25237;&#24433;&#21040;&#32479;&#19968;&#30340;&#32593;&#26684;&#22320;&#22270;&#20013;&#65292;&#36825;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#29615;&#22659;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#20174;&#23616;&#37096;&#35270;&#35282;&#26469;&#30475;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#20196;&#30456;&#20851;&#24615;&#32858;&#21512;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#27599;&#20010;&#32593;&#26684;&#21306;&#22495;&#20013;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#32447;&#32034;&#12290;&#22312;&#31163;&#25955;&#29615;&#22659;&#20013;&#23545;REVERIE&#12289;R2R&#12289;SOON&#25968;&#25454;&#38598;&#20197;&#21450;&#36830;&#32493;&#29615;&#22659;&#20013;&#30340;R2R-CE&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03833</link><description>&lt;p&gt;
&#22238;&#24402;&#20248;&#21270;&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26679;&#26412;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#30340;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;&#25361;&#25112;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#23427;&#20204;&#20027;&#23548;&#20102;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#37326;&#22806;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20173;&#28982;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#65292;&#26080;&#35770;&#26159;2D-3D&#25552;&#21319;&#65292;&#22270;&#20687;&#21040;3D&#36824;&#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#35757;&#32451;&#30340;&#32593;&#32476;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#30456;&#26426;&#20869;&#21442;&#21644;&#22522;&#20110;&#39046;&#22495;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#24179;&#22343;&#26469;&#20272;&#35745;&#23039;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#21487;&#20197;&#36880;&#26696;&#20363;&#20272;&#35745;&#32467;&#26524;&#65292;&#33021;&#22815;&#22312;&#37326;&#22806;&#39044;&#27979;&#26356;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#30340;&#20154;&#20307;&#23039;&#21183;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#25193;&#25955;&#20248;&#21270;&#65288;ZeDO&#65289;&#31649;&#36947;&#29992;&#20110;&#35299;&#20915;&#36328;&#39046;&#22495;&#21644;&#37326;&#22806;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22810;&#20551;&#35774;ZeDO&#22312;Human3.6M&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;minMPJPE 51.4mm&#65289;&#65292;&#24182;&#19988;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based methods have dominated the 3D human pose estimation (HPE) tasks with significantly better performance in most benchmarks than traditional optimization-based methods. Nonetheless, 3D HPE in the wild is still the biggest challenge of learning-based models, whether with 2D-3D lifting, image-to-3D, or diffusion-based methods, since the trained networks implicitly learn camera intrinsic parameters and domain-based 3D human pose distributions and estimate poses by statistical average. On the other hand, the optimization-based methods estimate results case-by-case, which can predict more diverse and sophisticated human poses in the wild. By combining the advantages of optimization-based and learning-based methods, we propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm without training with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00252</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00252
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#20013;&#30340;&#26368;&#20248;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#36817;&#26399;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#26041;&#31243;&#32452;&#30340;&#35299;&#38598;&#36890;&#24120;&#21253;&#21547;&#19981;&#20809;&#28369;&#12289;&#22855;&#24322;&#30340;&#28857;&#12290;&#35299;&#20915;&#22855;&#28857;&#26159;&#20960;&#20309;&#20013;&#30340;&#22522;&#26412;&#36807;&#31243;&#65292;&#25105;&#20204;&#23558;&#22855;&#28857;&#26367;&#25442;&#20026;&#20809;&#28369;&#28857;&#65292;&#21516;&#26102;&#20445;&#25345;&#35299;&#38598;&#30340;&#21097;&#20313;&#37096;&#20998;&#19981;&#21464;&#12290;&#35299;&#20915;&#22855;&#28857;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#65306;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#21453;&#22797;&#36827;&#34892;&#34987;&#31216;&#20026;&#8220;blowing-up&#8221;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#35299;&#20915;&#30340;&#22797;&#26434;&#24615;&#39640;&#24230;&#20381;&#36182;&#20110;&#26576;&#20123;&#36873;&#25321;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#25104;&#19981;&#21516;&#29256;&#26412;&#30340;&#20004;&#20154;&#21338;&#24328;&#65292;&#21363;&#25152;&#35859;&#30340;Hironaka&#28216;&#25103;&#65292;&#32780;&#31532;&#19968;&#20301;&#29609;&#23478;&#30340;&#33719;&#32988;&#31574;&#30053;&#25552;&#20379;&#20102;&#35299;&#20915;&#22855;&#28857;&#38382;&#39064;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;Hironaka&#28216;&#25103;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23547;&#25214;&#22855;&#28857;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#39033;&#24335;&#30456;&#21152;&#30340;&#24635;&#25968;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36873;&#25321;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#35777;&#26126;&#20102;&#26368;&#36817;&#21457;&#23637;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2306.15782</link><description>&lt;p&gt;
UTRNet: &#21360;&#21047;&#25991;&#26723;&#20013;&#39640;&#20998;&#36776;&#29575;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
UTRNet: High-Resolution Urdu Text Recognition In Printed Documents. (arXiv:2306.15782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#21360;&#21047;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;OCR&#30340;&#31471;&#21040;&#31471;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#26469;&#35299;&#20915;&#21360;&#21047;&#20044;&#23572;&#37117;&#25991;&#26412;&#35782;&#21035;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#23610;&#24230;&#30340;&#35821;&#20041;&#29305;&#24449;&#25552;&#21462;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;UTRNet&#26550;&#26500;&#65292;&#19968;&#20010;&#28151;&#21512;CNN-RNN&#27169;&#22411;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#21069;&#24037;&#20316;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#24037;&#20316;&#24456;&#38590;&#25512;&#24191;&#21040;&#20044;&#23572;&#37117;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UTRSet-Real&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;11,000&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;UTRSet-Synth&#65292;&#19968;&#20010;&#19982;&#23454;&#38469;&#19990;&#30028;&#38750;&#24120;&#30456;&#20284;&#30340;&#21547;&#26377;20,000&#34892;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;IIITH&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#30495;&#23454;&#24615;&#36827;&#34892;&#20102;&#20462;&#27491;&#65292;&#20351;&#20854;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#26356;&#21487;&#38752;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;UrduDoc&#65292;&#19968;&#31181;&#29992;&#20110;&#25195;&#25551;&#25991;&#26723;&#20013;&#20044;&#23572;&#37117;&#25991;&#26412;&#34892;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#24037;&#20855;&#65292;&#36890;&#36807;&#23558;UTRNet&#19982;&#25991;&#26412;&#30340;&#31471;&#21040;&#31471;&#20044;&#23572;&#37117;OCR&#38598;&#25104;&#22312;&#21360;&#21047;&#25991;&#26723;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to address the challenges of printed Urdu text recognition using high-resolution, multi-scale semantic feature extraction. Our proposed UTRNet architecture, a hybrid CNN-RNN model, demonstrates state-of-the-art performance on benchmark datasets. To address the limitations of previous works, which struggle to generalize to the intricacies of the Urdu script and the lack of sufficient annotated real-world data, we have introduced the UTRSet-Real, a large-scale annotated real-world dataset comprising over 11,000 lines and UTRSet-Synth, a synthetic dataset with 20,000 lines closely resembling real-world and made corrections to the ground truth of the existing IIITH dataset, making it a more reliable resource for future research. We also provide UrduDoc, a benchmark dataset for Urdu text line detection in scanned documents. Additionally, we have developed an online tool for end-to-end Urdu OCR from printed documents by integrating UTRNet with a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#25915;&#20987;&#25216;&#26415;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14062</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions. (arXiv:2306.14062v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#27169;&#31946;&#30340;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#25915;&#20987;&#25216;&#26415;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#27934;&#21644;&#25915;&#20987;&#30340;&#25968;&#37327;&#12289;&#31181;&#31867;&#21644;&#36895;&#24230;&#30340;&#21464;&#21270;&#20351;&#24471;&#20154;&#31867;&#19987;&#23478;&#21644;&#32463;&#39564;&#22312;&#20107;&#20214;&#23041;&#32961;&#20998;&#26512;&#20013;&#21464;&#24471;&#22256;&#38590;&#12290;MITRE AT&#65286;CK&#26694;&#26550;&#20351;&#29992;&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTP&#65289;&#25551;&#36848;&#25915;&#20987;&#32773;&#22914;&#20309;&#21644;&#20026;&#20309;&#21033;&#29992;&#28431;&#27934;&#12290;&#20294;&#26159;&#65292;&#19968;&#20010;&#23433;&#20840;&#19987;&#19994;&#20154;&#22763;&#25776;&#20889;&#30340;TTP&#25551;&#36848;&#21487;&#33021;&#34987;&#21478;&#19968;&#20010;&#20154;&#35299;&#37322;&#24471;&#38750;&#24120;&#19981;&#21516;&#65292;&#36825;&#20250;&#23548;&#33268;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#29978;&#33267;&#21830;&#19994;&#12289;&#25919;&#31574;&#21644;&#27861;&#24459;&#20915;&#31574;&#30340;&#28151;&#28102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#24050;&#32463;&#23548;&#33268;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#22312;&#32593;&#32476;&#25805;&#20316;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;LLM&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;NLP&#20219;&#21153;&#24471;&#21040;&#20102;&#26174;&#30528;&#30340;&#25913;&#21892;&#12290;&#36825;&#35753;&#25105;&#20204;&#36136;&#30097;LLM&#22312;&#22914;&#20309;&#35299;&#37322;TTP&#25110;&#19968;&#33324;&#32593;&#32476;&#25915;&#20987;&#25551;&#36848;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#30452;&#25509;&#20351;&#29992;LLMs&#20197;&#21450;&#35757;&#32451;BaseLLMs&#20197;&#25551;&#36848;TTP&#24182;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20197;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;BaseLLM&#22823;&#22823;&#25913;&#21892;&#20102;TTP&#30340;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#32988;&#36807;&#39046;&#22495;&#19987;&#23478;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#20998;&#26512;&#30340;&#24433;&#21709;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The volume, variety, and velocity of change in vulnerabilities and exploits have made incident threat analysis challenging with human expertise and experience along. The MITRE AT&amp;CK framework employs Tactics, Techniques, and Procedures (TTPs) to describe how and why attackers exploit vulnerabilities. However, a TTP description written by one security professional can be interpreted very differently by another, leading to confusion in cybersecurity operations or even business, policy, and legal decisions. Meanwhile, advancements in AI have led to the increasing use of Natural Language Processing (NLP) algorithms to assist the various tasks in cyber operations. With the rise of Large Language Models (LLMs), NLP tasks have significantly improved because of the LLM's semantic understanding and scalability. This leads us to question how well LLMs can interpret TTP or general cyberattack descriptions. We propose and analyze the direct use of LLMs as well as training BaseLLMs with ATT&amp;CK desc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;-&#23616;&#37096;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GL-MAE&#65289;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26500;&#20840;&#23616;&#21644;&#23616;&#37096;&#25513;&#30721;&#35270;&#22270;&#26469;&#25913;&#21892;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.08913</link><description>&lt;p&gt;
&#36890;&#36807;&#20840;&#23616;&#23616;&#37096;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#25512;&#36827;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Advancing Volumetric Medical Image Segmentation via Global-Local Masked Autoencoder. (arXiv:2306.08913v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;-&#23616;&#37096;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GL-MAE&#65289;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26500;&#20840;&#23616;&#21644;&#23616;&#37096;&#25513;&#30721;&#35270;&#22270;&#26469;&#25913;&#21892;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23558;MAE&#30452;&#25509;&#24212;&#29992;&#20110;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#65288;&#19968;&#65289;&#32570;&#20047;&#23545;&#25972;&#20307;&#25968;&#25454;&#20020;&#24202;&#32972;&#26223;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#65288;&#20108;&#65289;&#26080;&#27861;&#20445;&#35777;&#20174;&#38543;&#26426;&#25513;&#30721;&#36755;&#20837;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#31283;&#23450;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;-&#23616;&#37096;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;GL-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#38500;&#20102;&#37325;&#26500;&#23616;&#37096;&#25513;&#30721;&#35270;&#22270;&#65292;GL-MAE&#36824;&#36890;&#36807;&#37325;&#26500;&#20840;&#23616;&#25513;&#30721;&#35270;&#22270;&#26469;&#34701;&#20837;&#20840;&#23616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23436;&#25972;&#30340;&#20840;&#23616;&#35270;&#22270;&#34987;&#20316;&#20026;&#38170;&#28857;&#26469;&#25351;&#23548;&#37325;&#26500;&#24182;&#36890;&#36807;&#20840;&#23616;&#19968;&#33268;&#24615;&#23398;&#20064;&#21644;&#20840;&#23616;-&#23616;&#37096;&#19968;&#33268;&#24615;&#23398;&#20064;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoder (MAE) is a promising self-supervised pre-training technique that can improve the representation learning of a neural network without human intervention. However, applying MAE directly to volumetric medical images poses two challenges: (i) a lack of global information that is crucial for understanding the clinical context of the holistic data, (ii) no guarantee of stabilizing the representations learned from randomly masked inputs. To address these limitations, we propose the \textbf{G}lobal-\textbf{L}ocal \textbf{M}asked \textbf{A}uto\textbf{E}ncoder (GL-MAE), a simple yet effective self-supervised pre-training strategy. In addition to reconstructing masked local views, as in previous methods, GL-MAE incorporates global context learning by reconstructing masked global views. Furthermore, a complete global view is integrated as an anchor to guide the reconstruction and stabilize the learning process through global-to-global consistency learning and global-to-local con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.06770</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#20998;&#26512;&#25552;&#39640; LLM &#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis. (arXiv:2306.06770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#20294;&#26159;&#21333;&#29420;&#30340;&#25552;&#31034;&#24037;&#31243;&#24182;&#19981;&#33021;&#20026;&#26426;&#22120;&#20154;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#24773;&#22659;&#30456;&#20851;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#25193;&#23637;&#21644;&#34917;&#20805;&#25552;&#31034;&#24037;&#31243;&#65292;&#32531;&#35299;&#20854;&#23616;&#38480;&#24615;&#65292;&#20197;&#27492;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#23545; LLMs &#20135;&#29983;&#30340;&#20505;&#36873;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#12289;&#20462;&#22797;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#20154;&#20174; LLM &#20013;&#26816;&#32034;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#19981;&#21516;&#21709;&#24212;&#21518;&#21487;&#20197;&#36798;&#21040;&gt;75% &#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#65292;&#26080;&#38656;&#29992;&#25143;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) offer significant promise as a knowledge source for robotic task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM but alone is insufficient for acquiring relevant, situationally grounded knowledge for an embodied robotic agent learning novel tasks. We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous robot, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how a robot, by retrieving and evaluating a breadth of responses from the LLM, can achieve &gt;75% task completion in one-shot learning without user oversight. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01792</link><description>&lt;p&gt;
&#20219;&#21153;&#20851;&#31995;&#24863;&#30693;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Relation-aware Continual User Representation Learning. (arXiv:2306.01792v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#23427;&#33021;&#22815;&#23398;&#20064;&#36890;&#29992;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23454;&#29992;&#24615;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#26159;&#22522;&#20110;&#20854;&#36807;&#21435;&#34892;&#20026;&#23398;&#20064;&#23558;&#29992;&#25143;&#34920;&#31034;&#20026;&#20302;&#32500;&#34920;&#31034;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#23427;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#20852;&#36259;&#28608;&#22686;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#24314;&#27169;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23398;&#20064;&#20026;&#21333;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20026;&#27599;&#20010;&#20219;&#21153;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#34920;&#31034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#27010;&#24565;&#65292;&#21363;&#19982;&#22810;&#31181;&#20219;&#21153;&#30456;&#20851;&#30340;&#26356;&#24191;&#20041;&#29992;&#25143;&#34920;&#31034;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38656;&#27714;&#12289;&#28798;&#38590;&#24615;&#36951;&#24536;&#20197;&#21450;&#20026;&#25345;&#32493;&#28155;&#21152;&#30340;&#20219;&#21153;&#25552;&#20379;&#26377;&#38480;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;&#23398;&#20064;&#36890;&#29992;&#29992;&#25143;&#34920;&#31034;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25345;&#32493;&#29992;&#25143;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;TERACON&#65292;&#20854;&#23398;&#20064;&#33021;&#21147;&#19981;&#21463;&#20219;&#21153;&#25968;&#37327;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
User modeling, which learns to represent users into a low-dimensional representation space based on their past behaviors, got a surge of interest from the industry for providing personalized services to users. Previous efforts in user modeling mainly focus on learning a task-specific user representation that is designed for a single task. However, since learning task-specific user representations for every task is infeasible, recent studies introduce the concept of universal user representation, which is a more generalized representation of a user that is relevant to a variety of tasks. Despite their effectiveness, existing approaches for learning universal user representations are impractical in real-world applications due to the data requirement, catastrophic forgetting and the limited learning capability for continually added tasks. In this paper, we propose a novel continual user representation learning method, called TERACON, whose learning capability is not limited as the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#25351;&#21335;&#65292;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#65292;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#34920;&#31034;&#65292;&#35814;&#32454;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#24182;&#35299;&#20915;&#20102;&#35768;&#22810;&#26041;&#27861;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10410</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#65306;&#32508;&#36848;&#19982;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review. (arXiv:2304.10410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#25351;&#21335;&#65292;&#20027;&#35201;&#20851;&#27880;&#24863;&#30693;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#65292;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#22788;&#29702;&#21644;&#34920;&#31034;&#65292;&#35814;&#32454;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#65292;&#24182;&#35299;&#20915;&#20102;&#35768;&#22810;&#26041;&#27861;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25512;&#21160;&#65292;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#24863;&#30693;&#25216;&#26415;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#20026;&#20102;&#23454;&#29616;&#31934;&#20934;&#21644;&#31283;&#20581;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#36890;&#24120;&#37197;&#22791;&#22810;&#31181;&#20256;&#24863;&#22120;&#65292;&#20351;&#24471;&#20256;&#24863;&#22120;&#34701;&#21512;&#25104;&#20026;&#24863;&#30693;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#22312;&#36825;&#20123;&#20256;&#24863;&#22120;&#20013;&#65292;&#38647;&#36798;&#21644;&#30456;&#26426;&#36890;&#36807;&#20114;&#34917;&#30340;&#26041;&#24335;&#65292;&#22312;&#20219;&#20309;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#19979;&#37117;&#33021;&#22815;&#25552;&#20379;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#21608;&#22260;&#29615;&#22659;&#24863;&#30693;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#30340;&#32508;&#21512;&#25351;&#21335;&#65292;&#29305;&#21035;&#20851;&#27880;&#19982;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#30456;&#20851;&#30340;&#24863;&#30693;&#20219;&#21153;&#12290;&#22522;&#20110;&#38647;&#36798;&#21644;&#30456;&#26426;&#20256;&#24863;&#22120;&#30340;&#21407;&#29702;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#25968;&#25454;&#22788;&#29702;&#36807;&#31243;&#21644;&#34920;&#31034;&#65292;&#25509;&#30528;&#35814;&#32454;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#12290;&#22312;&#25506;&#35752;&#38647;&#36798;&#30456;&#26426;&#34701;&#21512;&#30340;&#26041;&#27861;&#23398;&#26102;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#35768;&#22810;&#38382;&#39064;&#65292;&#21253;&#25324;&#8220;&#20026;&#20160;&#20040;&#34701;&#21512;&#8221;&#65292;&#8220;&#34701;&#21512;&#20160;&#20040;&#8221;&#65292;&#8220;&#22312;&#21738;&#37324;&#34701;&#21512;&#8221;
&lt;/p&gt;
&lt;p&gt;
Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including "why to fuse", "what to fuse", "where to fus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#30340;&#22810;&#27169;&#24577;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26381;&#35013;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26102;&#23578;&#25554;&#22270;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02051</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26381;&#35013;&#35774;&#35745;&#24072;&#65306;&#38754;&#21521;&#20154;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26102;&#23578;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing. (arXiv:2304.02051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#30340;&#22810;&#27169;&#24577;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26381;&#35013;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26102;&#23578;&#25554;&#22270;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#25554;&#22270;&#26159;&#35774;&#35745;&#24072;&#29992;&#26469;&#20256;&#36798;&#20182;&#20204;&#30340;&#35270;&#35273;&#21644;&#23558;&#35774;&#35745;&#29702;&#24565;&#20174;&#26500;&#24605;&#21040;&#23454;&#29616;&#65292;&#23637;&#31034;&#26381;&#35013;&#22914;&#20309;&#19982;&#20154;&#20307;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#26102;&#23578;&#35774;&#35745;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#26465;&#20214;&#30340;&#26102;&#23578;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#36890;&#36807;&#36319;&#38543;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#22914;&#25991;&#26412;&#12289;&#20154;&#20307;&#23039;&#21183;&#21644;&#26381;&#35013;&#33609;&#22270;&#65292;&#25351;&#23548;&#29983;&#25104;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26102;&#23578;&#22270;&#20687;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#21512;&#36825;&#39033;&#20219;&#21153;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23637;&#24320;&#20102;&#20004;&#20010;&#29616;&#26377;&#26102;&#23578;&#25968;&#25454;&#38598; Dress Code &#21644; VITON-HD&#65292;&#29992;&#21322;&#33258;&#21160;&#30340;&#26041;&#27861;&#34917;&#20805;&#20102;&#22810;&#27169;&#24577;&#27880;&#37322;&#12290;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion illustration is used by designers to communicate their vision and to bring the design idea from conceptualization to realization, showing how clothes interact with the human body. In this context, computer vision can thus be used to improve the fashion design process. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an approach that has not been used before in the fashion domain. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental results on these new datasets demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.17155</link><description>&lt;p&gt;
&#22522;&#20110;&#21028;&#21035;&#24615;&#31867;&#26631;&#30340;&#25991;&#26412;&#22270;&#29255;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discriminative Class Tokens for Text-to-Image Diffusion Models. (arXiv:2303.17155v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#24615;&#20449;&#24687;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;&#38750;&#20405;&#20837;&#24335;&#24494;&#35843;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#39640;&#20934;&#30830;&#29575;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#29255;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#27495;&#20041;&#65292;&#29983;&#25104;&#30340;&#22270;&#29255;&#24120;&#24120;&#26080;&#27861;&#25551;&#32472;&#20986;&#24494;&#22937;&#30340;&#32454;&#33410;&#19988;&#26131;&#20110;&#20986;&#38169;&#12290;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#22312;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#65306;&#65288;i&#65289;&#19982;&#29992;&#20110;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#29228;&#21462;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#26377;&#31867;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#36739;&#23567;&#65292;&#22240;&#27492;&#29983;&#25104;&#30340;&#22270;&#29255;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20250;&#20005;&#37325;&#21463;&#24433;&#21709;&#65292;&#25110;&#65288;ii&#65289;&#36755;&#20837;&#26159;&#30828;&#32534;&#30721;&#30340;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#33258;&#30001;&#25991;&#26412;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#24615;&#20449;&#21495;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26082;&#21457;&#25381;&#20102;&#33258;&#30001;&#25991;&#26412;&#30340;&#34920;&#36798;&#28508;&#21147;&#65292;&#21448;&#33021;&#22815;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have enabled the generation of diverse and high-quality images. However, generated images often fall short of depicting subtle details and are susceptible to errors due to ambiguity in the input text. One way of alleviating these issues is to train diffusion models on class-labeled datasets. This comes with a downside, doing so limits their expressive power: (i) supervised datasets are generally small compared to large-scale scraped text-image datasets on which text-to-image models are trained, and so the quality and diversity of generated images are severely affected, or (ii) the input is a hard-coded label, as opposed to free-form text, which limits the control over the generated images.  In this work, we propose a non-invasive fine-tuning technique that capitalizes on the expressive potential of free-form text while achieving high accuracy through discriminative signals from a pretrained classifier, which guides the generation. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13991</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#23398;&#20064;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#27867;&#21270;&#65306;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#30340;&#32763;&#35793;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize towards Unseen Domains via a Content-Aware Style Invariant Model for Disease Detection from Chest X-rays. (arXiv:2302.13991v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13991
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#23481;&#24863;&#30693;&#30340;&#39118;&#26684;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#26469;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#65292;&#22312;&#33016;&#37096;X&#23556;&#32447;&#30142;&#30149;&#26816;&#27979;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#19981;&#21305;&#37197;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65292;&#22810;&#39046;&#22495;&#28151;&#21512;&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#26126;&#30830;&#35268;&#33539;&#25552;&#21462;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#23545;&#39118;&#26684;&#65288;&#20363;&#22914;&#65292;&#26080;&#20449;&#24687;&#30340;&#32441;&#29702;&#65289;&#26377;&#24456;&#24378;&#30340;&#20559;&#22909;&#65292;&#32780;&#19981;&#26159;&#23545;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#24418;&#29366;&#65289;&#30340;&#20559;&#22909;&#65292;&#36825;&#19982;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25918;&#23556;&#31185;&#21307;&#24072;&#20542;&#21521;&#20110;&#20174;CXR&#22270;&#20687;&#20013;&#23398;&#20064;&#35270;&#35273;&#32447;&#32034;&#65292;&#24182;&#22240;&#27492;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#20174;CXR&#22270;&#20687;&#36827;&#34892;&#30149;&#29702;&#35786;&#26029;&#30340;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#27169;&#22411;&#24212;&#35813;&#25552;&#21462;&#26082;&#26159;&#39118;&#26684;&#19981;&#21464;&#21448;&#26159;&#20869;&#23481;&#20559;&#22909;&#30340;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#39118;&#26684;&#38543;&#26426;&#21270;&#27169;&#22359;&#65288;SRMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance degradation due to source domain mismatch is a longstanding challenge in deep learning-based medical image analysis, particularly for chest X-rays (CXRs). Several methods (e.g., adversarial training, multi-domain mixups) have been proposed to extract domain-invariant high-level features to address this domain shift. However, these methods do not explicitly regularize the content and style characteristics of the extracted domain-invariant features. Recent studies have demonstrated that CNN models exhibit a strong bias toward styles (e.g., uninformative textures) rather than content (e.g., shape), in stark contrast to the human-vision system. Radiologists tend to learn visual cues from CXRs and thus perform well across multiple domains. Therefore, in medical imaging for pathology diagnosis from CXR images, models should extract domain-invariant features that are style-invariant and content-biased. Motivated by this, we employ the novel style randomization modules (SRMs) at bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#39550;&#39542;&#34892;&#20026;&#25968;&#25454;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20026;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#36866;&#24212;&#24615;&#21453;&#39304;&#21644;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;</title><link>http://arxiv.org/abs/2302.10898</link><description>&lt;p&gt;
&#20174;&#36947;&#36335;&#34892;&#39542;&#25968;&#25454;&#20013;&#20272;&#35745;&#39550;&#39542;&#21592;&#20010;&#24615;&#29305;&#36136;
&lt;/p&gt;
&lt;p&gt;
Estimating Driver Personality Traits from On-Road Driving Data. (arXiv:2302.10898v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#39550;&#39542;&#34892;&#20026;&#25968;&#25454;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20026;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#25552;&#20379;&#36866;&#24212;&#24615;&#21453;&#39304;&#21644;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#21033;&#29992;&#39550;&#39542;&#25968;&#25454;&#26469;&#20272;&#35745;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#29305;&#24449;&#65292;&#20197;&#29992;&#20110;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#12290;&#36866;&#24212;&#20010;&#20307;&#24515;&#29702;&#29305;&#24449;&#30340;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#36866;&#24403;&#30340;&#21453;&#39304;&#24182;&#39044;&#38450;&#20132;&#36890;&#20107;&#25925;&#12290;&#20316;&#20026;&#23454;&#29616;&#36825;&#26679;&#33258;&#36866;&#24212;&#36741;&#21161;&#31995;&#32479;&#30340;&#31532;&#19968;&#27493;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20174;&#22312;&#36335;&#34892;&#39542;&#30340;&#34892;&#20026;&#25968;&#25454;&#20013;&#24320;&#21457;&#19968;&#20010;&#20272;&#35745;&#39550;&#39542;&#21592;&#24515;&#29702;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#35748;&#30693;&#21151;&#33021;&#12289;&#24515;&#29702;&#39550;&#39542;&#39118;&#26684;&#21644;&#24037;&#20316;&#36127;&#33655;&#25935;&#24863;&#24230;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22238;&#24402;&#24314;&#27169;&#30740;&#31350;&#20102;&#39550;&#39542;&#34892;&#20026;&#19982;&#21508;&#31181;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#36861;&#36394;&#21046;&#22270;&#27979;&#35797;&#65288;TMT&#65289;&#21644;&#26377;&#29992;&#35270;&#37326;&#65288;UFOV&#65289;&#27979;&#35797;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20851;&#27880;&#36947;&#36335;&#31867;&#22411;&#20449;&#24687;&#65292;&#24182;&#25429;&#25417;&#20174;&#39550;&#39542;&#34892;&#20026;&#20013;&#35266;&#23519;&#21040;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25345;&#32493;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#39550;&#39542;&#26102;&#38388;&#36827;&#34892;&#20998;&#27573;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on the estimation of a driver's psychological characteristics using driving data for driving assistance systems. Driving assistance systems that support drivers by adapting individual psychological characteristics can provide appropriate feedback and prevent traffic accidents. As a first step toward implementing such adaptive assistance systems, this research aims to develop a model to estimate drivers' psychological characteristics, such as cognitive function, psychological driving style, and workload sensitivity, from on-road driving behavioral data using machine learning and deep learning techniques. We also investigated the relationship between driving behavior and various cognitive functions, including the Trail Making Test (TMT) and Useful Field of View (UFOV) test, through regression modeling. The proposed method focuses on road type information and captures various durations of time-series data observed from driving behaviors. First, we segment the driving ti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#35268;&#27169;&#40065;&#26834;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#21327;&#35758;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2302.08835</link><description>&lt;p&gt;
h&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
h-analysis and data-parallel physics-informed neural networks. (arXiv:2302.08835v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#21487;&#20197;&#23454;&#29616;&#20855;&#26377;&#35268;&#27169;&#40065;&#26834;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#21327;&#35758;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20250;&#24433;&#21709;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21487;&#25511;&#24615;&#65292;&#20026;&#23454;&#29616;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#29289;&#29702;&#21551;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#26041;&#26696;&#30340;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#65292;&#22312;&#22810;&#20010;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#65288;GPUs&#65289;&#20307;&#31995;&#32467;&#26500;&#19979;&#20851;&#27880;&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#12290;&#20026;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#22797;&#26434;&#24212;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;&#21644;&#39640;&#21534;&#21520;&#37327;&#30340;PIML&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#28041;&#21450;&#22797;&#26434;&#21644;&#39640;&#32500;&#39046;&#22495;&#12289;&#38750;&#32447;&#24615;&#25805;&#20316;&#31526;&#25110;&#22810;&#29289;&#29702;&#23398;&#65289;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$h$-&#20998;&#26512;&#21644;&#36890;&#36807;Horovod&#35757;&#32451;&#26694;&#26550;&#36827;&#34892;&#25968;&#25454;&#24182;&#34892;&#21152;&#36895;&#30340;&#26032;&#21327;&#35758;&#12290;&#35813;&#21327;&#35758;&#22522;&#20110;&#23545;&#27867;&#21270;&#35823;&#24046;&#21644;&#35757;&#32451;-&#27979;&#35797;&#24046;&#36317;&#30340;&#26032;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#34920;&#26126;&#21152;&#36895;&#23454;&#29616;&#31616;&#21333;&#65292;&#19981;&#20250;&#25439;&#23475;&#35757;&#32451;&#65292;&#24182;&#19988;&#35777;&#26126;&#26159;&#39640;&#25928;&#21644;&#21487;&#25511;&#30340;&#65292;&#20026;&#36890;&#29992;&#30340;&#35268;&#27169;&#40065;&#26834;PIML&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#36890;&#36807;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;
&lt;/p&gt;
&lt;p&gt;
We explore the data-parallel acceleration of physics-informed machine learning (PIML) schemes, with a focus on physics-informed neural networks (PINNs) for multiple graphics processing units (GPUs) architectures. In order to develop scale-robust and high-throughput PIML models for sophisticated applications which may require a large number of training points (e.g., involving complex and high-dimensional domains, non-linear operators or multi-physics), we detail a novel protocol based on $h$-analysis and data-parallel acceleration through the Horovod training framework. The protocol is backed by new convergence bounds for the generalization error and the train-test gap. We show that the acceleration is straightforward to implement, does not compromise training, and proves to be highly efficient and controllable, paving the way towards generic scale-robust PIML. Extensive numerical experiments with increasing complexity illustrate its robustness and consistency, offering a wide range of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2302.06912</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#24724;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regret-Based Optimization for Robust Reinforcement Learning. (arXiv:2302.06912v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#24724;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#35266;&#27979;&#20013;&#30340;&#23545;&#25239;&#24615;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#23545;&#35266;&#27979;&#20013;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#22122;&#22768;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#31181;&#23545;&#25239;&#24615;&#22122;&#22768;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#29616;&#26377;&#30340;&#20351;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35266;&#27979;&#25200;&#21160;&#30340;&#23545;&#25239;&#31574;&#30053;&#20027;&#35201;&#38598;&#20013;&#22312;&#36845;&#20195;&#25913;&#36827;&#27599;&#20010;&#36845;&#20195;&#20013;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#26222;&#36890;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#26159;&#34987;&#21160;&#24615;&#30340;&#65292;&#22914;&#26524;&#26576;&#20123;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20135;&#29983;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36861;&#27714;&#19968;&#31181;&#26356;&#31215;&#26497;&#30340;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#30452;&#25509;&#20248;&#21270;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#40065;&#26834;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable to small adversarial noise in observations. Such adversarial noise can have disastrous consequences in safety-critical environments. For instance, a self-driving car receiving adversarially perturbed sensory observations about nearby signs (e.g., a stop sign physically altered to be perceived as a speed limit sign) or objects (e.g., cars altered to be recognized as trees) can be fatal. Existing approaches for making RL algorithms robust to an observation-perturbing adversary have focused on reactive approaches that iteratively improve against adversarial examples generated at each iteration. While such approaches have been shown to provide improvements over regular RL methods, they are reactive and can fare significantly worse if certain categories of adversarial examples are not generated during training. To that end, we pursue a more proactive approach that relies on directly optimizing a well-studied robustn
&lt;/p&gt;</description></item><item><title>BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;</title><link>http://arxiv.org/abs/2301.09091</link><description>&lt;p&gt;
BallGAN: &#24102;&#26377;&#29699;&#24418;&#32972;&#26223;&#30340;3D&#24863;&#30693;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
BallGAN: 3D-aware Image Synthesis with a Spherical Background. (arXiv:2301.09091v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09091
&lt;/p&gt;
&lt;p&gt;
BallGAN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;3D&#24863;&#30693;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#32422;&#26463;&#26469;&#20943;&#23569;&#32972;&#26223;&#33258;&#30001;&#24230;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#24863;&#30693;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26088;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;3D&#22330;&#26223;&#65292;&#20197;&#20415;&#21487;&#20197;&#20197;&#20219;&#24847;&#35282;&#24230;&#36827;&#34892;&#28210;&#26579;&#20197;&#20135;&#29983;&#22270;&#20687;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#23384;&#22312;&#19981;&#33258;&#28982;&#30340;3D&#20960;&#20309;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;3D&#20960;&#20309;&#22312;&#32422;&#26463;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#30830;&#23450;&#30340;&#65292;&#21363;&#20165;&#23558;&#20854;&#20998;&#31867;&#20026;&#30495;&#23454;&#22270;&#20687;&#23545;&#20110;&#37492;&#21035;&#22120;&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32972;&#26223;&#36817;&#20284;&#20026;&#29699;&#24418;&#34920;&#38754;&#65292;&#24182;&#23558;&#22330;&#26223;&#34920;&#31034;&#20026;&#25918;&#32622;&#22312;&#29699;&#20307;&#20013;&#30340;&#21069;&#26223;&#21644;&#34180;&#29699;&#24418;&#32972;&#26223;&#30340;&#32852;&#21512;&#12290;&#36825;&#26679;&#21487;&#20197;&#20943;&#23569;&#32972;&#26223;&#22330;&#30340;&#33258;&#30001;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20307;&#28210;&#26579;&#26041;&#31243;&#65292;&#24182;&#21152;&#20837;&#20102;&#19987;&#29992;&#30340;&#32422;&#26463;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;BallGAN&#30340;&#26032;&#22411;3D&#24863;&#30693;GAN&#26694;&#26550;&#12290; BallGAN&#20855;&#26377;&#20197;&#19979;&#22810;&#20010;&#20248;&#28857;&#12290;1&#65289;&#23427;&#20135;&#29983;&#20102;&#26356;&#21512;&#29702;&#30340;3D&#20960;&#20309;&#65307;&#22330;&#26223;&#22312;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22270;&#20687;&#20855;&#26377;&#26356;&#22909;&#30340;&#20809;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D-aware GANs aim to synthesize realistic 3D scenes such that they can be rendered in arbitrary perspectives to produce images. Although previous methods produce realistic images, they suffer from unstable training or degenerate solutions where the 3D geometry is unnatural. We hypothesize that the 3D geometry is underdetermined due to the insufficient constraint, i.e., being classified as real image to the discriminator is not enough. To solve this problem, we propose to approximate the background as a spherical surface and represent a scene as a union of the foreground placed in the sphere and the thin spherical background. It reduces the degree of freedom in the background field. Accordingly, we modify the volume rendering equation and incorporate dedicated constraints to design a novel 3D-aware GAN framework named BallGAN. BallGAN has multiple advantages as follows. 1) It produces more reasonable 3D geometry; the images of a scene across different viewpoints have better photometric 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.04785</link><description>&lt;p&gt;
&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Phase-shifted Adversarial Training. (arXiv:2301.04785v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#35748;&#20026;&#26159;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#26356;&#26032;&#27493;&#39588;&#30340;&#25968;&#37327;&#12289;&#20351;&#29992;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#21450;&#23558;&#38543;&#26426;&#24615;&#27880;&#20837;&#21040;&#25915;&#20987;&#20013;&#26469;&#29983;&#25104;&#24378;&#26377;&#21147;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#21709;&#24212;&#39057;&#29575;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#23545;&#25239;&#35757;&#32451;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#25910;&#25947;&#24615;&#36739;&#20302;&#65292;&#20174;&#32780;&#22312;&#27599;&#20010;&#25968;&#25454;&#38468;&#36817;&#20135;&#29983;&#39640;&#24230;&#25391;&#33633;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#39057;&#29575;&#21407;&#29702;&#30340;&#26222;&#36941;&#29616;&#35937;&#65292;&#21363;\textit{&#36739;&#20302;&#30340;&#39057;&#29575;&#20808;&#23398;&#20064;}&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20173;&#28982;&#25104;&#31435;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20301;&#20559;&#31227;&#23545;&#25239;&#35757;&#32451;(PhaseAT)&#65292;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#39640;&#39057;&#20869;&#23481;&#26469;&#25913;&#21892;&#23545;&#25239;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been considered an imperative component for safely deploying neural network-based applications to the real world. To achieve stronger robustness, existing methods primarily focus on how to generate strong attacks by increasing the number of update steps, regularizing the models with the smoothed loss function, and injecting the randomness into the attack. Instead, we analyze the behavior of adversarial training through the lens of response frequency. We empirically discover that adversarial training causes neural networks to have low convergence to high-frequency information, resulting in highly oscillated predictions near each data. To learn high-frequency contents efficiently and effectively, we first prove that a universal phenomenon of frequency principle, i.e., \textit{lower frequencies are learned first}, still holds in adversarial training. Based on that, we propose phase-shifted adversarial training (PhaseAT) in which the model learns high-frequency com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.14055</link><description>&lt;p&gt;
&#24102;&#21453;&#39304;&#30340;&#31574;&#30053;&#24341;&#23548;&#25042;&#24816;&#25628;&#32034;&#29992;&#20110;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Policy-Guided Lazy Search with Feedback for Task and Motion Planning. (arXiv:2210.14055v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;LAZY&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#23398;&#20064;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a solver LAZY for PDDLStream problems, which maintains a single integrated search over action skeletons, gradually becoming more geometrically informed as samples of possible motions are lazily drawn during motion planning. Meanwhile, learned models of goal-directed policies and current motion sampling data are incorporated in LAZY to adaptively guide the task planner, leading to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions.
&lt;/p&gt;
&lt;p&gt;
PDDLStream&#27714;&#35299;&#22120;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;PDDL&#25193;&#23637;&#21040;&#20855;&#26377;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;PDDLStream&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#31995;&#21015;PDDL&#35268;&#21010;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#35268;&#21010;&#22120;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#36816;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LAZY&#65292;&#19968;&#31181;&#29992;&#20110;PDDLStream&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#65292;&#23427;&#22312;&#21160;&#20316;&#39592;&#26550;&#19978;&#32500;&#25252;&#21333;&#20010;&#38598;&#25104;&#25628;&#32034;&#65292;&#38543;&#30528;&#22312;&#36816;&#21160;&#35268;&#21010;&#26399;&#38388;&#25042;&#24816;&#22320;&#32472;&#21046;&#21487;&#33021;&#36816;&#21160;&#30340;&#26679;&#26412;&#65292;&#36880;&#28176;&#21464;&#24471;&#26356;&#20855;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#30446;&#26631;&#23548;&#21521;&#31574;&#30053;&#30340;&#23398;&#20064;&#27169;&#22411;&#21644;&#24403;&#21069;&#36816;&#21160;&#37319;&#26679;&#25968;&#25454;&#21512;&#24182;&#21040;LAZY&#20013;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24341;&#23548;&#20219;&#21153;&#35268;&#21010;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#23545;&#35937;&#12289;&#30446;&#26631;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#26410;&#35265;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#21487;&#34892;&#35299;&#25628;&#32034;&#30340;&#26174;&#30528;&#21152;&#36895;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;TAMP&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed, as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;ProtoBandit&#31639;&#27861;&#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#21407;&#22411;&#36873;&#25321;&#65292;&#36991;&#20813;&#20102;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#26114;&#36149;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#32452;&#32039;&#20945;&#30340;&#21407;&#22411;&#23454;&#20363;&#65292;&#26377;&#25928;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.01860</link><description>&lt;p&gt;
ProtoBandit: &#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#23454;&#29616;&#39640;&#25928;&#21407;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
ProtoBandit: Efficient Prototype Selection via Multi-Armed Bandits. (arXiv:2210.01860v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;ProtoBandit&#31639;&#27861;&#36890;&#36807;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#21407;&#22411;&#36873;&#25321;&#65292;&#36991;&#20813;&#20102;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#19979;&#36827;&#34892;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#26114;&#36149;&#24615;&#65292;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#32452;&#32039;&#20945;&#30340;&#21407;&#22411;&#23454;&#20363;&#65292;&#26377;&#25928;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#28304;&#25968;&#25454;&#38598;S&#20013;&#35782;&#21035;&#19968;&#32452;&#32039;&#20945;&#30340;&#20449;&#24687;&#25968;&#25454;&#23454;&#20363;&#65288;&#21363;&#21407;&#22411;&#65289;&#65292;&#20197;&#26368;&#22909;&#22320;&#20195;&#34920;&#32473;&#23450;&#30340;&#30446;&#26631;&#38598;T&#12290;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#31034;&#20363;&#25552;&#20379;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#21487;&#35299;&#37322;&#24615;&#27934;&#23519;&#65292;&#24182;&#22312;&#22522;&#20110;&#23454;&#20363;&#30340;&#25512;&#29702;&#20013;&#36215;&#21040;&#36741;&#21161;&#20316;&#29992;&#65292;&#20174;&#32780;&#24433;&#21709;&#20154;&#31867;&#20915;&#31574;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#38656;&#35201;&#22312;&#28304;&#25968;&#25454;&#28857;&#21644;&#30446;&#26631;&#25968;&#25454;&#28857;&#20043;&#38388;&#36827;&#34892;O&#65288;|S| |T|&#65289;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#35774;&#32622;&#26469;&#35828;&#26174;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22312;&#21407;&#22411;&#31034;&#20363;&#31354;&#38388;&#20013;&#37319;&#29992;&#38543;&#26426;&#36138;&#23146;&#25628;&#32034;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#26469;&#20943;&#23569;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#25968;&#37327;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;ProtoBandit&#33021;&#22815;&#22312;&#20135;&#29983;O&#65288;k^3 |S|&#65289;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#20986;&#19968;&#32452;k&#20010;&#21407;&#22411;&#65292;&#36825;&#19982;&#30446;&#26631;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a multi-armed bandit-based framework for identifying a compact set of informative data instances (i.e., the prototypes) from a source dataset $S$ that best represents a given target set $T$. Prototypical examples of a given dataset offer interpretable insights into the underlying data distribution and assist in example-based reasoning, thereby influencing every sphere of human decision-making. Current state-of-the-art prototype selection approaches require $O(|S||T|)$ similarity comparisons between source and target data points, which becomes prohibitively expensive for large-scale settings. We propose to mitigate this limitation by employing stochastic greedy search in the space of prototypical examples and multi-armed bandits for reducing the number of similarity comparisons. Our randomized algorithm, ProtoBandit, identifies a set of $k$ prototypes incurring $O(k^3|S|)$ similarity comparisons, which is independent of the size of the target set. An interesting
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.11355</link><description>&lt;p&gt;
&#20174;&#21018;&#20307;&#22270;&#20687;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65288;&#26410;&#30693;&#36136;&#37327;&#20998;&#24067;&#65289;
&lt;/p&gt;
&lt;p&gt;
Learning to predict 3D rotational dynamics from images of a rigid body with unknown mass distribution. (arXiv:2209.11355v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#65292;&#20174;&#21018;&#20307;&#22270;&#20687;&#24207;&#21015;&#20013;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#24403;&#20302;&#32500;&#24230;&#27979;&#37327;&#19981;&#21487;&#29992;&#26102;&#65292;&#20250;&#26377;&#33258;&#30001;&#26059;&#36716;&#30340;3D&#21018;&#20307;&#30340;&#22270;&#20687;&#35266;&#23519;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#25968;&#25454;&#30340;&#39640;&#32500;&#25968;&#38459;&#27490;&#20102;&#20351;&#29992;&#32463;&#20856;&#20272;&#35745;&#25216;&#26415;&#26469;&#23398;&#20064;&#21160;&#24577;&#12290;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#20063;&#21463;&#38480;&#20110;&#19968;&#20010;&#21018;&#20307;&#22270;&#20687;&#26080;&#27861;&#25581;&#31034;&#20307;&#20869;&#36136;&#37327;&#20998;&#24067;&#65292;&#32780;&#36136;&#37327;&#20998;&#24067;&#19982;&#21021;&#22987;&#35282;&#36895;&#24230;&#19968;&#36215;&#20915;&#23450;&#21018;&#20307;&#26059;&#36716;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#20272;&#35745;&#21644;&#39044;&#27979;3D&#26059;&#36716;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#32423;&#39044;&#27979;&#27969;&#31243;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#27969;&#31243;&#23558;&#21333;&#20010;&#22270;&#20687;&#26144;&#23556;&#21040;&#19982; $\mathbf{SO}(3)$ &#21516;&#32986;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#20174;&#28508;&#22312;&#23545;&#20013;&#35745;&#31639;&#35282;&#36895;&#24230;&#65292;&#24182;&#20351;&#29992;Hamilton&#36816;&#21160;&#26041;&#31243;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#26059;&#36716;&#21018;&#20307;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world settings, image observations of freely rotating 3D rigid bodies, may be available when low-dimensional measurements are not. However, the high-dimensionality of image data precludes the use of classical estimation techniques to learn the dynamics. The usefulness of standard deep learning methods is also limited because an image of a rigid body reveals nothing about the distribution of mass inside the body, which, together with initial angular velocity, is what determines how the body will rotate. We present a physics-informed neural network model to estimate and predict 3D rotational dynamics from image sequences. We achieve this using a multi-stage prediction pipeline that maps individual images to a latent representation homeomorphic to $\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts future latent states using the Hamiltonian equations of motion. We demonstrate the efficacy of our approach on new rotating rigid-body datasets of sequenc
&lt;/p&gt;</description></item><item><title>EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.08996</link><description>&lt;p&gt;
EDO-Net: &#20174;&#22270;&#21160;&#21147;&#23398;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
EDO-Net: Learning Elastic Properties of Deformable Objects from Graph Dynamics. (arXiv:2209.08996v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08996
&lt;/p&gt;
&lt;p&gt;
EDO-Net&#26159;&#19968;&#20010;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#24377;&#24615;&#23646;&#24615;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#26410;&#26469;&#29366;&#24577;&#30340;&#39044;&#27979;&#21644;&#36716;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#22270;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#21487;&#25552;&#21462;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#24377;&#24615;&#29289;&#29702;&#23646;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20363;&#22914;&#20174;&#25289;&#20280;&#20132;&#20114;&#20013;&#25552;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EDO-Net&#65288;&#24377;&#24615;&#21487;&#21464;&#24418;&#23545;&#35937;-Net&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#20855;&#26377;&#19981;&#21516;&#24377;&#24615;&#23646;&#24615;&#30340;&#22823;&#37327;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#22270;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#19981;&#20381;&#36182;&#20110;&#23646;&#24615;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;EDO-Net&#20849;&#21516;&#23398;&#20064;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#22359;&#21644;&#19968;&#20010;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22359;&#12290;&#21069;&#32773;&#36127;&#36131;&#25552;&#21462;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#39044;&#27979;&#20197;&#22270;&#24418;&#34920;&#31034;&#30340;&#31867;&#20284;&#24067;&#26009;&#30340;&#23545;&#35937;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#20102;EDO-Net&#30340;&#33021;&#21147;&#65306;1&#65289;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;2&#65289;&#36716;&#31227;&#23398;&#20064;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned rep
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65292;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#65292;&#24182;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#36319;&#36394;&#38556;&#30861;&#29289;&#12290;</title><link>http://arxiv.org/abs/2209.08258</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#23548;&#33322;&#21644;&#36991;&#38556;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65288;arXiv:2209.08258v3 [cs.RO] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A real-time dynamic obstacle tracking and mapping system for UAV navigation and collision avoidance with an RGB-D camera. (arXiv:2209.08258v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08258
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#65292;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;&#31995;&#32479;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#65292;&#24182;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#36319;&#36394;&#38556;&#30861;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#30340;&#31354;&#38388;&#20013;&#65292;&#23454;&#26102;&#21160;&#24577;&#29615;&#22659;&#24863;&#30693;&#23545;&#20110;&#33258;&#20027;&#26426;&#22120;&#20154;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#27969;&#34892;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#24314;&#22270;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#20855;&#26377;&#20219;&#24847;&#22797;&#26434;&#24418;&#29366;&#30340;3D&#38556;&#30861;&#29289;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#21306;&#20998;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#65292;&#23548;&#33268;&#36991;&#38556;&#24615;&#33021;&#26377;&#38480;&#12290;&#34429;&#28982;&#33258;&#20027;&#39550;&#39542;&#20013;&#23384;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#26080;&#27861;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#23454;&#26102;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RGB-D&#30456;&#26426;&#30340;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#36991;&#38556;&#30340;&#23454;&#26102;&#21160;&#24577;&#38556;&#30861;&#29289;&#36319;&#36394;&#21644;&#24314;&#22270;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#39318;&#20808;&#21033;&#29992;&#24102;&#26377;&#21344;&#25454;&#20307;&#32032;&#22320;&#22270;&#30340;&#28145;&#24230;&#22270;&#20687;&#29983;&#25104;&#28508;&#22312;&#30340;&#21160;&#24577;&#38556;&#30861;&#29289;&#21306;&#22495;&#20316;&#20026;&#20505;&#36873;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#25105;&#20204;&#30340;&#36830;&#32493;&#24615;&#28388;&#27874;&#22120;&#26469;&#36319;&#36394;&#27599;&#20010;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;&#29615;&#22659;&#27169;&#22411;&#21644;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#25552;&#20379;&#23548;&#33322;&#21644;&#36991;&#38556;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
The real-time dynamic environment perception has become vital for autonomous robots in crowded spaces. Although the popular voxel-based mapping methods can efficiently represent 3D obstacles with arbitrarily complex shapes, they can hardly distinguish between static and dynamic obstacles, leading to the limited performance of obstacle avoidance. While plenty of sophisticated learning-based dynamic obstacle detection algorithms exist in autonomous driving, the quadcopter's limited computation resources cannot achieve real-time performance using those approaches. To address these issues, we propose a real-time dynamic obstacle tracking and mapping system for quadcopter obstacle avoidance using an RGB-D camera. The proposed system first utilizes a depth image with an occupancy voxel map to generate potential dynamic obstacle regions as proposals. With the obstacle region proposals, the Kalman filter and our continuity filter are applied to track each dynamic obstacle. Finally, the environ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.07003</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#30340;&#35270;&#35273;&#36741;&#21161;&#26080;&#20154;&#26426;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;
&lt;/p&gt;
&lt;p&gt;
Vision-aided UAV navigation and dynamic obstacle avoidance using gradient-based B-spline trajectory optimization. (arXiv:2209.07003v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#26080;&#20154;&#26426;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23548;&#33322;&#21644;&#21160;&#24577;&#36991;&#38556;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#38656;&#35201;&#26426;&#22120;&#20154;&#29983;&#25104;&#26080;&#30896;&#25758;&#36712;&#36857;&#24182;&#20027;&#21160;&#36991;&#24320;&#31227;&#21160;&#38556;&#30861;&#29289;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#35774;&#35745;&#20102;&#22522;&#20110;&#21333;&#19968;&#22320;&#22270;&#34920;&#31034;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#65292;&#22914;&#20960;&#20309;&#22320;&#22270;&#12289;&#21344;&#29992;&#22320;&#22270;&#25110;ESDF&#22320;&#22270;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#22320;&#22270;&#34920;&#31034;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#21487;&#38752;&#22320;&#21516;&#26102;&#22788;&#29702;&#38745;&#24577;&#21644;&#21160;&#24577;&#38556;&#30861;&#29289;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;B&#26679;&#26465;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#20869;&#32622;&#35270;&#35273;&#12290;&#28145;&#24230;&#35270;&#35273;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22522;&#20110;&#20307;&#32032;&#22320;&#22270;&#23545;&#21160;&#24577;&#29289;&#20307;&#36827;&#34892;&#20960;&#20309;&#36319;&#36394;&#21644;&#34920;&#31034;&#12290;&#25552;&#20986;&#30340;&#20248;&#21270;&#39318;&#20808;&#37319;&#29992;&#22522;&#20110;&#22278;&#30340;&#23548;&#21521;&#28857;&#31639;&#27861;&#26469;&#36817;&#20284;&#36991;&#24320;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#20195;&#20215;&#21644;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#35270;&#35273;&#26816;&#27979;&#21040;&#30340;&#31227;&#21160;&#29289;&#20307;&#65292;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#22238;&#36864;&#35270;&#37326;&#36317;&#31163;&#22330;&#26469;&#38450;&#27490;&#21160;&#24577;&#30896;&#25758;&#12290;&#26368;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Navigating dynamic environments requires the robot to generate collision-free trajectories and actively avoid moving obstacles. Most previous works designed path planning algorithms based on one single map representation, such as the geometric, occupancy, or ESDF map. Although they have shown success in static environments, due to the limitation of map representation, those methods cannot reliably handle static and dynamic obstacles simultaneously. To address the problem, this paper proposes a gradient-based B-spline trajectory optimization algorithm utilizing the robot's onboard vision. The depth vision enables the robot to track and represent dynamic objects geometrically based on the voxel map. The proposed optimization first adopts the circle-based guide-point algorithm to approximate the costs and gradients for avoiding static obstacles. Then, with the vision-detected moving objects, our receding-horizon distance field is simultaneously used to prevent dynamic collisions. Finally,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#35780;&#20272;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Kernel SHAP&#22312;&#32508;&#21512;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2205.12423</link><description>&lt;p&gt;
&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22238;&#24402;&#27169;&#22411;&#20013;&#35780;&#20272;&#21024;&#38500;&#21644;&#25554;&#20837;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31639;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;Kernel SHAP&#22312;&#32508;&#21512;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#25351;&#26631;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#26159;&#30830;&#23450;&#40657;&#30418;&#20989;&#25968;$f$&#39044;&#27979;&#32972;&#21518;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;Petsiuk&#31561;&#20154;&#65288;2018&#65289;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#27979;&#35797;&#21487;&#20197;&#29992;&#26469;&#35780;&#21028;&#23545;&#20110;&#20998;&#31867;&#20013;&#20687;&#32032;&#20174;&#37325;&#35201;&#21040;&#19981;&#37325;&#35201;&#36827;&#34892;&#25490;&#24207;&#30340;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24335;&#65292;&#20197;$f$&#30340;&#20027;&#25928;&#24212;&#21644;&#20132;&#20114;&#20316;&#29992;&#26469;&#34913;&#37327;&#20854;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22312;&#36755;&#20837;&#38543;&#26426;&#39034;&#24207;&#19979;AUC&#30340;&#26399;&#26395;&#20540;&#30340;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#22238;&#24402;&#35774;&#32622;&#30340;&#30452;&#32447;&#19978;&#26041;&#38754;&#31215;&#30340;&#26367;&#20195;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25351;&#26631;&#23558;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#19982;Kernel SHAP&#65288;KS&#65289;&#12289;LIME&#12289;DeepLIFT&#12289;vanilla gradient&#21644;input$\times$gradient&#26041;&#27861;&#35745;&#31639;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#25105;&#20204;&#32771;&#34385;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;KS&#30340;&#25972;&#20307;&#34920;&#29616;&#26368;&#22909;&#65292;&#20294;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#25105;&#20204;&#21457;&#29616;IG&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#21644;KS&#34920;&#29616;&#30456;&#36817;&#65292;&#20294;&#35745;&#31639;&#26356;&#24555;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26816;&#26597;&#20849;&#20139;&#27169;&#22411;&#26356;&#26032;&#26469;&#35782;&#21035;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#26799;&#24230;&#23376;&#38598;&#30340;&#32479;&#35745;&#20998;&#24067;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#35782;&#21035;&#21518;&#38376;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2202.04311</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#24120;&#26816;&#27979;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Identifying Backdoor Attacks in Federated Learning via Anomaly Detection. (arXiv:2202.04311v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26816;&#26597;&#20849;&#20139;&#27169;&#22411;&#26356;&#26032;&#26469;&#35782;&#21035;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#26799;&#24230;&#23376;&#38598;&#30340;&#32479;&#35745;&#20998;&#24067;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#35782;&#21035;&#21518;&#38376;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#38544;&#31169;&#30340;&#30417;&#31649;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#32852;&#37030;&#23398;&#20064;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#36879;&#26126;&#30340;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20063;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#24544;&#35802;&#24230;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#32852;&#37030;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#21518;&#38376;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#19979;&#65292;&#34987;&#20837;&#20405;&#21442;&#19982;&#32773;&#21487;&#20197;&#24708;&#24708;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;&#20849;&#20139;&#27169;&#22411;&#26356;&#26032;&#26469;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#21518;&#38376;&#30340;&#23884;&#20837;&#20250;&#20197;&#27169;&#22411;&#26799;&#24230;&#30340;&#22823;&#23567;&#21644;&#26041;&#21521;&#30340;&#24418;&#24335;&#24433;&#21709;&#21442;&#19982;&#32773;&#30340;&#26412;&#22320;&#27169;&#22411;&#26435;&#37325;&#65292;&#36825;&#21487;&#20197;&#34920;&#29616;&#20026;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#26799;&#24230;&#23376;&#38598;&#30340;&#32479;&#35745;&#20998;&#24067;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#21518;&#38376;&#36827;&#34892;&#40065;&#26834;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has seen increased adoption in recent years in response to the growing regulatory demand for data privacy. However, the opaque local training process of federated learning also sparks rising concerns about model faithfulness. For instance, studies have revealed that federated learning is vulnerable to backdoor attacks, whereby a compromised participant can stealthily modify the model's behavior in the presence of backdoor triggers. This paper proposes an effective defense against the attack by examining shared model updates. We begin with the observation that the embedding of backdoors influences the participants' local model weights in terms of the magnitude and orientation of their model gradients, which can manifest as distinguishable disparities. We enable a robust identification of backdoors by studying the statistical distribution of the models' subsets of gradients. Concretely, we first segment the model gradients into fragment vectors that represent small por
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#21453;&#39304;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#36825;&#19968;&#20851;&#38190;&#32852;&#31995;&#19978;&#30340;&#25506;&#32034;&#20173;&#26377;&#24453;&#21152;&#24378;&#12290;</title><link>http://arxiv.org/abs/2112.01575</link><description>&lt;p&gt;
&#21521;&#20855;&#26377;&#20869;&#22312;&#21453;&#39304;&#30340;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Interactive Reinforcement Learning with Intrinsic Feedback. (arXiv:2112.01575v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01575
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#21453;&#39304;&#30340;&#20851;&#31995;&#65292;&#24378;&#35843;&#20102;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#36825;&#19968;&#20851;&#38190;&#32852;&#31995;&#19978;&#30340;&#25506;&#32034;&#20173;&#26377;&#24453;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#38543;&#30528;&#20154;&#20204;&#23545;&#20154;&#26426;&#21327;&#21516;&#65288;HITL&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#23558;&#20154;&#31867;&#36755;&#20837;&#19982;RL&#31639;&#27861;&#30456;&#32467;&#21512;&#24050;&#32463;&#20652;&#29983;&#20102;&#20132;&#20114;&#24335;RL&#30340;&#23376;&#39046;&#22495;&#12290;&#21516;&#26102;&#65292;BCI&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#20174;&#31070;&#32463;&#27963;&#21160;&#20013;&#25552;&#21462;&#26377;&#20851;&#20154;&#26426;&#20132;&#20114;&#30340;&#20449;&#24687;&#24615;&#33041;&#20449;&#21495;&#12290;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#20851;&#38190;&#32852;&#31995;&#22312;&#20110;&#23558;&#31070;&#32463;&#27963;&#21160;&#35299;&#37322;&#20026;&#21453;&#39304;&#65292;&#20197;&#20415;&#21487;&#20197;&#24212;&#29992;&#20132;&#20114;&#24335;RL&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#20852;&#30340;&#21453;&#39304;&#20171;&#36136;&#31216;&#20026;&#20869;&#22312;&#21453;&#39304;&#12290;&#23613;&#31649;&#20869;&#22312;&#21453;&#39304;&#33021;&#22815;&#33258;&#21160;&#20256;&#36798;&#29978;&#33267;&#26080;&#24847;&#35782;&#22320;&#20256;&#36798;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#20851;&#38190;&#32852;&#31995;&#30340;&#36866;&#24403;&#25506;&#32034;&#20960;&#20046;&#26410;&#21463;&#21040;&#20004;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20419;&#36827;&#28145;&#20837;&#29702;&#35299;&#21644;&#26356;&#26377;&#25928;&#30340;&#21033;&#29992;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#39118;&#26684;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#21160;&#26426;&#12289;&#26041;&#27861;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2111.13144</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#20351;&#29992;&#27969;&#36827;&#34892;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning to Search in Task and Motion Planning with Streams. (arXiv:2111.13144v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22312;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#20013;&#30340;&#38271;&#26399;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#30340;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations, improving the long-term reasoning ability in task and motion planning. The algorithm is applied to a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20013;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#23558;&#31163;&#25955;&#20219;&#21153;&#21464;&#37327;&#19978;&#30340;&#31526;&#21495;&#35268;&#21010;&#19982;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#21464;&#37327;&#19978;&#30340;&#36816;&#21160;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#26368;&#36817;&#30340;&#20316;&#21697;&#65292;&#22914;PDDLStream&#65292;&#19987;&#27880;&#20110;&#20048;&#35266;&#35268;&#21010;&#65292;&#20351;&#29992;&#36880;&#27493;&#22686;&#38271;&#30340;&#23545;&#35937;&#38598;&#65292;&#30452;&#21040;&#25214;&#21040;&#21487;&#34892;&#30340;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38598;&#21512;&#26159;&#20197;&#24191;&#24230;&#20248;&#20808;&#30340;&#26041;&#24335;&#31351;&#20030;&#25193;&#23637;&#30340;&#65292;&#32780;&#19981;&#32771;&#34385;&#25163;&#22836;&#38382;&#39064;&#30340;&#36923;&#36753;&#21644;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#20351;&#24471;&#20855;&#26377;&#22823;&#37327;&#23545;&#35937;&#30340;&#38271;&#26399;&#25512;&#29702;&#21464;&#24471;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20449;&#24687;&#30340;&#31526;&#21495;&#35268;&#21010;&#22120;&#65292;&#20197;&#26368;&#20339;&#20248;&#20808;&#26041;&#24335;&#25193;&#23637;&#23545;&#35937;&#21644;&#20107;&#23454;&#38598;&#21512;&#65292;&#30001;&#20808;&#21069;&#30340;&#25628;&#32034;&#35745;&#31639;&#23398;&#20064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22256;&#38590;&#24773;&#20917;&#19979;&#35268;&#21010;&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;7&#33258;&#30001;&#24230;&#26426;&#26800;&#33218;&#22312;&#22534;&#21472;&#25805;&#32437;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task and motion planning problems in robotics combine symbolic planning over discrete task variables with motion optimization over continuous state and action variables. Recent works such as PDDLStream have focused on optimistic planning with an incrementally growing set of objects until a feasible trajectory is found. However, this set is exhaustively expanded in a breadth-first manner, regardless of the logical and geometric structure of the problem at hand, which makes long-horizon reasoning with large numbers of objects prohibitively time-consuming. To address this issue, we propose a geometrically informed symbolic planner that expands the set of objects and facts in a best-first manner, prioritized by a Graph Neural Network that is learned from prior search computations. We evaluate our approach on a diverse set of problems and demonstrate an improved ability to plan in difficult scenarios. We also apply our algorithm on a 7DOF robotic arm in block-stacking manipulation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#23545;&#21098;&#26525;&#21518;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#20197;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#23545;&#25968;&#20851;&#31995;&#20915;&#23450;&#20102;&#39044;&#35757;&#32451;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2108.00259</link><description>&lt;p&gt;
&#29992;&#22810;&#23569;&#39044;&#35757;&#32451;&#36275;&#20197;&#21457;&#29616;&#19968;&#20010;&#22909;&#30340;&#23376;&#32593;&#32476;&#65311;
&lt;/p&gt;
&lt;p&gt;
How much pre-training is enough to discover a good subnetwork?. (arXiv:2108.00259v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#20013;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#23545;&#21098;&#26525;&#21518;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#20197;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#23545;&#25968;&#20851;&#31995;&#20915;&#23450;&#20102;&#39044;&#35757;&#32451;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#23545;&#20110;&#22312;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#32593;&#32476;&#32467;&#26500;&#20013;&#21457;&#29616;&#39640;&#25928;&#12289;&#39640;&#24615;&#33021;&#30340;&#23376;&#32593;&#32476;&#38750;&#24120;&#26377;&#29992;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#23427;&#28041;&#21450;&#21040;&#19968;&#20010;&#19977;&#27493;&#36807;&#31243;&#8212;&#8212;&#39044;&#35757;&#32451;&#12289;&#21098;&#26525;&#21644;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#20026;&#23494;&#38598;&#27169;&#22411;&#24517;&#39035;&#23436;&#20840;&#39044;&#35757;&#32451;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#30340;&#25968;&#37327;&#19982;&#21098;&#26525;&#32593;&#32476;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#20381;&#36182;&#20851;&#31995;&#30340;&#29702;&#35770;&#25551;&#36848;&#20173;&#28982;&#32570;&#22833;&#12290;&#20026;&#20102;&#25968;&#23398;&#20998;&#26512;&#23494;&#38598;&#32593;&#32476;&#39044;&#35757;&#32451;&#25152;&#38656;&#30340;&#25968;&#37327;&#65292;&#20197;&#20415;&#21098;&#26525;&#21518;&#30340;&#32593;&#32476;&#33021;&#22815;&#34920;&#29616;&#33391;&#22909;&#65292;&#25105;&#20204;&#22312;&#20108;&#23618;&#20840;&#36830;&#25509;&#32593;&#32476;&#19978;&#21457;&#29616;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#36229;&#36807;&#36825;&#20010;&#30028;&#38480;&#65292;&#36890;&#36807;&#36138;&#23146;&#21069;&#21521;&#36873;&#25321;&#30340;&#21098;&#26525;&#21487;&#20197;&#36798;&#21040;&#33391;&#22909;&#30340;&#35757;&#32451;&#35823;&#24046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20010;&#38408;&#20540;&#34987;&#35777;&#26126;&#19982;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning is useful for discovering efficient, high-performing subnetworks within pre-trained, dense network architectures. More often than not, it involves a three-step process -- pre-training, pruning, and re-training -- that is computationally expensive, as the dense model must be fully pre-trained. While previous work has revealed through experiments the relationship between the amount of pre-training and the performance of the pruned network, a theoretical characterization of such dependency is still missing. Aiming to mathematically analyze the amount of dense network pre-training needed for a pruned network to perform well, we discover a simple theoretical bound in the number of gradient descent pre-training iterations on a two-layer, fully-connected network, beyond which pruning via greedy forward selection [61] yields a subnetwork that achieves good training error. Interestingly, this threshold is shown to be logarithmically dependent upon the size of the dataset,
&lt;/p&gt;</description></item><item><title>&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2106.11342</link><description>&lt;p&gt;
&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dive into Deep Learning. (arXiv:2106.11342v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11342
&lt;/p&gt;
&lt;p&gt;
&#12298;&#28145;&#20837;&#28145;&#24230;&#23398;&#20064;&#12299;&#26159;&#19968;&#26412;&#26088;&#22312;&#20351;&#28145;&#24230;&#23398;&#20064;&#26131;&#20110;&#29702;&#35299;&#30340;&#24320;&#28304;&#20070;&#31821;&#65292;&#25552;&#20379;&#20174;&#27010;&#24565;&#21040;&#20195;&#30721;&#30340;&#25945;&#23398;&#36164;&#28304;&#65292;&#26088;&#22312;&#25104;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;&#65292;&#24182;&#20801;&#35768;&#31038;&#21306;&#24555;&#36895;&#26356;&#26032;&#21644;&#20114;&#21160;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26412;&#24320;&#28304;&#20070;&#26159;&#25105;&#20204;&#30340;&#21162;&#21147;&#65292;&#35753;&#28145;&#24230;&#23398;&#20064;&#21464;&#24471;&#26131;&#20110;&#29702;&#35299;&#65292;&#25945;&#35835;&#32773;&#27010;&#24565;&#12289;&#32972;&#26223;&#21644;&#20195;&#30721;&#12290;&#25972;&#26412;&#20070;&#37117;&#26159;&#22312;Jupyter&#31508;&#35760;&#26412;&#20013;&#36215;&#33609;&#30340;&#65292;&#19982;&#29420;&#31435;&#30340;&#20195;&#30721;&#26080;&#32541;&#38598;&#25104;&#20102;&#35828;&#26126;&#22270;&#12289;&#25968;&#23398;&#21644;&#20114;&#21160;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#36164;&#28304;&#65292;&#26082;&#21487;&#20197;&#33258;&#30001;&#20351;&#29992;&#65292;&#21448;&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#25216;&#26415;&#28145;&#24230;&#65292;&#20026;&#25104;&#20026;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31185;&#23398;&#23478;&#30340;&#36215;&#28857;; &#21253;&#25324;&#21487;&#36816;&#34892;&#30340;&#20195;&#30721;&#65292;&#21521;&#35835;&#32773;&#23637;&#31034;&#22914;&#20309;&#23454;&#36341;&#35299;&#20915;&#38382;&#39064;; &#20801;&#35768;&#24555;&#36895;&#26356;&#26032;&#65292;&#19981;&#20165;&#30001;&#25105;&#20204;&#65292;&#36824;&#30001;&#25972;&#20010;&#31038;&#21306;&#26356;&#26032;; &#25509;&#21463;&#25216;&#26415;&#32454;&#33410;&#30340;&#20114;&#21160;&#35752;&#35770;&#21644;&#35299;&#31572;&#38382;&#39064;&#30340;&#35770;&#22363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This open-source book represents our attempt to make deep learning approachable, teaching readers the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code. Our goal is to offer a resource that could (i) be freely available for everyone; (ii) offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist; (iii) include runnable code, showing readers how to solve problems in practice; (iv) allow for rapid updates, both by us and also by the community at large; (v) be complemented by a forum for interactive discussion of technical details and to answer questions.
&lt;/p&gt;</description></item></channel></rss>