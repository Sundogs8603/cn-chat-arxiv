<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#21644;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.05741</link><description>&lt;p&gt;
&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Neural Progressive Meshes. (arXiv:2308.05741v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#28176;&#36827;&#32593;&#26684;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#21644;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#20197;&#22312;&#25163;&#25345;&#35774;&#22791;&#19978;&#28040;&#36153;&#30340;3D&#20869;&#23481;&#30340;&#28608;&#22686;&#38656;&#35201;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#36890;&#36807;&#20114;&#32852;&#32593;&#20256;&#36755;&#22823;&#37327;&#30340;&#20960;&#20309;&#25968;&#25454;&#65292;&#20363;&#22914;3D&#32593;&#26684;&#12290;&#35814;&#32454;&#30340;&#39640;&#20998;&#36776;&#29575;&#36164;&#28304;&#23545;&#20110;&#23384;&#20648;&#21644;&#20256;&#36755;&#24102;&#23485;&#37117;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36890;&#24120;&#20351;&#29992;&#23618;&#27425;&#32454;&#33410;&#25216;&#26415;&#22312;&#36866;&#24403;&#30340;&#24102;&#23485;&#39044;&#31639;&#19979;&#20256;&#36755;&#36164;&#28304;&#12290;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#26469;&#35828;&#65292;&#20197;&#28176;&#36827;&#30340;&#26041;&#24335;&#20256;&#36755;&#25968;&#25454;&#65292;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21152;&#20837;&#65292;&#25913;&#36827;&#20960;&#20309;&#30340;&#36136;&#37327;&#23588;&#20854;&#21487;&#21462;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;3D&#32593;&#26684;&#30340;&#20960;&#20309;&#32454;&#33410;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#24418;&#29366;&#20043;&#38388;&#37117;&#21576;&#29616;&#20986;&#30456;&#20284;&#30340;&#23616;&#37096;&#27169;&#24335;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20849;&#20139;&#30340;&#23398;&#20064;&#29983;&#25104;&#31354;&#38388;&#26469;&#26377;&#25928;&#34920;&#31034;&#36825;&#20123;&#32454;&#33410;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32454;&#20998;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#25552;&#21069;&#35757;&#32451;&#22823;&#37327;&#34920;&#38754;&#65292;&#22312;&#27492;&#31354;&#38388;&#20013;&#23398;&#20064;&#36825;&#20123;&#32454;&#33410;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#22312;&#20013;&#38388;&#32454;&#20998;&#32423;&#21035;&#20043;&#38388;&#21487;&#20197;&#36880;&#28176;&#20256;&#36755;&#39069;&#22806;&#30340;&#27531;&#20313;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#28176;&#36827;&#24335;&#20256;&#36755;&#21644;&#25913;&#21892;&#20960;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of 3D content that can be consumed on hand-held devices necessitates efficient tools for transmitting large geometric data, e.g., 3D meshes, over the Internet. Detailed high-resolution assets can pose a challenge to storage as well as transmission bandwidth, and level-of-detail techniques are often used to transmit an asset using an appropriate bandwidth budget. It is especially desirable for these methods to transmit data progressively, improving the quality of the geometry with more data. Our key insight is that the geometric details of 3D meshes often exhibit similar local patterns even across different shapes, and thus can be effectively represented with a shared learned generative space. We learn this space using a subdivision-based encoder-decoder architecture trained in advance on a large collection of surfaces. We further observe that additional residual features can be transmitted progressively between intermediate levels of subdivision that enable the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.05734</link><description>&lt;p&gt;
AudioLDM 2: &#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#36827;&#34892;&#25972;&#20307;&#38899;&#39057;&#29983;&#25104;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (arXiv:2308.05734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;LOA&#65292;&#23558;&#20219;&#20309;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38899;&#39057;&#29983;&#25104;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38899;&#39057;&#20013;&#20849;&#20139;&#19968;&#20123;&#20849;&#24615;&#65292;&#27604;&#22914;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#65292;&#20294;&#20026;&#27599;&#31181;&#31867;&#22411;&#35774;&#35745;&#27169;&#22411;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#29305;&#23450;&#30340;&#30446;&#26631;&#21644;&#20559;&#24046;&#65292;&#36825;&#20123;&#20559;&#24046;&#21487;&#33021;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#30446;&#26631;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23454;&#29616;&#38899;&#39057;&#29983;&#25104;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#38899;&#25928;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#35821;&#35328;&#38899;&#39057;&#65288;LOA&#65289;&#8221;&#30340;&#38899;&#39057;&#36890;&#29992;&#34920;&#31034;&#12290;&#20219;&#20309;&#38899;&#39057;&#37117;&#21487;&#20197;&#22522;&#20110;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#27169;&#22411;AudioMAE&#36716;&#25442;&#20026;LOA&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-2&#27169;&#22411;&#23558;&#20219;&#20309;&#24418;&#24335;&#30340;&#38899;&#39057;&#36716;&#25442;&#20026;LOA&#65292;&#24182;&#21033;&#29992;&#20197;LOA&#20026;&#26465;&#20214;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#38899;&#39057;&#29983;&#25104;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33258;&#28982;&#22320;&#24102;&#26469;&#20102;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#21487;&#37325;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;AudioMAE&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework introduces a general representation of audio, called language of audio (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate any modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on LOA. The proposed framework naturally brings advantages such as in-context learning abilities and reusable self-supervised pretrained AudioMAE
&lt;/p&gt;</description></item><item><title>PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05732</link><description>&lt;p&gt;
PDE-Refiner: &#21033;&#29992;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05732
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20256;&#32479;&#35299;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#36825;&#20123;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#23454;&#29992;&#20215;&#20540;&#20381;&#36182;&#20110;&#23427;&#20204;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#24403;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#30340;&#26102;&#38388;&#23637;&#24320;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#21457;&#29616;&#24573;&#30053;&#38750;&#20027;&#23548;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65288;&#36890;&#24120;&#19982;PDE&#35299;&#20013;&#30340;&#39640;&#39057;&#29575;&#30456;&#20851;&#65289;&#26159;&#38480;&#21046;&#31283;&#23450;&#12289;&#20934;&#30830;&#23637;&#24320;&#24615;&#33021;&#30340;&#20027;&#35201;&#38519;&#38449;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24341;&#20837;&#20102;PDE-Refiner&#65307;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#36890;&#36807;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#23454;&#29616;&#23545;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;PDE-Refiner&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05713</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#25968;&#23398;&#21644;&#31185;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#27979;&#35797;GPT-4
&lt;/p&gt;
&lt;p&gt;
Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems. (arXiv:2308.05713v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#38382;&#39064;&#19978;&#20351;&#29992;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#25554;&#20214;&#26174;&#33879;&#25552;&#21319;&#20102;GPT&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#20294;&#25509;&#21475;&#25925;&#38556;&#20173;&#28982;&#26159;&#20854;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25551;&#36848;&#20102;&#22312;2023&#24180;6&#26376;&#33267;8&#26376;&#26399;&#38388;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#31185;&#23398;&#21644;&#25968;&#23398;&#39046;&#22495;&#36827;&#34892;&#30340;105&#20010;&#21407;&#21019;&#38382;&#39064;&#30340;&#27979;&#35797;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;Wolfram Alpha&#21644;Code Interpreter&#25554;&#20214;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#20123;&#25554;&#20214;&#26174;&#33879;&#22686;&#24378;&#20102;GPT&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#32463;&#24120;&#20986;&#29616;&#8220;&#25509;&#21475;&#8221;&#25925;&#38556;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;GPT&#32463;&#24120;&#22312;&#38382;&#39064;&#30340;&#34920;&#36848;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#26080;&#27861;&#20174;&#25554;&#20214;&#20013;&#24471;&#21040;&#26377;&#29992;&#30340;&#31572;&#26696;&#12290;&#35299;&#20915;&#36825;&#20123;&#25509;&#21475;&#25925;&#38556;&#20284;&#20046;&#26159;&#20351;GPT&#25104;&#20026;&#21487;&#38752;&#30340;&#22823;&#23398;&#32423;&#35745;&#31639;&#38382;&#39064;&#24037;&#20855;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often "interface" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30740;&#31350;&#30340;&#27010;&#36848;&#21644;&#32452;&#25104;&#37096;&#20998;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.05701</link><description>&lt;p&gt;
&#25506;&#32034;&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of World Models for Anomaly Detection in Autonomous Driving. (arXiv:2308.05701v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30740;&#31350;&#30340;&#27010;&#36848;&#21644;&#32452;&#25104;&#37096;&#20998;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#23553;&#38381;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#65292;&#20294;&#22312;&#38754;&#23545;&#24847;&#22806;&#24773;&#20917;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#20316;&#20026;&#19968;&#31181;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#26681;&#25454;&#28508;&#22312;&#34892;&#21160;&#39044;&#27979;&#26410;&#26469;&#30340;&#26041;&#24335;&#12290;&#36825;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#22914;&#20309;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#23558;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#19982;&#20808;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#24037;&#20316;&#30456;&#20851;&#32852;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years there have been remarkable advancements in autonomous driving. While autonomous vehicles demonstrate high performance in closed-set conditions, they encounter difficulties when confronted with unexpected situations. At the same time, world models emerged in the field of model-based reinforcement learning as a way to enable agents to predict the future depending on potential actions. This led to outstanding results in sparse reward and complex control tasks. This work provides an overview of how world models can be leveraged to perform anomaly detection in the domain of autonomous driving. We provide a characterization of world models and relate individual components to previous works in anomaly detection to facilitate further research in the field.
&lt;/p&gt;</description></item><item><title>SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05697</link><description>&lt;p&gt;
SSLRec: &#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;
&lt;/p&gt;
&lt;p&gt;
SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05697
&lt;/p&gt;
&lt;p&gt;
SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;SSL&#31639;&#27861;&#26469;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#24615;&#33021;&#65288;&#20363;&#22914;&#22270;&#21327;&#21516;&#36807;&#28388;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#31038;&#20132;&#25512;&#33616;&#12289;&#30693;&#35782;&#22270;&#22686;&#24378;&#25512;&#33616;&#65289;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#25512;&#33616;&#31639;&#27861;&#30340;&#22522;&#30707;&#65292;&#32479;&#19968;&#29616;&#26377;&#26041;&#27861;&#30340;&#39564;&#35777;&#65292;&#24182;&#25512;&#21160;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SSLRec&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;SSLRec&#24211;&#20855;&#26377;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21487;&#20197;&#26041;&#20415;&#29992;&#25143;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2308.05681</link><description>&lt;p&gt;
&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#38754;&#20020;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#21644;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient. (arXiv:2308.05681v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#30340;&#33030;&#24369;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30828;&#24615;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27969;&#24418;&#21644;&#23450;&#20041;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#26469;&#25915;&#20987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#33030;&#24369;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#39592;&#39612;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#26041;&#27861;&#35201;&#27714;&#35201;&#20040;&#23436;&#20840;&#20102;&#35299;&#21463;&#23475;&#32773;&#65288;&#21363;&#30333;&#30418;&#25915;&#20987;&#65289;&#65292;&#35201;&#20040;&#26377;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#22522;&#20110;&#36716;&#31227;&#30340;&#25915;&#20987;&#65289;&#65292;&#25110;&#32773;&#39057;&#32321;&#26597;&#35810;&#27169;&#22411;&#65288;&#21363;&#40657;&#30418;&#25915;&#20987;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#37117;&#38750;&#24120;&#38480;&#21046;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#33030;&#24369;&#24615;&#30340;&#36136;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33030;&#24369;&#24615;&#30830;&#23454;&#23384;&#22312;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#20219;&#21153;&#65306;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#21463;&#23475;&#32773;&#27169;&#22411;&#25110;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#31614;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#30828;&#24615;&#26080;&#30418;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#36816;&#21160;&#27969;&#24418;&#65292;&#28982;&#21518;&#23450;&#20041;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#25915;&#20987;&#30340;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#39592;&#39612;-&#21160;&#20316;&#30693;&#24773;&#26799;&#24230;&#65288;SMI&#26799;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#26799;&#24230;&#21253;&#21547;&#36816;&#21160;&#21160;&#21147;&#23398;&#30340;&#20449;&#24687;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#20551;&#35774;&#25439;&#22833;&#26799;&#24230;&#26159;&#36890;&#36807;&#35745;&#31639;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#20102;NHTS&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#21592;&#21644;&#36710;&#36742;&#20986;&#34892;&#30340;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#23545;&#20256;&#32479;&#20132;&#36890;&#35268;&#21010;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#35828;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.05665</link><description>&lt;p&gt;
&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#20154;&#21592;&#21644;&#36710;&#36742;&#20986;&#34892;&#65306;&#23545;NHTS&#25968;&#25454;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring Deep Learning Approaches to Predict Person and Vehicle Trips: An Analysis of NHTS Data. (arXiv:2308.05665v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20998;&#26512;&#20102;NHTS&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#20154;&#21592;&#21644;&#36710;&#36742;&#20986;&#34892;&#30340;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;98%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#23545;&#20256;&#32479;&#20132;&#36890;&#35268;&#21010;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#35828;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20132;&#36890;&#35268;&#21010;&#22312;&#20934;&#30830;&#39044;&#27979;&#20154;&#21592;&#21644;&#36710;&#36742;&#20986;&#34892;&#26041;&#38754;&#20381;&#36182;&#36739;&#22810;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#35268;&#21010;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#32771;&#34385;&#20986;&#34892;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#65292;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#20339;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#20197;&#25913;&#21464;&#25105;&#20204;&#23545;&#20986;&#34892;&#39044;&#27979;&#21644;&#20132;&#36890;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#20840;&#22269;&#23478;&#24237;&#20986;&#34892;&#35843;&#26597;&#65288;NHTS&#65289;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#20154;&#21592;&#21644;&#36710;&#36742;&#20986;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;NHTS&#25968;&#25454;&#20013;&#30340;&#22823;&#37327;&#20449;&#24687;&#65292;&#25429;&#25417;&#20197;&#21069;&#20256;&#32479;&#27169;&#22411;&#24573;&#35270;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#21592;&#20986;&#34892;&#26041;&#38754;&#36798;&#21040;&#20102;98%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#36710;&#36742;&#20986;&#34892;&#20272;&#35745;&#26041;&#38754;&#36798;&#21040;&#20102;96%&#30340;&#20934;&#30830;&#29575;&#12290;&#30456;&#27604;&#20256;&#32479;&#20132;&#36890;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#20195;&#34920;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern transportation planning relies heavily on accurate predictions of person and vehicle trips. However, traditional planning models often fail to account for the intricacies and dynamics of travel behavior, leading to less-than-optimal accuracy in these predictions. This study explores the potential of deep learning techniques to transform the way we approach trip predictions, and ultimately, transportation planning. Utilizing a comprehensive dataset from the National Household Travel Survey (NHTS), we developed and trained a deep learning model for predicting person and vehicle trips. The proposed model leverages the vast amount of information in the NHTS data, capturing complex, non-linear relationships that were previously overlooked by traditional models. As a result, our deep learning model achieved an impressive accuracy of 98% for person trip prediction and 96% for vehicle trip estimation. This represents a significant improvement over the performances of traditional transpo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#36710;&#36742;&#36712;&#36857;&#20998;&#27573;&#24182;&#29983;&#25104;&#36947;&#36335;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;YOLOv5&#31639;&#27861;&#20934;&#30830;&#20998;&#31867;&#30452;&#32447;&#36947;&#36335;&#27573;&#21644;&#20132;&#21449;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05658</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#30456;&#20851;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
Automatic Extraction of Relevant Road Infrastructure using Connected vehicle data and Deep Learning Model. (arXiv:2308.05658v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#36710;&#36742;&#36712;&#36857;&#20998;&#27573;&#24182;&#29983;&#25104;&#36947;&#36335;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;YOLOv5&#31639;&#27861;&#20934;&#30830;&#20998;&#31867;&#30452;&#32447;&#36947;&#36335;&#27573;&#21644;&#20132;&#21449;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#36895;&#21457;&#23637;&#30340;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#25928;&#20934;&#30830;&#22320;&#32472;&#21046;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#23545;&#20110;&#20248;&#21270;&#20132;&#36890;&#31995;&#32479;&#12289;&#22686;&#24378;&#36947;&#36335;&#23433;&#20840;&#24182;&#25913;&#21892;&#39550;&#39542;&#21592;&#21644;&#36890;&#21220;&#32773;&#30340;&#25972;&#20307;&#20986;&#34892;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20005;&#23803;&#30340;&#29942;&#39048;&#38459;&#30861;&#20102;&#36827;&#23637;-&#32321;&#29712;&#32791;&#26102;&#30340;&#25163;&#21160;&#20132;&#21449;&#28857;&#35782;&#21035;&#12290;&#32771;&#34385;&#21040;&#38656;&#35201;&#35782;&#21035;&#30340;&#20132;&#21449;&#28857;&#25968;&#37327;&#21644;&#27599;&#20010;&#20132;&#21449;&#28857;&#25152;&#38656;&#30340;&#24037;&#26102;&#65292;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#21644;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#22320;&#29702;&#25955;&#21015;&#23545;&#36710;&#36742;&#36712;&#36857;&#36827;&#34892;&#20998;&#27573;&#65292;&#24182;&#29983;&#25104;&#36947;&#36335;&#27573;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;YOLOv5&#65288;You Only Look Once version 5&#65289;&#31639;&#27861;&#20934;&#30830;&#20998;&#31867;&#20102;&#30452;&#32447;&#36947;&#36335;&#27573;&#21644;&#20132;&#21449;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's rapidly evolving urban landscapes, efficient and accurate mapping of road infrastructure is critical for optimizing transportation systems, enhancing road safety, and improving the overall mobility experience for drivers and commuters. Yet, a formidable bottleneck obstructs progress - the laborious and time-intensive manual identification of intersections. Simply considering the shear number of intersections that need to be identified, and the labor hours required per intersection, the need for an automated solution becomes undeniable. To address this challenge, we propose a novel approach that leverages connected vehicle data and cutting-edge deep learning techniques. By employing geohashing to segment vehicle trajectories and then generating image representations of road segments, we utilize the YOLOv5 (You Only Look Once version 5) algorithm for accurate classification of both straight road segments and intersections. Experimental results demonstrate an impressive overall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26356;&#26032;&#27169;&#22411;&#24341;&#20837;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#33021;&#20135;&#29983;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#24182;&#20445;&#25345;&#21028;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05619</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#26356;&#26032;&#20020;&#24202;&#39118;&#38505;&#20998;&#23618;&#27169;&#22411;&#65306;&#35780;&#20272;&#21644;&#20248;&#21270;&#20020;&#24202;&#21307;&#29983;-&#27169;&#22411;&#22242;&#38431;&#24615;&#33021;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Updating Clinical Risk Stratification Models Using Rank-Based Compatibility: Approaches for Evaluating and Optimizing Clinician-Model Team Performance. (arXiv:2308.05619v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#26356;&#26032;&#27169;&#22411;&#24341;&#20837;&#30340;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#33021;&#20135;&#29983;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#24182;&#20445;&#25345;&#21028;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#30340;&#21464;&#21270;&#25110;&#26032;&#25968;&#25454;&#30340;&#20986;&#29616;&#65292;&#26356;&#26032;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20445;&#25345;&#25110;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26356;&#26032;&#27169;&#22411;&#21487;&#33021;&#20250;&#24341;&#20837;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24403;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#30340;&#34892;&#20026;&#19982;&#29992;&#25143;&#30340;&#26399;&#26395;&#19981;&#19968;&#33268;&#26102;&#65292;&#20250;&#23548;&#33268;&#29992;&#25143;-&#27169;&#22411;&#22242;&#38431;&#34920;&#29616;&#19981;&#20339;&#12290;&#29616;&#26377;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#20915;&#31574;&#38408;&#20540;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22522;&#20110;&#20272;&#35745;&#39118;&#38505;&#30340;&#25490;&#21517;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#20860;&#23481;&#24615;&#24230;&#37327;&#65292;$C^R$&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#20248;&#21270;&#21028;&#21035;&#24615;&#33021;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#40723;&#21169;&#33391;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;&#22312;&#21033;&#29992;MIMIC&#25968;&#25454;&#30340;&#30149;&#27515;&#29575;&#39118;&#38505;&#20998;&#23618;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#36873;&#25321;&#25216;&#26415;&#65292;&#20135;&#29983;&#20102;&#26356;&#20860;&#23481;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21028;&#21035;&#24615;&#33021;&#65292;$C^R$&#25552;&#39640;&#20102;0.019&#65288;$95\%$&#32622;&#20449;&#21306;&#38388;&#65306;...
&lt;/p&gt;
&lt;p&gt;
As data shift or new data become available, updating clinical machine learning models may be necessary to maintain or improve performance over time. However, updating a model can introduce compatibility issues when the behavior of the updated model does not align with user expectations, resulting in poor user-model team performance. Existing compatibility measures depend on model decision thresholds, limiting their applicability in settings where models are used to generate rankings based on estimated risk. To address this limitation, we propose a novel rank-based compatibility measure, $C^R$, and a new loss function that aims to optimize discriminative performance while encouraging good compatibility. Applied to a case study in mortality risk stratification leveraging data from MIMIC, our approach yields more compatible models while maintaining discriminative performance compared to existing model selection techniques, with an increase in $C^R$ of $0.019$ ($95\%$ confidence interval: 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#36141;&#20080;&#27010;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32435;&#20837;&#21830;&#21697;&#32452;&#21512;&#25928;&#26524;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05617</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21830;&#21697;&#32452;&#21512;&#20248;&#21270;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Neural Network Based Choice Model for Assortment Optimization. (arXiv:2308.05617v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#36141;&#20080;&#27010;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22914;&#20309;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32435;&#20837;&#21830;&#21697;&#32452;&#21512;&#25928;&#26524;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38626;&#25955;&#36873;&#25321;&#27169;&#22411;&#22312;&#32463;&#27982;&#23398;&#12289;&#24066;&#22330;&#33829;&#38144;&#21644;&#25910;&#30410;&#31649;&#29702;&#20013;&#34987;&#29992;&#26469;&#39044;&#27979;&#23458;&#25143;&#36141;&#20080;&#27010;&#29575;&#65292;&#20363;&#22914;&#26681;&#25454;&#20215;&#26684;&#21644;&#20854;&#20182;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#23427;&#20204;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#25429;&#25417;&#21040;&#23458;&#25143;&#30340;&#24322;&#36136;&#24615;&#21644;&#34892;&#20026;&#65292;&#20294;&#26159;&#23427;&#20204;&#24456;&#38590;&#20272;&#35745;&#65292;&#36890;&#24120;&#22522;&#20110;&#35768;&#22810;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#25928;&#29992;&#65292;&#32780;&#19988;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#25429;&#25417;&#21040;&#23458;&#25143;&#34892;&#20026;&#30340;&#35768;&#22810;&#26174;&#33879;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#23601;&#26159;&#65292;&#37492;&#20110;&#23427;&#20204;&#22312;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#65292;&#26159;&#21542;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#28040;&#38500;&#20180;&#32454;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#23458;&#25143;&#34892;&#20026;&#27169;&#22411;&#20197;&#21450;&#25163;&#24037;&#32534;&#30721;&#21644;&#35843;&#25972;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#21830;&#21697;&#32452;&#21512;&#25928;&#26524;&#32435;&#20837;&#36825;&#26679;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#36825;&#26679;&#19968;&#20010;&#40657;&#30418;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21830;&#21697;&#32452;&#21512;&#20248;&#21270;&#30340;&#38382;&#39064;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#36141;&#20080;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models are used in economics, marketing and revenue management to predict customer purchase probabilities, say as a function of prices and other features of the offered assortment. While they have been shown to be expressive, capturing customer heterogeneity and behaviour, they are also hard to estimate, often based on many unobservables like utilities; and moreover, they still fail to capture many salient features of customer behaviour. A natural question then, given their success in other contexts, is if neural networks can eliminate the necessity of carefully building a context-dependent customer behaviour model and hand-coding and tuning the estimation. It is unclear however how one would incorporate assortment effects into such a neural network, and also how one would optimize the assortment with such a black-box generative model of choice probabilities. In this paper we investigate first whether a single neural network architecture can predict purchase probabiliti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#26597;&#21270;&#24037;&#29983;&#20135;&#21378;&#30340;&#23436;&#25972;&#24615;&#24182;&#25552;&#20379;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.05612</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#24037;&#21378;&#30417;&#25511;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Smart Robotic System for Industrial Plant Supervision. (arXiv:2308.05612v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05612
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#26816;&#26597;&#21270;&#24037;&#29983;&#20135;&#21378;&#30340;&#23436;&#25972;&#24615;&#24182;&#25552;&#20379;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#21270;&#24037;&#29983;&#20135;&#21378;&#20013;&#65292;&#20154;&#24037;&#25805;&#20316;&#21592;&#32463;&#24120;&#23545;&#24037;&#21378;&#30340;&#23436;&#25972;&#24615;&#36827;&#34892;&#26816;&#26597;&#65292;&#20197;&#30830;&#20445;&#39640;&#23433;&#20840;&#26631;&#20934;&#65292;&#22240;&#27492;&#21487;&#33021;&#26159;&#31532;&#19968;&#20010;&#36935;&#21040;&#21361;&#38505;&#25805;&#20316;&#26465;&#20214;&#30340;&#20154;&#12290;&#20026;&#20102;&#20943;&#36731;&#20182;&#20204;&#22312;&#25925;&#38556;&#26816;&#27979;&#21644;&#30417;&#25511;&#26041;&#38754;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#30001;&#19968;&#20010;&#33258;&#20027;&#23548;&#33322;&#26426;&#22120;&#20154;&#21644;&#21508;&#31181;&#20256;&#24863;&#22120;&#21644;&#25968;&#25454;&#22788;&#29702;&#22120;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#20154;&#31867;&#30340;&#35270;&#35273;&#12289;&#21957;&#35273;&#21644;&#21548;&#35273;&#24863;&#30693;&#21644;&#35299;&#37322;&#33021;&#21147;&#65292;&#20197;&#36827;&#34892;&#33258;&#21160;&#21270;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24223;&#27700;&#22788;&#29702;&#35774;&#26045;&#20013;&#23545;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#22320;&#23548;&#33322;&#24037;&#21378;&#24182;&#25552;&#20379;&#26377;&#20851;&#20851;&#38190;&#25805;&#20316;&#26465;&#20214;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's chemical production plants, human field operators perform frequent checks on the plant's integrity to guarantee high safety standards, and thus are possibly the first to encounter dangerous operating conditions. To alleviate their tasks of failure detection and monitoring by audio, visual, and olfactory perceptions, we present a robotic system that consists of an autonomously navigating robot integrated with various sensors and data processing. We aim to resemble the human sensing and interpretation capabilities of sight, smell, and hearing, for providing automated inspection. We evaluate our system extensively at a wastewater facility in full working conditions. Our results demonstrate that the system is able to robustly navigate a plant and to provide useful information about critical operating conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#12290;&#36890;&#36807;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.05601</link><description>&lt;p&gt;
&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-graph Spatio-temporal Graph Convolutional Network for Traffic Flow Prediction. (arXiv:2308.05601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22270;&#31354;&#26102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#12290;&#36890;&#36807;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#20043;&#38388;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#23545;&#20110;&#22478;&#24066;&#29983;&#27963;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#21151;&#33021;&#20043;&#19968;&#65292;&#20132;&#36890;&#35780;&#20272;&#22312;&#29616;&#20170;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#27599;&#26085;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#22312;&#25972;&#20010;&#32593;&#32476;&#33539;&#22260;&#30340;&#25910;&#36153;&#31449;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#23454;&#38469;&#20013;&#21508;&#20010;&#20301;&#32622;&#20043;&#38388;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#29366;&#20917;&#21152;&#21095;&#20102;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#30456;&#20851;&#31354;&#26102;&#22240;&#32032;&#26080;&#27861;&#20840;&#38754;&#22320;&#24212;&#29992;&#20110;&#38271;&#26399;&#25345;&#32493;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#26102;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#36895;&#20844;&#36335;&#27599;&#26085;&#20132;&#36890;&#27969;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#25968;&#25454;&#24402;&#19968;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#30001;&#20110;&#32593;&#32476;&#33539;&#22260;&#30340;&#25910;&#36153;&#31449;&#20132;&#36890;&#27969;&#30340;&#38271;&#23614;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20855;&#26377;&#19981;&#21516;&#35821;&#20041;&#30340;&#32593;&#32476;&#26469;&#25429;&#25417;&#31354;&#26102;&#29305;&#24449;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20351;&#29992;&#20102;&#27668;&#35937;&#21644;&#26085;&#21382;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inter-city highway transportation is significant for urban life. As one of the key functions in intelligent transportation system (ITS), traffic evaluation always plays significant role nowadays, and daily traffic flow prediction still faces challenges at network-wide toll stations. On the one hand, the data imbalance in practice among various locations deteriorates the performance of prediction. On the other hand, complex correlative spatio-temporal factors cannot be comprehensively employed in long-term duration. In this paper, a prediction method is proposed for daily traffic flow in highway domain through spatio-temporal deep learning. In our method, data normalization strategy is used to deal with data imbalance, due to long-tail distribution of traffic flow at network-wide toll stations. And then, based on graph convolutional network, we construct networks in distinct semantics to capture spatio-temporal features. Beside that, meteorology and calendar features are used by our mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#26469;&#25805;&#20316;&#27169;&#22411;&#36755;&#20986;&#26631;&#35760;&#22120;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#20854;&#20182;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05585</link><description>&lt;p&gt;
&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#23454;&#25112;&#65306;&#25805;&#20316;&#36755;&#20986;&#26631;&#35760;&#22120;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length. (arXiv:2308.05585v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#26469;&#25805;&#20316;&#27169;&#22411;&#36755;&#20986;&#26631;&#35760;&#22120;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#20854;&#20182;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#22312;&#22609;&#36896;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23545;&#20110;&#25511;&#21046;&#36755;&#20986;&#30340;&#27602;&#24615;&#21644;&#36873;&#25321;&#36755;&#20986;&#39118;&#26684;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#21253;&#21547;&#35823;&#23548;&#24615;&#20869;&#23481;&#65292;&#36843;&#20999;&#38656;&#35201;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#20197;&#30830;&#20445;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#20027;&#35201;&#23384;&#22312;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#24615;&#31561;&#29305;&#28857;&#65292;&#36825;&#20351;&#24471;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#20013;&#22870;&#21169;&#27169;&#22411;&#30340;&#35780;&#20272;&#21464;&#24471;&#22256;&#38590;&#65292;&#36827;&#32780;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#20351;&#29992;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#30340;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#31616;&#21333;&#20219;&#21153;&#65292;&#26088;&#22312;&#21033;&#29992;Gloden&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#65292;&#39564;&#35777;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20174;&#20013;&#33719;&#24471;&#21551;&#31034;&#65292;&#20027;&#35201;&#35299;&#37322;&#20102;&#21033;&#29992;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#26469;&#25805;&#32437;&#27169;&#22411;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#26631;&#35760;&#22120;&#38271;&#24230;&#30340;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#65292;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#19981;&#20165;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#33021;&#22815;&#26377;&#25928;&#22320;&#25805;&#32437;&#36755;&#20986;&#30340;&#26631;&#35760;&#22120;&#38271;&#24230;&#65292;&#36824;&#23637;&#31034;&#20102;&#20854;&#20182;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reinforcement Learning from Human Feedback (RLHF) plays a pivotal role in shaping the impact of large language models (LLMs), contributing significantly to controlling output toxicity and selecting output styles, particularly as LLMs often harbor misleading content, highlighting the urgency to align them with human values for secure AI systems. The RLHF, characterized by complexity, instability, and sensitivity to hyperparameters, makes the evaluation of the reward model for complex tasks challenging, thereby further complicating the use of Proximal Policy Optimization (PPO). In this paper, we introduce a simple task designed to employ Gloden as a reward model that validates the effectiveness of PPO and inspires it, primarily explaining the task of utilizing PPO to manipulate the tokenizer length of the output generated by the model. Experiments confirm that PPO is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20449;&#36947;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#21512;&#25104;&#26377;&#38480;&#25968;&#25454;&#30340;&#20449;&#36947;&#23454;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110; GAN &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#31283;&#23450;&#19988;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20449;&#36947;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.05583</link><description>&lt;p&gt;
&#26080;&#32447;&#30005;&#26080;&#32447;&#20449;&#36947;&#24314;&#27169;&#21644;&#25277;&#26679;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Diffusion Models for Radio Wireless Channel Modelling and Sampling. (arXiv:2308.05583v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20449;&#36947;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#21512;&#25104;&#26377;&#38480;&#25968;&#25454;&#30340;&#20449;&#36947;&#23454;&#29616;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110; GAN &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#31283;&#23450;&#19988;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20449;&#36947;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#36947;&#24314;&#27169;&#23545;&#20110;&#35774;&#35745;&#29616;&#20195;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20449;&#36947;&#24314;&#27169;&#30340;&#22797;&#26434;&#24615;&#21644;&#25910;&#38598;&#39640;&#36136;&#37327;&#26080;&#32447;&#20449;&#36947;&#25968;&#25454;&#30340;&#25104;&#26412;&#26085;&#30410;&#22686;&#21152;&#65292;&#24050;&#25104;&#20026;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#20449;&#36947;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#26377;&#38480;&#25968;&#25454;&#30340;&#20449;&#36947;&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#39057;&#29575;&#31354;&#38388;&#22495;&#20013;&#25805;&#20316;&#30340;&#22522;&#20110; U Net &#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22914;&#20309;&#20934;&#30830;&#22320;&#37325;&#29616;&#20449;&#36947;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#65306;$i)$ &#21453;&#26144;&#22825;&#32447;&#21644;&#39057;&#29575;&#39046;&#22495;&#20013;&#24402;&#19968;&#21270;&#21151;&#29575;&#35889;&#30340;&#23454;&#38469;&#20998;&#24067;&#21644;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#30340;&#36817;&#20284; $2$-Wasserstein &#36317;&#31163;&#65292;&#21644; $ii)$ &#20998;&#24067;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#24230;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110; GAN &#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#35757;&#32451;&#31283;&#23450;&#19988;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#20449;&#36947;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Channel modelling is essential to designing modern wireless communication systems. The increasing complexity of channel modelling and the cost of collecting high-quality wireless channel data have become major challenges. In this paper, we propose a diffusion model based channel sampling approach for rapidly synthesizing channel realizations from limited data. We use a diffusion model with a U Net based architecture operating in the frequency space domain. To evaluate how well the proposed model reproduces the true distribution of channels in the training dataset, two evaluation metrics are used: $i)$ the approximate $2$-Wasserstein distance between real and generated distributions of the normalized power spectrum in the antenna and frequency domains and $ii)$ precision and recall metric for distributions. We show that, compared to existing GAN based approaches which suffer from mode collapse and unstable training, our diffusion based approach trains stably and generates diverse and hi
&lt;/p&gt;</description></item><item><title>C5&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#36951;&#24536;&#21644;&#27169;&#22411;&#19978;&#19979;&#25991;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#21319;ChatGPT&#30340;&#23545;&#35805;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05567</link><description>&lt;p&gt;
C5&#65306;&#20026;ChatGPT&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#35805;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
C5: Towards Better Conversation Comprehension and Contextual Continuity for ChatGPT. (arXiv:2308.05567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05567
&lt;/p&gt;
&lt;p&gt;
C5&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#21487;&#35270;&#21270;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#20154;&#31867;&#36951;&#24536;&#21644;&#27169;&#22411;&#19978;&#19979;&#25991;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#21319;ChatGPT&#30340;&#23545;&#35805;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#22797;&#26434;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#20542;&#21521;&#20110;&#19982;ChatGPT&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#65292;&#20197;&#20445;&#25345;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#33719;&#24471;&#20840;&#38754;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36951;&#24536;&#21644;&#27169;&#22411;&#19978;&#19979;&#25991;&#36951;&#24536;&#20173;&#28982;&#26159;&#22810;&#36718;&#23545;&#35805;&#22330;&#26223;&#20013;&#31361;&#20986;&#30340;&#38382;&#39064;&#65292;&#36825;&#25361;&#25112;&#20102;&#29992;&#25143;&#23545;ChatGPT&#30340;&#23545;&#35805;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#23545;&#35805;&#21487;&#35270;&#21270;&#31995;&#32479;C5&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#23616;&#35270;&#22270;&#12289;&#20027;&#39064;&#35270;&#22270;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38382;&#31572;&#35270;&#22270;&#12290;&#20840;&#23616;&#35270;&#22270;&#20351;&#29992;GitLog&#22270;&#34920;&#30340;&#38544;&#21947;&#26469;&#34920;&#31034;&#23545;&#35805;&#32467;&#26500;&#65292;&#23637;&#31034;&#23545;&#35805;&#28436;&#21464;&#30340;&#36235;&#21183;&#65292;&#24182;&#25903;&#25345;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#30340;&#25506;&#32034;&#12290;&#20027;&#39064;&#35270;&#22270;&#26088;&#22312;&#26174;&#31034;&#25152;&#26377;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have demonstrated outstanding performance in various fields, particularly in natural language understanding and generation tasks. In complex application scenarios, users tend to engage in multi-turn conversations with ChatGPT to keep contextual information and obtain comprehensive responses. However, human forgetting and model contextual forgetting remain prominent issues in multi-turn conversation scenarios, which challenge the users' conversation comprehension and contextual continuity for ChatGPT. To address these challenges, we propose an interactive conversation visualization system called C5, which includes Global View, Topic View, and Context-associated Q\&amp;A View. The Global View uses the GitLog diagram metaphor to represent the conversation structure, presenting the trend of conversation evolution and supporting the exploration of locally salient features. The Topic View is designed to display all the question and answer nodes and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#20266;&#36896;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#24694;&#24847;&#28145;&#24230;&#20266;&#36896;&#30340;&#21019;&#24314;&#21644;&#32570;&#20047;&#26222;&#36866;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#21644;&#36827;&#23637;&#36827;&#34892;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20840;&#38754;&#12289;&#26222;&#36866;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05563</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advancements In The Field Of Deepfake Detection. (arXiv:2308.05563v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#20266;&#36896;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#24694;&#24847;&#28145;&#24230;&#20266;&#36896;&#30340;&#21019;&#24314;&#21644;&#32570;&#20047;&#26222;&#36866;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#21644;&#36827;&#23637;&#36827;&#34892;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20840;&#38754;&#12289;&#26222;&#36866;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#25351;&#19968;&#20010;&#20154;&#30340;&#29031;&#29255;&#25110;&#35270;&#39057;&#34987;&#25968;&#23383;&#21270;&#25913;&#21464;&#25110;&#37096;&#20998;&#26367;&#25442;&#20026;&#20854;&#20182;&#20154;&#30340;&#22270;&#20687;&#12290;&#28145;&#24230;&#20266;&#36896;&#21487;&#33021;&#23548;&#33268;&#21508;&#31181;&#38382;&#39064;&#65292;&#24120;&#34987;&#24694;&#24847;&#20351;&#29992;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#29992;&#27861;&#26159;&#20462;&#25913;&#33879;&#21517;&#25919;&#27835;&#20154;&#29289;&#21644;&#21517;&#20154;&#30340;&#35270;&#39057;&#12290;&#36825;&#20123;&#28145;&#24230;&#20266;&#36896;&#21487;&#20197;&#23637;&#31034;&#20182;&#20204;&#21457;&#34920;&#20882;&#29359;&#12289;&#26377;&#38382;&#39064;&#21644;/&#25110;&#19981;&#30495;&#23454;&#30340;&#22768;&#26126;&#12290;&#24403;&#21069;&#30340;&#28145;&#24230;&#20266;&#36896;&#38750;&#24120;&#36924;&#30495;&#65292;&#24403;&#20197;&#36825;&#31181;&#26041;&#24335;&#20351;&#29992;&#26102;&#65292;&#21487;&#33021;&#24341;&#21457;&#24656;&#24908;&#65292;&#29978;&#33267;&#24433;&#21709;&#36873;&#20030;&#21644;&#25919;&#27835;&#35266;&#28857;&#12290;&#30446;&#21069;&#26377;&#35768;&#22810;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31574;&#30053;&#65292;&#20294;&#25214;&#21040;&#26368;&#20840;&#38754;&#12289;&#26222;&#36866;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23558;&#35299;&#20915;&#24694;&#24847;&#28145;&#24230;&#20266;&#36896;&#21019;&#24314;&#21644;&#32570;&#20047;&#26222;&#36866;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#21644;&#20998;&#26512;&#24403;&#21069;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#39046;&#22495;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deepfake is a photo or video of a person whose image has been digitally altered or partially replaced with an image of someone else. Deepfakes have the potential to cause a variety of problems and are often used maliciously. A common usage is altering videos of prominent political figures and celebrities. These deepfakes can portray them making offensive, problematic, and/or untrue statements. Current deepfakes can be very realistic, and when used in this way, can spread panic and even influence elections and political opinions. There are many deepfake detection strategies currently in use but finding the most comprehensive and universal method is critical. So, in this survey we will address the problems of malicious deepfake creation and the lack of universal deepfake detection methods. Our objective is to survey and analyze a variety of current methods and advances in the field of deepfake detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#25216;&#26415;&#30340;&#21382;&#21490;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#22686;&#24191;Lagrange&#20132;&#26367;&#26041;&#21521;&#38750;&#31934;&#30830;&#29275;&#39039;(ALADIN)&#31639;&#27861;&#20197;&#21450;Alternating Direction Method of Multipliers (ADMM)&#31561;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#31361;&#20986;&#20102;&#36817;&#31471;&#20013;&#24515;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;ALADIN&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.05548</link><description>&lt;p&gt;
&#23398;&#20064;&#65288;&#19982;&#65289;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning (With) Distributed Optimization. (arXiv:2308.05548v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#25216;&#26415;&#30340;&#21382;&#21490;&#21457;&#23637;&#65292;&#20171;&#32461;&#20102;&#22686;&#24191;Lagrange&#20132;&#26367;&#26041;&#21521;&#38750;&#31934;&#30830;&#29275;&#39039;(ALADIN)&#31639;&#27861;&#20197;&#21450;Alternating Direction Method of Multipliers (ADMM)&#31561;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#21516;&#26102;&#31361;&#20986;&#20102;&#36817;&#31471;&#20013;&#24515;&#26041;&#27861;&#30340;&#24212;&#29992;&#21644;ALADIN&#30340;&#29420;&#29305;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#20998;&#24067;&#24335;&#20248;&#21270;&#25216;&#26415;&#30340;&#21382;&#21490;&#21457;&#23637;&#65292;&#36861;&#28335;&#21040;20&#19990;&#32426;60&#24180;&#20195;&#20025;&#40784;&#26684;&#12289;&#27779;&#23572;&#22827;&#21644;&#26412;&#24503;&#26031;&#24320;&#21019;&#30340;&#22522;&#20110;&#23545;&#20598;&#24615;&#30340;&#26041;&#27861;&#65292;&#30452;&#21040;&#22686;&#24191;Lagrange&#20132;&#26367;&#26041;&#21521;&#38750;&#31934;&#30830;&#29275;&#39039;(ALADIN)&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#26368;&#21021;&#30340;&#37325;&#28857;&#26159;&#23545;&#20984;&#38382;&#39064;&#30340;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#21644;&#20998;&#35299;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102;Alternating Direction Method of Multipliers (ADMM)&#31561;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;2000&#24180;&#20195;&#26411;&#20998;&#24067;&#24335;&#20248;&#21270;&#20877;&#24230;&#21463;&#21040;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#25104;&#20687;&#39046;&#22495;&#65292;&#35777;&#26126;&#20102;ADMM&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#21644;&#32479;&#19968;&#28508;&#21147;&#12290;&#26412;&#25991;&#36824;&#31361;&#20986;&#20102;&#36817;&#31471;&#20013;&#24515;&#26041;&#27861;&#30340;&#20986;&#29616;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;ALADIN&#30340;&#29420;&#29305;&#29305;&#28857;&#65292;&#23427;&#20026;&#38750;&#20984;&#24773;&#20917;&#25552;&#20379;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#26080;&#38656;&#24341;&#20837;&#36741;&#21161;&#21464;&#37327;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#26377;&#25152;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides an overview of the historical progression of distributed optimization techniques, tracing their development from early duality-based methods pioneered by Dantzig, Wolfe, and Benders in the 1960s to the emergence of the Augmented Lagrangian Alternating Direction Inexact Newton (ALADIN) algorithm. The initial focus on Lagrangian relaxation for convex problems and decomposition strategies led to the refinement of methods like the Alternating Direction Method of Multipliers (ADMM). The resurgence of interest in distributed optimization in the late 2000s, particularly in machine learning and imaging, demonstrated ADMM's practical efficacy and its unifying potential. This overview also highlights the emergence of the proximal center method and its applications in diverse domains. Furthermore, the paper underscores the distinctive features of ALADIN, which offers convergence guarantees for non-convex scenarios without introducing auxiliary variables, differentiating it fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;AUV&#25511;&#21046;&#20013;&#20351;&#29992;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#25511;&#21046;&#65288;MPPI&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#20570;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#22312;&#19982;&#20256;&#32479;PID&#21644;&#32423;&#32852;PID&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05547</link><description>&lt;p&gt;
&#29992;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#25511;&#21046;&#22686;&#24378;AUV&#30340;&#33258;&#20027;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing AUV Autonomy With Model Predictive Path Integral Control. (arXiv:2308.05547v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;AUV&#25511;&#21046;&#20013;&#20351;&#29992;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#25511;&#21046;&#65288;MPPI&#65289;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#20570;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#22312;&#19982;&#20256;&#32479;PID&#21644;&#32423;&#32852;PID&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#27700;&#19979;&#36710;&#36742;&#65288;AUV&#65289;&#22312;&#21208;&#27979;&#28023;&#27915;&#29615;&#22659;&#12289;&#36827;&#34892;&#27700;&#19979;&#26816;&#26597;&#20219;&#21153;&#21644;&#28023;&#27915;&#25506;&#32034;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;AUV&#33021;&#22815;&#25104;&#21151;&#25191;&#34892;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#26465;&#20214;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#23433;&#20840;&#36816;&#34892;&#65292;&#26426;&#36733;&#25511;&#21046;&#22120;&#24212;&#35813;&#33021;&#22815;&#22312;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#27169;&#22411;&#39044;&#27979;&#36335;&#24452;&#31215;&#20998;&#25511;&#21046;&#65288;MPPI&#65289;&#29992;&#20110;AUV&#25511;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;AUV&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#20256;&#25773;MPPI&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#26102;&#35745;&#31639;&#20986;&#25511;&#21046;&#34892;&#21160;&#12290;&#25105;&#20204;&#35814;&#32454;&#35780;&#20272;&#20102;&#20027;&#35201;&#36229;&#21442;&#25968;&#23545;MPPI&#25511;&#21046;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#32463;&#20856;PID&#21644;&#32423;&#32852;PID&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous underwater vehicles (AUVs) play a crucial role in surveying marine environments, carrying out underwater inspection tasks, and ocean exploration. However, in order to ensure that the AUV is able to carry out its mission successfully, a control system capable of adapting to changing environmental conditions is required. Furthermore, to ensure the robotic platform's safe operation, the onboard controller should be able to operate under certain constraints. In this work, we investigate the feasibility of Model Predictive Path Integral Control (MPPI) for the control of an AUV. We utilise a non-linear model of the AUV to propagate the samples of the MPPI, which allow us to compute the control action in real time. We provide a detailed evaluation of the effect of the main hyperparameters on the performance of the MPPI controller. Furthermore, we compared the performance of the proposed method with a classical PID and Cascade PID approach, demonstrating the superiority of our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#24182;&#19981;&#19968;&#23450;&#33021;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#65292;&#24378;&#35843;&#20102;&#23558;&#21333;&#27493;&#27169;&#22411;&#19982;&#21512;&#25104;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05522</link><description>&lt;p&gt;
&#27169;&#22411;&#24456;&#37325;&#35201;&#65306;&#21333;&#27493;&#21453;&#21512;&#25104;&#23545;&#21512;&#25104;&#35268;&#21010;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Models Matter: The Impact of Single-Step Retrosynthesis on Synthesis Planning. (arXiv:2308.05522v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#24182;&#19981;&#19968;&#23450;&#33021;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#65292;&#24378;&#35843;&#20102;&#23558;&#21333;&#27493;&#27169;&#22411;&#19982;&#21512;&#25104;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#26159;&#23558;&#21270;&#23398;&#21270;&#21512;&#29289;&#36880;&#27493;&#36882;&#24402;&#22320;&#20998;&#35299;&#20026;&#20998;&#23376;&#21069;&#20307;&#65292;&#30452;&#21040;&#25214;&#21040;&#19968;&#32452;&#21830;&#19994;&#19978;&#21487;&#29992;&#30340;&#20998;&#23376;&#20026;&#27490;&#65292;&#20197;&#25552;&#20379;&#21512;&#25104;&#36335;&#32447;&#12290;&#23427;&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#21644;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#36923;&#36753;&#21644;&#25214;&#21040;&#27491;&#30830;&#30340;&#21453;&#24212;&#39034;&#24207;&#65292;&#20108;&#32773;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#32852;&#31995;&#22312;&#24403;&#21069;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#24471;&#21040;&#20307;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27493;&#21512;&#25104;&#35268;&#21010;&#24182;&#20351;&#29992;&#20844;&#24320;&#21644;&#19987;&#26377;&#30340;&#21453;&#24212;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#23558;&#36825;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#21333;&#27493;&#24615;&#33021;&#36739;&#22909;&#19982;&#25214;&#21040;&#28508;&#22312;&#30340;&#21453;&#24212;&#36335;&#24452;&#25104;&#21151;&#20043;&#38388;&#23384;&#22312;&#26029;&#35010;&#65292;&#36825;&#34920;&#26126;&#23558;&#26469;&#24517;&#39035;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#35780;&#20272;&#21333;&#27493;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis consists of breaking down a chemical compound recursively step-by-step into molecular precursors until a set of commercially available molecules is found with the goal to provide a synthesis route. Its two primary research directions, single-step retrosynthesis prediction, which models the chemical reaction logic, and multi-step synthesis planning, which tries to find the correct sequence of reactions, are inherently intertwined. Still, this connection is not reflected in contemporary research. In this work, we combine these two major research directions by applying multiple single-step retrosynthesis models within multi-step synthesis planning and analyzing their impact using public and proprietary reaction data. We find a disconnection between high single-step performance and potential route-finding success, suggesting that single-step models must be evaluated within synthesis planning in the future. Furthermore, we show that the commonly used single-step retrosynthesi
&lt;/p&gt;</description></item><item><title>Mono-Hydra&#26159;&#19968;&#20010;&#23454;&#26102;&#31354;&#38388;&#24863;&#30693;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#30446;&#30456;&#26426;&#21644;IMU&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#23454;&#26102;&#26500;&#24314;&#23460;&#20869;&#22330;&#26223;&#30340;3D&#22270;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20174;&#21333;&#30446;&#30456;&#26426;&#36755;&#20837;&#20013;&#25512;&#23548;&#28145;&#24230;&#21644;&#35821;&#20041;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#26041;&#26681;&#20449;&#24687;&#30340;&#26426;&#22120;&#20154;&#35270;&#35273;&#24815;&#24615;&#27979;&#36317;(VIO)&#31639;&#27861;&#26469;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05515</link><description>&lt;p&gt;
Mono-hydra: &#21333;&#30446;&#30456;&#26426;&#36755;&#20837;&#19982;IMU&#23454;&#26102;3D&#22330;&#26223;&#22270;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Mono-hydra: Real-time 3D scene graph construction from monocular camera input with IMU. (arXiv:2308.05515v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05515
&lt;/p&gt;
&lt;p&gt;
Mono-Hydra&#26159;&#19968;&#20010;&#23454;&#26102;&#31354;&#38388;&#24863;&#30693;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#21333;&#30446;&#30456;&#26426;&#21644;IMU&#20256;&#24863;&#22120;&#65292;&#33021;&#22815;&#23454;&#26102;&#26500;&#24314;&#23460;&#20869;&#22330;&#26223;&#30340;3D&#22270;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20174;&#21333;&#30446;&#30456;&#26426;&#36755;&#20837;&#20013;&#25512;&#23548;&#28145;&#24230;&#21644;&#35821;&#20041;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#26041;&#26681;&#20449;&#24687;&#30340;&#26426;&#22120;&#20154;&#35270;&#35273;&#24815;&#24615;&#27979;&#36317;(VIO)&#31639;&#27861;&#26469;&#20445;&#35777;&#20934;&#30830;&#24615;&#21644;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;3D&#29615;&#22659;&#20013;&#33258;&#20027;&#23548;&#33322;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#23545;&#31354;&#38388;&#27010;&#24565;&#30340;&#29702;&#35299;&#65292;&#20174;&#20302;&#23618;&#20960;&#20309;&#21040;&#39640;&#23618;&#35821;&#20041;&#65292;&#22914;&#29289;&#20307;&#12289;&#22320;&#28857;&#21644;&#24314;&#31569;&#29289;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#26679;&#30340;&#29702;&#35299;&#65292;3D&#22330;&#26223;&#22270;&#25104;&#20026;&#34920;&#31034;&#29615;&#22659;&#30340;&#20998;&#23618;&#27010;&#24565;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21333;&#30446;&#35270;&#35273;&#31995;&#32479;&#23454;&#26102;&#26500;&#24314;&#36825;&#20123;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#31354;&#38388;&#24863;&#30693;&#31995;&#32479;Mono-Hydra&#65292;&#32467;&#21512;&#20102;&#21333;&#30446;&#30456;&#26426;&#21644;IMU&#20256;&#24863;&#22120;&#65292;&#19987;&#27880;&#20110;&#23460;&#20869;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#25143;&#22806;&#24212;&#29992;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#28508;&#22312;&#30340;&#29992;&#36884;&#12290;&#31995;&#32479;&#37319;&#29992;&#19968;&#22871;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#25512;&#23548;&#28145;&#24230;&#21644;&#35821;&#20041;&#12290;&#23427;&#20351;&#29992;&#20102;&#22522;&#20110;&#26041;&#26681;&#20449;&#24687;&#30340;&#26426;&#22120;&#20154;&#35270;&#35273;&#24815;&#24615;&#27979;&#36317;(VIO)&#31639;&#27861;&#65292;&#20174;&#32780;&#30830;&#20445;
&lt;/p&gt;
&lt;p&gt;
The ability of robots to autonomously navigate through 3D environments depends on their comprehension of spatial concepts, ranging from low-level geometry to high-level semantics, such as objects, places, and buildings. To enable such comprehension, 3D scene graphs have emerged as a robust tool for representing the environment as a layered graph of concepts and their relationships. However, building these representations using monocular vision systems in real-time remains a difficult task that has not been explored in depth. This paper puts forth a real-time spatial perception system Mono-Hydra, combining a monocular camera and an IMU sensor setup, focusing on indoor scenarios. However, the proposed approach is adaptable to outdoor applications, offering flexibility in its potential uses. The system employs a suite of deep learning algorithms to derive depth and semantics. It uses a robocentric visual-inertial odometry (VIO) algorithm based on square-root information, thereby ensuring 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;EDDA&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20998;&#21035;&#35299;&#20915;&#20102;&#30693;&#35782;&#35299;&#32806;&#21644;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05508</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#23884;&#20837;&#35299;&#32806;&#19982;&#39046;&#22495;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Multi-domain Recommendation with Embedding Disentangling and Domain Alignment. (arXiv:2308.05508v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#39046;&#22495;&#25512;&#33616;&#26041;&#27861;EDDA&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#20998;&#21035;&#35299;&#20915;&#20102;&#30693;&#35782;&#35299;&#32806;&#21644;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39046;&#22495;&#25512;&#33616;(MDR)&#26088;&#22312;&#20026;&#20855;&#26377;&#37325;&#21472;&#29992;&#25143;/&#29289;&#21697;&#30340;&#19981;&#21516;&#39046;&#22495;(&#20363;&#22914;&#20135;&#21697;&#31867;&#22411;)&#25552;&#20379;&#25512;&#33616;&#65292;&#23545;&#20110;&#25317;&#26377;&#22810;&#20010;&#26381;&#21153;&#30340;&#24179;&#21488;&#22914;&#20122;&#39532;&#36874;&#12289;Facebook&#21644;LinkedIn&#26159;&#24120;&#35265;&#30340;&#12290;&#29616;&#26377;&#30340;MDR&#27169;&#22411;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#24456;&#38590;&#35299;&#32806;&#21487;&#20197;&#27867;&#21270;&#21040;&#25152;&#26377;&#39046;&#22495;&#30340;&#30693;&#35782;(&#20363;&#22914;&#65292;&#29992;&#25143;&#21916;&#27426;&#24265;&#20215;&#30340;&#29289;&#21697;)&#19982;&#29305;&#23450;&#20110;&#21333;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;(&#20363;&#22914;&#65292;&#29992;&#25143;&#21916;&#27426;&#34013;&#33394;&#30340;&#26381;&#35013;&#20294;&#19981;&#21916;&#27426;&#34013;&#33394;&#30340;&#27773;&#36710;)&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#22312;&#20855;&#26377;&#23567;&#37325;&#21472;&#30340;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#30693;&#35782;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDDA&#30340;&#26032;&#30340;MDR&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#21644;&#39046;&#22495;&#23545;&#40784;&#65292;&#20998;&#21035;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#23884;&#20837;&#35299;&#32806;&#25512;&#33616;&#22120;&#20998;&#31163;&#20102;&#36328;&#39046;&#22495;&#37096;&#20998;&#21644;&#21333;&#39046;&#22495;&#37096;&#20998;&#30340;&#27169;&#22411;&#21644;&#23884;&#20837;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;MDR&#26041;&#27861;&#21482;&#20851;&#27880;&#27169;&#22411;&#23618;&#38754;&#30340;&#35299;&#32806;&#12290;&#39046;&#22495;&#23545;&#40784;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#23545;&#25239;&#35757;&#32451;&#26469;&#25552;&#21319;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-domain recommendation (MDR) aims to provide recommendations for different domains (e.g., types of products) with overlapping users/items and is common for platforms such as Amazon, Facebook, and LinkedIn that host multiple services. Existing MDR models face two challenges: First, it is difficult to disentangle knowledge that generalizes across domains (e.g., a user likes cheap items) and knowledge specific to a single domain (e.g., a user likes blue clothing but not blue cars). Second, they have limited ability to transfer knowledge across domains with small overlaps. We propose a new MDR method named EDDA with two key components, i.e., embedding disentangling recommender and domain alignment, to tackle the two challenges respectively. In particular, the embedding disentangling recommender separates both the model and embedding for the inter-domain part and the intra-domain part, while most existing MDR methods only focus on model-level disentangling. The domain alignment leverag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;EFX&#20998;&#37197;&#30340;&#23384;&#22312;&#24615;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#23545;&#20108;&#36827;&#21046;&#21644;&#27425;&#27169;&#20272;&#20540;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;EFX&#20998;&#37197;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05503</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#20272;&#20540;&#30340;EFX&#20998;&#37197;&#23384;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
EFX Allocations Exist for Binary Valuations. (arXiv:2308.05503v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#20013;&#30340;EFX&#20998;&#37197;&#30340;&#23384;&#22312;&#24615;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#23545;&#20108;&#36827;&#21046;&#21644;&#27425;&#27169;&#20272;&#20540;&#30340;&#32467;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;EFX&#20998;&#37197;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#24179;&#20998;&#37197;&#38382;&#39064;&#20197;&#21450;&#28385;&#36275;&#20844;&#24179;&#26465;&#20214;&#65288;EFX&#65289;&#30340;&#20998;&#37197;&#30340;&#23384;&#22312;&#24615;&#12290;EFX&#20998;&#37197;&#30340;&#23384;&#22312;&#24615;&#26159;&#20844;&#24179;&#20998;&#37197;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20108;&#36827;&#21046;&#20272;&#20540;&#65292;&#20854;&#20013;&#36890;&#36807;&#33719;&#24471;&#39069;&#22806;&#29289;&#21697;&#30340;&#36793;&#38469;&#22686;&#30410;&#20026;0&#25110;1&#12290;Babaioff&#31561;&#20154;[2021]&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#36827;&#21046;&#21644;&#27425;&#27169;&#30340;&#20272;&#20540;&#65292;EFX&#20998;&#37197;&#24635;&#26159;&#23384;&#22312;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#23384;&#22312;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#19968;&#33324;&#30340;&#20108;&#36827;&#21046;&#20272;&#20540;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;EFX&#20998;&#37197;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fair division problem and the existence of allocations satisfying the fairness criterion envy-freeness up to any item (EFX). The existence of EFX allocations is a major open problem in the fair division literature. We consider binary valuations where the marginal gain of the value by receiving an extra item is either $0$ or $1$. Babaioff et al. [2021] proved that EFX allocations always exist for binary and submodular valuations. In this paper, by using completely different techniques, we extend this existence result to general binary valuations that are not necessarily submodular, and we present a polynomial time algorithm for computing an EFX allocation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05502</link><description>&lt;p&gt;
&#23558;&#39034;&#24207;&#24102;&#20837;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#27861;&#24459;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bringing order into the realm of Transformer-based language models for artificial intelligence and law. (arXiv:2308.05502v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#23545;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#30340;&#31995;&#32479;&#27010;&#36848;&#12290;&#25991;&#31456;&#26088;&#22312;&#31361;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#20197;&#21450;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;TLM&#65289;&#34987;&#24191;&#27867;&#35748;&#21487;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#24320;&#21457;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#19982;&#20854;&#20182;&#25991;&#26412;&#39046;&#22495;&#19968;&#26679;&#65292;TLM&#30830;&#23454;&#25512;&#21160;&#20102;&#27861;&#24459;&#39046;&#22495;&#35768;&#22810;&#24863;&#20852;&#36259;&#20219;&#21153;&#23545;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#31532;&#19968;&#20010;Transformer&#27169;&#22411;&#25552;&#20986;&#20102;&#22823;&#32422;6&#24180;&#26102;&#38388;&#65292;&#20294;&#36825;&#39033;&#25216;&#26415;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#36805;&#29467;&#21457;&#23637;&#65292;BERT&#21644;&#30456;&#20851;&#27169;&#22411;&#25104;&#20026;&#20027;&#35201;&#21442;&#32771;&#65292;&#20063;&#22312;&#27861;&#24459;&#39046;&#22495;&#21344;&#26377;&#37325;&#35201;&#22320;&#20301;&#12290;&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#27010;&#36848;&#20102;TLM&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#38382;&#39064;&#21644;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#31361;&#20986;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#19968;&#26041;&#38754;&#20102;&#35299;Transformer&#22312;&#25903;&#25345;&#27861;&#24459;&#27969;&#31243;&#20013;&#21462;&#24471;&#30340;AI&#25104;&#21151;&#36129;&#29486;&#26159;&#20160;&#20040;&#65292;&#21478;&#19968;&#26041;&#38754;&#20102;&#35299;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#26159;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitati
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#25163;&#26415;&#23460;&#20013;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#36890;&#36807;&#22788;&#29702;&#30417;&#35270;&#22120;&#35013;&#32622;&#30340;&#25668;&#20687;&#22836;&#25910;&#38598;&#36830;&#32493;&#30340;&#34892;&#20026;&#25968;&#25454;&#65292;&#20197;&#21450;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05501</link><description>&lt;p&gt;
&#36229;&#20046;&#23547;&#24120;&#30340;&#30524;&#35266;&#20845;&#36335;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#25163;&#26415;&#23460;&#20013;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
More Than Meets the Eye: Analyzing Anesthesiologists' Visual Attention in the Operating Room Using Deep Learning Models. (arXiv:2308.05501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05501
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#25163;&#26415;&#23460;&#20013;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#20998;&#24067;&#65292;&#36890;&#36807;&#22788;&#29702;&#30417;&#35270;&#22120;&#35013;&#32622;&#30340;&#25668;&#20687;&#22836;&#25910;&#38598;&#36830;&#32493;&#30340;&#34892;&#20026;&#25968;&#25454;&#65292;&#20197;&#21450;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#22312;&#30417;&#35270;&#22120;&#19978;&#26174;&#31034;&#65292;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#26159;&#23433;&#20840;&#31649;&#29702;&#20840;&#40635;&#24739;&#32773;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65307;&#27492;&#22806;&#65292;&#35270;&#35273;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#22312;&#20840;&#31243;&#40635;&#37257;&#36807;&#31243;&#20013;&#33719;&#21462;&#29305;&#23450;&#32447;&#32034;&#30340;&#33021;&#21147;&#21487;&#33021;&#30452;&#25509;&#24433;&#21709;&#24739;&#32773;&#30340;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20351;&#29992;&#21487;&#31359;&#25140;&#30340;&#30524;&#21160;&#36861;&#36394;&#25216;&#26415;&#26469;&#20998;&#26512;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27169;&#24335;&#12290;&#34429;&#28982;&#33021;&#22815;&#20135;&#29983;&#35814;&#32454;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#31359;&#25140;&#35774;&#22791;&#23545;&#20110;&#25163;&#26415;&#23460;&#20013;&#30340;&#22823;&#35268;&#27169;&#25110;&#38271;&#26399;&#25968;&#25454;&#25910;&#38598;&#24182;&#19981;&#21487;&#25345;&#32493;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22788;&#29702;&#20197;&#30417;&#35270;&#22120;&#20026;&#22522;&#30784;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#21160;&#36861;&#36394;&#26041;&#27861;&#65292;&#25910;&#38598;&#36830;&#32493;&#30340;&#34892;&#20026;&#25968;&#25454;&#65292;&#24182;&#22312;&#26368;&#23567;&#24178;&#25200;&#19979;&#20102;&#35299;&#40635;&#37257;&#24072;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#26694;&#26550;&#25910;&#38598;&#25163;&#26415;&#23460;&#30340;&#35270;&#39057;&#24405;&#20687;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patient's vital signs, which are displayed on monitors, make the anesthesiologist's visual attention (VA) a key component in the safe management of patients under general anesthesia; moreover, the distribution of said VA and the ability to acquire specific cues throughout the anesthetic, may have a direct impact on patient's outcome. Currently, most studies employ wearable eye-tracking technologies to analyze anesthesiologists' visual patterns. Albeit being able to produce meticulous data, wearable devices are not a sustainable solution for large-scale or long-term use for data collection in the operating room (OR). Thus, by utilizing a novel eye-tracking method in the form of deep learning models that process monitor-mounted webcams, we collected continuous behavioral data and gained insight into the anesthesiologist's VA distribution with minimal disturbance to their natural workflow. In this study, we collected OR video recordings using the proposed framework and compared different 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;XAI&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#38899;&#20048;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#28508;&#31354;&#38388;&#27491;&#21017;&#21270;&#12289;&#29992;&#25143;&#30028;&#38754;&#21453;&#39304;&#24490;&#29615;&#21644;&#38899;&#20048;&#23646;&#24615;&#30340;&#21487;&#35270;&#21270;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05496</link><description>&lt;p&gt;
&#25506;&#32034;XAI&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#35299;&#37322;&#29983;&#25104;&#38899;&#20048;&#20013;&#30340;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring XAI for the Arts: Explaining Latent Space in Generative Music. (arXiv:2308.05496v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;XAI&#22312;&#33402;&#26415;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#38899;&#20048;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#28508;&#31354;&#38388;&#27491;&#21017;&#21270;&#12289;&#29992;&#25143;&#30028;&#38754;&#21453;&#39304;&#24490;&#29615;&#21644;&#38899;&#20048;&#23646;&#24615;&#30340;&#21487;&#35270;&#21270;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26377;&#28508;&#21147;&#25903;&#25345;&#26356;&#20855;&#20114;&#21160;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#21327;&#20316;&#21019;&#36896;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#36827;&#34892;&#21019;&#36896;&#24615;&#30340;&#21512;&#20316;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#21019;&#36896;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38656;&#35201;&#20855;&#22791;&#21487;&#35843;&#35797;&#30340;&#29305;&#24615;&#65292;&#21363;&#21487;&#26816;&#26597;&#12289;&#21487;&#29702;&#35299;&#21644;&#21487;&#20462;&#25913;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#33402;&#26415;&#39046;&#22495;&#20013;&#20960;&#20046;&#27809;&#26377;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#38899;&#20048;&#29983;&#25104;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26356;&#21152;&#21487;&#35299;&#37322;&#65307;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29983;&#25104;&#38899;&#20048;&#23567;&#33410;&#30340;MeasureVAE&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;i&#65289;&#20351;&#29992;&#28508;&#31354;&#38388;&#27491;&#21017;&#21270;&#65292;&#24378;&#21046;&#19968;&#20123;&#29305;&#23450;&#32500;&#24230;&#30340;&#28508;&#31354;&#38388;&#26144;&#23556;&#21040;&#26377;&#24847;&#20041;&#30340;&#38899;&#20048;&#23646;&#24615;&#65292;ii&#65289;&#25552;&#20379;&#29992;&#25143;&#30028;&#38754;&#21453;&#39304;&#24490;&#29615;&#65292;&#20801;&#35768;&#29992;&#25143;&#35843;&#25972;&#28508;&#31354;&#38388;&#30340;&#32500;&#24230;&#24182;&#23454;&#26102;&#35266;&#23519;&#36825;&#20123;&#21464;&#21270;&#30340;&#32467;&#26524;&#65292;iii&#65289;&#25552;&#20379;&#38899;&#20048;&#23646;&#24615;&#22312;&#28508;&#31354;&#38388;&#20013;&#30340;&#21487;&#35270;&#21270;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI has the potential to support more interactive and fluid co-creative AI systems which can creatively collaborate with people. To do this, creative AI models need to be amenable to debugging by offering eXplainable AI (XAI) features which are inspectable, understandable, and modifiable. However, currently there is very little XAI for the arts. In this work, we demonstrate how a latent variable model for music generation can be made more explainable; specifically we extend MeasureVAE which generates measures of music. We increase the explainability of the model by: i) using latent space regularisation to force some specific dimensions of the latent space to map to meaningful musical attributes, ii) providing a user interface feedback loop to allow people to adjust dimensions of the latent space and observe the results of these changes in real-time, iii) providing a visualisation of the musical attributes in the latent space to help people understand and predict the effect o
&lt;/p&gt;</description></item><item><title>LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2308.05481</link><description>&lt;p&gt;
LLM&#21464;&#25104;DBA
&lt;/p&gt;
&lt;p&gt;
LLM As DBA. (arXiv:2308.05481v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05481
&lt;/p&gt;
&lt;p&gt;
LLM&#21464;&#25104;DBA&#65292;&#25552;&#20379;&#25968;&#25454;&#24211;&#32500;&#25252;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#32463;&#39564;&#21644;&#22810;&#20010;LLMs&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65288;DBA&#65289;&#22312;&#31649;&#29702;&#12289;&#32500;&#25252;&#21644;&#20248;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20197;&#30830;&#20445;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;DBA&#26469;&#35828;&#65292;&#31649;&#29702;&#22823;&#37327;&#25968;&#25454;&#24211;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#20113;&#25968;&#25454;&#24211;&#19978;&#30340;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#65289;&#26159;&#22256;&#38590;&#21644;&#32321;&#29712;&#30340;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#29702;&#35299;&#26377;&#20215;&#20540;&#25991;&#20214;&#24182;&#29983;&#25104;&#21512;&#29702;&#31572;&#26696;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;D-Bot&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#24211;&#31649;&#29702;&#21592;&#65292;&#23427;&#21487;&#20197;&#25345;&#32493;&#20174;&#25991;&#26412;&#26469;&#28304;&#20013;&#33719;&#21462;&#25968;&#25454;&#24211;&#32500;&#25252;&#32463;&#39564;&#65292;&#24182;&#20026;&#30446;&#26631;&#25968;&#25454;&#24211;&#25552;&#20379;&#21512;&#29702;&#12289;&#26377;&#29702;&#12289;&#21450;&#26102;&#30340;&#35786;&#26029;&#21644;&#20248;&#21270;&#24314;&#35758;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#20197;LLM&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#24211;&#32500;&#25252;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#20174;&#25991;&#26723;&#21644;&#24037;&#20855;&#20013;&#26816;&#27979;&#25968;&#25454;&#24211;&#32500;&#25252;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#24605;&#32500;&#26641;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#21327;&#20316;&#35786;&#26029;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experiment
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#39640;&#20998;&#36776;&#29575;3+1D&#38647;&#36798;&#30340;&#32972;&#26223;&#19979;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#30001;&#20110;3D&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#19982;3+1D&#38647;&#36798;&#28857;&#20113;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#29616;&#26377;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#21487;&#20316;&#20026;&#22522;&#20110;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#36215;&#28857;&#12290;&#20294;&#20026;&#20102;&#36866;&#24212;&#38647;&#36798;&#39046;&#22495;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#29616;&#26377;&#26816;&#27979;&#22120;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2308.05478</link><description>&lt;p&gt;
&#22312;&#39640;&#20998;&#36776;&#29575;3+1D&#38647;&#36798;&#32972;&#26223;&#19979;&#65292;&#23545;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#36827;&#34892;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reviewing 3D Object Detectors in the Context of High-Resolution 3+1D Radar. (arXiv:2308.05478v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05478
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#39640;&#20998;&#36776;&#29575;3+1D&#38647;&#36798;&#30340;&#32972;&#26223;&#19979;&#65292;&#32508;&#36848;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#30001;&#20110;3D&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#19982;3+1D&#38647;&#36798;&#28857;&#20113;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#29616;&#26377;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#21487;&#20316;&#20026;&#22522;&#20110;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#36215;&#28857;&#12290;&#20294;&#20026;&#20102;&#36866;&#24212;&#38647;&#36798;&#39046;&#22495;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#29616;&#26377;&#26816;&#27979;&#22120;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#39640;&#20998;&#36776;&#29575;4D(3+1D)&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#21457;&#23637;&#21644;&#24066;&#22330;&#24341;&#20837;&#25512;&#21160;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#24863;&#30693;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38647;&#36798;&#28857;&#20113;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#12290;3D&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#19978;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#26159;3D&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#29087;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#21508;&#20855;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#30001;&#20110;3D&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#21644;3+1D&#38647;&#36798;&#28857;&#20113;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#29616;&#26377;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#26159;&#22522;&#20110;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#30446;&#26631;&#26816;&#27979;&#30340;&#33258;&#28982;&#22522;&#30784;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#27493;&#26159;&#20998;&#26512;&#29616;&#26377;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#12290;&#20026;&#20102;&#23558;&#38024;&#23545;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#24320;&#21457;&#30340;&#29616;&#26377;3D&#28857;&#20113;&#30446;&#26631;&#26816;&#27979;&#22120;&#24212;&#29992;&#20110;&#38647;&#36798;&#39046;&#22495;&#65292;&#39318;&#20808;&#38656;&#35201;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#19968;&#20123;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#36866;&#24212;&#24615;&#35843;&#25972;&#65292;&#20363;&#22914;PointPillars&#65292;&#20197;&#20351;&#20854;&#36866;&#29992;&#20110;&#38647;&#36798;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments and the beginning market introduction of high-resolution imaging 4D (3+1D) radar sensors have initialized deep learning-based radar perception research. We investigate deep learning-based models operating on radar point clouds for 3D object detection. 3D object detection on lidar point cloud data is a mature area of 3D vision. Many different architectures have been proposed, each with strengths and weaknesses. Due to similarities between 3D lidar point clouds and 3+1D radar point clouds, those existing 3D object detectors are a natural basis to start deep learning-based 3D object detection on radar data. Thus, the first step is to analyze the detection performance of the existing models on the new data modality and evaluate them in depth. In order to apply existing 3D point cloud object detectors developed for lidar point clouds to the radar domain, they need to be adapted first. While some detectors, such as PointPillars, have already been adapted to be applicable 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#25506;&#35752;&#20102;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#32467;&#26524;&#21457;&#29616;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#20351;&#29992;&#65292;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#21307;&#29983;&#30340;&#21442;&#19982;&#20173;&#36739;&#23569;&#25253;&#36947;&#12290;</title><link>http://arxiv.org/abs/2308.05411</link><description>&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI applications in the Medical Domain: a systematic review. (arXiv:2308.05411v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#25506;&#35752;&#20102;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#20013;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#32467;&#26524;&#21457;&#29616;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#20351;&#29992;&#65292;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#21307;&#29983;&#30340;&#21442;&#19982;&#20173;&#36739;&#23569;&#25253;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#12289;&#24739;&#32773;&#25252;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#36825;&#20123;&#24212;&#29992;&#22312;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23454;&#38469;&#19978;&#24456;&#23569;&#26377;&#24212;&#29992;&#12290;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38754;&#20020;&#30528;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#12289;&#36981;&#23432;&#27861;&#35268;&#12289;&#21512;&#29702;&#20351;&#29992;&#25968;&#25454;&#31561;&#21508;&#31181;&#25361;&#25112;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#20351;&#20154;&#31867;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#24182;&#20449;&#20219;&#20854;&#32467;&#26524;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20960;&#24180;&#21457;&#34920;&#30340;198&#31687;&#25991;&#31456;&#65292;&#23545;&#21307;&#23398;&#20915;&#31574;&#25903;&#25345;&#30340;XAI&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;&#30456;&#20851;&#25991;&#31456;&#30340;&#31995;&#32479;&#32508;&#21512;&#20135;&#29983;&#20102;&#20960;&#20010;&#21457;&#29616;&#65306;&#65288;1&#65289;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#37319;&#29992;&#20102;&#36890;&#29992;&#27169;&#22411;&#30340;XAI&#25216;&#26415;&#65292;&#65288;2&#65289;&#30456;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#26356;&#22810;&#22320;&#20351;&#29992;&#65292;&#65288;3&#65289;&#35299;&#37322;&#24615;&#34987;&#24212;&#29992;&#20110;&#25552;&#39640;&#20449;&#20219;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#25253;&#36947;&#20102;&#21307;&#29983;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence in Medicine has made significant progress with emerging applications in medical imaging, patient care, and other areas. While these applications have proven successful in retrospective studies, very few of them were applied in practice.The field of Medical AI faces various challenges, in terms of building user trust, complying with regulations, using data ethically.Explainable AI (XAI) aims to enable humans understand AI and trust its results. This paper presents a literature review on the recent developments of XAI solutions for medical decision support, based on a representative sample of 198 articles published in recent years. The systematic synthesis of the relevant articles resulted in several findings. (1) model-agnostic XAI techniques were mostly employed in these solutions, (2) deep learning models are utilized more than other types of machine learning models, (3) explainability was applied to promote trust, but very few works reported the physicians par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#22312;&#20892;&#20316;&#29289;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#35270;&#35282;&#21644;&#20808;&#21069;&#26041;&#27861;&#65292;&#26681;&#25454;&#27979;&#35797;&#21306;&#22495;&#36873;&#25321;&#26368;&#20339;&#34701;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05407</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#29992;&#20110;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Assessment of Multi-view fusion learning for Crop Classification. (arXiv:2308.05407v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#22810;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#22312;&#20892;&#20316;&#29289;&#20998;&#31867;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#21333;&#19968;&#35270;&#35282;&#21644;&#20808;&#21069;&#26041;&#27861;&#65292;&#26681;&#25454;&#27979;&#35797;&#21306;&#22495;&#36873;&#25321;&#26368;&#20339;&#34701;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36965;&#24863;&#25968;&#25454;&#28304;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#22810;&#35270;&#35282;&#23398;&#20064;&#24314;&#27169;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36965;&#24863;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#12289;&#24133;&#24230;&#21644;&#22122;&#22768;&#24046;&#24322;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#22312;&#36755;&#20837;&#32423;&#21035;&#36827;&#34892;&#34701;&#21512;&#65292;&#20294;&#20854;&#20182;&#26356;&#39640;&#32423;&#30340;&#34701;&#21512;&#31574;&#30053;&#21487;&#33021;&#20250;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;CropHarvest&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#20892;&#20316;&#29289;&#20998;&#31867;&#30340;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;&#34701;&#21512;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#21333;&#20010;&#35270;&#35282;&#21644;&#20808;&#21069;&#30340;&#34701;&#21512;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#34701;&#21512;&#26041;&#27861;&#19968;&#30452;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22810;&#35270;&#35282;&#34701;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;&#65292;&#22312;&#27979;&#35797;&#21306;&#22495;&#19981;&#21516;&#30340;&#26041;&#27861;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#36873;&#25321;&#34701;&#21512;&#26041;&#27861;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a rapidly increasing amount and diversity of remote sensing (RS) data sources, there is a strong need for multi-view learning modeling. This is a complex task when considering the differences in resolution, magnitude, and noise of RS data. The typical approach for merging multiple RS sources has been input-level fusion, but other - more advanced - fusion strategies may outperform this traditional approach. This work assesses different fusion strategies for crop classification in the CropHarvest dataset. The fusion methods proposed in this work outperform models based on individual views and previous fusion methods. We do not find one single fusion method that consistently outperforms all other approaches. Instead, we present a comparison of multi-view fusion methods for three different datasets and show that, depending on the test region, different methods obtain the best performance. Despite this, we suggest a preliminary criterion for the selection of fusion methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;AI&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#20998;&#26512;&#20102;AI&#20195;&#29702;&#20449;&#20219;&#30340;&#20027;&#35201;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;&#36825;&#19968;&#26032;&#19968;&#20195;&#33258;&#21160;&#21270;&#20195;&#29702;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#32771;&#34385;&#22240;&#32032;&#21644;&#25361;&#25112;&#12290;&#26368;&#32456;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#30028;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.05391</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;LLM&#30340;AI&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#20449;&#20219;:&#26032;&#30340;&#32771;&#34385;&#22240;&#32032;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust in LLM-Based AI Automation Agents: New Considerations and Future Challenges. (arXiv:2308.05391v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;AI&#33258;&#21160;&#21270;&#20195;&#29702;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;&#65292;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#20998;&#26512;&#20102;AI&#20195;&#29702;&#20449;&#20219;&#30340;&#20027;&#35201;&#26041;&#38754;&#65292;&#24182;&#38024;&#23545;&#36825;&#19968;&#26032;&#19968;&#20195;&#33258;&#21160;&#21270;&#20195;&#29702;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#32771;&#34385;&#22240;&#32032;&#21644;&#25361;&#25112;&#12290;&#26368;&#32456;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#30028;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20195;&#29702;&#30340;&#20449;&#20219;&#24050;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20174;&#32780;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#29702;&#35299;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#22522;&#20110;LLM&#30340;AI&#20195;&#29702;&#26694;&#26550;&#30340;&#20986;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#22312;&#27969;&#31243;&#33258;&#21160;&#21270;&#39046;&#22495;&#65292;&#20986;&#29616;&#20102;&#19968;&#20195;&#26032;&#30340;&#22522;&#20110;AI&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#21644;&#22521;&#35757;&#26426;&#21046;&#65292;&#24314;&#31435;&#33258;&#21160;&#21270;&#27969;&#31243;&#21464;&#24471;&#26356;&#21152;&#21487;&#34892;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#20123;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#35752;&#35770;&#30340;AI&#20195;&#29702;&#30340;&#20449;&#20219;&#20027;&#35201;&#26041;&#38754;&#65292;&#24182;&#30830;&#23450;&#20102;&#19982;&#36825;&#19968;&#26032;&#19968;&#20195;&#33258;&#21160;&#21270;&#20195;&#29702;&#30456;&#20851;&#30340;&#20855;&#20307;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27492;&#31867;&#20135;&#21697;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#30740;&#31350;&#30028;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust in AI agents has been extensively studied in the literature, resulting in significant advancements in our understanding of this field. However, the rapid advancements in Large Language Models (LLMs) and the emergence of LLM-based AI agent frameworks pose new challenges and opportunities for further research. In the field of process automation, a new generation of AI-based agents has emerged, enabling the execution of complex tasks. At the same time, the process of building automation has become more accessible to business users via user-friendly no-code tools and training mechanisms. This paper explores these new challenges and opportunities, analyzes the main aspects of trust in AI agents discussed in existing literature, and identifies specific considerations and challenges relevant to this new generation of automation agents. We also evaluate how nascent products in this category address these considerations. Finally, we highlight several challenges that the research community
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12289;&#26435;&#21033;&#20154;&#20449;&#24687;&#21644;IPC&#20195;&#30721;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.05385</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20998;&#31867;&#23398;&#20064;&#21644;&#21382;&#21490;&#27169;&#24335;&#24314;&#27169;&#29992;&#20110;&#19987;&#21033;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Taxonomy Learning and Historical Patterns Modelling for Patent Classification. (arXiv:2308.05385v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#32508;&#21512;&#20102;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12289;&#26435;&#21033;&#20154;&#20449;&#24687;&#21644;IPC&#20195;&#30721;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19987;&#21033;&#20998;&#31867;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#20998;&#31867;&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#19987;&#21033;&#20998;&#37197;&#22810;&#20010;&#22269;&#38469;&#19987;&#21033;&#20998;&#31867;&#65288;IPC&#65289;&#20195;&#30721;&#12290;&#26368;&#36817;&#29992;&#20110;&#33258;&#21160;&#20998;&#31867;&#19987;&#21033;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#20998;&#26512;&#19987;&#21033;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#25991;&#26412;&#20043;&#22806;&#65292;&#27599;&#20010;&#19987;&#21033;&#36824;&#19982;&#19968;&#20123;&#26435;&#21033;&#20154;&#30456;&#20851;&#32852;&#65292;&#20102;&#35299;&#20182;&#20204;&#30003;&#35831;&#30340;&#19987;&#21033;&#23545;&#20110;&#20998;&#31867;&#24448;&#24448;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#27492;&#22806;&#65292;IPC&#31995;&#32479;&#21046;&#23450;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#19988;&#20351;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;IPC&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32508;&#21512;&#32771;&#34385;&#19978;&#36848;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#32771;&#34385;&#19987;&#21033;&#20449;&#24687;&#30340;&#25972;&#21512;&#26694;&#26550;&#26469;&#36827;&#34892;&#19987;&#21033;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;IPC&#20195;&#30721;&#30456;&#20851;&#24615;&#23398;&#20064;&#27169;&#22359;&#65292;&#36890;&#36807;&#22312;&#21516;&#19968;&#23618;&#32423;&#21644;&#19981;&#21516;&#23618;&#32423;&#20043;&#38388;&#33258;&#36866;&#24212;&#22320;&#20256;&#36882;&#21644;&#32858;&#21512;&#28040;&#24687;&#65292;&#20174;&#32780;&#24471;&#21040;&#23427;&#20204;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patent classification aims to assign multiple International Patent Classification (IPC) codes to a given patent. Recent methods for automatically classifying patents mainly focus on analyzing the text descriptions of patents. However, apart from the texts, each patent is also associated with some assignees, and the knowledge of their applied patents is often valuable for classification. Furthermore, the hierarchical taxonomy formulated by the IPC system provides important contextual information and enables models to leverage the correlations between IPC codes for more accurate classification. However, existing methods fail to incorporate the above aspects. In this paper, we propose an integrated framework that comprehensively considers the information on patents for patent classification. To be specific, we first present an IPC codes correlations learning module to derive their semantic representations via adaptively passing and aggregating messages within the same level and across dif
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05379</link><description>&lt;p&gt;
&#36229;&#36234;&#35821;&#20041;&#65306;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning. (arXiv:2308.05379v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34892;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#26597;&#35810;-&#39033;&#30446;&#21305;&#37197;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#24314;&#27169;&#26088;&#22312;&#23450;&#20301;&#19982;&#23545;&#24212;&#26597;&#35810;&#30456;&#20851;&#30340;&#29702;&#24819;&#39033;&#30446;&#65292;&#36825;&#23545;&#20110;&#25628;&#32034;&#24341;&#25806;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#35780;&#20272;&#26597;&#35810;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32431;&#35821;&#20041;&#21305;&#37197;&#24182;&#19981;&#26159;&#21807;&#19968;&#30340;&#26041;&#27861;&#12290;&#23454;&#38469;&#19978;&#65292;&#20174;&#29992;&#25143;&#25628;&#32034;&#35760;&#24405;&#30340;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#36741;&#21161;&#26597;&#35810;-&#39033;&#30446;&#20132;&#20114;&#21487;&#20197;&#25552;&#20379;&#36827;&#19968;&#27493;&#25581;&#31034;&#29992;&#25143;&#25628;&#32034;&#24847;&#22270;&#30340;&#32447;&#32034;&#12290;&#24471;&#30410;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34892;&#20026;&#22686;&#24378;&#30456;&#20851;&#23398;&#20064;&#27169;&#22411;&#30340;&#25903;&#20184;&#23453;&#25628;&#32034;&#27169;&#22411;&#65288;BARL-ASe&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#30446;&#26631;&#39033;&#30446;&#30340;&#30456;&#37051;&#26597;&#35810;&#21644;&#30446;&#26631;&#26597;&#35810;&#30340;&#30456;&#37051;&#39033;&#30446;&#26469;&#34917;&#20805;&#30446;&#26631;&#26597;&#35810;-&#39033;&#30446;&#30340;&#35821;&#20041;&#21305;&#37197;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#22810;&#23618;&#20849;&#21516;&#27880;&#24847;&#21147;&#65292;&#20174;&#30456;&#37051;&#21644;&#30446;&#26631;&#35270;&#22270;&#20013;&#25552;&#21462;&#20102;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#27169;&#22411;&#38543;&#21518;&#37319;&#29992;&#37051;&#23621;-&#30446;&#26631;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevance modeling aims to locate desirable items for corresponding queries, which is crucial for search engines to ensure user experience. Although most conventional approaches address this problem by assessing the semantic similarity between the query and item, pure semantic matching is not everything. In reality, auxiliary query-item interactions extracted from user historical behavior data of the search log could provide hints to reveal users' search intents further. Drawing inspiration from this, we devise a novel Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that leverages neighbor queries of target item and neighbor items of target query to complement target query-item semantic matching. Specifically, our model builds multi-level co-attention for distilling coarse-grained and fine-grained semantic representations from both neighbor and target views. The model subsequently employs neighbor-target self-supervised learning to improve the accuracy and robu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.05374</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;LLMs&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#35843;&#26597;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. (arXiv:2308.05374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20043;&#21069;&#65292;&#30830;&#20445;&#23545;&#40784;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26469;&#35780;&#20272;LLMs&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#12289;&#20215;&#20540;&#35266;&#21644;&#27861;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#26102;&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;LLM&#21487;&#20449;&#24230;&#30340;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#12290;&#27599;&#20010;&#20027;&#35201;&#31867;&#21035;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#33509;&#24178;&#23376;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24555;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;CNN&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#24110;&#21161;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#22312;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;</title><link>http://arxiv.org/abs/2308.05364</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Machine Learning aided Computer Architecture Design for CNN Inferencing Systems. (arXiv:2308.05364v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#35745;&#31639;&#26426;&#26550;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#24555;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;CNN&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#24110;&#21161;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#22312;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21644;&#21450;&#26102;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#21644;&#36793;&#32536;&#35745;&#31639;&#31561;&#26032;&#20852;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;ML&#31639;&#27861;&#20043;&#19968;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#38656;&#35201;&#39640;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#28385;&#36275;&#35774;&#35745;&#32422;&#26463;&#65292;&#20154;&#20204;&#20351;&#29992;ML&#21152;&#36895;&#22120;&#22914;GPGPUs&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#21152;&#36895;&#22120;&#36890;&#24120;&#28041;&#21450;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#38656;&#35201;&#22823;&#37327;&#25163;&#24037;&#21162;&#21147;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#24555;DSE&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#36866;&#21512;CNN&#25512;&#29702;&#31995;&#32479;&#30340;GPGPU&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;CNN&#21151;&#32791;&#21644;&#24615;&#33021;&#39044;&#27979;&#65292;MAPE&#20998;&#21035;&#20026;5.03%&#21644;5.94%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#35745;&#31639;&#26426;&#26550;&#26500;&#24072;&#33021;&#22815;&#22312;&#24320;&#21457;&#30340;&#26089;&#26399;&#20272;&#35745;&#21151;&#32791;&#21644;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#23569;&#24320;&#21457;&#21608;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient and timely calculations of Machine Learning (ML) algorithms are essential for emerging technologies like autonomous driving, the Internet of Things (IoT), and edge computing. One of the primary ML algorithms used in such systems is Convolutional Neural Networks (CNNs), which demand high computational resources. This requirement has led to the use of ML accelerators like GPGPUs to meet design constraints. However, selecting the most suitable accelerator involves Design Space Exploration (DSE), a process that is usually time-consuming and requires significant manual effort. Our work presents approaches to expedite the DSE process by identifying the most appropriate GPGPU for CNN inferencing systems. We have developed a quick and precise technique for forecasting the power and performance of CNNs during inference, with a MAPE of 5.03% and 5.94%, respectively. Our approach empowers computer architects to estimate power and performance in the early stages of development, reducing 
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20998;&#31867;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20197;&#38590;&#20197;&#34987;&#20154;&#31867;&#36776;&#35748;&#30340;&#26041;&#24335;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#26102;&#30340;&#26356;&#39640;&#32423;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#65292;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;</title><link>http://arxiv.org/abs/2308.05341</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#65306;&#25506;&#32034;ChatGPT&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT. (arXiv:2308.05341v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20998;&#31867;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#19982;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20197;&#38590;&#20197;&#34987;&#20154;&#31867;&#36776;&#35748;&#30340;&#26041;&#24335;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#26102;&#30340;&#26356;&#39640;&#32423;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#65292;&#22312;&#21306;&#20998;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#26102;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#38754;&#21521;&#20844;&#20247;&#25552;&#20379;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#34987;&#23398;&#29983;&#29992;&#26469;&#29983;&#25104;&#25955;&#25991;&#25110;&#25972;&#20010;&#35770;&#25991;&#12290;&#20294;&#26159;&#32769;&#24072;&#22914;&#20309;&#30693;&#36947;&#19968;&#31687;&#25991;&#26412;&#26159;&#30001;&#23398;&#29983;&#36824;&#26159;&#30001;&#20154;&#24037;&#26234;&#33021;&#32534;&#20889;&#30340;&#65311;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20256;&#32479;&#21644;&#26032;&#30340;&#29305;&#24449;&#26469;(1)&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;(2)&#30001;&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#30340;&#25991;&#26412;&#12290;&#30001;&#20110;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#20154;&#24037;&#26234;&#33021;&#34987;&#25351;&#31034;&#20197;&#20154;&#31867;&#38590;&#20197;&#36776;&#35748;&#30340;&#26041;&#24335;&#21019;&#24314;&#25991;&#26412;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#25105;&#20204;&#36824;&#23545;&#36825;&#31181;&#26356;&#39640;&#32423;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#20316;&#20102;&#28085;&#30422;10&#20010;&#23398;&#26657;&#35805;&#39064;&#30340;&#26032;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#30340;F1&#20998;&#25968;&#36229;&#36807;96%&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;&#23545;&#22522;&#30784;&#21644;&#39640;&#32423;&#20154;&#24037;&#29983;&#25104;/&#20154;&#24037;&#26234;&#33021;&#37325;&#26032;&#34920;&#36798;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#30340;F1&#20998;&#25968;&#36229;&#36807;78%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, generative AIs like ChatGPT have become available to the wide public. These tools can for instance be used by students to generate essays or whole theses. But how does a teacher know whether a text is written by a student or an AI? In our work, we explore traditional and new features to (1) detect text generated by AI from scratch and (2) text rephrased by AI. Since we found that classification is more difficult when the AI has been instructed to create the text in a way that a human would not recognize that it was generated by an AI, we also investigate this more advanced case. For our experiments, we produced a new text corpus covering 10 school topics. Our best systems to classify basic and advanced human-generated/AI-generated texts have F1-scores of over 96%. Our best systems for classifying basic and advanced human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems use a combination of perplexity, semantic, list lookup, error-based, readability, A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adv-Inpainting&#30340;&#21019;&#26032;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#22270;&#26696;&#21644;&#36793;&#30028;&#26041;&#38754;&#26356;&#21152;&#33258;&#28982;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05320</link><description>&lt;p&gt;
Adv-Inpainting:&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;
&lt;/p&gt;
&lt;p&gt;
Adv-Inpainting: Generating Natural and Transferable Adversarial Patch via Attention-guided Feature Fusion. (arXiv:2308.05320v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Adv-Inpainting&#30340;&#21019;&#26032;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#29305;&#24449;&#34701;&#21512;&#29983;&#25104;&#33258;&#28982;&#19988;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#22270;&#26696;&#21644;&#36793;&#30028;&#26041;&#38754;&#26356;&#21152;&#33258;&#28982;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21033;&#29992;&#21152;&#24615;&#22122;&#22768;&#25915;&#20987;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#25805;&#20316;&#25972;&#20010;&#33080;&#37096;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#33080;&#35782;&#21035;&#25915;&#20987;&#37117;&#22522;&#20110;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#23558;&#25200;&#21160;&#38480;&#21046;&#22312;&#19968;&#20010;&#36739;&#23567;&#30340;&#21306;&#22495;&#20869;&#12290;&#20808;&#21069;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#25915;&#20987;&#24120;&#24120;&#23548;&#33268;&#19981;&#33258;&#28982;&#30340;&#22270;&#26696;&#21644;&#26126;&#26174;&#30340;&#36793;&#30028;&#65292;&#23481;&#26131;&#34987;&#23519;&#35273;&#12290;&#25105;&#20204;&#35748;&#20026;&#29983;&#25104;&#24102;&#26377;&#21512;&#29702;&#20869;&#23481;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#20250;&#27604;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#25110;&#30452;&#25509;&#20174;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#26356;&#20855;&#26377;&#26356;&#24378;&#30340;&#36801;&#31227;&#24615;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#19988;&#39640;&#24230;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20004;&#38454;&#27573;&#31895;&#21040;&#31934;&#30340;&#25915;&#20987;&#26694;&#26550;&#65292;&#31216;&#20026;Adv-Inpainting&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;StyleGAN&#65288;Att-StyleGAN&#65289;&#65292;&#26681;&#25454;&#27880;&#24847;&#21147;&#22270;&#33258;&#36866;&#24212;&#22320;&#32467;&#21512;&#32441;&#29702;&#21644;&#36523;&#20221;&#29305;&#24449;&#65292;&#29983;&#25104;&#39640;&#24230;&#21487;&#36801;&#31227;&#21644;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#36148;&#32440;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rudimentary adversarial attacks utilize additive noise to attack facial recognition (FR) models. However, because manipulating the total face is impractical in the physical setting, most real-world FR attacks are based on adversarial patches, which limit perturbations to a small area. Previous adversarial patch attacks often resulted in unnatural patterns and clear boundaries that were easily noticeable. In this paper, we argue that generating adversarial patches with plausible content can result in stronger transferability than using additive noise or directly sampling from the latent space. To generate natural-looking and highly transferable adversarial patches, we propose an innovative two-stage coarse-to-fine attack framework called Adv-Inpainting. In the first stage, we propose an attention-guided StyleGAN (Att-StyleGAN) that adaptively combines texture and identity features based on the attention map to generate high-transferable and natural adversarial patches. In the second
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>DC-GCT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#38142;&#35774;&#35745;&#26469;&#32422;&#26463;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#23427;&#32467;&#21512;&#20102;GCN&#21644;Transformer&#30340;&#20248;&#28857;&#65292;&#20805;&#20998;&#25429;&#25417;&#20102;&#20154;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.05298</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#21452;&#38142;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Double-chain Constraints for 3D Human Pose Estimation in Images and Videos. (arXiv:2308.05298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05298
&lt;/p&gt;
&lt;p&gt;
DC-GCT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#38142;&#35774;&#35745;&#26469;&#32422;&#26463;&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65292;&#23427;&#32467;&#21512;&#20102;GCN&#21644;Transformer&#30340;&#20248;&#28857;&#65292;&#20805;&#20998;&#25429;&#25417;&#20102;&#20154;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#28145;&#24230;&#20449;&#24687;&#30340;2D&#23039;&#21183;&#37325;&#24314;3D&#23039;&#21183;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20154;&#31867;&#36816;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#26159;&#20027;&#35201;&#21407;&#22240;&#12290;&#20851;&#38190;&#22312;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#20851;&#33410;&#20043;&#38388;&#30340;&#31354;&#38388;&#32422;&#26463;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#30340;&#22266;&#26377;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#21452;&#38142;&#22270;&#21367;&#31215;&#21464;&#25442;&#22120; (DC-GCT)&#65292;&#36890;&#36807;&#30001;&#23616;&#37096;&#21040;&#20840;&#23616;&#21644;&#20840;&#23616;&#21040;&#23616;&#37096;&#26500;&#25104;&#30340;&#21452;&#38142;&#35774;&#35745;&#26469;&#32422;&#26463;&#23039;&#21183;&#65292;&#20197;&#33719;&#24471;&#36866;&#29992;&#20110;&#24403;&#21069;&#20154;&#20307;&#23039;&#21183;&#30340;&#22797;&#26434;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;GCN&#21644;Transformer&#30340;&#20248;&#28857;&#65292;&#24182;&#22522;&#20110;GCN&#35774;&#35745;&#20102;&#22522;&#20110;&#26412;&#22320;&#32422;&#26463;&#27169;&#22359; (LCM) &#21644;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#20840;&#23616;&#32422;&#26463;&#27169;&#22359; (GCM)&#65292;&#20197;&#21450;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359; (FIM)&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20805;&#20998;&#25429;&#25417;&#20102;&#20154;&#20307;&#20851;&#33410;&#20043;&#38388;&#30340;&#22810;&#32423;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#20248;&#21270;&#27169;&#22411;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26102;&#38388;&#20449;&#24687;&#24341;&#20837;&#21333;&#24103;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing 3D poses from 2D poses lacking depth information is particularly challenging due to the complexity and diversity of human motion. The key is to effectively model the spatial constraints between joints to leverage their inherent dependencies. Thus, we propose a novel model, called Double-chain Graph Convolutional Transformer (DC-GCT), to constrain the pose through a double-chain design consisting of local-to-global and global-to-local chains to obtain a complex representation more suitable for the current human pose. Specifically, we combine the advantages of GCN and Transformer and design a Local Constraint Module (LCM) based on GCN and a Global Constraint Module (GCM) based on self-attention mechanism as well as a Feature Interaction Module (FIM). The proposed method fully captures the multi-level dependencies between human body joints to optimize the modeling capability of the model. Moreover, we propose a method to use temporal information into the single-frame model 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#26469;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2308.05295</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#20915;&#31574;&#65306;&#32508;&#21512;&#12289;&#39564;&#35777;&#12289;&#22522;&#30784;&#21644;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Multimodal Pretrained Models for Sequential Decision-Making: Synthesis, Verification, Grounding, and Perception. (arXiv:2308.05295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#26469;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#32534;&#30721;&#20197;&#22810;&#31181;&#24418;&#24335;&#34920;&#36798;&#30340;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#65292;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#33021;&#38598;&#25104;&#21040;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#26500;&#24314;&#21644;&#39564;&#35777;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#27979;&#23558;&#36825;&#20123;&#25511;&#21046;&#22120;&#19982;&#20219;&#21153;&#29615;&#22659;&#30456;&#36830;&#25509;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#25551;&#36848;&#26597;&#35810;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#26500;&#24314;&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#32534;&#30721;&#27169;&#22411;&#30340;&#20219;&#21153;&#30456;&#20851;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#23427;&#39564;&#35777;&#25511;&#21046;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#26159;&#21542;&#19982;&#20854;&#20182;&#29420;&#31435;&#21487;&#29992;&#30340;&#30693;&#35782;&#19968;&#33268;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#33021;&#21253;&#25324;&#29615;&#22659;&#30340;&#25277;&#35937;&#20449;&#24687;&#25110;&#29992;&#25143;&#25552;&#20379;&#30340;&#35268;&#33539;&#12290;&#22914;&#26524;&#39564;&#35777;&#27493;&#39588;&#21457;&#29616;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#65292;&#31639;&#27861;&#20250;&#33258;&#21160;&#32416;&#27491;&#24182;&#37325;&#26032;&#39564;&#35777;&#65292;&#30452;&#21040;&#25152;&#26377;&#30693;&#35782;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decision-making tasks, and to ground these controllers to task environments through visual observations. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model's output to construct an automaton-based controller that encodes the model's task-relevant knowledge. It then verifies whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or user-provided specifications. If this verification step discovers any inconsistency, the algorithm automaticall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20803;&#27169;&#24335;&#21644;&#20351;&#29992;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#24182;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.05275</link><description>&lt;p&gt;
&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-heterogeneity Graph Few-shot Learning. (arXiv:2308.05275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#21462;&#20803;&#27169;&#24335;&#21644;&#20351;&#29992;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#24182;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#24322;&#36136;&#22270;&#20013;&#26631;&#31614;&#31232;&#30095;&#38382;&#39064;&#65292;&#24322;&#36136;&#22270;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#28304;&#24322;&#36136;&#22270;&#20013;&#20016;&#23500;&#26631;&#35760;&#31867;&#25552;&#21462;&#30340;&#27867;&#21270;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#24322;&#36136;&#22270;&#20013;&#30340;&#23569;&#26631;&#35760;&#31867;&#26469;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#32771;&#34385;&#28304;&#21644;&#30446;&#26631;&#24322;&#36136;&#22270;&#20849;&#20139;&#19968;&#32452;&#22266;&#23450;&#30340;&#33410;&#28857;/&#36793;&#31867;&#22411;&#30340;&#24773;&#20917;&#65292;&#24573;&#30053;&#20102;&#26356;&#19968;&#33324;&#30340;&#36328;&#24322;&#36136;&#22270;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#24322;&#36136;&#22270;&#21487;&#20197;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#12289;&#38750;&#22266;&#23450;&#30340;&#33410;&#28857;/&#36793;&#31867;&#22411;&#38598;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26410;&#34987;&#25506;&#32034;&#30340;&#36328;&#24322;&#36136;&#22270;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#24322;&#36136;&#22270;&#23569;&#26679;&#26412;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;CGFL&#12290;&#22312;CGFL&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#20803;&#27169;&#24335;&#26469;&#25429;&#33719;&#24322;&#36136;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#22270;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MHGN&#65289;&#26469;&#23398;&#20064;&#36328;&#24322;&#36136;&#22270;&#30340;&#20803;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33539;&#22260;&#23376;..
&lt;/p&gt;
&lt;p&gt;
In recent years, heterogeneous graph few-shot learning has been proposed to address the label sparsity issue in heterogeneous graphs (HGs), which contain various types of nodes and edges. The existing methods have achieved good performance by transferring generalized knowledge extracted from rich-labeled classes in source HG(s) to few-labeled classes in a target HG. However, these methods only consider the single-heterogeneity scenario where the source and target HGs share a fixed set of node/edge types, ignoring the more general scenario of cross-heterogeneity, where each HG can have a different and non-fixed set of node/edge types. To this end, we focus on the unexplored cross-heterogeneity scenario and propose a novel model for Cross-heterogeneity Graph Few-shot Learning, namely CGFL. In CGFL, we first extract meta-patterns to capture heterogeneous information and propose a multi-view heterogeneous graph neural network (MHGN) to learn meta-patterns across HGs. Then, we propose a sco
&lt;/p&gt;</description></item><item><title>AI4GCC&#31454;&#36187;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20256;&#32479;&#32463;&#27982;&#25919;&#31574;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#21521;&#19978;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#12290;&#24314;&#35758;&#25913;&#36827;&#35780;&#20272;&#26631;&#20934;&#65292;&#32771;&#34385;&#28040;&#36153;/&#25928;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#29702;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#21450;&#35848;&#21028;&#21327;&#35758;&#30340;&#21338;&#24328;&#35770;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05260</link><description>&lt;p&gt;
AI4GCC -- &#31532;&#19977;&#39033;&#36187;&#36947;&#65306;&#28040;&#36153;&#21644;&#22810;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
AI4GCC -- Track 3: Consumption and the Challenges of Multi-Agent RL. (arXiv:2308.05260v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05260
&lt;/p&gt;
&lt;p&gt;
AI4GCC&#31454;&#36187;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20256;&#32479;&#32463;&#27982;&#25919;&#31574;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#21521;&#19978;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#12290;&#24314;&#35758;&#25913;&#36827;&#35780;&#20272;&#26631;&#20934;&#65292;&#32771;&#34385;&#28040;&#36153;/&#25928;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#29702;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#21450;&#35848;&#21028;&#21327;&#35758;&#30340;&#21338;&#24328;&#35770;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI4GCC&#31454;&#36187;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20256;&#32479;&#32463;&#27982;&#25919;&#31574;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#21521;&#19978;&#36808;&#20986;&#20102;&#22823;&#32966;&#30340;&#19968;&#27493;&#12290;&#22312;&#19979;&#38754;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20004;&#20010;&#28508;&#22312;&#30340;&#25913;&#36827;&#39046;&#22495;&#65292;&#21487;&#20197;&#22686;&#24378;&#27604;&#36187;&#30340;&#33021;&#21147;&#65292;&#20197;&#35782;&#21035;&#21644;&#35780;&#20272;&#25552;&#20986;&#30340;&#35848;&#21028;&#21327;&#35758;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#35780;&#20272;&#26631;&#20934;&#20013;&#22686;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#25351;&#26631;&#65292;&#32771;&#34385;&#21040;&#28040;&#36153;/&#25928;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#35758;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#25311;&#22120;&#20013;&#20195;&#29702;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#20197;&#21450;&#26469;&#33258;&#25552;&#20986;&#30340;&#35848;&#21028;&#21327;&#35758;&#30340;&#21338;&#24328;&#35770;&#23646;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#24314;&#35758;&#23545;&#26410;&#26469;&#30340;&#27604;&#36187;/&#27169;&#25311;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
The AI4GCC competition presents a bold step forward in the direction of integrating machine learning with traditional economic policy analysis. Below, we highlight two potential areas for improvement that could enhance the competition's ability to identify and evaluate proposed negotiation protocols. Firstly, we suggest the inclusion of an additional index that accounts for consumption/utility as part of the evaluation criteria. Secondly, we recommend further investigation into the learning dynamics of agents in the simulator and the game theoretic properties of outcomes from proposed negotiation protocols. We hope that these suggestions can be of use for future iterations of the competition/simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#20998;&#26512;&#30740;&#31350;&#20102;&#22270;&#20687;&#21512;&#25104;&#20013;&#21521;&#37327;&#37327;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;VQGAN&#65289;&#30340;&#24433;&#21709;&#65292;&#38598;&#20013;&#20851;&#27880;&#20102;&#21521;&#37327;&#37327;&#21270;&#25439;&#22833;&#12289;&#30721;&#20070;&#22823;&#23567;&#20248;&#21270;&#21644;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#34429;&#28982;&#32467;&#26524;&#26410;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#25968;&#25454;&#38598;&#19979;VQGAN&#30340;&#34892;&#20026;&#21644;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.05242</link><description>&lt;p&gt;
VQGAN&#20013;&#30340;&#21521;&#37327;&#37327;&#21270;&#25439;&#22833;&#20998;&#26512;&#65306;&#22522;&#20110;&#21333;&#20010;GPU&#30340;&#22270;&#20687;&#21512;&#25104;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Vector quantization loss analysis in VQGANs: a single-GPU ablation study for image-to-image synthesis. (arXiv:2308.05242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28040;&#34701;&#20998;&#26512;&#30740;&#31350;&#20102;&#22270;&#20687;&#21512;&#25104;&#20013;&#21521;&#37327;&#37327;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;VQGAN&#65289;&#30340;&#24433;&#21709;&#65292;&#38598;&#20013;&#20851;&#27880;&#20102;&#21521;&#37327;&#37327;&#21270;&#25439;&#22833;&#12289;&#30721;&#20070;&#22823;&#23567;&#20248;&#21270;&#21644;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#34429;&#28982;&#32467;&#26524;&#26410;&#36229;&#36234;&#29616;&#26377;&#22522;&#20934;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#25968;&#25454;&#38598;&#19979;VQGAN&#30340;&#34892;&#20026;&#21644;&#30456;&#20851;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20351;&#29992;&#21333;&#20010;NVIDIA A100 GPU&#30340;&#22270;&#20687;&#21512;&#25104;&#36827;&#34892;&#20102;&#23545;&#21521;&#37327;&#37327;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;VQGAN&#65289;&#30340;&#28040;&#34701;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#32422;&#26463;&#19979;&#65292;&#21253;&#25324;&#32441;&#29702;&#25968;&#37327;&#12289;&#30721;&#20070;&#21521;&#37327;&#23646;&#24615;&#21644;&#28508;&#22312;&#32500;&#24230;&#31561;&#20851;&#38190;&#21442;&#25968;&#30340;&#24494;&#22937;&#24433;&#21709;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21521;&#37327;&#37327;&#21270;&#25439;&#22833;&#65292;&#22266;&#23450;&#20854;&#20182;&#36229;&#21442;&#25968;&#21644;&#25439;&#22833;&#32452;&#20214;&#65288;GAN&#25439;&#22833;&#65289;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20197;&#21450;&#21464;&#21270;&#22823;&#23567;&#23545;&#37325;&#26500;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#27809;&#26377;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;VQGAN&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#20266;&#24433;&#12289;&#30721;&#20070;&#22823;&#23567;&#20248;&#21270;&#21644;&#19982;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#27604;&#36739;&#20998;&#26512;&#26041;&#38754;&#65292;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study performs an ablation analysis of Vector Quantized Generative Adversarial Networks (VQGANs), concentrating on image-to-image synthesis utilizing a single NVIDIA A100 GPU. The current work explores the nuanced effects of varying critical parameters including the number of epochs, image count, and attributes of codebook vectors and latent dimensions, specifically within the constraint of limited resources. Notably, our focus is pinpointed on the vector quantization loss, keeping other hyperparameters and loss components (GAN loss) fixed. This was done to delve into a deeper understanding of the discrete latent space, and to explore how varying its size affects the reconstruction. Though, our results do not surpass the existing benchmarks, however, our findings shed significant light on VQGAN's behaviour for a smaller dataset, particularly concerning artifacts, codebook size optimization, and comparative analysis with Principal Component Analysis (PCA). The study also uncovers t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#12290;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26435;&#34913;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05234</link><description>&lt;p&gt;
&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Edge and Cloud for V2X-Based Real-Time Object Detection in Autonomous Driving. (arXiv:2308.05234v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21033;&#29992;&#36793;&#32536;&#21644;&#20113;&#31471;&#25216;&#26415;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#29289;&#20307;&#26816;&#27979;&#12290;&#36890;&#36807;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#36827;&#34892;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26435;&#34913;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#24863;&#30693;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#24863;&#30693;&#27169;&#22359;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#20250;&#24433;&#21709;&#26680;&#24515;&#39550;&#39542;&#20915;&#31574;&#12290;&#23454;&#26102;&#24863;&#30693;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#19968;&#22823;&#25361;&#25112;&#22312;&#20110;&#22312;&#26816;&#27979;&#36136;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#12290;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23454;&#26102;&#24863;&#30693;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#35745;&#31639;&#21644;&#21151;&#32791;&#26041;&#38754;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#36739;&#22823;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24448;&#24448;&#33021;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#22312;&#36816;&#34892;&#26102;&#20063;&#26356;&#24930;&#12290;&#30001;&#20110;&#26368;&#20934;&#30830;&#30340;&#26816;&#27979;&#22120;&#26080;&#27861;&#22312;&#26412;&#22320;&#23454;&#26102;&#36816;&#34892;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#35745;&#31639;&#22806;&#37096;&#21270;&#21040;&#36793;&#32536;&#21644;&#20113;&#24179;&#21488;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#24179;&#21488;&#36164;&#28304;&#21463;&#38480;&#36739;&#23569;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#19981;&#21516;&#30340;&#22806;&#37096;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#30828;&#20214;&#21644;&#32593;&#32476;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#39044;&#27979;&#36136;&#37327;&#21644;&#31471;&#21040;&#31471;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#30001;&#20110;&#36890;&#36807;&#32593;&#32476;&#20256;&#36865;&#21407;&#22987;&#24103;&#30340;&#23454;&#29616;&#23384;&#22312;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#25968;&#25454;&#20256;&#36755;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power have to be taken into account for real-time perception in autonomous vehicles. Larger object detection models tend to produce the best results, but are also slower at runtime. Since the most accurate detectors cannot run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. Using real hardware and network simulations, we compare different trade-offs between prediction quality and end-to-end delay. Since sending raw frames over the network impl
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#23626;Alexa&#22870;Embibod AI SimBot&#25361;&#25112;&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#22823;&#23398;&#22242;&#38431;&#38656;&#35201;&#26500;&#24314;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#21161;&#25163;&#12290;&#35770;&#25991;&#24635;&#32467;&#20102;&#25361;&#25112;&#30340;&#27010;&#36848;&#21644;&#20026;&#22242;&#38431;&#25552;&#20379;&#30340;&#22522;&#30784;&#35774;&#26045;&#21644;&#25903;&#25345;&#65292;&#20197;&#21450;&#21442;&#19982;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.05221</link><description>&lt;p&gt;
Alexa&#65292;&#19982;&#26426;&#22120;&#20154;&#19968;&#36215;&#29609;&#32781;&#65306;&#24341;&#20837;&#31532;&#19968;&#23626;Alexa&#22870;Embibod AI SimBot&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Embodied AI. (arXiv:2308.05221v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05221
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#23626;Alexa&#22870;Embibod AI SimBot&#25361;&#25112;&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#22823;&#23398;&#22242;&#38431;&#38656;&#35201;&#26500;&#24314;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#21161;&#25163;&#12290;&#35770;&#25991;&#24635;&#32467;&#20102;&#25361;&#25112;&#30340;&#27010;&#36848;&#21644;&#20026;&#22242;&#38431;&#25552;&#20379;&#30340;&#22522;&#30784;&#35774;&#26045;&#21644;&#25903;&#25345;&#65292;&#20197;&#21450;&#21442;&#19982;&#22242;&#38431;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alexa Prize&#35745;&#21010;&#36171;&#20104;&#20102;&#35768;&#22810;&#22823;&#23398;&#29983;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#23637;&#31034;&#20182;&#20204;&#22312;&#26500;&#24314;&#20250;&#35805;&#20195;&#29702;&#26041;&#38754;&#30340;&#25165;&#33021;&#30340;&#26426;&#20250;&#65292;&#36890;&#36807;SocialBot&#22823;&#25361;&#25112;&#21644;TaskBot&#25361;&#25112;&#31561;&#25361;&#25112;&#12290;&#38543;&#30528;&#20250;&#35805;&#20195;&#29702;&#22312;&#22810;&#27169;&#24577;&#21644;&#20855;&#20307;&#21270;&#30340;&#24773;&#22659;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20986;&#29616;&#65292;&#25506;&#32034;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#29289;&#29702;&#20855;&#20307;&#21270;&#22686;&#24378;&#30340;&#20250;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;SimBot&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#22312;&#20854;&#20013;&#22823;&#23398;&#22242;&#38431;&#31454;&#30456;&#26500;&#24314;&#33021;&#22815;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#21161;&#25163;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;SimBot&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#32447;&#21644;&#31163;&#32447;&#25361;&#25112;&#38454;&#27573;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20026;&#22242;&#38431;&#25552;&#20379;&#30340;&#22522;&#30784;&#35774;&#26045;&#21644;&#25903;&#25345;&#65292;&#21253;&#25324;Alexa Arena&#65292;&#27169;&#25311;&#29615;&#22659;&#20197;&#21450;&#25552;&#20379;&#32473;&#22242;&#38431;&#21152;&#36895;&#26500;&#24314;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;ML&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#21442;&#19982;&#22242;&#38431;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Alexa Prize program has empowered numerous university students to explore, experiment, and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challenge in which university teams compete to build robot assistants that complete tasks in a simulated physical environment. This paper provides an overview of the SimBot Challenge, which included both online and offline challenge phases. We describe the infrastructure and support provided to the teams including Alexa Arena, the simulated environment, and the ML toolkit provided to teams to accelerate their building of vision and language models. We summarize the approaches the participating teams to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.05201</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;"&#29983;&#25104;"&#24037;&#20316;&#65306;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#32463;&#39564;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
"Generate" the Future of Work through AI: Empirical Evidence from Online Labor Markets. (arXiv:2308.05201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05201
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;ChatGPT&#20316;&#20026;&#22806;&#29983;&#20914;&#20987;&#65292;&#25581;&#31034;&#20102;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#65292;&#20294;&#36866;&#24212;&#26032;&#25216;&#26415;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#20173;&#33021;&#33719;&#24471;&#21033;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#23545;&#20854;&#23545;&#21171;&#21160;&#24066;&#22330;&#30340;&#24433;&#21709;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#20026;&#20102;&#22635;&#34917;&#29616;&#26377;&#30340;&#23454;&#35777;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;ChatGPT&#30340;&#25512;&#20986;&#35299;&#37322;&#20026;&#19968;&#31181;&#22806;&#29983;&#20914;&#20987;&#65292;&#24182;&#37319;&#29992;&#24046;&#24322;&#27861;&#26469;&#37327;&#21270;&#20854;&#23545;&#22312;&#32447;&#21171;&#21160;&#24066;&#22330;&#20013;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#24037;&#20316;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#30452;&#25509;&#25509;&#35302;ChatGPT&#30340;&#20219;&#21153;&#21644;&#33258;&#30001;&#32844;&#19994;&#32773;&#30340;&#20132;&#26131;&#37327;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#19979;&#38477;&#22312;&#30456;&#23545;&#36739;&#39640;&#30340;&#36807;&#21435;&#20132;&#26131;&#37327;&#25110;&#36739;&#20302;&#30340;&#36136;&#37327;&#26631;&#20934;&#19979;&#23588;&#20026;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#26381;&#21153;&#25552;&#20379;&#21830;&#37117;&#26222;&#36941;&#32463;&#21382;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#36825;&#20010;&#36716;&#22411;&#26399;&#38388;&#65292;&#33021;&#22815;&#36866;&#24212;&#26032;&#36827;&#23637;&#24182;&#25552;&#20379;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26381;&#21153;&#30340;&#33258;&#30001;&#32844;&#19994;&#32773;&#21487;&#20197;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#30410;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#30340;&#20986;&#29616;&#26377;&#21487;&#33021;&#26367;&#20195;&#20154;&#21147;&#21171;&#21160;
&lt;/p&gt;
&lt;p&gt;
With the advent of general-purpose Generative AI, the interest in discerning its impact on the labor market escalates. In an attempt to bridge the extant empirical void, we interpret the launch of ChatGPT as an exogenous shock, and implement a Difference-in-Differences (DID) approach to quantify its influence on text-related jobs and freelancers within an online labor marketplace. Our results reveal a significant decrease in transaction volume for gigs and freelancers directly exposed to ChatGPT. Additionally, this decline is particularly marked in units of relatively higher past transaction volume or lower quality standards. Yet, the negative effect is not universally experienced among service providers. Subsequent analyses illustrate that freelancers proficiently adapting to novel advancements and offering services that augment AI technologies can yield substantial benefits amidst this transformative period. Consequently, even though the advent of ChatGPT could conceivably substitute
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#65292;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#24314;&#27169;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#39046;&#22495;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.05189</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Representations for Spatio-Temporal Visual Attention Modeling and Understanding. (arXiv:2308.05189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#65292;&#22312;&#35270;&#39057;&#24207;&#21015;&#20013;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#20197;&#21450;&#29992;&#20110;&#24314;&#27169;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#39046;&#22495;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#21338;&#22763;&#35770;&#25991;&#30740;&#31350;&#21644;&#24320;&#21457;&#20102;&#22522;&#20110;&#23618;&#27425;&#21270;&#34920;&#31034;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#26041;&#27861;&#65292;&#38024;&#23545;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#31639;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#27880;&#24847;&#21147;&#24314;&#27169;&#21644;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#24314;&#27169;&#19978;&#34892;&#30340;&#26102;&#31354;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#24182;&#26368;&#32456;&#29992;&#20110;&#24314;&#27169;&#26102;&#38388;&#39046;&#22495;&#20013;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This PhD. Thesis concerns the study and development of hierarchical representations for spatio-temporal visual attention modeling and understanding in video sequences. More specifically, we propose two computational models for visual attention. First, we present a generative probabilistic model for context-aware visual attention modeling and understanding. Secondly, we develop a deep network architecture for visual attention modeling, which first estimates top-down spatio-temporal visual attention, and ultimately serves for modeling attention in the temporal domain.
&lt;/p&gt;</description></item><item><title>PromptPaint&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25991;&#23383;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#27169;&#25311;&#32472;&#30011;&#23186;&#20171;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#34920;&#36798;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25903;&#25345;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#36845;&#20195;&#22609;&#36896;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;PromptPaint&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.05184</link><description>&lt;p&gt;
PromptPaint: &#36890;&#36807;&#32472;&#30011;&#23186;&#20171;&#33324;&#30340;&#20132;&#20114;&#24341;&#23548;&#25991;&#26412;&#36716;&#22270;&#29255;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions. (arXiv:2308.05184v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05184
&lt;/p&gt;
&lt;p&gt;
PromptPaint&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25991;&#23383;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#27169;&#25311;&#32472;&#30011;&#23186;&#20171;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#28151;&#21512;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#34920;&#36798;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25903;&#25345;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#36827;&#34892;&#36845;&#20195;&#22609;&#36896;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;PromptPaint&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#36716;&#22270;&#29255;(T2I)&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#24335;&#65292;&#20294;&#26159;&#24341;&#23548;&#36825;&#31181;&#29983;&#25104;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23545;&#20110;&#38590;&#20197;&#36890;&#36807;&#35821;&#35328;&#25551;&#36848;&#30340;&#27010;&#24565;&#65292;&#29992;&#25143;&#21487;&#33021;&#38590;&#20197;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#21478;&#22806;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#20197;&#31471;&#21040;&#31471;&#31995;&#32479;&#30340;&#24418;&#24335;&#26500;&#24314;&#30340;&#65292;&#32570;&#20047;&#23545;&#22270;&#20687;&#30340;&#36845;&#20195;&#22609;&#36896;&#25903;&#25345;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PromptPaint&#65292;&#23427;&#23558;T2I&#29983;&#25104;&#19982;&#27169;&#25311;&#25105;&#20204;&#20351;&#29992;&#24425;&#33394;&#27833;&#28422;&#30340;&#20132;&#20114;&#30456;&#32467;&#21512;&#12290;PromptPaint&#20351;&#29992;&#25143;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#65292;&#28151;&#21512;&#34920;&#36798;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27010;&#24565;&#30340;&#25552;&#31034;&#12290;&#23601;&#20687;&#25105;&#20204;&#36890;&#36807;&#22312;&#29289;&#29702;&#30011;&#24067;&#19978;&#23618;&#21472;&#25918;&#32622;&#27833;&#28422;&#26469;&#36845;&#20195;&#35843;&#25972;&#39068;&#33394;&#19968;&#26679;&#65292;PromptPaint&#21516;&#26679;&#20801;&#35768;&#29992;&#25143;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#19981;&#21516;&#30011;&#24067;&#21306;&#22495;&#21644;&#26102;&#38388;&#24212;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23545;&#28151;&#21512;&#25552;&#31034;&#30340;&#19981;&#21516;&#26041;&#27861;&#12289;&#35774;&#35745;&#26435;&#34913;&#20197;&#21450;&#29983;&#25104;&#27169;&#22411;&#30340;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#36890;&#36807;PromptPaint&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
While diffusion-based text-to-image (T2I) models provide a simple and powerful way to generate images, guiding this generation remains a challenge. For concepts that are difficult to describe through language, users may struggle to create prompts. Moreover, many of these models are built as end-to-end systems, lacking support for iterative shaping of the image. In response, we introduce PromptPaint, which combines T2I generation with interactions that model how we use colored paints. PromptPaint allows users to go beyond language to mix prompts that express challenging concepts. Just as we iteratively tune colors through layered placements of paint on a physical canvas, PromptPaint similarly allows users to apply different prompts to different canvas areas and times of the generative process. Through a set of studies, we characterize different approaches for mixing prompts, design trade-offs, and socio-technical challenges for generative models. With PromptPaint we provide insight into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#20026;&#30315;&#30187;&#30149;&#20154;&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#25252;&#29702;&#25552;&#20379;&#20102;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.05176</link><description>&lt;p&gt;
&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#25506;&#32034;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models. (arXiv:2308.05176v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#65292;&#36890;&#36807;&#24212;&#29992;&#22810;&#26679;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20248;&#21270;&#27169;&#22411;&#24615;&#33021;&#65292;&#20026;&#30315;&#30187;&#30149;&#20154;&#30340;&#26377;&#25928;&#31649;&#29702;&#21644;&#25252;&#29702;&#25552;&#20379;&#20102;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#20854;&#29305;&#28857;&#26159;&#21453;&#22797;&#21457;&#20316;&#21644;&#19981;&#21487;&#39044;&#27979;&#30340;&#30315;&#30187;&#21457;&#20316;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#39044;&#27979;&#20197;&#23454;&#26045;&#26377;&#25928;&#30340;&#31649;&#29702;&#21644;&#24739;&#32773;&#25252;&#29702;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#35760;&#24405;&#65292;&#20197;&#21450;&#20854;&#22312;&#30315;&#30187;&#21457;&#20316;&#26399;&#38388;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#33041;&#27963;&#21160;&#27934;&#23519;&#30340;&#33021;&#21147;&#65292;&#20351;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;EEG&#25968;&#25454;&#36827;&#34892;&#30315;&#30187;&#21457;&#20316;&#39044;&#27979;&#30340;&#20116;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#20998;&#26512; - &#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#26497;&#31471;&#26862;&#26519;&#65288;ET&#65289;&#65292;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#21644;&#26799;&#24230;&#25552;&#21319;&#65288;GB&#65289;&#12290;&#25968;&#25454;&#38598;&#32463;&#36807;&#20102;&#32454;&#33268;&#30340;&#39044;&#22788;&#29702;&#65292; &#21253;&#25324;&#28165;&#29702;&#12289;&#24402;&#19968;&#21270;&#12289;&#24322;&#24120;&#20540;&#22788;&#29702;&#21644;&#36807;&#37319;&#26679;&#65292;&#20197;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#21644;&#20415;&#20110;&#20934;&#30830;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#36825;&#20123;&#39044;&#22788;&#29702;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20462;&#21098;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20462;&#21098;&#36807;&#31243;&#20013;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05170</link><description>&lt;p&gt;
FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. (arXiv:2308.05170v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FPGA&#36164;&#28304;&#24863;&#30693;&#30340;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20462;&#21098;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20462;&#21098;&#36807;&#31243;&#20013;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#38899;&#35782;&#21035;&#12289;&#31185;&#23398;&#20998;&#26512;&#31561;&#20247;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#23454;&#26102;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#23545;&#26356;&#24555;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#26356;&#20302;&#30340;&#21151;&#32791;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;FPGA&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#30340;&#21512;&#36866;&#35774;&#22791;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#12289;&#37327;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#20462;&#21098;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20943;&#23569;&#20102;&#20056;&#27861;&#21644;&#20869;&#23384;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#20462;&#21098;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#24213;&#23618;&#30828;&#20214;&#30340;&#29305;&#24615;&#65292;&#23548;&#33268;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#21644;&#36127;&#36733;&#24179;&#34913;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36164;&#28304;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#30828;&#20214;&#20026;&#20013;&#24515;&#30340;&#20462;&#21098;&#20844;&#24335;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#20855;&#26377;&#36164;&#28304;&#24863;&#30693;&#24352;&#37327;&#32467;&#26500;&#30340;&#32972;&#21253;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks achieve state-of-the-art performance in image classification, speech recognition, scientific analysis and many more application areas. With the ever-increasing need for faster computation and lower power consumption, driven by real-time systems and Internet-of-Things (IoT) devices, FPGAs have emerged as suitable devices for deep learning inference. Due to the high computational complexity and memory footprint of neural networks, various compression techniques, such as pruning, quantization and knowledge distillation, have been proposed in literature. Pruning sparsifies a neural network, reducing the number of multiplications and memory. However, pruning often fails to capture properties of the underlying hardware, causing unstructured sparsity and load-balance inefficiency, thus bottlenecking resource improvements. We propose a hardware-centric formulation of pruning, by formulating it as a knapsack problem with resource-aware tensor structures. The primary emphasis is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;&#29983;&#25104;&#29305;&#27530;&#26597;&#35810;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#31363;&#21462;&#12290;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#30028;&#26694;&#22352;&#26631;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2308.05127</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Data-Free Model Extraction Attacks in the Context of Object Detection. (arXiv:2308.05127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;&#29983;&#25104;&#29305;&#27530;&#26597;&#35810;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#30446;&#26631;&#27169;&#22411;&#30340;&#31363;&#21462;&#12290;&#36890;&#36807;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#23545;&#36793;&#30028;&#26694;&#22352;&#26631;&#30340;&#39044;&#27979;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#36890;&#36807;&#20351;&#29992;&#38024;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#29305;&#27530;&#26597;&#35810;&#26469;&#31363;&#21462;&#27169;&#22411;&#30340;&#12290;&#36825;&#39033;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#25110;&#20195;&#29702;&#25968;&#25454;&#38598;&#26469;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#35757;&#32451;&#19968;&#20010;&#27169;&#20223;&#30446;&#26631;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#26469;&#23436;&#25104;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#27169;&#22411;&#26159;&#22312;&#23545;&#25163;&#26080;&#27861;&#35775;&#38382;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#26080;&#25968;&#25454;&#27169;&#22411;&#25552;&#21462;&#25216;&#26415;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#31867;&#20284;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#20351;&#29992;&#30340;&#29983;&#25104;&#22120;&#26469;&#20154;&#24037;&#29983;&#25104;&#26597;&#35810;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#36793;&#30028;&#26694;&#22352;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23450;&#20041;&#25439;&#22833;&#20989;&#25968;&#21644;&#20351;&#29992;&#26032;&#39062;&#30340;&#29983;&#25104;&#22120;&#35774;&#32622;&#26159;&#25552;&#21462;&#30446;&#26631;&#27169;&#22411;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant number of machine learning models are vulnerable to model extraction attacks, which focus on stealing the models by using specially curated queries against the target model. This task is well accomplished by using part of the training data or a surrogate dataset to train a new model that mimics a target model in a white-box environment. In pragmatic situations, however, the target models are trained on private datasets that are inaccessible to the adversary. The data-free model extraction technique replaces this problem when it comes to using queries artificially curated by a generator similar to that used in Generative Adversarial Nets. We propose for the first time, to the best of our knowledge, an adversary black box attack extending to a regression problem for predicting bounding box coordinates in object detection. As part of our study, we found that defining a loss function and using a novel generator setup is one of the key aspects in extracting the target model. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05106</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#26102;&#38388;&#65306;&#26292;&#21147;&#26816;&#27979;&#22312;&#30417;&#25511;&#35270;&#39057;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Balancing Accuracy and Training Time in Federated Learning for Violence Detection in Surveillance Videos: A Study of Neural Network Architectures. (arXiv:2308.05106v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#19979;&#35270;&#39057;&#26292;&#21147;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#23427;&#20204;&#30340;&#36866;&#24212;&#24615;&#12290;&#30740;&#31350;&#21253;&#25324;&#23545;&#22522;&#20934;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#26412;&#30340;&#8220;Flow-Gated&#8221;&#26550;&#26500;&#65292;&#31216;&#20026;&#8220;Diff-Gated&#8221;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#36229;&#25910;&#25947;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#23558;&#38598;&#20013;&#24335;&#25968;&#25454;&#38598;&#36866;&#24212;&#21040;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#35757;&#32451;&#26368;&#20339;&#26292;&#21147;&#26816;&#27979;&#27169;&#22411;&#65292;&#30740;&#31350;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an investigation into machine learning techniques for violence detection in videos and their adaptation to a federated learning context. The study includes experiments with spatio-temporal features extracted from benchmark video datasets, comparison of different methods, and proposal of a modified version of the "Flow-Gated" architecture called "Diff-Gated." Additionally, various machine learning techniques, including super-convergence and transfer learning, are explored, and a method for adapting centralized datasets to a federated learning context is developed. The research achieves better accuracy results compared to state-of-the-art models by training the best violence detection model in a federated learning context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04952</link><description>&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20856;&#22411;&#26680;&#23398;&#20064;&#19982;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#65292;&#35299;&#20915;&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20998;&#21106;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#26680;&#20197;&#21450;&#20856;&#22411;&#23398;&#20064;&#21644;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;GFSS&#65289;&#23558;&#23569;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#65288;FSS&#65289;&#25193;&#23637;&#21040;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#26102;&#20998;&#21106;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#24050;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#39069;&#22806;&#30340;&#20998;&#25903;&#25110;&#20856;&#22411;&#32858;&#21512;&#26469;&#28040;&#38500;FSS&#30340;&#32422;&#26463;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#34920;&#31034;&#20998;&#21106;&#21644;&#23884;&#20837;&#20559;&#35265;&#65292;&#20005;&#37325;&#24433;&#21709;GFSS&#30340;&#24615;&#33021;&#65292;&#23578;&#26410;&#32508;&#21512;&#32771;&#34385;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20856;&#22411;&#26680;&#23398;&#20064;&#21644;&#24320;&#25918;&#38598;&#21069;&#26223;&#24863;&#30693;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#26680;&#26469;&#23545;&#27599;&#20010;&#31867;&#21035;&#36827;&#34892;&#20998;&#21106;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20856;&#22411;&#23398;&#20064;&#19982;&#22522;&#31867;&#26680;&#30340;&#26356;&#26032;&#30456;&#32467;&#21512;&#65292;&#36825;&#19982;&#23569;&#26679;&#26412;&#26032;&#31867;&#21035;&#30340;&#21407;&#22411;&#30693;&#35782;&#32858;&#21512;&#30456;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#19982;&#26465;&#20214;&#20559;&#24046;&#22522;&#20110;&#25512;&#29702;&#30340;&#21069;&#26223;&#19978;&#19979;&#25991;&#24863;&#30693;&#27169;&#22359;&#65292;&#29992;&#20110;&#25191;&#34892;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic Segmentation (FSS) to simultaneously segment unseen classes and seen classes during evaluation. Previous works leverage additional branch or prototypical aggregation to eliminate the constrained setting of FSS. However, representation division and embedding prejudice, which heavily results in poor performance of GFSS, have not been synthetical considered. We address the aforementioned problems by jointing the prototypical kernel learning and open-set foreground perception. Specifically, a group of learnable kernels is proposed to perform segmentation with each kernel in charge of a stuff class. Then, we explore to merge the prototypical learning to the update of base-class kernels, which is consistent with the prototype knowledge aggregation of few-shot novel classes. In addition, a foreground contextual perception module cooperating with conditional bias based inference is adopted to perform class-agnostic as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.04371</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32047;&#31215;&#25512;&#29702;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Cumulative Reasoning With Large Language Models. (arXiv:2308.04371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#27169;&#25311;&#20154;&#31867;&#24605;&#32500;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#31616;&#21270;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;24&#28857;&#28216;&#25103;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#24378;&#22823;&#19988;&#22810;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#35299;&#20915;&#39640;&#24230;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#38656;&#35201;&#28145;&#24605;&#29087;&#34385;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27492;&#21482;&#26377;&#26368;&#23567;&#31243;&#24230;&#30340;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32047;&#31215;&#25512;&#29702;&#65288;CR&#65289;&#65292;&#23427;&#20197;&#32047;&#31215;&#21644;&#36845;&#20195;&#30340;&#26041;&#24335;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#24605;&#32500;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#32452;&#20214;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#65292;&#20351;&#20854;&#26356;&#26131;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#12290;&#23545;&#20110;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;CR&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#36229;&#36807;&#29616;&#26377;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22810;&#36798;9.3&#65285;&#65292;&#24182;&#22312;&#32463;&#36807;&#31574;&#21010;&#30340;FOLIO&#32500;&#22522;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24778;&#20154;&#30340;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#22312;24&#28857;&#28216;&#25103;&#30340;&#32972;&#26223;&#19979;&#65292;CR&#23454;&#29616;&#20102;94&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language models are powerful and versatile, they often fail to address highly complex problems. This is because solving complex problems requires deliberate thinking, which has been only minimally guided during training. In this paper, we propose a new method called Cumulative Reasoning (CR), which employs language models in a cumulative and iterative manner to emulate human thought processes. By decomposing tasks into smaller components, \ournameb streamlines the problem-solving process, rendering it both more manageable and effective. For logical inference tasks, CR consistently outperforms existing methods with an improvement up to 9.3\%, and achieves the astonishing accuracy of 98.04\% on the curated FOLIO wiki dataset. In the context of the Game of 24, CR achieves an accuracy of 94\%, which signifies a substantial enhancement of 20\% over the previous state-of-the-art method.
&lt;/p&gt;</description></item><item><title>&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.04313</link><description>&lt;p&gt;
Apple Vision Pro for Healthcare: &#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#65311;&#65288;arXiv:2308.04313v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Apple Vision Pro for Healthcare: "The Ultimate Display"?. (arXiv:2308.04313v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04313
&lt;/p&gt;
&lt;p&gt;
&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#65292;&#19968;&#27454;&#20855;&#26377;&#28151;&#21512;&#29616;&#23454;&#21644;&#22686;&#24378;&#29616;&#23454;&#21151;&#33021;&#30340;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#65292;&#25317;&#26377;&#29420;&#29305;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#23637;&#31034;&#20329;&#25140;&#32773;&#30340;&#30524;&#30555;&#20197;&#21450;&#25968;&#23383;&#30343;&#20896;&#25353;&#38062;&#30340;&#34701;&#21512;&#21151;&#33021;&#12290;&#36825;&#27454;&#26080;&#32447;&#35774;&#22791;&#21487;&#33021;&#23454;&#29616;&#20102;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;6&#26376;&#30340;&#20840;&#29699;&#24320;&#21457;&#32773;&#22823;&#20250;&#65288;WWDC&#65289;&#19978;&#65292;&#33529;&#26524;&#25512;&#20986;&#20102;Vision Pro&#12290;Vision Pro&#26159;&#19968;&#27454;&#28151;&#21512;&#29616;&#23454;&#65288;MR&#65289;&#22836;&#30420;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23427;&#26159;&#19968;&#27454;&#20855;&#26377;&#39069;&#22806;&#35270;&#39057;&#36879;&#35270;&#65288;VST&#65289;&#33021;&#21147;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#12290;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#36890;&#36807;&#25668;&#20687;&#22836;&#20256;&#36755;&#21040;&#29992;&#25143;&#30524;&#21069;&#30340;&#65288;VR&#65289;&#23631;&#24149;&#65292;&#20351;&#24471;Vision Pro&#20063;&#25104;&#20026;&#20102;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#35774;&#22791;&#12290;&#24403;&#28982;&#65292;&#36825;&#24182;&#19981;&#29420;&#29305;&#65292;&#19982;Varjo XR-3&#31561;&#20854;&#20182;&#35774;&#22791;&#31867;&#20284;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;Vision Pro&#20855;&#26377;&#19968;&#20123;&#26377;&#36259;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#20869;&#37096;&#23631;&#24149;&#21487;&#20197;&#21521;&#8220;&#22806;&#30028;&#8221;&#26174;&#31034;&#20329;&#25140;&#22836;&#30420;&#32773;&#30340;&#30524;&#30555;&#65292;&#25110;&#32773;&#39030;&#37096;&#30340;&#19968;&#20010;&#25353;&#38062;&#31216;&#20026;&#8220;&#25968;&#23383;&#30343;&#20896;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26059;&#36716;&#26080;&#32541;&#22320;&#34701;&#21512;&#25968;&#23383;&#20869;&#23481;&#19982;&#29289;&#29702;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;Vision Pro&#26159;&#26080;&#32447;&#30340;&#65292;&#21482;&#26377;&#30005;&#27744;&#30340;&#30005;&#32518;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#22836;&#30420;&#27604;Varjo XR-3&#26356;&#21152;&#28789;&#27963;&#12290;&#36825;&#21487;&#33021;&#26356;&#25509;&#36817;&#8220;&#32456;&#26497;&#26174;&#31034;&#22120;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more specifically it is a Virtual Reality (VR) device with an additional Video See-Through (VST) capability. The VST capability turns the Vision Pro also into an Augmented Reality (AR) device. The AR feature is enabled by streaming the real world via cameras to the (VR) screens in front of the user's eyes. This is of course not unique and similar to other devices, like the Varjo XR-3. Nevertheless, the Vision Pro has some interesting features, like an inside-out screen that can show the headset wearers' eyes to "outsiders" or a button on the top, called "Digital Crown", that allows you to seamlessly blend digital content with your physical space by turning it. In addition, it is untethered, except for the cable to the battery, which makes the headset more agile, compared to the Varjo XR-3. This could actually come closer to the "Ultimate Display",
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04033</link><description>&lt;p&gt;
&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adapting Foundation Models for Information Synthesis of Wireless Communication Specifications. (arXiv:2308.04033v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#23427;&#37319;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#12289;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#21453;&#39304;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#19988;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#12289;&#24320;&#21457;&#21644;&#30740;&#31350;&#29616;&#20195;&#26080;&#32447;&#36890;&#20449;&#25216;&#26415;&#30340;&#29616;&#26377;&#26041;&#27861;&#28041;&#21450;&#32791;&#26102;&#19988;&#32321;&#29712;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#31579;&#36873;&#22823;&#37327;&#30340;&#32593;&#39029;&#21644;&#25216;&#26415;&#35268;&#33539;&#25991;&#20214;&#65292;&#25910;&#38598;&#25152;&#38656;&#20449;&#24687;&#24182;&#36827;&#34892;&#32508;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NextGen Communications Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#35268;&#33539;&#20449;&#24687;&#32508;&#21512;&#30340;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#12290;&#35813;&#31995;&#32479;&#22522;&#20110;&#26368;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#23637;&#65292;&#24182;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#30340;&#38468;&#21152;&#32452;&#20214;&#65306;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#24211;&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#20174;&#26080;&#32447;&#25216;&#26415;&#35268;&#33539;&#25968;&#25454;&#24211;&#20013;&#25552;&#21462;&#31616;&#27905;&#30340;&#12289;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#19987;&#23478;&#21453;&#39304;&#21644;&#25968;&#25454;&#36129;&#29486;&#24037;&#20855;&#12290;&#22312;&#20351;&#29992;&#30001;&#19987;&#23478;&#21019;&#24314;&#30340;&#26597;&#35810;&#21644;&#21442;&#32771;&#21709;&#24212;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to understanding, developing and researching modern wireless communication technologies involves time-intensive and arduous process of sifting through numerous webpages and technical specification documents, gathering the required information and synthesizing it. This paper presents NextGen Communications Copilot, a conversational artificial intelligence tool for information synthesis of wireless communication specifications. The system builds on top of recent advancements in foundation models and consists of three key additional components: a domain-specific database, a context extractor, and a feedback mechanism. The system appends user queries with concise and query-dependent contextual information extracted from a database of wireless technical specifications and incorporates tools for expert feedback and data contributions. On evaluation using a benchmark dataset of queries and reference responses created by subject matter experts, the system demonstrated more 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24182;&#21033;&#29992;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#25913;&#21892;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#21046;&#23450;&#38024;&#23545;&#24615;&#30340;&#25919;&#31574;&#21644;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2308.03880</link><description>&lt;p&gt;
&#20445;&#25252;&#23432;&#25252;&#32773;&#65306;&#22312;&#32447;&#20799;&#31461;&#24615;&#34384;&#24453;&#30340;&#33258;&#21160;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse. (arXiv:2308.03880v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03880
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#24182;&#21033;&#29992;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#20197;&#25913;&#21892;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#24110;&#21161;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#21046;&#23450;&#38024;&#23545;&#24615;&#30340;&#25919;&#31574;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20799;&#31461;&#30340;&#22312;&#32447;&#26292;&#21147;&#20107;&#20214;&#26377;&#25152;&#22686;&#21152;&#65292;&#35201;&#27714;&#25105;&#20204;&#20104;&#20197;&#32039;&#24613;&#20851;&#27880;&#12290;&#26377;&#20851;&#37096;&#38376;&#38656;&#35201;&#25163;&#21160;&#20998;&#26512;&#28389;&#29992;&#25237;&#35785;&#20197;&#20102;&#35299;&#29359;&#32618;&#21160;&#24577;&#24182;&#35782;&#21035;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25237;&#35785;&#30340;&#25163;&#21160;&#20998;&#26512;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#26292;&#38706;&#20998;&#26512;&#21592;&#25509;&#35302;&#21040;&#26377;&#23475;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#19968;&#31181;&#19987;&#20026;&#20840;&#38754;&#20998;&#26512;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#36890;&#36807;&#23545;&#25253;&#21578;&#22312;&#20027;&#39064;&#12289;&#29359;&#32618;&#31243;&#24230;&#21644;&#20260;&#23475;&#31243;&#24230;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#25509;&#35302;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#25105;&#20204;&#22810;&#23398;&#31185;&#22242;&#38431;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23545;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#23454;&#29616;&#23545;&#25253;&#21578;&#30340;&#26356;&#28145;&#20837;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#22522;&#26412;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#29702;&#35299;&#65292;&#20351;&#25191;&#27861;&#37096;&#38376;&#21644;&#20915;&#31574;&#32773;&#33021;&#22815;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Online violence against children has increased globally recently, demanding urgent attention. Competent authorities manually analyze abuse complaints to comprehend crime dynamics and identify patterns. However, the manual analysis of these complaints presents a challenge because it exposes analysts to harmful content during the review process. Given these challenges, we present a novel solution, an automated tool designed to analyze children's sexual abuse reports comprehensively. By automating the analysis process, our tool significantly reduces the risk of exposure to harmful content by categorizing the reports on three dimensions: Subject, Degree of Criminality, and Damage. Furthermore, leveraging our multidisciplinary team's expertise, we introduce a novel approach to annotate the collected data, enabling a more in-depth analysis of the reports. This approach improves the comprehension of fundamental patterns and trends, enabling law enforcement agencies and policymakers to create 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.03152</link><description>&lt;p&gt;
AI-GOMS: &#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AI-GOMS: Large AI-Driven Global Ocean Modeling System. (arXiv:2308.03152v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03152
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#27169;&#25311;&#26159;&#27169;&#25311;&#28023;&#27915;&#30340;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#36807;&#31243;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#21644;&#36816;&#33829;&#28023;&#27915;&#23398;&#30340;&#22522;&#30784;&#12290;&#29616;&#20195;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#20027;&#35201;&#21253;&#25324;&#25511;&#21046;&#26041;&#31243;&#21644;&#25968;&#20540;&#31639;&#27861;&#12290;&#38750;&#32447;&#24615;&#19981;&#31283;&#23450;&#24615;&#12289;&#35745;&#31639;&#24320;&#38144;&#12289;&#20302;&#21487;&#37325;&#29992;&#25928;&#29575;&#21644;&#39640;&#32806;&#21512;&#25104;&#26412;&#36880;&#28176;&#25104;&#20026;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#36827;&#19968;&#27493;&#21457;&#23637;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31185;&#23398;&#35745;&#31639;&#27169;&#22411;&#23637;&#31034;&#20102;&#25968;&#23383;&#23402;&#29983;&#21644;&#31185;&#23398;&#27169;&#25311;&#30340;&#38761;&#21629;&#28508;&#21147;&#65292;&#20294;&#25968;&#20540;&#28023;&#27915;&#27169;&#25311;&#30340;&#29942;&#39048;&#23578;&#26410;&#24471;&#21040;&#36827;&#19968;&#27493;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AI-GOMS&#65292;&#19968;&#20010;&#22823;&#22411;AI&#39537;&#21160;&#30340;&#20840;&#29699;&#28023;&#27915;&#27169;&#25311;&#31995;&#32479;&#65292;&#29992;&#20110;&#20934;&#30830;&#39640;&#25928;&#30340;&#20840;&#29699;&#28023;&#27915;&#27599;&#26085;&#39044;&#27979;&#12290;AI-GOMS&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#39592;&#24178;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#26412;&#28023;&#27915;&#21464;&#37327;&#39044;&#27979;&#21644;&#36731;&#37327;&#32423;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.02312</link><description>&lt;p&gt;
&#35841;&#22238;&#31572;&#30340;&#26356;&#22909;&#65311;&#23545;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Who Answers It Better? An In-Depth Analysis of ChatGPT and Stack Overflow Answers to Software Engineering Questions. (arXiv:2308.02312v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;ChatGPT&#21644;Stack Overflow&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#21644;&#21487;&#29992;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22238;&#31572;&#20013;&#26377;52%&#38169;&#35823;&#65292;77%&#20887;&#38271;&#65292;&#20294;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;&#20173;&#28982;&#22312;39.34%&#30340;&#24773;&#20917;&#19979;&#34987;&#20351;&#29992;&#32773;&#20559;&#22909;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Q&amp;A&#24179;&#21488;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#19968;&#30452;&#26159;&#31243;&#24207;&#21592;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;&#32593;&#19978;&#27714;&#21161;&#34892;&#20026;&#30340;&#33539;&#24335;&#27491;&#22312;&#21457;&#29983;&#21464;&#21270;&#12290;&#23613;&#31649;ChatGPT&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#26469;&#35780;&#20272;ChatGPT&#22238;&#31572;&#36719;&#20214;&#24037;&#31243;&#38382;&#39064;&#30340;&#29305;&#28857;&#25110;&#21487;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22238;&#31572;517&#20010;Stack Overflow&#65288;SO&#65289;&#38382;&#39064;&#36827;&#34892;&#20102;&#39318;&#27425;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#23545;ChatGPT&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32508;&#21512;&#24615;&#21644;&#31616;&#27905;&#24615;&#36827;&#34892;&#20102;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#20998;&#26512;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;ChatGPT&#22238;&#31572;&#22312;&#35821;&#35328;&#21644;&#20154;&#31867;&#26041;&#38754;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;52&#65285;&#30340;ChatGPT&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;77&#65285;&#30340;&#22238;&#31572;&#20887;&#38271;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#20854;&#32508;&#21512;&#24615;&#21644;&#28165;&#26224;&#30340;&#35821;&#35328;&#34920;&#36798;&#65292;ChatGPT&#22238;&#31572;&#20173;&#28982;&#22312;39.34&#65285;&#30340;&#24773;&#20917;&#19979;&#21463;&#21040;&#38738;&#30544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Q&amp;A platforms have been an integral part of the web-help-seeking behavior of programmers over the past decade. However, with the recent introduction of ChatGPT, the paradigm of web-help-seeking behavior is experiencing a shift. Despite the popularity of ChatGPT, no comprehensive study has been conducted to evaluate the characteristics or usability of ChatGPT's answers to software engineering questions. To bridge the gap, we conducted the first in-depth analysis of ChatGPT's answers to 517 Stack Overflow (SO) questions and examined the correctness, consistency, comprehensiveness, and conciseness of ChatGPT's answers. Furthermore, we conducted a large-scale linguistic analysis, and a user study to understand the characteristics of ChatGPT answers from linguistic and human aspects. Our analysis shows that 52\% of ChatGPT answers are incorrect and 77\% are verbose. Nonetheless, ChatGPT answers are still preferred 39.34\% of the time due to their comprehensiveness and well-articulated langu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;</title><link>http://arxiv.org/abs/2307.15644</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaling Data Generation in Vision-and-Language Navigation. (arXiv:2307.15644v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#20013;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#26377;&#25928;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36924;&#30495;&#30340;&#29615;&#22659;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24050;&#23384;&#22312;&#30340;&#20195;&#29702;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35821;&#35328;&#24341;&#23548;&#30340;&#35270;&#35273;&#23548;&#33322;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36941;&#21382;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#21487;&#27867;&#21270;&#20195;&#29702;&#30340;&#30417;&#30563;&#25968;&#37327;&#26377;&#20102;&#26126;&#26174;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;HM3D&#21644;Gibson&#25968;&#25454;&#38598;&#20013;&#30340;1200&#22810;&#20010;&#36924;&#30495;&#30340;&#29615;&#22659;&#65292;&#24182;&#21033;&#29992;&#32593;&#32476;&#19978;&#30340;&#36164;&#28304;&#21512;&#25104;&#20102;490&#19975;&#20010;&#25351;&#20196;&#36712;&#36857;&#23545;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#33539;&#24335;&#20013;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#20195;&#29702;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#24688;&#24403;&#22320;&#24212;&#29992;&#25193;&#22686;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20195;&#29702;&#12290;&#24471;&#30410;&#20110;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#29616;&#26377;&#20195;&#29702;&#30340;&#24615;&#33021;&#21487;&#20197;&#22823;&#24133;&#25552;&#21319;&#65288;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#32477;&#23545;&#20540;&#22686;&#21152;&#20102;11%&#65289;&#65292;&#22312;R2R&#27979;&#35797;&#38598;&#20013;&#21333;&#27425;&#36816;&#34892;&#25104;&#21151;&#29575;&#26174;&#33879;&#25552;&#21319;&#33267;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research in language-guided visual navigation has demonstrated a significant demand for the diversity of traversable environments and the quantity of supervision for training generalizable agents. To tackle the common data scarcity issue in existing vision-and-language navigation datasets, we propose an effective paradigm for generating large-scale data for learning, which applies 1200+ photo-realistic environments from HM3D and Gibson datasets and synthesizes 4.9 million instruction trajectory pairs using fully-accessible resources on the web. Importantly, we investigate the influence of each component in this paradigm on the agent's performance and study how to adequately apply the augmented data to pre-train and fine-tune an agent. Thanks to our large-scale dataset, the performance of an existing agent can be pushed up (+11% absolute with regard to previous SoTA) to a significantly new best of 80% single-run success rate on the R2R test split by simple imitation learning. The
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14527</link><description>&lt;p&gt;
&#29992;&#20110;&#33618;&#37326;SAR&#21644;&#23547;&#25214;Patricia Wu-Murad&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14527
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#33618;&#37326;&#25628;&#25937;&#20013;&#24212;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;EfficientDET&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#20294;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#20551;&#38451;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#23558;&#20004;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#65292;EfficientDET&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#26469;&#33258;&#26085;&#26412;Wu-Murad&#37326;&#22806;&#25628;&#25937;&#65288;WSAR&#65289;&#21162;&#21147;&#30340;98.9 GB&#26080;&#20154;&#26426;&#22270;&#20687;&#30340;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;3&#20010;&#26041;&#21521;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#33267;&#23569;19&#31181;&#26041;&#27861;&#21644;3&#20010;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22312;&#26080;&#20154;&#26426;&#22270;&#20687;&#20013;&#23450;&#20301;&#22833;&#36394;&#20154;&#21592;&#65292;&#20294;&#21482;&#26377;3&#31181;&#26041;&#27861;&#65288;2&#31181;&#26080;&#30417;&#30563;&#21644;1&#31181;&#26410;&#30693;&#32467;&#26500;&#65289;&#22312;&#25991;&#29486;&#20013;&#34987;&#24341;&#29992;&#20026;&#23454;&#38469;WSAR&#25805;&#20316;&#20013;&#20351;&#29992;&#36807;&#12290;&#22312;&#36825;&#20123;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;EfficientDET&#26550;&#26500;&#21644;&#26080;&#30417;&#30563;&#30340;RX&#20809;&#35889;&#20998;&#31867;&#22120;&#34987;&#36873;&#25321;&#20026;&#26368;&#36866;&#21512;&#27492;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;EfficientDET&#27169;&#22411;&#24212;&#29992;&#20110;HERIDAL&#25968;&#25454;&#38598;&#65292;&#23613;&#31649;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#25216;&#26415;&#30456;&#24403;&#30340;&#27700;&#24179;&#65292;&#20294;&#27169;&#22411;&#22312;&#20551;&#38451;&#24615;&#26041;&#38754;&#26080;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26377;&#25928;&#35782;&#21035;&#65288;&#20363;&#22914;&#65292;&#35782;&#21035;&#26641;&#26525;&#21644;&#23721;&#30707;&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper details the challenges in applying two computer vision systems, an EfficientDET supervised learning model and the unsupervised RX spectral classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and rescue (WSAR) effort in Japan and identifies 3 directions for future research. There have been at least 19 proposed approaches and 3 datasets aimed at locating missing persons in drone imagery, but only 3 approaches (2 unsupervised and 1 of an unknown structure) are referenced in the literature as having been used in an actual WSAR operation. Of these proposed approaches, the EfficientDET architecture and the unsupervised spectral RX classifier were selected as the most appropriate for this setting. The EfficientDET model was applied to the HERIDAL dataset and despite achieving performance that is statistically equivalent to the state-of-the-art, the model fails to translate to the real world in terms of false positives (e.g., identifying tree limbs and rocks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2307.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#26631;&#31614;&#26469;&#37325;&#26032;&#23457;&#35270;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#26469;&#24341;&#23548;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19982;&#20266;&#26631;&#31614;&#25216;&#26415;&#30340;&#36741;&#21161;&#24050;&#32463;&#25104;&#20026;&#39046;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#22810;&#31867;&#35757;&#32451;&#35774;&#32622;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#20302;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#20849;&#23384;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#21516;&#26102;&#23398;&#20064;&#26816;&#27979;&#25152;&#26377;&#31867;&#21035;&#30340;&#26032;&#22411;ReDB&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#21644;&#31867;&#24179;&#34913;&#30340;&#20266;&#19977;&#32500;&#26694;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#24341;&#23548;&#19981;&#21516;&#20998;&#24067;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#29615;&#22659;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20809;&#26463;&#25968;&#37327;&#65289;&#24102;&#26469;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#22495;&#26816;&#26597;&#65288;CDE&#65289;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#23454;&#20363;&#22797;&#21046;&#31896;&#36148;&#21040;&#28304;&#29615;&#22659;&#20013;&#24182;&#27979;&#37327;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#35780;&#20272;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#21644;&#32531;&#35299;&#29289;&#20307;&#30340;&#36716;&#31227;&#65288;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16021</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32467;&#26500;&#65306;&#35843;&#26597;&#19982;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16021
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#26500;&#30340;&#35282;&#33394;&#21644;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#20010;&#23376;&#39046;&#22495;&#22312;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#26041;&#38754;&#25152;&#20570;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20511;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20989;&#25968;&#36924;&#36817;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#23545;&#22810;&#26679;&#19988;&#19981;&#21487;&#39044;&#27979;&#30340;&#21160;&#24577;&#12289;&#22024;&#26434;&#20449;&#21495;&#20197;&#21450;&#24222;&#22823;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#31561;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#26102;&#65292;&#20854;&#23454;&#29992;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;&#35832;&#22914;&#25968;&#25454;&#25928;&#29575;&#20302;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12289;&#32570;&#23569;&#23433;&#20840;&#20445;&#35777;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#36825;&#20123;&#20851;&#38190;&#25351;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#26159;&#23558;&#38382;&#39064;&#30340;&#38468;&#21152;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#21508;&#20010;&#23376;&#39046;&#22495;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#32435;&#20837;&#36825;&#26679;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#26694;&#26550;&#19979;&#65292;&#25581;&#31034;&#32467;&#26500;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.09378</link><description>&lt;p&gt;
&#22312;Lenia&#20013;&#25429;&#33719;&#26032;&#20852;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Capturing Emerging Complexity in Lenia. (arXiv:2305.09378v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09378
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;Lenia&#65292;&#36890;&#36807;&#35782;&#21035;&#22797;&#26434;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#20135;&#29983;&#19981;&#21516;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#20197;&#36827;&#21270;&#20986;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#25506;&#35752;&#20102;Lenia&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#25311;&#25968;&#23383;&#29983;&#29289;&#31995;&#32479;&#30340;&#20154;&#24037;&#29983;&#21629;&#24179;&#21488;&#12290;Lenia&#30340;&#29983;&#24577;&#31995;&#32479;&#30001;&#31616;&#21333;&#30340;&#20154;&#24037;&#29983;&#29289;&#32452;&#25104;&#65292;&#23427;&#20204;&#21487;&#20197;&#31227;&#21160;&#12289;&#28040;&#32791;&#12289;&#29983;&#38271;&#21644;&#32321;&#27542;&#12290;&#35813;&#24179;&#21488;&#26159;&#19968;&#20010;&#30740;&#31350;&#20154;&#24037;&#29983;&#21629;&#21644;&#36827;&#21270;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#28789;&#27963;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#22810;&#26679;&#21270;&#29983;&#29289;&#12290;&#35813;&#30740;&#31350;&#30340;&#20851;&#38190;&#26159;&#22312;Lenia&#20013;&#27979;&#37327;&#22797;&#26434;&#24615;&#65292;&#35782;&#21035;&#27979;&#37327;&#35268;&#21017;&#30340;&#38271;&#26399;&#22797;&#26434;&#24615;&#26032;&#20852;&#34892;&#20026;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#36827;&#21270;&#20986;&#23578;&#26410;&#21457;&#29616;&#30340;&#26356;&#22909;&#30340;Lenia&#34892;&#20026;&#12290;&#36951;&#20256;&#31639;&#27861;&#20351;&#29992;&#30456;&#37051;&#21306;&#22495;&#25110;&#26680;&#20316;&#20026;&#22522;&#22240;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;Lenia&#30340;&#20854;&#20182;&#21442;&#25968;&#65288;&#20363;&#22914;&#29983;&#38271;&#20989;&#25968;&#65289;&#19981;&#21464;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#20154;&#21475;&#34892;&#20026;&#30340;&#32467;&#26524;&#65292;&#28982;&#21518;&#27979;&#37327;&#36866;&#24212;&#24230;&#20540;&#20197;&#20915;&#23450;&#25152;&#24471;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#21464;&#21270;&#20316;&#20026;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
This research project investigates Lenia, an artificial life platform that simulates ecosystems of digital creatures. Lenia's ecosystem consists of simple, artificial organisms that can move, consume, grow, and reproduce. The platform is important as a tool for studying artificial life and evolution, as it provides a scalable and flexible environment for creating a diverse range of organisms with varying abilities and behaviors. Measuring complexity in Lenia is a key aspect of the study, which identifies the metrics for measuring long-term complex emerging behavior of rules, with the aim of evolving better Lenia behaviors which are yet not discovered. The Genetic Algorithm uses neighborhoods or kernels as genotype while keeping the rest of the parameters of Lenia as fixed, for example growth function, to produce different behaviors respective to the population and then measures fitness value to decide the complexity of the resulting behavior. First, we use Variation over Time as a fitn
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#24341;&#23548;&#30340;&#29289;&#20307;&#35745;&#25968;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#23494;&#38598;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07304</link><description>&lt;p&gt;
CLIP-Count&#65306;&#38754;&#21521;&#25991;&#26412;&#24341;&#23548;&#19979;&#38646;&#26679;&#26412;&#29289;&#20307;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
CLIP-Count: Towards Text-Guided Zero-Shot Object Counting. (arXiv:2305.07304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#19968;&#31181;&#22522;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#24341;&#23548;&#30340;&#29289;&#20307;&#35745;&#25968;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#24341;&#20837;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#33719;&#24471;&#20102;&#39640;&#25928;&#30340;&#23494;&#38598;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;-&#22270;&#20687;&#21305;&#37197;&#33021;&#21147;&#65292;&#21487;&#36716;&#31227;&#21040;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#30446;&#26631;&#35745;&#25968;&#8212;&#8212;&#20272;&#35745;&#22270;&#20687;&#20013;&#23545;&#35937;&#30340;&#25968;&#37327;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#39318;&#27425;&#25506;&#32034;&#65292;&#23558;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#29992;&#20110;&#26080;&#31867;&#21035;&#20559;&#35265;&#30340;&#29289;&#20307;&#35745;&#25968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP-Count&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#31243;&#65292;&#23427;&#36890;&#36807;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#24341;&#23548;&#65292;&#20026;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#20272;&#35745;&#23494;&#24230;&#22270;&#65292;&#32780;&#19981;&#38656;&#35201;&#22312;&#29305;&#23450;&#23545;&#35937;&#31867;&#21035;&#19978;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#20026;&#20102;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#23494;&#38598;&#22270;&#20687;&#29305;&#24449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#19969;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#65292;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#26377;&#29992;&#30340;&#34917;&#19969;&#32423;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#23494;&#38598;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#30340;patch-text&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20256;&#36882;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in visual-language models have shown remarkable zero-shot text-image matching ability that is transferable to down-stream tasks such as object detection and segmentation. However, adapting these models for object counting, which involves estimating the number of objects in an image, remains a formidable challenge. In this study, we conduct the first exploration of transferring visual-language models for class-agnostic object counting. Specifically, we propose CLIP-Count, a novel pipeline that estimates density maps for open-vocabulary objects with text guidance in a zero-shot manner, without requiring any finetuning on specific object classes. To align the text embedding with dense image features, we introduce a patch-text contrastive loss that guides the model to learn informative patch-level image representations for dense prediction. Moreover, we design a hierarchical patch-text interaction module that propagates semantic information across different resolution level
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.03216</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#23454;&#29616;&#36817;&#23454;&#26102;&#38754;&#37096;&#21160;&#30011;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution. (arXiv:2305.03216v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;3D&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#12289;&#36924;&#30495;&#22320;&#22686;&#24378;&#20302;&#25104;&#26412;&#12289;&#23454;&#26102;&#29289;&#29702;&#27169;&#25311;&#20135;&#29983;&#30340;&#38754;&#37096;&#34920;&#29616;&#65292;&#20351;&#20854;&#25509;&#36817;&#20110;&#20855;&#26377;&#26356;&#39640;&#20998;&#36776;&#29575;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#39640;&#36798;26&#20493;&#30340;&#20803;&#32032;&#25968;&#65289;&#21644;&#20934;&#30830;&#29289;&#29702;&#24314;&#27169;&#30340;&#21442;&#32771;&#36136;&#37327;&#31163;&#32447;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28304;&#20110;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#26500;&#24314;&#19968;&#32452;&#37197;&#23545;&#24103;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#24207;&#21015;&#20998;&#21035;&#26469;&#33258;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#30456;&#20114;&#23545;&#24212;&#12290;&#25105;&#20204;&#20197;&#38754;&#37096;&#21160;&#30011;&#20026;&#20363;&#65292;&#21019;&#36896;&#36825;&#31181;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#26041;&#24335;&#23601;&#26159;&#22312;&#20004;&#20010;&#27169;&#25311;&#22120;&#20013;&#35843;&#25972;&#21516;&#26679;&#30340;&#32908;&#32905;&#28608;&#27963;&#25511;&#21046;&#21644;&#39592;&#26550;&#23039;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36229;&#20998;&#36776;&#29575;&#26694;&#26550;&#20174;&#36825;&#20010;&#35757;&#32451;&#38598;&#20013;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#34920;&#24773;&#65292;&#24182;&#19988;&#34917;&#20607;&#20004;&#20010;&#27169;&#25311;&#20043;&#38388;&#30340;&#24314;&#27169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a neural network-based simulation super-resolution framework that can efficiently and realistically enhance a facial performance produced by a low-cost, realtime physics-based simulation to a level of detail that closely approximates that of a reference-quality off-line simulator with much higher resolution (26x element count in our examples) and accurate physical modeling. Our approach is rooted in our ability to construct - via simulation - a training set of paired frames, from the low- and high-resolution simulators respectively, that are in semantic correspondence with each other. We use face animation as an exemplar of such a simulation domain, where creating this semantic congruence is achieved by simply dialing in the same muscle actuation controls and skeletal pose in the two simulators. Our proposed neural network super-resolution framework generalizes from this training set to unseen expressions, compensates for modeling discrepancies between the two simulations du
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.05874</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;EEG&#25968;&#25454;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21487;&#35299;&#37322;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Adaptive Gated Graph Convolutional Network for Explainable Diagnosis of Alzheimer's Disease using EEG Data. (arXiv:2304.05874v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#21367;&#31215;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#21644;&#21151;&#33021;&#36830;&#25509;&#24230;&#37327;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#65292;&#24182;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#33041;&#21306;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20998;&#31867;&#33041;&#30005;&#22270;(EEG)&#25968;&#25454;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#30340;&#35786;&#26029;&#20173;&#28982;&#26159;&#30456;&#23545;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#38376;&#25511;&#22270;&#21367;&#31215;&#32593;&#32476;(AGGCN)&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;AGGCN&#36890;&#36807;&#23558;&#22522;&#20110;&#21367;&#31215;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#19982;&#22522;&#20110;&#21151;&#33021;&#36830;&#25509;&#24615;&#30340;&#33879;&#21517;&#30456;&#20851;&#24230;&#37327;&#30456;&#32467;&#21512;&#26469;&#33258;&#36866;&#24212;&#23398;&#20064;&#22270;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#38376;&#25511;&#22270;&#21367;&#31215;&#21487;&#20197;&#21160;&#24577;&#22320;&#21152;&#26435;&#32771;&#34385;&#21508;&#31181;&#31354;&#38388;&#23610;&#24230;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38381;&#30524;&#21644;&#30529;&#30524;&#29366;&#24577;&#19979;&#22343;&#33021;&#21462;&#24471;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#32467;&#26524;&#30340;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;AGGCN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;AD&#26368;&#21463;&#24433;&#21709;&#30340;&#33041;&#21306;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) models are increasingly being used for the classification of electroencephalography (EEG) data. However, GNN-based diagnosis of neurological disorders, such as Alzheimer's disease (AD), remains a relatively unexplored area of research. Previous studies have relied on functional connectivity methods to infer brain graph structures and used simple GNN architectures for the diagnosis of AD. In this work, we propose a novel adaptive gated graph convolutional network (AGGCN) that can provide explainable predictions. AGGCN adaptively learns graph structures by combining convolution-based node feature enhancement with a well-known correlation-based measure of functional connectivity. Furthermore, the gated graph convolution can dynamically weigh the contribution of various spatial scales. The proposed model achieves high accuracy in both eyes-closed and eyes-open conditions, indicating the stability of learned representations. Finally, we demonstrate that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.14961</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#36827;&#34892;&#35748;&#35777;&#21644;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#19981;&#26029;&#25193;&#23637;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35782;&#21035;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#65292;&#25110;&#32773;&#26159;&#19968;&#20010;&#8220;&#26679;&#26412;&#22806;&#8221;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#21487;&#20197;&#20197;&#19968;&#31181;&#23548;&#33268;&#20998;&#31867;&#22120;&#20570;&#20986;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#24335;&#25805;&#32437;OOD&#26679;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;L2&#33539;&#22260;&#20869;&#35777;&#26126;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#19981;&#38656;&#35201;&#29305;&#23450;&#32452;&#20214;&#25110;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;OOD&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#26679;&#26412;&#30340;&#39640;&#27700;&#24179;&#30340;&#35748;&#35777;&#21644;&#23545;&#25239;&#30340;&#32467;&#26524;&#12290;&#22312;CIFAR10/100&#30340;&#25152;&#26377;OOD&#26816;&#27979;&#25351;&#26631;&#30340;&#24179;&#22343;&#20540;&#26174;&#31034;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;&#32422;13&#65285;/ 5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a $\ell_2$-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$ relative to previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2302.13817</link><description>&lt;p&gt;
&#35753;&#25105;&#20204;&#26469;&#32842;&#32842;&#21543;&#65281;&#19982;ChatGPT&#30340;&#23545;&#35805;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations. (arXiv:2302.13817v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#20854;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#20197;&#21450;&#24403;&#21069;&#29256;&#26412;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27454;&#33021;&#22815;&#29983;&#25104;&#20687;&#20154;&#31867;&#19968;&#26679;&#30340;&#21477;&#23376;&#21644;&#20889;&#20986;&#36830;&#36143;&#25991;&#31456;&#30340;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#27010;&#36848;&#20197;&#21450;ChatGPT&#32972;&#21518;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36824;&#24378;&#35843;&#20102;ChatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#25945;&#32946;&#21644;&#30740;&#31350;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;ChatGPT&#21608;&#22260;&#23384;&#22312;&#30528;&#19968;&#20123;&#38544;&#31169;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#21521;ChatGPT&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#20415;&#23427;&#34920;&#36798;&#33258;&#24049;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention. This paper discusses the historical overview of chatbots and the technology behind Chat Generative Pre-trained Transformer, better known as ChatGPT. Moreover, potential applications of ChatGPT in various domains, including healthcare, education, and research, are highlighted. Despite promising results, there are several privacy and ethical concerns surrounding ChatGPT. In addition, we highlight some of the important limitations of the current version of ChatGPT. We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20026;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.10822</link><description>&lt;p&gt;
RobustPdM&#65306;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;
&lt;/p&gt;
&lt;p&gt;
RobustPdM: Designing Robust Predictive Maintenance against Adversarial Attacks. (arXiv:2301.10822v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#25239;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20026;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#32500;&#25252;&#25216;&#26415;&#22312;&#38477;&#20302;&#22797;&#26434;&#26426;&#22120;&#30340;&#32500;&#25252;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#65292;&#25552;&#39640;&#25972;&#20307;&#29983;&#20135;&#29575;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36890;&#36807;&#24191;&#27867;&#21033;&#29992;&#29289;&#32852;&#32593;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#37117;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25935;&#24863;&#24615;&#24050;&#32463;&#34987;&#20844;&#35748;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32500;&#25252;&#39046;&#22495;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#20219;&#21153;&#30340;&#39044;&#27979;&#32500;&#25252;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#39044;&#27979;&#32500;&#25252;&#31995;&#32479;&#65292;&#36890;&#36807;&#24191;&#27867;&#20998;&#26512;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#32500;&#25252;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art predictive maintenance (PdM) techniques have shown great success in reducing maintenance costs and downtime of complicated machines while increasing overall productivity through extensive utilization of Internet-of-Things (IoT) and Deep Learning (DL). Unfortunately, IoT sensors and DL algorithms are both prone to cyber-attacks. For instance, DL algorithms are known for their susceptibility to adversarial examples. Such adversarial attacks are vastly under-explored in the PdM domain. This is because the adversarial attacks in the computer vision domain for classification tasks cannot be directly applied to the PdM domain for multivariate time series (MTS) regression tasks. In this work, we propose an end-to-end methodology to design adversarially robust PdM systems by extensively analyzing the effect of different types of adversarial attacks and proposing a novel adversarial defense technique for DL-enabled PdM models. First, we propose novel MTS Projected Gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#26426;&#20316;&#29289;&#34892;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20892;&#30000;&#29615;&#22659;&#20013;&#24212;&#23545;&#22810;&#26679;&#30340;&#30000;&#38388;&#26465;&#20214;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#30456;&#26426;&#23454;&#29616;&#20316;&#29289;&#34892;&#26816;&#27979;&#21644;&#23548;&#33322;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04278</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#30000;&#23548;&#33322;&#20892;&#26426;&#20316;&#29289;&#34892;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based Crop Row Detection for Infield Navigation of Agri-Robots. (arXiv:2209.04278v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#26426;&#20316;&#29289;&#34892;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20892;&#30000;&#29615;&#22659;&#20013;&#24212;&#23545;&#22810;&#26679;&#30340;&#30000;&#38388;&#26465;&#20214;&#65292;&#36890;&#36807;&#20302;&#25104;&#26412;&#30456;&#26426;&#23454;&#29616;&#20316;&#29289;&#34892;&#26816;&#27979;&#21644;&#23548;&#33322;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#26449;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#38754;&#20020;&#30528;&#19981;&#26029;&#21464;&#21270;&#30340;&#30000;&#38388;&#26465;&#20214;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26114;&#36149;&#30340;&#30828;&#20214;&#65292;&#22914;RTK-GNSS&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#12289;&#33021;&#22815;&#24212;&#23545;&#36825;&#31181;&#30000;&#38388;&#21464;&#21270;&#30340;&#20302;&#25104;&#26412;&#30456;&#26426;&#20316;&#29289;&#34892;&#26816;&#27979;&#31639;&#27861;&#12290;&#29616;&#26377;&#30340;&#20316;&#29289;&#34892;&#26816;&#27979;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#20195;&#34920;&#25152;&#26377;&#21487;&#33021;&#30340;&#30000;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;11&#20010;&#30000;&#38388;&#21464;&#21270;&#30340;&#29980;&#33756;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#20010;&#29983;&#38271;&#38454;&#27573;&#12289;&#20809;&#29031;&#27700;&#24179;&#21464;&#21270;&#12289;&#26434;&#33609;&#23494;&#24230;&#19981;&#21516;&#12289;&#26354;&#32447;&#20316;&#29289;&#34892;&#21644;&#19981;&#36830;&#32493;&#20316;&#29289;&#34892;&#31561;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#20316;&#29289;&#34892;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#21033;&#29992;&#39044;&#27979;&#30340;&#20998;&#21106;&#25513;&#27169;&#26469;&#25552;&#21462;&#20013;&#22830;&#20316;&#29289;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#22830;&#20316;&#29289;&#34892;&#36873;&#25321;&#31639;&#27861;&#12290;&#26032;&#39062;&#30340;&#20316;&#29289;&#34892;&#26816;&#27979;&#31639;&#27861;&#32463;&#36807;&#27979;&#35797;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20316;&#29289;&#34892;&#26816;&#27979;&#24615;&#33021;&#21644;&#21487;&#35270;&#20282;&#26381;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous navigation in agricultural environments is challenged by varying field conditions that arise in arable fields. State-of-the-art solutions for autonomous navigation in such environments require expensive hardware such as RTK-GNSS. This paper presents a robust crop row detection algorithm that withstands such field variations using inexpensive cameras. Existing datasets for crop row detection does not represent all the possible field variations. A dataset of sugar beet images was created representing 11 field variations comprised of multiple grow stages, light levels, varying weed densities, curved crop rows and discontinuous crop rows. The proposed pipeline segments the crop rows using a deep learning-based method and employs the predicted segmentation mask for extraction of the central crop using a novel central crop row selection algorithm. The novel crop row detection algorithm was tested for crop row detection performance and the capability of visual servoing along a crop
&lt;/p&gt;</description></item><item><title>VAE&#35299;&#32439;&#32467;&#26524;&#30340;&#25104;&#22240;&#26377;&#24453;&#37325;&#26032;&#23457;&#35270;&#65292;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#19982;&#37325;&#24314;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#19982;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.13341</link><description>&lt;p&gt;
VAE&#35299;&#32439;&#25928;&#26524;&#30340;&#37325;&#24314;&#25439;&#22833;&#34987;&#24573;&#35270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Overlooked Implications of the Reconstruction Loss for VAE Disentanglement. (arXiv:2202.13341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13341
&lt;/p&gt;
&lt;p&gt;
VAE&#35299;&#32439;&#32467;&#26524;&#30340;&#25104;&#22240;&#26377;&#24453;&#37325;&#26032;&#23457;&#35270;&#65292;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#19982;&#37325;&#24314;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#19982;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#35299;&#32806;&#34920;&#31034;&#36890;&#24120;&#24402;&#22240;&#20110;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#32452;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#25968;&#25454;&#19982;&#25439;&#22833;&#30340;&#37325;&#24314;&#39033;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;VAEs&#20013;&#35299;&#32806;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26631;&#20934;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26681;&#25454;&#20856;&#22411;&#30340;VAE&#37325;&#24314;&#25439;&#22833;&#20250;&#23548;&#33268;&#23427;&#20204;&#30340;&#20027;&#35266;&#30495;&#23454;&#22240;&#32032;&#19982;&#25968;&#25454;&#20013;&#30340;&#24863;&#30693;&#36724;&#20043;&#38388;&#23384;&#22312;&#24847;&#22806;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#20026;&#22312;&#32473;&#23450;&#37325;&#24314;&#25439;&#22833;&#19979;&#26500;&#24314;&#23545;&#25239;&#25968;&#25454;&#38598;&#30340;&#29702;&#35770;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31034;&#20363;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#36825;&#19968;&#28857;&#65292;&#35813;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#26368;&#20808;&#36827;&#26694;&#26550;&#20013;&#30340;&#35299;&#32806;&#25928;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#30452;&#35266;&#30340;&#30495;&#23454;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#31034;&#20363;&#37325;&#24314;&#25439;&#22833;&#65292;&#20877;&#27425;&#33021;&#22815;&#24863;&#30693;&#21040;&#30495;&#23454;&#22240;&#32032;&#26469;&#37325;&#26032;&#23454;&#29616;&#35299;&#32806;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35777;&#26126;&#20102;&#35299;&#32806;&#30340;&#20027;&#35266;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations with variational autoencoders (VAEs) is often attributed to the regularisation component of the loss. In this work, we highlight the interaction between data and the reconstruction term of the loss as the main contributor to disentanglement in VAEs. We show that standard benchmark datasets have unintended correlations between their subjective ground-truth factors and perceived axes in the data according to typical VAE reconstruction losses. Our work exploits this relationship to provide a theory for what constitutes an adversarial dataset under a given reconstruction loss. We verify this by constructing an example dataset that prevents disentanglement in state-of-the-art frameworks while maintaining human-intuitive ground-truth factors. Finally, we re-enable disentanglement by designing an example reconstruction loss that is once again able to perceive the ground-truth factors. Our findings demonstrate the subjective nature of disentanglement and t
&lt;/p&gt;</description></item></channel></rss>