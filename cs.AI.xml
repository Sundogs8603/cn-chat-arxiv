<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.06949</link><description>&lt;p&gt;
HyperDreamBooth&#65306;&#29992;&#20110;&#24555;&#36895;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06949
&lt;/p&gt;
&lt;p&gt;
HyperDreamBooth&#26159;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#20013;&#24555;&#36895;&#29983;&#25104;&#20010;&#24615;&#21270;&#26435;&#37325;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#24182;&#21516;&#26102;&#20445;&#30041;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#39118;&#26684;&#19979;&#21512;&#25104;&#20010;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#20010;&#24615;&#21270;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27599;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#26102;&#38388;&#25237;&#20837;&#65292;&#20026;&#27599;&#20010;&#20027;&#39064;&#23384;&#20648;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#20250;&#23545;&#23384;&#20648;&#23481;&#37327;&#25552;&#20986;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperDreamBooth-&#19968;&#31181;&#33021;&#22815;&#20174;&#19968;&#20010;&#20154;&#30340;&#21333;&#24352;&#22270;&#29255;&#26377;&#25928;&#29983;&#25104;&#19968;&#32452;&#20010;&#24615;&#21270;&#26435;&#37325;&#30340;&#36229;&#32593;&#32476;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#26435;&#37325;&#32452;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#24182;&#25645;&#37197;&#24555;&#36895;&#24494;&#35843;&#65292;HyperDreamBooth&#33021;&#22815;&#20197;&#22810;&#31181;&#32972;&#26223;&#21644;&#39118;&#26684;&#29983;&#25104;&#19968;&#20010;&#20154;&#30340;&#38754;&#37096;&#65292;&#20445;&#25345;&#39640;&#20027;&#39064;&#32454;&#33410;&#21516;&#26102;&#20063;&#20445;&#25345;&#27169;&#22411;&#23545;&#22810;&#26679;&#21270;&#39118;&#26684;&#21644;&#35821;&#20041;&#20462;&#25913;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#32422;50&#20493;&#20307;&#29616;&#20102;&#38754;&#37096;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization has emerged as a prominent aspect within the field of generative AI, enabling the synthesis of individuals in diverse contexts and styles, while retaining high-fidelity to their identities. However, the process of personalization presents inherent challenges in terms of time and memory requirements. Fine-tuning each personalized model needs considerable GPU time investment, and storing a personalized model per subject can be demanding in terms of storage capacity. To overcome these challenges, we propose HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of personalized weights from a single image of a person. By composing these weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth can generate a person's face in various contexts and styles, with high subject details while also preserving the model's crucial knowledge of diverse styles and semantic modifications. Our method achieves personalization on faces in roughly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.06947</link><description>&lt;p&gt;
&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65306;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#36890;&#36807;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26469;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#32467;&#21512;&#20102;Transformer&#21644;&#21367;&#31215;&#35774;&#35745;&#30340;&#20248;&#28857;&#65292;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#39057;&#35782;&#21035;&#27169;&#22411;&#21033;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#38271;&#36317;&#31163;&#26102;&#31354;&#19978;&#19979;&#25991;&#24314;&#27169;&#12290;&#35270;&#39057;Transformer&#35774;&#35745;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#20197;&#39640;&#35745;&#31639;&#25104;&#26412;&#27169;&#25311;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29992;&#20110;&#35270;&#39057;&#30340;&#21367;&#31215;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#38271;&#36317;&#31163;&#20381;&#36182;&#24314;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20004;&#31181;&#35774;&#35745;&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#65288;Video-FocalNet&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#35270;&#39057;&#35782;&#21035;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#12290;&#35270;&#39057;&#28966;&#28857;&#32593;&#32476;&#22522;&#20110;&#26102;&#31354;&#28966;&#28857;&#35843;&#21046;&#26550;&#26500;&#65292;&#23545;&#33258;&#27880;&#24847;&#21147;&#30340;&#20132;&#20114;&#21644;&#32858;&#21512;&#27493;&#39588;&#36827;&#34892;&#20102;&#39072;&#20498;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#32858;&#21512;&#27493;&#39588;&#21644;&#20132;&#20114;&#27493;&#39588;&#37117;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#21367;&#31215;&#21644;&#36880;&#20803;&#32032;&#20056;&#27861;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20854;&#35745;&#31639;&#25104;&#26412;&#27604;&#35270;&#39057;&#34920;&#36798;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#23545;&#24212;&#37096;&#20998;&#35201;&#20302;&#24471;&#22810;&#12290;&#25105;&#20204;&#24191;&#27867;&#25506;&#32034;&#20102;&#28966;&#28857;&#35843;&#21046;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent video recognition models utilize Transformer models for long-range spatio-temporal context modeling. Video transformer designs are based on self-attention that can model global context at a high computational cost. In comparison, convolutional designs for videos offer an efficient alternative but lack long-range dependency modeling. Towards achieving the best of both designs, this work proposes Video-FocalNet, an effective and efficient architecture for video recognition that models both local and global contexts. Video-FocalNet is based on a spatio-temporal focal modulation architecture that reverses the interaction and aggregation steps of self-attention for better efficiency. Further, the aggregation step and the interaction step are both implemented using efficient convolution and element-wise multiplication operations that are computationally less expensive than their self-attention counterparts on video representations. We extensively explore the design space of focal modu
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#32852;&#31995;&#65292;&#23558;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21464;&#25442;&#21644;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#20204;&#26159;&#31561;&#20215;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25351;&#20986;&#20102;&#20165;&#20165;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06941</link><description>&lt;p&gt;
&#20851;&#20110;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations. (arXiv:2307.06941v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#29702;&#35770;&#32852;&#31995;&#65292;&#23558;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21464;&#25442;&#21644;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#23427;&#20204;&#26159;&#31561;&#20215;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25351;&#20986;&#20102;&#20165;&#20165;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#20004;&#31181;&#26368;&#21463;&#27426;&#36814;&#30340;&#35299;&#37322;&#31867;&#22411;&#26159;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36825;&#20004;&#31867;&#26041;&#27861;&#19968;&#30452;&#22312;&#29420;&#31435;&#22320;&#36827;&#34892;&#30740;&#31350;&#65292;&#32780;&#23545;&#23427;&#20204;&#30340;&#35843;&#21644;&#21482;&#26377;&#23569;&#25968;&#35797;&#22270;&#26159;&#32463;&#39564;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#21338;&#24328;&#35770;&#29305;&#24449;&#24402;&#22240;&#65288;&#20027;&#35201;&#20851;&#27880;&#20294;&#19981;&#38480;&#20110;SHAP&#65289;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20043;&#38388;&#30340;&#26126;&#30830;&#29702;&#35770;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;Shapley&#20540;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#36827;&#34892;&#26377;&#25928;&#21464;&#25442;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#35777;&#26126;&#23427;&#20204;&#23454;&#38469;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#31561;&#20215;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;Shapley&#20540;&#20197;&#22806;&#30340;&#21338;&#24328;&#35770;&#35299;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#31561;&#20215;&#26465;&#20214;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21482;&#31616;&#21333;&#22320;&#20351;&#29992;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23450;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) has received widespread interest in recent years, and two of the most popular types of explanations are feature attributions, and counterfactual explanations. These classes of approaches have been largely studied independently and the few attempts at reconciling them have been primarily empirical. This work establishes a clear theoretical connection between game-theoretic feature attributions, focusing on but not limited to SHAP, and counterfactuals explanations. After motivating operative changes to Shapley values based feature attributions and counterfactual explanations, we prove that, under conditions, they are in fact equivalent. We then extend the equivalency result to game-theoretic solution concepts beyond Shapley values. Moreover, through the analysis of the conditions of such equivalence, we shed light on the limitations of naively using counterfactual explanations to provide feature importances. Experiments on three datasets quantita
&lt;/p&gt;</description></item><item><title>FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.06933</link><description>&lt;p&gt;
FDAPT: &#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
FDAPT: Federated Domain-adaptive Pre-training for Language Models. (arXiv:2307.06933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06933
&lt;/p&gt;
&lt;p&gt;
FDAPT&#26159;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;&#20110;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;FDAPT&#33021;&#22815;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#25552;&#20986;&#30340;FFDAPT&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;FDAPT&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20063;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;DAPT&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26356;&#25935;&#24863;&#21644;&#20998;&#24067;&#24335;&#25968;&#25454;&#26469;&#22686;&#24378;&#27169;&#22411;&#36866;&#24212;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FDAPT&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FDAPT&#22312;IID&#21644;&#38750;IID&#24773;&#20917;&#19979;&#37117;&#33021;&#32500;&#25345;&#19982;&#20013;&#22830;&#22522;&#32447;&#30456;&#31454;&#20105;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#20923;&#32467;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;FFDAPT&#65289;&#12290;FFDAPT&#24179;&#22343;&#25552;&#39640;&#20102;12.1%&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;FDAPT&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#65292;&#19968;&#33324;&#24615;&#33021;&#27874;&#21160;&#20445;&#25345;&#22312;1%&#20197;&#19979;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#24037;&#20316;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Domain-adaptive Pre-training (DAPT) with Federated Learning (FL) can enhance model adaptation by leveraging more sensitive and distributed data while preserving data privacy. However, few studies have focused on this method. Therefore, we conduct the first comprehensive empirical study to evaluate the performance of Federated Domain-adaptive Pre-training (FDAPT). We demonstrate that FDAPT can maintain competitive downstream task performance to the centralized baseline in both IID and non-IID situations. Furthermore, we propose a novel algorithm, Frozen Federated Domain-adaptive Pre-training (FFDAPT). FFDAPT improves the computational efficiency by 12.1% on average and exhibits similar downstream task performance to standard FDAPT, with general performance fluctuations remaining less than 1%. Finally, through a critical evaluation of our work, we identify promising future research directions for this new research area.
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06917</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;: ChatGPT&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
LLM-assisted Knowledge Graph Engineering: Experiments with ChatGPT. (arXiv:2307.06917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;LLM&#36741;&#21161;&#30340;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#12289;&#28789;&#27963;&#12289;&#36879;&#26126;&#12289;&#36328;&#31995;&#32479;&#21644;&#21327;&#20316;&#30340;&#26041;&#24335;&#26469;&#32452;&#32455;&#31038;&#20250;&#21644;&#24037;&#19994;&#20197;&#21450;&#31185;&#23398;&#23398;&#31185;&#20013;&#21508;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#21644;&#25968;&#25454;&#12290;&#30693;&#35782;&#22270;&#35889;&#22312;&#26377;&#25928;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20219;&#20309;&#20854;&#20182;&#24418;&#24335;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#38656;&#35201;&#23545;&#22270;&#32467;&#26500;&#12289;&#32593;&#32476;&#25216;&#26415;&#12289;&#29616;&#26377;&#27169;&#22411;&#21644;&#35789;&#27719;&#12289;&#35268;&#21017;&#38598;&#12289;&#36923;&#36753;&#20197;&#21450;&#26368;&#20339;&#23454;&#36341;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#21516;&#26102;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#12290;&#32771;&#34385;&#21040;&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21450;&#20854;&#25509;&#21475;&#21644;&#24212;&#29992;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#25506;&#32034;&#20854;&#22312;&#25903;&#25345;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#20013;&#19968;&#20123;&#23454;&#39564;&#21450;&#20854;&#32467;&#26524;&#65292;&#20197;&#23637;&#31034;ChatGPT&#22914;&#20309;&#36741;&#21161;&#25105;&#20204;&#24320;&#21457;&#21644;&#31649;&#29702;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KG) provide us with a structured, flexible, transparent, cross-system, and collaborative way of organizing our knowledge and data across various domains in society and industrial as well as scientific disciplines. KGs surpass any other form of representation in terms of effectiveness. However, Knowledge Graph Engineering (KGE) requires in-depth experiences of graph structures, web technologies, existing models and vocabularies, rule sets, logic, as well as best practices. It also demands a significant amount of work. Considering the advancements in large language models (LLMs) and their interfaces and applications in recent years, we have conducted comprehensive experiments with ChatGPT to explore its potential in supporting KGE. In this paper, we present a selection of these experiments and their results to demonstrate how ChatGPT can assist us in the development and management of KGs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06913</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#25581;&#31034;&#29420;&#29305;&#30340;&#27010;&#24565;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06913
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20998;&#35299;&#21644;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#27010;&#24565;&#21521;&#37327;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#19988;&#20855;&#26377;&#35821;&#20041;&#30340;&#29420;&#29305;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#36825;&#20123;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26131;&#20110;&#29702;&#35299;&#21644;&#19982;&#20219;&#21153;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#23545;&#20110;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#27169;&#22411;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26356;&#26131;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#20687;&#32032;&#26174;&#33879;&#24615;&#31561;&#29305;&#24449;&#24402;&#22240;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#23450;&#20041;&#35299;&#37322;&#20998;&#26512;&#30340;&#27010;&#24565;&#20250;&#21463;&#21040;&#29992;&#25143;&#23545;&#27010;&#24565;&#26399;&#26395;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#25581;&#31034;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#20998;&#35299;&#19968;&#20010;&#23618;&#30340;&#28508;&#22312;&#31354;&#38388;&#25104;&#22855;&#24322;&#21521;&#37327;&#65292;&#24182;&#36890;&#36807;&#26080;&#30417;&#30563;&#32858;&#31867;&#23545;&#20854;&#36827;&#34892;&#31934;&#28860;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#39640;&#26041;&#24046;&#26041;&#21521;&#19978;&#30340;&#27010;&#24565;&#21521;&#37327;&#65292;&#24182;&#25351;&#21521;&#35821;&#20041;&#19978;&#29420;&#29305;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#22823;&#37096;&#20998;&#27010;&#24565;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26131;&#20110;&#29702;&#35299;&#30340;&#65292;&#20855;&#26377;&#19968;&#33268;&#24615;&#65292;&#24182;&#19982;&#25152;&#38656;&#20219;&#21153;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.06908</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Generating Benchmarks for Factuality Evaluation of Language Models. (arXiv:2307.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06908
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FACTOR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20107;&#23454;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#19981;&#27491;&#30830;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#22312;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#26102;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#12290;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#24635;&#26159;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37096;&#32626;&#21040;&#29305;&#23450;&#39046;&#22495;&#20043;&#21069;&#65292;&#34913;&#37327;&#20854;&#22312;&#35813;&#39046;&#22495;&#20013;&#29983;&#25104;&#20107;&#23454;&#38169;&#35823;&#20449;&#24687;&#30340;&#20542;&#21521;&#24456;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#29983;&#25104;&#35780;&#20272;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;LM&#33258;&#36523;&#20013;&#37319;&#26679;&#30340;&#20107;&#23454;&#65292;&#22240;&#27492;&#26080;&#27861;&#25511;&#21046;&#35780;&#20272;&#20107;&#23454;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#33021;&#20302;&#20272;&#20102;&#32597;&#35265;&#21644;&#19981;&#22826;&#21487;&#33021;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FACTOR&#65306;&#36890;&#36807;&#35821;&#26009;&#24211;&#21464;&#25442;&#36827;&#34892;&#20107;&#23454;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;LM&#30340;&#20107;&#23454;&#24615;&#12290;FACTOR&#20250;&#33258;&#21160;&#23558;&#24863;&#20852;&#36259;&#30340;&#20107;&#23454;&#35821;&#26009;&#24211;&#36716;&#21270;&#20026;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;LM&#26681;&#25454;&#35821;&#26009;&#24211;&#29983;&#25104;&#30495;&#23454;&#20107;&#23454;&#30340;&#20542;&#21521;&#19982;&#29983;&#25104;&#31867;&#20284;&#20294;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#21019;&#24314;&#20102;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;Wiki-FACTOR&#21644;News-FACTOR&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#38543;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#24182;&#19988;&#24403;LM&#19982;&#26816;&#32034;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#24615;&#33021;&#24471;&#21040;&#25913;&#21892;&#65307;&#65288;ii&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#19982;&#22256;&#24785;&#24230;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#36825;&#20004;&#20010;&#25351;&#26631;&#22312;&#27169;&#22411;&#25490;&#24207;&#19978;&#24182;&#19981;&#24635;&#26159;&#19968;&#33268;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#24403;&#22256;&#24785;&#24230;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#20998;&#25968;&#26356;&#33021;&#20934;&#30830;&#21453;&#26144;LM&#30340;&#20107;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing factual generation evaluation methods focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent rare and unlikely facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create two benchmarks: Wiki-FACTOR and News-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score correlates with perplexity, but the two metrics do not always agree on model ranking; and (iii) when perplexity and benchmark score 
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20462;&#25913;&#27010;&#29575;&#25110;&#22870;&#21169;&#26102;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20445;&#25345;&#20540;&#20989;&#25968;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#24182;&#19988;&#36825;&#20010;&#25361;&#25112;&#19982;&#29366;&#24577;&#25968;&#30446;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.06877</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The complexity of non-stationary reinforcement learning. (arXiv:2307.06877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06877
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#24179;&#31283;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20462;&#25913;&#27010;&#29575;&#25110;&#22870;&#21169;&#26102;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#20445;&#25345;&#20540;&#20989;&#25968;&#30340;&#26368;&#26032;&#29366;&#24577;&#65292;&#24182;&#19988;&#36825;&#20010;&#25361;&#25112;&#19982;&#29366;&#24577;&#25968;&#30446;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#34987;&#35748;&#20026;&#26159;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#24688;&#22909;&#25429;&#25417;&#21040;&#20102;&#36825;&#20010;&#25361;&#25112;&#65306;&#20462;&#25913;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#19968;&#20010;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#27010;&#29575;&#25110;&#22870;&#21169;&#65292;&#38656;&#35201;&#33457;&#36153;&#20960;&#20046;&#19982;&#29366;&#24577;&#25968;&#30446;&#19968;&#26679;&#22810;&#30340;&#26102;&#38388;&#26469;&#21450;&#26102;&#26356;&#26032;&#20540;&#20989;&#25968;&#65292;&#38500;&#38750;&#24378;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;(SETH)&#26159;&#38169;&#35823;&#30340;&#65307;SETH&#26159;P&#8800;NP&#29468;&#24819;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#21152;&#24378;&#29256;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#29366;&#24577;&#25968;&#30446;&#36890;&#24120;&#26159;&#22825;&#25991;&#25968;&#23383;&#32423;&#21035;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20165;&#20165;"&#28155;&#21152;"&#19968;&#20010;&#26032;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of continual learning in the domain of reinforcement learning, often called non-stationary reinforcement learning, has been identified as an important challenge to the application of reinforcement learning. We prove a worst-case complexity result, which we believe captures this challenge: Modifying the probabilities or the reward of a single state-action pair in a reinforcement learning problem requires an amount of time almost as large as the number of states in order to keep the value function up to date, unless the strong exponential time hypothesis (SETH) is false; SETH is a widely accepted strengthening of the P $\neq$ NP conjecture. Recall that the number of states in current applications of reinforcement learning is typically astronomical. In contrast, we show that just $\textit{adding}$ a new state-action pair is considerably easier to implement.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.06870</link><description>&lt;p&gt;
&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Lifelong Learning for Task and Motion Planning. (arXiv:2307.06870v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23478;&#20013;&#38271;&#26399;&#37096;&#32626;&#30340;&#26426;&#22120;&#20154;&#25152;&#38754;&#20020;&#30340;&#20855;&#36523;&#21270;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#32972;&#26223;&#19979;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#38382;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#26469;&#20135;&#29983;&#35268;&#21010;&#22120;&#30340;&#20505;&#36873;&#21442;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#22312;&#32447;&#36873;&#25321;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#38271;&#26102;&#38388;&#20869;&#37096;&#32626;&#22312;&#23478;&#20013;&#30340;&#26426;&#22120;&#20154;&#38754;&#20020;&#30528;&#30495;&#27491;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#12290;&#20316;&#20026;&#26426;&#22120;&#20154;&#23547;&#27714;&#20026;&#29992;&#25143;&#25552;&#20379;&#24110;&#21161;&#65292;&#23427;&#24212;&#35813;&#21033;&#29992;&#20219;&#20309;&#31215;&#32047;&#30340;&#32463;&#39564;&#26469;&#25913;&#36827;&#33258;&#24049;&#30340;&#30693;&#35782;&#65292;&#25104;&#20026;&#19968;&#20010;&#26356;&#29087;&#32451;&#30340;&#21161;&#25163;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#36825;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#32456;&#36523;&#23398;&#20064;&#38382;&#39064;&#24314;&#27169;&#12290;&#21033;&#29992;TAMP&#31995;&#32479;&#30340;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#20026;&#35268;&#21010;&#22120;&#29983;&#25104;&#20505;&#36873;&#36830;&#32493;&#21442;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;&#39044;&#20808;&#30830;&#23450;&#25968;&#25454;&#22914;&#20309;&#22312;&#20219;&#21153;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20849;&#20139;&#21644;&#38750;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#20195;&#29702;&#20219;&#21153;&#26469;&#20915;&#23450;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#22312;&#32447;&#20351;&#29992;&#21738;&#20010;&#27169;&#22411;&#65292;&#36825;&#20123;&#20219;&#21153;&#20316;&#20026;&#27599;&#20010;&#27169;&#22411;&#23545;&#29366;&#24577;&#30340;&#29702;&#35299;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#30340;2D&#39046;&#22495;&#21644;BEHAVIOR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#35268;&#21010;&#25104;&#21151;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot deployed in a home over long stretches of time faces a true lifelong learning problem. As it seeks to provide assistance to its users, the robot should leverage any accumulated experience to improve its own knowledge to become a more proficient assistant. We formalize this setting with a novel lifelong learning problem formulation in the context of learning for task and motion planning (TAMP). Exploiting the modularity of TAMP systems, we develop a generative mixture model that produces candidate continuous parameters for a planner. Whereas most existing lifelong learning approaches determine a priori how data is shared across task models, our approach learns shared and non-shared models and determines which to use online during planning based on auxiliary tasks that serve as a proxy for each model's understanding of a state. Our method exhibits substantial improvements in planning success on simulated 2D domains and on several problems from the BEHAVIOR benchmark.
&lt;/p&gt;</description></item><item><title>DecompEval&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#23558;&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34913;&#37327;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06869</link><description>&lt;p&gt;
DecompEval&#65306;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#20316;&#20026;&#26080;&#30417;&#30563;&#20998;&#35299;&#38382;&#31572;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering. (arXiv:2307.06869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06869
&lt;/p&gt;
&lt;p&gt;
DecompEval&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#23558;&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34913;&#37327;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#35780;&#20272;&#25351;&#26631;&#38754;&#20020;&#30528;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22823;&#22810;&#25968;&#34920;&#29616;&#33391;&#22909;&#30340;&#25351;&#26631;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;NLG&#20219;&#21153;&#21644;&#35780;&#20272;&#32500;&#24230;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#20165;&#25552;&#20379;&#27599;&#20010;&#32500;&#24230;&#30340;&#35780;&#20272;&#20998;&#25968;&#65292;&#32780;&#19981;&#25581;&#31034;&#22914;&#20309;&#33719;&#24471;&#35813;&#20998;&#25968;&#30340;&#35777;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25351;&#26631;&#31216;&#20026;DecompEval&#12290;&#36825;&#20010;&#25351;&#26631;&#23558;NLG&#35780;&#20272;&#24314;&#27169;&#20026;&#19968;&#31181;&#31867;&#20284;&#25351;&#20196;&#30340;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#32780;&#19981;&#26159;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26088;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20351;&#35780;&#20272;&#36807;&#31243;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23558;&#20851;&#20110;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#35774;&#35745;&#25351;&#20196;&#24335;&#38382;&#39064;&#20998;&#35299;&#20026;&#34913;&#37327;&#23376;&#38382;&#39064;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06865</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#24212;&#34987;&#35270;&#20026;&#31192;&#23494;&#65306;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#26469;&#25511;&#21046;&#65292;&#20854;&#20013;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#20197;&#26088;&#22312;&#25351;&#23548;&#27169;&#22411;&#22312;&#35813;&#26597;&#35810;&#19978;&#30340;&#34892;&#20026;&#30340;&#25552;&#31034;&#20316;&#20026;&#21069;&#32512;&#12290;&#20844;&#21496;&#29992;&#20110;&#25351;&#23548;&#20854;&#27169;&#22411;&#30340;&#25552;&#31034;&#36890;&#24120;&#34987;&#35270;&#20026;&#31192;&#23494;&#65292;&#38544;&#34255;&#22312;&#26597;&#35810;&#30340;&#29992;&#25143;&#20043;&#22806;&#12290;&#23427;&#20204;&#29978;&#33267;&#34987;&#35270;&#20026;&#21487;&#20197;&#20080;&#21334;&#30340;&#21830;&#21697;&#12290;&#28982;&#32780;&#65292;&#26377;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#26174;&#31034;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#29992;&#25143;&#20173;&#28982;&#21487;&#20197;&#25552;&#21462;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#28304;&#21644;&#22810;&#20010;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#23454;&#38469;&#19978;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21644;&#36719;&#35745;&#31639;&#30340;&#26234;&#33021;&#35748;&#30693;&#25511;&#21046;&#31995;&#32479;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21462;&#37327;&#23376;&#33258;&#32452;&#32455;&#30693;&#35782;&#24211;&#65292;&#25913;&#21892;&#20102;&#22312;&#21361;&#38505;&#25511;&#21046;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#37327;&#23376;&#27169;&#31946;&#25512;&#29702;&#38376;&#35774;&#35745;&#22312;&#23884;&#20837;&#24335;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06858</link><description>&lt;p&gt;
&#38750;&#20256;&#32479;&#30340;&#35748;&#30693;&#26234;&#33021;&#26426;&#22120;&#20154;&#25511;&#21046;&#65306;&#20154;&#31867;&#24773;&#24863;&#20272;&#35745;&#20013;&#30340;&#37327;&#23376;&#36719;&#35745;&#31639;&#26041;&#27861;-- QCOptKB&#24037;&#20855;&#21253;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unconventional Cognitive Intelligent Robotic Control: Quantum Soft Computing Approach in Human Being Emotion Estimation -- QCOptKB Toolkit Application. (arXiv:2307.06858v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#21644;&#36719;&#35745;&#31639;&#30340;&#26234;&#33021;&#35748;&#30693;&#25511;&#21046;&#31995;&#32479;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#21462;&#37327;&#23376;&#33258;&#32452;&#32455;&#30693;&#35782;&#24211;&#65292;&#25913;&#21892;&#20102;&#22312;&#21361;&#38505;&#25511;&#21046;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#37327;&#23376;&#27169;&#31946;&#25512;&#29702;&#38376;&#35774;&#35745;&#22312;&#23884;&#20837;&#24335;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#37327;&#23376;&#21644;&#36719;&#35745;&#31639;&#30340;&#26234;&#33021;&#35748;&#30693;&#25511;&#21046;&#31995;&#32479;&#30340;&#31574;&#30053;&#12290;&#25551;&#36848;&#20102;&#20174;&#26234;&#33021;&#27169;&#31946;&#25511;&#21046;&#22120;&#30340;&#19981;&#23436;&#32654;&#30693;&#35782;&#24211;&#20013;&#25552;&#21462;&#37327;&#23376;&#33258;&#32452;&#32455;&#30693;&#35782;&#24211;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#35813;&#25216;&#26415;&#25913;&#21892;&#20102;&#22312;&#21361;&#38505;&#25511;&#21046;&#24773;&#20917;&#19979;&#26234;&#33021;&#35748;&#30693;&#25511;&#21046;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;&#35748;&#30693;&#31070;&#32463;&#25509;&#21475;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#26426;&#22120;&#20154;&#21512;&#20316;&#12290;&#36890;&#36807;&#31034;&#20363;&#23637;&#31034;&#20102;&#24341;&#20837;&#37327;&#23376;&#27169;&#31946;&#25512;&#29702;&#38376;&#35774;&#35745;&#20316;&#20026;&#23884;&#20837;&#24335;&#25511;&#21046;&#31995;&#32479;&#30340;&#21487;&#32534;&#31243;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#23637;&#31034;&#20102;&#22522;&#20110;&#35748;&#30693;&#22836;&#30420;&#21644;&#37327;&#23376;&#27169;&#31946;&#25511;&#21046;&#22120;&#30340;&#31070;&#32463;&#25509;&#21475;&#24212;&#29992;&#20110;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategy of intelligent cognitive control systems based on quantum and soft computing presented. Quantum self-organization knowledge base synergetic effect extracted from intelligent fuzzy controllers imperfect knowledge bases described. That technology improved of robustness of intelligent cognitive control systems in hazard control situations described with the cognitive neuro-interface and different types of robot cooperation. Examples demonstrated the introduction of quantum fuzzy inference gate design as prepared programmable algorithmic solution for board embedded control systems. The possibility of neuro-interface application based on cognitive helmet with quantum fuzzy controller for driving of the vehicle is shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24863;&#30693;&#25163;&#26415;&#32447;&#65292;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36319;&#36394;&#32447;&#30340;&#36335;&#24452;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#20027;&#36827;&#34892;&#25163;&#26415;&#32541;&#21512; "&#23614;&#37096;&#32553;&#30701;" &#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20108;&#32500;&#32447;&#26816;&#27979;&#21644;&#19977;&#32500;&#32447;&#37325;&#24314;&#23454;&#29616;&#20102;&#23545;&#32447;&#30340;&#20934;&#30830;&#24863;&#30693;&#21644;&#36319;&#36394;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06845</link><description>&lt;p&gt;
&#33258;&#20027;&#32541;&#21512;&#26415;&#23614;&#37096;&#32553;&#30701;&#30340;&#25163;&#26415;&#32447;&#20114;&#21160;&#24863;&#30693;&#30340;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Interactive Perception of Surgical Thread for Autonomous Suture Tail-Shortening. (arXiv:2307.06845v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24863;&#30693;&#25163;&#26415;&#32447;&#65292;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#36319;&#36394;&#32447;&#30340;&#36335;&#24452;&#65292;&#24182;&#24212;&#29992;&#20110;&#33258;&#20027;&#36827;&#34892;&#25163;&#26415;&#32541;&#21512; "&#23614;&#37096;&#32553;&#30701;" &#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20108;&#32500;&#32447;&#26816;&#27979;&#21644;&#19977;&#32500;&#32447;&#37325;&#24314;&#23454;&#29616;&#20102;&#23545;&#32447;&#30340;&#20934;&#30830;&#24863;&#30693;&#21644;&#36319;&#36394;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#25163;&#26415;&#32541;&#21512;&#20013;&#65292;&#20934;&#30830;&#24863;&#30693;&#32541;&#21512;&#32447;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#32447;&#30340;&#29366;&#24577;&#31354;&#38388;&#22797;&#26434;&#65292;&#32447;&#30340;&#34180;&#24230;&#21644;&#21487;&#21464;&#24418;&#24615;&#20197;&#21450;&#22841;&#20855;&#21644;&#32452;&#32455;&#30340;&#21487;&#33021;&#36974;&#25377;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#36319;&#36394;&#25163;&#26415;&#32447;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#36974;&#25377;&#21644;&#22797;&#26434;&#30340;&#32447;&#37197;&#32622;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#25163;&#26415;&#32541;&#21512; "&#23614;&#37096;&#32553;&#30701;" &#20219;&#21153;&#65306;&#23558;&#32447;&#25289;&#36890;&#36807;&#32452;&#32455;&#30452;&#21040;&#25152;&#38656;&#30340; "&#23614;&#37096;" &#38271;&#24230;&#20445;&#25345;&#26292;&#38706;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#20108;&#32500;&#25163;&#26415;&#32447;&#26816;&#27979;&#32593;&#32476;&#65292;&#22312;RGB&#22270;&#20687;&#20013;&#20998;&#21106;&#32541;&#21512;&#32447;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#20108;&#32500;&#20013;&#35782;&#21035;&#32447;&#30340;&#36335;&#24452;&#65292;&#24182;&#36890;&#36807;&#20174;&#20004;&#20010;&#31435;&#20307;&#30456;&#26426;&#26816;&#27979;&#36827;&#34892;&#19977;&#35282;&#21270;&#65292;&#23558;&#32447;&#37325;&#24314;&#20026;&#19977;&#32500;&#30340;&#38750;&#22343;&#21248;&#26377;&#29702;B&#26679;&#26465;&#26354;&#32447;&#12290;&#19968;&#26086;&#21021;&#22987;&#21270;&#20102;&#19977;&#32500;&#32447;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#20250;&#22312;&#38543;&#21518;&#30340;&#24103;&#20013;&#36319;&#36394;&#32447;&#30340;&#31227;&#21160;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21333;&#24103;&#19977;&#32500;&#32447;&#19978;&#36798;&#21040;1.33&#20687;&#32032;&#30340;&#24179;&#22343;&#37325;&#25237;&#24433;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D sensing of suturing thread is a challenging problem in automated surgical suturing because of the high state-space complexity, thinness and deformability of the thread, and possibility of occlusion by the grippers and tissue. In this work we present a method for tracking surgical thread in 3D which is robust to occlusions and complex thread configurations, and apply it to autonomously perform the surgical suture "tail-shortening" task: pulling thread through tissue until a desired "tail" length remains exposed. The method utilizes a learned 2D surgical thread detection network to segment suturing thread in RGB images. It then identifies the thread path in 2D and reconstructs the thread in 3D as a NURBS spline by triangulating the detections from two stereo cameras. Once a 3D thread model is initialized, the method tracks the thread across subsequent frames. Experiments suggest the method achieves a 1.33 pixel average reprojection error on challenging single-frame 3D thread 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#29359;&#32618;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#36136;&#37327;&#19981;&#36275;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.06844</link><description>&lt;p&gt;
&#22403;&#22334;&#36827;&#65292;&#22403;&#22334;&#20986;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#29359;&#32618;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Garbage in, garbage out: Zero-shot detection of crime using Large Language Models. (arXiv:2307.06844v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#29359;&#32618;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#36136;&#37327;&#19981;&#36275;&#20197;&#25903;&#25345;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#22312;&#32473;&#23450;&#30417;&#25511;&#35270;&#39057;&#30340;&#25991;&#26412;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20851;&#20110;&#29359;&#32618;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23558;&#35270;&#39057;&#25163;&#21160;&#36716;&#25442;&#20026;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25551;&#36848;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20165;&#36890;&#36807;&#38646;&#26679;&#26412;&#25512;&#29702;&#23454;&#29616;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#29359;&#32618;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#35270;&#39057;&#36716;&#25991;&#26412;&#26041;&#27861;&#26080;&#27861;&#29983;&#25104;&#36275;&#22815;&#36136;&#37327;&#30340;&#35270;&#39057;&#25551;&#36848;&#26469;&#25903;&#25345;&#25512;&#29702;&#65288;&#22403;&#22334;&#35270;&#39057;&#25551;&#36848;&#36827;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21518;&#65292;&#20135;&#29983;&#30340;&#32467;&#26524;&#20063;&#26159;&#22403;&#22334;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes exploiting the common sense knowledge learned by large language models to perform zero-shot reasoning about crimes given textual descriptions of surveillance videos. We show that when video is (manually) converted to high quality textual descriptions, large language models are capable of detecting and classifying crimes with state-of-the-art performance using only zero-shot reasoning. However, existing automated video-to-text approaches are unable to generate video descriptions of sufficient quality to support reasoning (garbage video descriptions into the large language model, garbage out).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#24605;&#24819;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#26041;&#27861;&#20851;&#31995;&#31561;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#21457;&#23637;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.06825</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Causal Framework to Unify Common Domain Generalization Approaches. (arXiv:2307.06825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#32479;&#19968;&#24120;&#35265;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#24605;&#24819;&#12289;&#29702;&#35770;&#22522;&#30784;&#21644;&#26041;&#27861;&#20851;&#31995;&#31561;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#21457;&#23637;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#25351;&#23398;&#20064;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#39046;&#22495;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#26032;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#23427;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#24050;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#19981;&#21516;&#30340;&#26041;&#27861;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#20351;&#24471;&#24456;&#38590;&#23545;&#36825;&#20010;&#39046;&#22495;&#26377;&#19968;&#20010;&#25972;&#20307;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#24182;&#22312;&#26694;&#26550;&#20013;&#23545;&#24120;&#35265;&#30340;DG&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20197;&#19979;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65306;(1)&#27599;&#20010;DG&#26041;&#27861;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20160;&#20040;&#65311;(2)&#29702;&#35770;&#19978;&#20026;&#20160;&#20040;&#26399;&#26395;&#23427;&#25913;&#21892;&#23545;&#26032;&#39046;&#22495;&#30340;&#25512;&#24191;&#33021;&#21147;&#65311;(3)&#19981;&#21516;&#30340;DG&#26041;&#27861;&#20043;&#38388;&#22914;&#20309;&#30456;&#20851;&#65292;&#26377;&#20160;&#20040;&#30456;&#23545;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65311;&#36890;&#36807;&#25552;&#20379;&#23545;DG&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20013;&#30340;&#21407;&#21017;&#24182;&#24320;&#21457;&#20986;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is about learning models that generalize well to new domains that are related to, but different from, the training domain(s). It is a fundamental problem in machine learning and has attracted much attention in recent years. A large number of approaches have been proposed. Different approaches are motivated from different perspectives, making it difficult to gain an overall understanding of the area. In this paper, we propose a causal framework for domain generalization and present an understanding of common DG approaches in the framework. Our work sheds new lights on the following questions: (1) What are the key ideas behind each DG method? (2) Why is it expected to improve generalization to new domains theoretically? (3) How are different DG methods related to each other and what are relative advantages and limitations? By providing a unified perspective on DG, we hope to help researchers better understand the underlying principles and develop more effective
&lt;/p&gt;</description></item><item><title>CLAIMED&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06824</link><description>&lt;p&gt;
CLAIMED -- &#22312;&#31185;&#23398;&#21152;&#36895;&#21457;&#29616;&#20013;&#26500;&#24314;&#31895;&#31890;&#24230;&#36816;&#31639;&#31526;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLAIMED -- the open source framework for building coarse-grained operators for accelerated discovery in science. (arXiv:2307.06824v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06824
&lt;/p&gt;
&lt;p&gt;
CLAIMED&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#65292;&#37325;&#22797;&#24615;&#21644;&#37325;&#29992;&#24615;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#31185;&#23398;&#23478;&#22312;&#20174;&#25968;&#25454;&#21040;&#20986;&#29256;&#30340;&#36807;&#31243;&#20013;&#26377;&#30528;&#20016;&#23500;&#30340;&#32463;&#39564;&#12290;&#23613;&#31649;&#19968;&#20123;&#20986;&#29256;&#28192;&#36947;&#35201;&#27714;&#28304;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#33719;&#24471;&#65292;&#20294;&#37325;&#26032;&#36816;&#34892;&#21644;&#39564;&#35777;&#23454;&#39564;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#26631;&#20934;&#12290;&#22240;&#27492;&#65292;&#37325;&#29992;&#29616;&#26377;&#30340;&#31185;&#23398;&#25968;&#25454;&#22788;&#29702;&#20195;&#30721;&#26469;&#33258;&#26368;&#26032;&#30740;&#31350;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#25105;&#20204;&#24341;&#20837;CLAIMED&#65292;&#23427;&#22312;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#20013;&#35299;&#20915;&#20102;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#38382;&#39064;&#30340;&#31185;&#23398;&#30740;&#31350;&#23454;&#36341;&#12290;CLAIMED&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#25903;&#25345;&#31185;&#23398;&#23478;&#20174;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#31185;&#23398;&#36816;&#31639;&#31526;&#24211;&#20013;&#37325;&#26032;&#32452;&#21512;&#24037;&#20316;&#27969;&#31243;&#65292;&#26469;&#26500;&#24314;&#21487;&#37325;&#29992;&#30340;&#36816;&#31639;&#31526;&#21644;&#21487;&#25193;&#23637;&#30340;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#23454;&#29616;&#65292;&#20294;CLAIMED&#26159;&#32534;&#31243;&#35821;&#35328;&#12289;&#31185;&#23398;&#24211;&#21644;&#25191;&#34892;&#29615;&#22659;&#26080;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern data-driven science, reproducibility and reusability are key challenges. Scientists are well skilled in the process from data to publication. Although some publication channels require source code and data to be made accessible, rerunning and verifying experiments is usually hard due to a lack of standards. Therefore, reusing existing scientific data processing code from state-of-the-art research is hard as well. This is why we introduce CLAIMED, which has a proven track record in scientific research for addressing the repeatability and reusability issues in modern data-driven science. CLAIMED is a framework to build reusable operators and scalable scientific workflows by supporting the scientist to draw from previous work by re-composing workflows from existing libraries of coarse-grained scientific operators. Although various implementations exist, CLAIMED is programming language, scientific library, and execution environment agnostic.
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#20250;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;GPT-3&#65292;&#24182;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06794</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21542;&#23450;&#24335;&#20114;&#34917;&#24120;&#35782;
&lt;/p&gt;
&lt;p&gt;
Negated Complementary Commonsense using Large Language Models. (arXiv:2307.06794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#36825;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#20250;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;GPT-3&#65292;&#24182;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#38750;&#24120;&#35268;&#38382;&#39064;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#22833;&#21435;&#35686;&#35273;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#22312;&#24120;&#35782;&#24773;&#26223;&#20013;&#23547;&#25214;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#36825;&#31867;&#38382;&#39064;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20174;GPT-3&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#65288;&#36229;&#36807;11&#20010;&#28857;&#65289;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24378;&#35843;&#20102;&#30740;&#31350;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21542;&#23450;&#24335;&#20114;&#34917;&#38382;&#39064;&#20013;&#30340;&#21709;&#24212;&#30340;&#37325;&#35201;&#24615;&#12290;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#23454;&#39564;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://github.com/navidre/negated_complementary_commonsense&#12290;
&lt;/p&gt;
&lt;p&gt;
Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25511;&#21046;&#21512;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#21160;&#24577;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.06758</link><description>&lt;p&gt;
&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#23618;&#25511;&#21046;&#21512;&#25104;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Layered controller synthesis for dynamic multi-agent systems. (arXiv:2307.06758v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25511;&#21046;&#21512;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#21160;&#24577;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#24314;&#31435;&#22312;&#21069;&#19968;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#35745;&#31639;&#31995;&#32479;&#30340;&#31895;&#30053;&#25277;&#35937;&#30340;&#39640;&#32423;&#35745;&#21010;&#65292;&#20351;&#29992;&#22686;&#21152;&#20102;&#31186;&#34920;&#30340;&#21442;&#25968;&#21270;&#23450;&#26102;&#33258;&#21160;&#26426;&#26469;&#26377;&#25928;&#22320;&#27169;&#25311;&#36825;&#31181;&#31995;&#32479;&#30340;&#31616;&#21270;&#21160;&#21147;&#23398;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#22522;&#20110;SMT&#34920;&#36848;&#30340;&#39640;&#32423;&#35745;&#21010;&#20027;&#35201;&#22788;&#29702;&#38382;&#39064;&#30340;&#32452;&#21512;&#26041;&#38754;&#65292;&#25552;&#20379;&#20102;&#26356;&#21160;&#24577;&#20934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#38454;&#27573;&#32479;&#31216;&#20026;SWA-SMT&#27714;&#35299;&#22120;&#65292;&#23427;&#20204;&#26159;&#26500;&#36896;&#27491;&#30830;&#30340;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65306;&#19981;&#33021;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;SWA-SMT&#30340;&#35299;&#20316;&#20026;&#26368;&#21518;&#19968;&#20010;&#38454;&#27573;&#30340;&#21021;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#35813;&#38454;&#27573;&#26088;&#22312;&#33719;&#24471;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#19988;&#35777;&#26126;&#21021;&#22987;&#25968;&#25454;&#38598;&#23545;&#20110;&#25972;&#20010;&#26041;&#27861;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a layered approach for multi-agent control problem, decomposed into three stages, each building upon the results of the previous one. First, a high-level plan for a coarse abstraction of the system is computed, relying on parametric timed automata augmented with stopwatches as they allow to efficiently model simplified dynamics of such systems. In the second stage, the high-level plan, based on SMT-formulation, mainly handles the combinatorial aspects of the problem, provides a more dynamically accurate solution. These stages are collectively referred to as the SWA-SMT solver. They are correct by construction but lack a crucial feature: they cannot be executed in real time. To overcome this, we use SWA-SMT solutions as the initial training dataset for our last stage, which aims at obtaining a neural network control policy. We use reinforcement learning to train the policy, and show that the initial dataset is crucial for the overall success of the method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06742</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling Services: A Multi-Agent Hierarchical Reinforcement Learning Approach. (arXiv:2307.06742v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#20195;&#29702;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#21363;&#26102;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#30340;&#36710;&#36742;&#35843;&#24230;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#35813;&#26694;&#26550;&#26377;&#25928;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#32676;&#19968;&#20307;&#21270;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23545;&#22478;&#38469;&#26053;&#34892;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#12290;&#22478;&#38469;&#25340;&#36710;&#26381;&#21153;&#36890;&#36807;&#23454;&#26045;&#38656;&#27714;&#21709;&#24212;&#24615;&#22686;&#24378;&#25514;&#26045;&#65292;&#26377;&#26395;&#21319;&#32423;&#20256;&#32479;&#30340;&#22478;&#38469;&#23458;&#36710;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#32447;&#25805;&#20316;&#21463;&#21040;&#36710;&#36742;&#36164;&#28304;&#20998;&#37197;&#21644;&#25340;&#36710;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#20043;&#38388;&#32806;&#21512;&#24615;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#23618;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22312;&#32447;&#36710;&#38431;&#31649;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#26694;&#26550;&#30340;&#19978;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#23553;&#24314;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21327;&#21516;&#20998;&#37197;&#38386;&#32622;&#36710;&#36742;&#21040;&#19981;&#21516;&#30340;&#22478;&#38469;&#32447;&#36335;&#65292;&#32780;&#22312;&#19979;&#23618;&#65292;&#21017;&#20351;&#29992;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#26356;&#26032;&#36710;&#36742;&#30340;&#36335;&#32447;&#12290;&#22522;&#20110;&#20013;&#22269;&#21414;&#38376;&#21450;&#20854;&#21608;&#36793;&#22478;&#24066;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20379;&#32473;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integrated development of city clusters has given rise to an increasing demand for intercity travel. Intercity ride-pooling service exhibits considerable potential in upgrading traditional intercity bus services by implementing demand-responsive enhancements. Nevertheless, its online operations suffer the inherent complexities due to the coupling of vehicle resource allocation among cities and pooled-ride vehicle routing. To tackle these challenges, this study proposes a two-level framework designed to facilitate online fleet management. Specifically, a novel multi-agent feudal reinforcement learning model is proposed at the upper level of the framework to cooperatively assign idle vehicles to different intercity lines, while the lower level updates the routes of vehicles using an adaptive large neighborhood search heuristic. Numerical studies based on the realistic dataset of Xiamen and its surrounding cities in China show that the proposed framework effectively mitigates the supp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPR-Net&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.06736</link><description>&lt;p&gt;
MPR-Net: &#22810;&#23610;&#24230;&#27169;&#24335;&#37325;&#29616;&#24341;&#23548;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#21487;&#35299;&#37322;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MPR-Net:Multi-Scale Pattern Reproduction Guided Universality Time Series Interpretable Forecasting. (arXiv:2307.06736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MPR-Net&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#24182;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#22266;&#26377;&#30340;&#25361;&#25112;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#30340;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#21382;&#21490;&#24207;&#21015;&#20013;&#30340;&#26377;&#25928;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#39044;&#27979;&#12290;&#22522;&#20110;&#28857;&#23545;&#28857;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;Transformer&#26550;&#26500;&#30340;&#20808;&#36827;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#65292;&#20294;&#20854;&#36741;&#21161;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26500;&#22266;&#26377;&#22320;&#30772;&#22351;&#20102;&#26102;&#38388;&#39034;&#24207;&#65292;&#38477;&#20302;&#20102;&#20449;&#24687;&#21033;&#29992;&#29575;&#65292;&#24182;&#20351;&#39044;&#27979;&#36807;&#31243;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#27169;&#22411;MPR-Net&#12290;&#23427;&#39318;&#20808;&#20351;&#29992;&#21367;&#31215;&#36816;&#31639;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#22810;&#23610;&#24230;&#21382;&#21490;&#24207;&#21015;&#27169;&#24335;&#65292;&#28982;&#21518;&#22522;&#20110;&#27169;&#24335;&#37325;&#29616;&#30340;&#20808;&#39564;&#30693;&#35782;&#26500;&#24314;&#19968;&#31181;&#27169;&#24335;&#25193;&#23637;&#39044;&#27979;&#26041;&#27861;&#65292;&#26368;&#21518;&#20351;&#29992;&#21453;&#21367;&#31215;&#36816;&#31639;&#23558;&#26410;&#26469;&#27169;&#24335;&#37325;&#24314;&#20026;&#26410;&#26469;&#24207;&#21015;&#12290;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;MPR-Net&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#37325;&#35201;&#27169;&#24335;&#24182;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting has received wide interest from existing research due to its broad applications and inherent challenging. The research challenge lies in identifying effective patterns in historical series and applying them to future forecasting. Advanced models based on point-wise connected MLP and Transformer architectures have strong fitting power, but their secondary computational complexity limits practicality. Additionally, those structures inherently disrupt the temporal order, reducing the information utilization and making the forecasting process uninterpretable. To solve these problems, this paper proposes a forecasting model, MPR-Net. It first adaptively decomposes multi-scale historical series patterns using convolution operation, then constructs a pattern extension forecasting method based on the prior knowledge of pattern reproduction, and finally reconstructs future patterns into future series using deconvolution operation. By leveraging the temporal dependencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;GRAN&#20248;&#20110;GraphRNN&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#23567;&#22411;&#22270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;GraphRNN&#25913;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2307.06709</link><description>&lt;p&gt;
GRAN&#20248;&#20110;GraphRNN&#65306;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#29992;&#20110;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
GRAN is superior to GraphRNN: node orderings, kernel- and graph embeddings-based metrics for graph generators. (arXiv:2307.06709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#33410;&#28857;&#25490;&#24207;&#12289;&#26680;&#20989;&#25968;&#21644;&#22270;&#23884;&#20837;&#30340;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;GRAN&#20248;&#20110;GraphRNN&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#23567;&#22411;&#22270;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;GraphRNN&#25913;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#34987;&#24191;&#27867;&#25552;&#20986;&#65292;&#24182;&#22312;&#33647;&#29289;&#21457;&#29616;&#12289;&#36947;&#36335;&#32593;&#32476;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#31243;&#24207;&#32508;&#21512;&#31561;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22270;&#38754;&#20020;&#30528;&#29702;&#35770;&#19978;&#30340;&#25361;&#25112;&#65292;&#22914;&#21516;&#26500;&#34920;&#31034;&#65292;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22256;&#38590;&#12290;&#22312;&#24212;&#29992;&#39046;&#22495;&#20013;&#65292;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#27169;&#22411;&#65311;&#25105;&#20204;&#23545;&#20998;&#24067;&#22270;&#19981;&#21464;&#37327;&#30340;&#26680;&#20989;&#25968;&#24230;&#37327;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#22312;&#22270;&#23884;&#20837;&#31354;&#38388;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#24418;&#21644;&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#24230;&#37327;&#12290;&#22522;&#20110;&#27969;&#24418;&#30340;&#24230;&#37327;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#26469;&#27604;&#36739;&#20004;&#20010;&#33879;&#21517;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;GraphRNN&#21644;GRAN&#65292;&#24182;&#25581;&#31034;&#20102;&#33410;&#28857;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;GRAN&#20248;&#20110;GraphRNN&#65292;&#32780;&#19988;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#25490;&#24207;&#30340;GraphRNN&#36866;&#29992;&#20110;&#23567;&#22411;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25968;&#25454;&#38598;&#36873;&#25321;&#21644;&#33410;&#28857;&#29305;&#24449;&#21021;&#22987;&#21270;&#30340;&#26368;&#20339;&#23454;&#36341;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of generative models for graphs have been proposed. They are used in drug discovery, road networks, neural architecture search, and program synthesis. Generating graphs has theoretical challenges, such as isomorphic representations -- evaluating how well a generative model performs is difficult. Which model to choose depending on the application domain?  We extensively study kernel-based metrics on distributions of graph invariants and manifold-based and kernel-based metrics in graph embedding space. Manifold-based metrics outperform kernel-based metrics in embedding space. We use these metrics to compare GraphRNN and GRAN, two well-known generative models for graphs, and unveil the influence of node orderings. It shows the superiority of GRAN over GraphRNN - further, our proposed adaptation of GraphRNN with a depth-first search ordering is effective for small-sized graphs.  A guideline on good practices regarding dataset selection and node feature initialization is prov
&lt;/p&gt;</description></item><item><title>S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06701</link><description>&lt;p&gt;
S-HR-VQVAE: &#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06701
&lt;/p&gt;
&lt;p&gt;
S-HR-VQVAE&#26159;&#19968;&#31181;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32467;&#21512;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#21644;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#22312;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#23558;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#20998;&#23618;&#27531;&#24046;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HR-VQVAE&#65289;&#19982;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;PixelCNN&#65288;ST-PixelCNN&#65289;&#30456;&#32467;&#21512;&#65292;&#29992;&#26469;&#35299;&#20915;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24207;&#21015;&#20998;&#23618;&#27531;&#24046;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;S-HR-VQVAE&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;HR-VQVAE&#22312;&#23545;&#38745;&#27490;&#22270;&#20687;&#36827;&#34892;&#24314;&#27169;&#26102;&#30340;&#20869;&#22312;&#33021;&#21147;&#21644;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#21450;ST-PixelCNN&#22788;&#29702;&#26102;&#31354;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292; S-HR-VQVAE&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#23545;&#35270;&#39057;&#39044;&#27979;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23398;&#20064;&#26102;&#31354;&#20449;&#24687;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#28040;&#38500;&#27169;&#31946;&#39044;&#27979;&#21644;&#38544;&#24335;&#24314;&#27169;&#29289;&#29702;&#29305;&#24615;&#12290;&#23545;KTH&#20154;&#20307;&#21160;&#20316;&#21644;Moving-MNIST&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#38754;&#19982;&#39030;&#32423;&#35270;&#39057;&#39044;&#27979;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the video prediction task by putting forth a novel model that combines (i) our recently proposed hierarchical residual vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN (ST-PixelCNN). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combined with the ST-PixelCNN's ability at handling spatiotemporal information, S-HR-VQVAE can better deal with chief challenges in video prediction. These include learning spatiotemporal information, handling high dimensional data, combating blurry prediction, and implicit modeling of physical characteristics. Extensive experimental results on the KTH Human Action and Moving-MNIST tasks demonstrate that our model compares favorably against top video prediction techniques both in quantitative and qualitative evalu
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#23454;&#29616;&#30340;&#26234;&#33021;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#25216;&#26415;&#65292;&#22312;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.06687</link><description>&lt;p&gt;
&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#25361;&#25112;&#12289;&#26041;&#27861;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and Opportunities. (arXiv:2307.06687v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06687
&lt;/p&gt;
&lt;p&gt;
&#36208;&#21521;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#23454;&#29616;&#30340;&#26234;&#33021;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#25216;&#26415;&#65292;&#22312;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#34987;&#30740;&#31350;&#29992;&#26469;&#38761;&#26032;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#21644;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29992;&#25143;&#30340;&#27785;&#28024;&#24335;&#32593;&#32476;&#34394;&#25311;&#20307;&#39564;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#35821;&#20041;&#29702;&#35299;&#19982;&#34920;&#31034;&#26469;&#23454;&#29616;&#22312;&#28151;&#21512;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#26080;&#32541;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#20013;&#22235;&#20010;&#22522;&#26412;&#31995;&#32479;&#32452;&#20214;&#30340;&#26234;&#33021;&#21644;&#26102;&#31354;&#29305;&#24449;&#65292;&#21363;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26102;&#31354;&#25968;&#25454;&#34920;&#31034;&#65288;STDR&#65289;&#12289;&#35821;&#20041;&#29289;&#32852;&#32593;&#65288;SIoT&#65289;&#21644;&#35821;&#20041;&#22686;&#24378;&#25968;&#23383;&#23402;&#29983;&#65288;SDT&#65289;&#12290;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#36825;&#22235;&#20010;&#22522;&#26412;&#31995;&#32479;&#32452;&#20214;&#30340;&#20856;&#22411;&#25216;&#26415;&#65292;&#20351;&#24471;&#22312;&#26222;&#36941;&#30340;&#35821;&#20041;&#20803;&#23431;&#23449;&#20013;&#33021;&#22815;&#36827;&#34892;&#26234;&#33021;&#12289;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20132;&#20114;&#65292;&#24182;&#32473;&#20986;&#20102;&#20856;&#22411;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#22914;&#36828;&#31243;&#25945;&#32946;&#12289;&#24037;&#20316;&#19982;&#21327;&#20316;&#12289;&#23089;&#20048;&#19982;&#31038;&#20132;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#33829;&#38144;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, ubiquitous semantic Metaverse has been studied to revolutionize immersive cyber-virtual experiences for augmented reality (AR) and virtual reality (VR) users, which leverages advanced semantic understanding and representation to enable seamless, context-aware interactions within mixed-reality environments. This survey focuses on the intelligence and spatio-temporal characteristics of four fundamental system components in ubiquitous semantic Metaverse, i.e., artificial intelligence (AI), spatio-temporal data representation (STDR), semantic Internet of Things (SIoT), and semantic-enhanced digital twin (SDT). We thoroughly survey the representative techniques of the four fundamental system components that enable intelligent, personalized, and context-aware interactions with typical use cases of the ubiquitous semantic Metaverse, such as remote education, work and collaboration, entertainment and socialization, healthcare, and e-commerce marketing. Furthermore, we outline 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#33258;&#30417;&#30563;&#22320;&#38663;&#21435;&#22122;&#65292;&#36890;&#36807;&#35266;&#23519;&#21435;&#22122;&#32593;&#32476;&#20013;&#30340;&#40657;&#30418;&#23376;&#65292;&#26367;&#20195;&#23545;&#22122;&#22768;&#32479;&#35745;&#30340;&#20808;&#39564;&#30693;&#35782;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#24179;&#22343;&#21270;&#36755;&#20837;&#20687;&#32032;&#30340;&#38597;&#21487;&#27604;&#36129;&#29486;&#65292;&#25552;&#20379;&#25233;&#21046;&#22320;&#38663;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#26368;&#26377;&#25928;&#33945;&#29256;&#12290;</title><link>http://arxiv.org/abs/2307.06682</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#25513;&#34109;&#35774;&#35745;&#36827;&#34892;&#33258;&#30417;&#30563;&#22320;&#38663;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence driven mask design for self-supervised seismic denoising. (arXiv:2307.06682v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#36827;&#34892;&#33258;&#30417;&#30563;&#22320;&#38663;&#21435;&#22122;&#65292;&#36890;&#36807;&#35266;&#23519;&#21435;&#22122;&#32593;&#32476;&#20013;&#30340;&#40657;&#30418;&#23376;&#65292;&#26367;&#20195;&#23545;&#22122;&#22768;&#32479;&#35745;&#30340;&#20808;&#39564;&#30693;&#35782;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#24179;&#22343;&#21270;&#36755;&#20837;&#20687;&#32032;&#30340;&#38597;&#21487;&#27604;&#36129;&#29486;&#65292;&#25552;&#20379;&#25233;&#21046;&#22320;&#38663;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#26368;&#26377;&#25928;&#33945;&#29256;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#30456;&#24178;&#22122;&#22768;&#20250;&#23548;&#33268;&#35823;&#24046;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#23613;&#26089;&#21644;&#39640;&#25928;&#22320;&#25233;&#21046;&#22122;&#22768;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#33258;&#30417;&#30563;&#21435;&#22122;&#32469;&#36807;&#20102;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#20855;&#26377;&#22122;&#22768;-&#24178;&#20928;&#35757;&#32451;&#23545;&#30340;&#24120;&#35268;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#30456;&#24178;&#22122;&#22768;&#25233;&#21046;&#26041;&#27861;&#38656;&#35201;&#23545;&#22122;&#22768;&#32479;&#35745;&#30340;&#24191;&#27867;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#35266;&#23519;&#21435;&#22122;&#32593;&#32476;&#30340;&#20869;&#37096;&#65292;&#24182;&#21033;&#29992;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#26367;&#20195;&#23545;&#22122;&#22768;&#26412;&#36523;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26080;&#20559;&#32593;&#32476;&#21644;&#30456;&#20851;&#38597;&#21487;&#27604;&#30697;&#38453;&#25152;&#25552;&#20379;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#30452;&#25509;&#32447;&#24615;&#20851;&#31995;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#38543;&#26426;&#36873;&#25321;&#30340;&#33509;&#24178;&#36755;&#20837;&#20687;&#32032;&#30340;&#38597;&#21487;&#27604;&#36129;&#29486;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#21270;&#65292;&#21487;&#20197;&#25552;&#20379;&#25233;&#21046;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#26368;&#26377;&#25928;&#33945;&#29256;&#30340;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The presence of coherent noise in seismic data leads to errors and uncertainties, and as such it is paramount to suppress noise as early and efficiently as possible. Self-supervised denoising circumvents the common requirement of deep learning procedures of having noisy-clean training pairs. However, self-supervised coherent noise suppression methods require extensive knowledge of the noise statistics. We propose the use of explainable artificial intelligence approaches to see inside the black box that is the denoising network and use the gained knowledge to replace the need for any prior knowledge of the noise itself. This is achieved in practice by leveraging bias-free networks and the direct linear link between input and output provided by the associated Jacobian matrix; we show that a simple averaging of the Jacobian contributions over a number of randomly selected input pixels, provides an indication of the most effective mask to suppress noise present in the data. The proposed me
&lt;/p&gt;</description></item><item><title>DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06647</link><description>&lt;p&gt;
DeepIPCv2&#65306;&#21033;&#29992;LiDAR&#24378;&#21270;&#33258;&#21160;&#39550;&#39542;&#29615;&#22659;&#24863;&#30693;&#19982;&#23548;&#33322;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle. (arXiv:2307.06647v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06647
&lt;/p&gt;
&lt;p&gt;
DeepIPCv2&#26159;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#28857;&#20113;&#20316;&#20026;&#24863;&#30693;&#36755;&#20837;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DeepIPCv2&#65292;&#19968;&#31181;&#21033;&#29992;LiDAR&#20256;&#24863;&#22120;&#24863;&#30693;&#29615;&#22659;&#30340;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#29031;&#26465;&#20214;&#36739;&#24046;&#30340;&#24773;&#20917;&#19979;&#12290;DeepIPCv2&#20351;&#29992;&#19968;&#32452;LiDAR&#28857;&#20113;&#20316;&#20026;&#20854;&#20027;&#35201;&#24863;&#30693;&#36755;&#20837;&#12290;&#30001;&#20110;&#28857;&#20113;&#19981;&#21463;&#20809;&#29031;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#28165;&#26224;&#30340;&#29615;&#22659;&#35266;&#23519;&#65292;&#26080;&#35770;&#26465;&#20214;&#22914;&#20309;&#12290;&#36825;&#20351;&#24471;&#24863;&#30693;&#27169;&#22359;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#22330;&#26223;&#29702;&#35299;&#21644;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25903;&#25345;&#25511;&#21046;&#27169;&#22359;&#20934;&#30830;&#20272;&#35745;&#23548;&#33322;&#25511;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#35813;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#32452;&#39550;&#39542;&#35760;&#24405;&#24182;&#22312;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#19979;&#36827;&#34892;&#30495;&#23454;&#33258;&#21160;&#39550;&#39542;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#21644;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#35777;&#26126;&#20854;&#24615;&#33021;&#12290;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#65292;DeepIPCv2&#22312;&#25152;&#26377;&#26465;&#20214;&#19979;&#22343;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39550;&#39542;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DeepIPCv2, an autonomous driving model that perceives the environment using a LiDAR sensor for more robust drivability, especially when driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR point clouds for its main perception input. As point clouds are not affected by illumination changes, they can provide a clear observation of the surroundings no matter what the condition is. This results in a better scene understanding and stable features provided by the perception module to support the controller module in estimating navigational control properly. To evaluate its performance, we conduct several tests by deploying the model to predict a set of driving records and perform real automated driving under three different conditions. We also conduct ablation and comparative studies with some recent models to justify its performance. Based on the experimental results, DeepIPCv2 shows a robust performance by achieving the best drivability in all conditions. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#36716;&#25442;&#24207;&#21015;&#26816;&#32034;&#65288;ITSR&#65289;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#21644;&#26368;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#37117;&#33021;&#32988;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06630</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#36716;&#25442;&#24207;&#21015;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Image Transformation Sequence Retrieval with General Reinforcement Learning. (arXiv:2307.06630v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#20687;&#36716;&#25442;&#24207;&#21015;&#26816;&#32034;&#65288;ITSR&#65289;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#21644;&#26368;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#37117;&#33021;&#32988;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#36716;&#25442;&#24207;&#21015;&#26816;&#32034;&#65288;ITSR&#65289;&#20219;&#21153;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#26816;&#32034;&#20986;&#20316;&#20026;&#28304;&#21644;&#30446;&#26631;&#30340;&#20004;&#20010;&#32473;&#23450;&#22270;&#20687;&#20043;&#38388;&#30340;&#36716;&#25442;&#24207;&#21015;&#12290;&#37492;&#20110;&#25361;&#25112;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#20363;&#22914;&#27491;&#30830;&#24207;&#21015;&#30340;&#22810;&#26679;&#24615;&#25110;&#36807;&#31243;&#30340;&#36830;&#32493;&#27493;&#39588;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;&#20363;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#30340;ITSR&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#30417;&#30563;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#21644;&#26368;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#37117;&#33021;&#32988;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;ITSR&#30340;&#26412;&#36136;&#21450;&#20854;&#30456;&#20851;&#25361;&#25112;&#25552;&#20986;&#20102;&#26377;&#36259;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, the novel Image Transformation Sequence Retrieval (ITSR) task is presented, in which a model must retrieve the sequence of transformations between two given images that act as source and target, respectively. Given certain characteristics of the challenge such as the multiplicity of a correct sequence or the correlation between consecutive steps of the process, we propose a solution to ITSR using a general model-based Reinforcement Learning such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural network. Our experiments provide a benchmark in both synthetic and real domains, where the proposed approach is compared with supervised training. The results report that a model trained with MCTS is able to outperform its supervised counterpart in both the simplest and the most complex cases. Our work draws interesting conclusions about the nature of ITSR and its associated challenges.
&lt;/p&gt;</description></item><item><title>SecureFalcon&#26159;&#22522;&#20110;FalconLLM&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;FalconLLM&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#65292;&#33021;&#22815;&#35782;&#21035;C&#20195;&#30721;&#26679;&#26412;&#20013;&#30340;&#28431;&#27934;&#21644;&#38750;&#28431;&#27934;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2307.06616</link><description>&lt;p&gt;
SecureFalcon:&#19979;&#19968;&#20195;&#38754;&#21521;&#32593;&#32476;&#23433;&#20840;&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SecureFalcon: The Next Cyber Reasoning System for Cyber Security. (arXiv:2307.06616v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06616
&lt;/p&gt;
&lt;p&gt;
SecureFalcon&#26159;&#22522;&#20110;FalconLLM&#30340;&#32593;&#32476;&#25512;&#29702;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;FalconLLM&#26469;&#23454;&#29616;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#65292;&#33021;&#22815;&#35782;&#21035;C&#20195;&#30721;&#26679;&#26412;&#20013;&#30340;&#28431;&#27934;&#21644;&#38750;&#28431;&#27934;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#28431;&#27934;&#23548;&#33268;&#21508;&#31181;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23849;&#28291;&#12289;&#25968;&#25454;&#20002;&#22833;&#21644;&#23433;&#20840;&#28431;&#27934;&#65292;&#20005;&#37325;&#24433;&#21709;&#36719;&#20214;&#24212;&#29992;&#21644;&#31995;&#32479;&#30340;&#24066;&#22330;&#37319;&#29992;&#29575;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#12289;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#26368;&#24120;&#29992;&#19988;&#26377;&#22266;&#26377;&#30340;&#35823;&#25253;&#29575;&#65292;&#32473;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#36825;&#20123;&#25345;&#20037;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;FalconLLM&#22312;&#35782;&#21035;&#22797;&#26434;&#27169;&#24335;&#21644;&#28431;&#27934;&#26041;&#38754;&#26174;&#31034;&#20986;&#37325;&#35201;&#28508;&#21147;&#65292;&#22240;&#27492;&#22312;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;FalconLLM&#36827;&#34892;&#20102;&#38024;&#23545;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#25512;&#20986;&#20102;SecureFalcon&#65292;&#36825;&#26159;&#22522;&#20110;FalconLLM&#30340;&#21019;&#26032;&#27169;&#22411;&#26550;&#26500;&#12290;SecureFalcon&#34987;&#35757;&#32451;&#29992;&#20110;&#21306;&#20998;&#26377;&#28431;&#27934;&#21644;&#26080;&#28431;&#27934;&#30340;C&#20195;&#30721;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software vulnerabilities leading to various detriments such as crashes, data loss, and security breaches, significantly hinder the quality, affecting the market adoption of software applications and systems. Although traditional methods such as automated software testing, fault localization, and repair have been intensively studied, static analysis tools are most commonly used and have an inherent false positives rate, posing a solid challenge to developer productivity. Large Language Models (LLMs) offer a promising solution to these persistent issues. Among these, FalconLLM has shown substantial potential in identifying intricate patterns and complex vulnerabilities, hence crucial in software vulnerability detection. In this paper, for the first time, FalconLLM is being fine-tuned for cybersecurity applications, thus introducing SecureFalcon, an innovative model architecture built upon FalconLLM. SecureFalcon is trained to differentiate between vulnerable and non-vulnerable C code sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#22522;&#20110;&#20415;&#25658;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#35270;&#32593;&#33180;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;635&#20010;&#30001;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#30524;&#24213;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#19978;&#23545;&#35270;&#32593;&#33180;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#21644;&#31934;&#30830;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#26088;&#22312;&#25512;&#36827;&#34880;&#31649;&#20998;&#21106;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.06577</link><description>&lt;p&gt;
&#22522;&#20110;&#20415;&#25658;&#35774;&#22791;&#30340;&#30524;&#24213;&#35270;&#39057;&#25968;&#25454;&#38598;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation. (arXiv:2307.06577v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#22522;&#20110;&#20415;&#25658;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#30340;&#22522;&#20110;&#35270;&#39057;&#30340;&#35270;&#32593;&#33180;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;635&#20010;&#30001;&#26234;&#33021;&#25163;&#26426;&#37319;&#38598;&#30340;&#30524;&#24213;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#19978;&#23545;&#35270;&#32593;&#33180;&#32467;&#26500;&#36827;&#34892;&#20840;&#38754;&#21644;&#31934;&#30830;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#26088;&#22312;&#25512;&#36827;&#34880;&#31649;&#20998;&#21106;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#24314;&#31435;&#22312;&#20351;&#29992;&#21488;&#24335;&#35774;&#22791;&#37319;&#38598;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#38745;&#24577;&#22270;&#20687;&#33258;&#28982;&#20007;&#22833;&#20102;&#35270;&#32593;&#33180;&#27874;&#21160;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#24615;&#38477;&#20302;&#65292;&#32780;&#21488;&#24335;&#35774;&#22791;&#30340;&#20351;&#29992;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#20854;&#21463;&#38480;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#20415;&#25658;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#33719;&#21462;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#30340;&#35270;&#32593;&#33180;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;&#22235;&#20010;&#19981;&#21516;&#35786;&#25152;&#25910;&#38598;&#30340;635&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30524;&#24213;&#35270;&#39057;&#65292;&#28041;&#21450;415&#21517;50&#33267;75&#23681;&#30340;&#24739;&#32773;&#12290;&#23427;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#19978;&#25552;&#20379;&#20102;&#35270;&#32593;&#33180;&#32467;&#26500;&#30340;&#20840;&#38754;&#21644;&#31934;&#30830;&#30340;&#26631;&#27880;&#65292;&#26088;&#22312;&#25512;&#36827;&#34880;&#31649;&#20998;&#21106;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#19977;&#20010;&#23618;&#27425;&#30340;&#31354;&#38388;&#26631;&#27880;&#65306;&#29992;&#20110;&#25972;&#20307;&#35270;&#32593;&#33180;&#32467;&#26500;&#25551;&#32472;&#30340;&#20108;&#20540;&#34880;&#31649;&#25513;&#33180;&#12289;&#29992;&#20110;&#21306;&#20998;&#38745;&#33033;&#21644;&#21160;&#33033;&#30340;&#24120;&#35268;&#38745;&#33033;-&#21160;&#33033;&#25513;&#33180;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices. The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility. Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition. The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old. It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation. Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06564</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#30340;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#65306;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach. (arXiv:2307.06564v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36164;&#28304;&#38480;&#21046;&#19979;&#36827;&#34892;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#65292;&#26469;&#35302;&#21457;&#24178;&#39044;&#65292;&#20174;&#32780;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#35302;&#21457;&#24178;&#39044;&#26469;&#20248;&#21270;&#19994;&#21153;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22686;&#21152;&#27491;&#38754;&#26696;&#20363;&#32467;&#26524;&#30340;&#27010;&#29575;&#12290;&#36825;&#20123;&#24178;&#39044;&#26159;&#26681;&#25454;&#24178;&#39044;&#31574;&#30053;&#35302;&#21457;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#24178;&#39044;&#31574;&#30053;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#20551;&#35774;&#21487;&#29992;&#20110;&#25191;&#34892;&#24178;&#39044;&#30340;&#36164;&#28304;&#25968;&#37327;&#26159;&#26080;&#38480;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#36164;&#28304;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#26041;&#36807;&#31243;&#30417;&#25511;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#22256;&#22659;&#26159;&#22522;&#20110;&#23545;&#24178;&#39044;&#38656;&#27714;&#12289;&#21450;&#26102;&#24615;&#25110;&#25928;&#26524;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#27700;&#24179;&#26469;&#35302;&#21457;&#24178;&#39044;&#12290;&#23454;&#38469;&#19978;&#65292;&#24403;&#23545;&#24178;&#39044;&#30340;&#24517;&#35201;&#24615;&#25110;&#25928;&#26524;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23558;&#26377;&#38480;&#30340;&#36164;&#28304;&#29992;&#20110;&#24178;&#39044;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescriptive process monitoring methods seek to optimize the performance of business processes by triggering interventions at runtime, thereby increasing the probability of positive case outcomes. These interventions are triggered according to an intervention policy. Reinforcement learning has been put forward as an approach to learning intervention policies through trial and error. Existing approaches in this space assume that the number of resources available to perform interventions in a process is unlimited, an unrealistic assumption in practice. This paper argues that, in the presence of resource constraints, a key dilemma in the field of prescriptive process monitoring is to trigger interventions based not only on predictions of their necessity, timeliness, or effect but also on the uncertainty of these predictions and the level of resource utilization. Indeed, committing scarce resources to an intervention when the necessity or effects of this intervention are highly uncertain m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;</title><link>http://arxiv.org/abs/2307.06541</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#32473;&#23450;&#26102;&#38388;&#35270;&#37326;&#30340;&#65288;&#21069;&#21521;&#65289;&#24378;&#21270;&#23398;&#20064;&#25110;&#35268;&#21010;&#26469;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#19982;&#19987;&#23478;&#28436;&#31034;&#21305;&#37197;&#12290;&#26102;&#38388;&#35270;&#37326;&#22312;&#30830;&#23450;&#22870;&#21169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;IRL&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#27604;&#22320;&#38754;&#23454;&#38469;&#20540;&#26356;&#30701;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#36890;&#24120;&#33021;&#26356;&#24555;&#22320;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#27492;&#29616;&#35937;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#24182;&#32473;&#20986;&#20102;&#35299;&#37322;&#65306;&#26102;&#38388;&#35270;&#37326;&#25511;&#21046;&#20102;&#24341;&#21457;&#31574;&#30053;&#31867;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#36825;&#19968;&#20998;&#26512;&#20026;IRL&#30340;&#26377;&#25928;&#35270;&#37326;&#36873;&#25321;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#12290;&#23427;&#20063;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;IRL&#20844;&#24335;&#65306;&#19982;&#20165;&#20855;&#26377;&#32473;&#23450;&#35270;&#37326;&#30340;&#22870;&#21169;&#30456;&#27604;&#65292;&#20849;&#21516;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#35270;&#37326;&#26356;&#21152;&#33258;&#28982;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24050;&#26377;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06521</link><description>&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Drug Discovery: Are We There Yet?. (arXiv:2307.06521v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#24050;&#26377;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#27491;&#22312;&#36866;&#24212;&#25968;&#25454;&#31185;&#23398;&#12289;&#20449;&#24687;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#26032;&#25216;&#26415;&#65292;&#20197;&#21152;&#24555;&#26377;&#25928;&#27835;&#30103;&#30340;&#24320;&#21457;&#65292;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#21160;&#29289;&#23454;&#39564;&#12290;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#33647;&#29289;&#21457;&#29616;&#65292;&#25237;&#36164;&#32773;&#12289;&#24037;&#19994;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#20197;&#21450;&#31435;&#27861;&#32773;&#23545;&#27492;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25104;&#21151;&#30340;&#33647;&#29289;&#21457;&#29616;&#38656;&#35201;&#20248;&#21270;&#19982;&#33647;&#29702;&#21160;&#21147;&#23398;&#12289;&#33647;&#20195;&#21160;&#21147;&#23398;&#21644;&#20020;&#24202;&#32467;&#26524;&#30456;&#20851;&#30340;&#24615;&#36136;&#12290;&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#30340;&#19977;&#20010;&#25903;&#26609;&#65306;&#30142;&#30149;&#12289;&#38774;&#28857;&#21644;&#27835;&#30103;&#27169;&#24335;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#33647;&#29289;&#12290;&#29983;&#25104;&#21270;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22810;&#23646;&#24615;&#20248;&#21270;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20351;&#24471;&#22810;&#31181;&#21270;&#21512;&#29289;&#36827;&#20837;&#20102;&#20020;&#24202;&#35797;&#39564;&#12290;&#31185;&#23398;&#30028;&#24517;&#39035;&#20180;&#32454;&#23457;&#26597;&#24050;&#30693;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#21482;&#26377;&#20855;&#26377;&#36275;&#22815;&#30340;&#30495;&#23454;&#22522;&#20934;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25165;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drug discovery is adapting to novel technologies such as data science, informatics, and artificial intelligence (AI) to accelerate effective treatment development while reducing costs and animal experiments. AI is transforming drug discovery, as indicated by increasing interest from investors, industrial and academic scientists, and legislators. Successful drug discovery requires optimizing properties related to pharmacodynamics, pharmacokinetics, and clinical outcomes. This review discusses the use of AI in the three pillars of drug discovery: diseases, targets, and therapeutic modalities, with a focus on small molecule drugs. AI technologies, such as generative chemistry, machine learning, and multi-property optimization, have enabled several compounds to enter clinical trials. The scientific community must carefully vet known information to address the reproducibility crisis. The full potential of AI in drug discovery can only be realized with sufficient ground truth and appropriate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20934;&#30830;&#26657;&#20934;AI&#31995;&#32479;&#20013;&#30340;&#20449;&#24565;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#25104;&#20004;&#31867;:&#20027;&#35266;&#24615;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#20449;&#24565;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06513</link><description>&lt;p&gt;
&#21033;&#29992;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#23454;&#29616;&#20449;&#24565;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Leveraging Contextual Counterfactuals Toward Belief Calibration. (arXiv:2307.06513v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#20934;&#30830;&#26657;&#20934;AI&#31995;&#32479;&#20013;&#30340;&#20449;&#24565;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#25104;&#20004;&#31867;:&#20027;&#35266;&#24615;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#22788;&#29702;&#20449;&#24565;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#25968;&#25454;&#37319;&#38598;&#21407;&#21017;&#25110;&#27491;&#21017;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#31561;&#26041;&#24335;&#65292;&#20154;&#30340;&#20449;&#24565;&#21644;&#20215;&#20540;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;AI&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#20803;&#23545;&#40784;&#38382;&#39064;&#26159;&#20154;&#31867;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#36328;&#32676;&#20307;&#30340;&#19981;&#23545;&#40784;&#24615;&#65292;&#32780;&#19988;&#21363;&#20351;&#22312;&#20154;&#31867;&#20043;&#38388;&#65292;&#27599;&#20010;&#20449;&#24565;&#30340;&#38544;&#21547;&#24378;&#24230;&#20063;&#21487;&#33021;&#19981;&#22909;&#26657;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#23581;&#35797;&#36328;&#19978;&#19979;&#25991;&#36827;&#34892;&#25512;&#24191;&#26102;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39640;&#21518;&#24724;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19978;&#19979;&#25991;&#21453;&#20107;&#23454;&#21644;&#34917;&#25937;&#25104;&#26412;&#23545;&#20110;&#26356;&#26032;&#20915;&#31574;&#32773;&#30340;&#20449;&#24565;&#20197;&#21450;&#25152;&#25345;&#20449;&#24565;&#30340;&#24378;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#24341;&#20837;&#21453;&#20107;&#23454;&#25512;&#29702;&#26159;&#20934;&#30830;&#26657;&#20934;&#20449;&#24565;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20449;&#24565;&#30340;&#22810;&#26679;&#24615;&#20998;&#20026;&#20004;&#31867;&#65306;&#20027;&#35266;&#24615;&#65288;&#21516;&#19968;&#32676;&#20307;&#20869;&#30340;&#20010;&#20307;&#38388;&#65289;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65288;&#21516;&#19968;&#20154;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#65289;
&lt;/p&gt;
&lt;p&gt;
Beliefs and values are increasingly being incorporated into our AI systems through alignment processes, such as carefully curating data collection principles or regularizing the loss function used for training. However, the meta-alignment problem is that these human beliefs are diverse and not aligned across populations; furthermore, the implicit strength of each belief may not be well calibrated even among humans, especially when trying to generalize across contexts. Specifically, in high regret situations, we observe that contextual counterfactuals and recourse costs are particularly important in updating a decision maker's beliefs and the strengths to which such beliefs are held. Therefore, we argue that including counterfactuals is key to an accurate calibration of beliefs during alignment. To do this, we first segment belief diversity into two categories: subjectivity (across individuals within a population) and epistemic uncertainty (within an individual across different contexts
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06507</link><description>&lt;p&gt;
&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25552;&#39640;&#20102;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#22312;&#35299;&#20915;&#21307;&#30103;&#25361;&#25112;&#21644;&#25552;&#20379;&#25913;&#36827;&#35786;&#26029;&#24037;&#20855;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24448;&#24448;&#38656;&#35201;&#26377;&#27880;&#37322;&#30340;&#21307;&#23398;&#22270;&#20687;&#25104;&#20026;&#20805;&#20998;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24378;&#22823;&#33021;&#21147;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#65288;NAFLD&#65289;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65306;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#30340;&#22270;&#20687;&#35745;&#31639;&#30340;Inception Score&#65288;IS&#65289;&#21644;Fr\'{e}chet Inception Distance&#65288;FID&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25193;&#25955;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26368;&#22823;IS&#24471;&#20998;&#20026;1.90&#65292;&#32780;GANs&#20026;1.67&#65292;&#26368;&#23567;FID&#24471;&#20998;&#20026;69.45&#65292;&#32780;GANs&#20026;99.53&#12290;&#21033;&#29992;&#37096;&#20998;&#20923;&#32467;&#30340;CNN&#39592;&#24178;&#65288;EfficientNet v1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools. However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models. Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance. We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fr\'{e}chet Inception Distance (FID), computed on diffusion-generated images and generative adversarial networks (GANs)-generated images. Our results show superior performance for the diffusion-generated images, with a maximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet v1),
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.06501</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#23454;&#29616;&#20154;&#24037;&#33008;&#33146;
&lt;/p&gt;
&lt;p&gt;
Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCPAP&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#38598;&#25104;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#20197;&#35299;&#20915;&#20154;&#24037;&#33008;&#33146;&#30340;&#22797;&#26434;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#34880;&#31958;&#27979;&#37327;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20154;&#24037;&#33008;&#33146;(AP)&#22312;&#23454;&#29616;1&#22411;&#31958;&#23615;&#30149;&#24739;&#32773;&#38381;&#29615;&#34880;&#31958;&#25511;&#21046;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#29983;&#29702;&#36807;&#31243;&#12289;&#24310;&#36831;&#30340;&#33008;&#23707;&#32032;&#21453;&#24212;&#21644;&#19981;&#20934;&#30830;&#30340;&#34880;&#31958;&#27979;&#37327;&#65292;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;AP&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#36890;&#36807;&#21160;&#24577;&#27169;&#22411;&#21644;&#23433;&#20840;&#32422;&#26463;&#25552;&#20379;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20294;&#20854;&#32570;&#20047;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#21463;&#21040;&#26410;&#23459;&#24067;&#30340;&#39278;&#39135;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#21644;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#20294;&#38754;&#20020;&#20998;&#24067;&#20559;&#31227;&#21644;&#22823;&#37327;&#25968;&#25454;&#38656;&#27714;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;HyCPAP&#65292;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;HyCPAP&#23558;MPC&#31574;&#30053;&#19982;&#38598;&#25104;DRL&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#24357;&#34917;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#26041;&#27861;&#65292;QuScore&#65292;&#23545;&#25239;&#23545;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.06496</link><description>&lt;p&gt;
&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#23545;&#25239;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (arXiv:2307.06496v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#30340;&#40657;&#31665;&#25915;&#20987;&#26041;&#27861;&#65292;QuScore&#65292;&#23545;&#25239;&#23545;&#35299;&#37322;&#27169;&#22411;&#36827;&#34892;&#32806;&#21512;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#65292;&#23454;&#29616;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#20013;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#25915;&#20987;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#23558;DNN&#27169;&#22411;&#19982;&#35299;&#37322;&#27169;&#22411;&#30456;&#32467;&#21512;&#21487;&#20197;&#22312;&#20154;&#31867;&#19987;&#23478;&#20171;&#20837;&#26102;&#25552;&#20379;&#19968;&#31181;&#23433;&#20840;&#24863;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#30830;&#23450;&#25152;&#32473;&#26679;&#26412;&#26159;&#33391;&#24615;&#30340;&#36824;&#26159;&#24694;&#24847;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;(IDLSes)&#26131;&#21463;&#24694;&#24847;&#31713;&#25913;&#30340;&#25915;&#20987;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#23545;IDLSes&#32452;&#20214;&#30340;&#35775;&#38382;&#21463;&#38480;&#65292;&#23545;&#25163;&#26356;&#38590;&#20197;&#27450;&#39575;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#25928;&#29575;&#21644;&#35780;&#20998;&#30340;&#40657;&#31665;&#25915;&#20987;IDLSes&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;QuScore&#65292;&#23427;&#19981;&#38656;&#35201;&#20102;&#35299;&#30446;&#26631;&#27169;&#22411;&#21450;&#20854;&#32806;&#21512;&#30340;&#35299;&#37322;&#27169;&#22411;&#12290;QuScore&#22522;&#20110;&#36716;&#31227;&#21644;&#35780;&#20998;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#31181;&#26377;&#25928;&#30340;&#24494;&#29983;&#29289;&#36951;&#20256;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#24517;&#35201;&#30340;&#26597;&#35810;&#25968;&#37327;&#20197;&#36827;&#34892;&#25104;&#21151;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are susceptible to adversarial samples in white and black-box environments. Although previous studies have shown high attack success rates, coupling DNN models with interpretation models could offer a sense of security when a human expert is involved, who can identify whether a given sample is benign or malicious. However, in white-box environments, interpretable deep learning systems (IDLSes) have been shown to be vulnerable to malicious manipulations. In black-box settings, as access to the components of IDLSes is limited, it becomes more challenging for the adversary to fool the system. In this work, we propose a Query-efficient Score-based black-box attack against IDLSes, QuScore, which requires no knowledge of the target model and its coupled interpretation model. QuScore is based on transfer-based and score-based methods by employing an effective microbial genetic algorithm. Our method is designed to reduce the number of queries necessary to carry out success
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;Cohn-Umans&#26694;&#26550;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#39564;&#35777;&#30340;&#24378;&#21807;&#19968;&#21487;&#35299;&#35868;&#39064;&#65288;SUSP&#65289;&#30340;&#26032;&#23376;&#31867;&#65292;&#21517;&#20026;&#21487;&#31616;&#21270;&#30340;SUSPs&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#31616;&#21270;&#30340;SUSPs&#21487;&#20197;&#36798;&#21040;&#19982;&#26080;&#31351;&#26063;SUSPs&#30456;&#21516;&#30340;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#36793;&#30028;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#26426;&#25628;&#32034;&#26500;&#36896;&#20102;&#26356;&#22823;&#30340;SUSPs&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#30340;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.06463</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#39564;&#35777;&#30340;&#24378;&#21807;&#19968;&#21487;&#35299;&#35868;&#39064;&#19982;&#30697;&#38453;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficiently-Verifiable Strong Uniquely Solvable Puzzles and Matrix Multiplication. (arXiv:2307.06463v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;Cohn-Umans&#26694;&#26550;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#39564;&#35777;&#30340;&#24378;&#21807;&#19968;&#21487;&#35299;&#35868;&#39064;&#65288;SUSP&#65289;&#30340;&#26032;&#23376;&#31867;&#65292;&#21517;&#20026;&#21487;&#31616;&#21270;&#30340;SUSPs&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#31616;&#21270;&#30340;SUSPs&#21487;&#20197;&#36798;&#21040;&#19982;&#26080;&#31351;&#26063;SUSPs&#30456;&#21516;&#30340;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#36793;&#30028;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#26426;&#25628;&#32034;&#26500;&#36896;&#20102;&#26356;&#22823;&#30340;SUSPs&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#36827;&#20102;Cohn-Umans&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#12289;&#20998;&#26512;&#21644;&#23547;&#25214;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21807;&#19968;&#21487;&#35299;&#35868;&#39064;&#65288;SUSP&#65289;&#30340;&#23376;&#31867;&#65292;&#31216;&#20026;&#21487;&#31616;&#21270;&#30340;SUSPs&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#35868;&#39064;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#39564;&#35777;&#65292;&#32780;&#23545;&#20110;&#19968;&#33324;&#30340;SUSPs&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#31616;&#21270;&#30340;SUSPs&#21487;&#20197;&#36798;&#21040;&#19982;&#26080;&#31351;&#26063;SUSPs&#30456;&#21516;&#30340;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;$\omega$&#36793;&#30028;&#12290;&#26681;&#25454;&#35745;&#31639;&#26426;&#25628;&#32034;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23485;&#24230;&#36739;&#23567;&#30340;SUSPs&#30340;&#26356;&#22823;&#26500;&#36896;&#65292;&#36825;&#19982;&#25105;&#20204;&#26356;&#32039;&#23494;&#30340;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#23558;&#30697;&#38453;&#20056;&#27861;&#25351;&#25968;&#30340;&#19978;&#30028;&#20174;$2.66$&#22686;&#24378;&#21040;$2.505$&#65292;&#25509;&#36817;Cohn&#31561;&#20154;&#25163;&#24037;&#26500;&#36896;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We advance the Cohn-Umans framework for developing fast matrix multiplication algorithms. We introduce, analyze, and search for a new subclass of strong uniquely solvable puzzles (SUSP), which we call simplifiable SUSPs. We show that these puzzles are efficiently verifiable, which remains an open question for general SUSPs. We also show that individual simplifiable SUSPs can achieve the same strength of bounds on the matrix multiplication exponent $\omega$ that infinite families of SUSPs can. We report on the construction, by computer search, of larger SUSPs than previously known for small width. This, combined with our tighter analysis, strengthens the upper bound on the matrix multiplication exponent from $2.66$ to $2.505$ obtainable via this computational approach, and nears the results of the handcrafted constructions of Cohn et al.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06440</link><description>&lt;p&gt;
&#27809;&#26377;&#35757;&#32451;&#23601;&#27809;&#26377;&#25910;&#30410;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#26550;&#26500;&#65292;&#25209;&#37327;&#36873;&#25321;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#36825;&#20123;&#31639;&#27861;&#39044;&#35757;&#32451;&#26102;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#28040;&#22833;&#20102;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#26469;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#37322;&#25918;&#20102;&#20195;&#30721;&#26469;&#20419;&#36827;&#39640;&#25928;&#35757;&#32451;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;Transformer-based&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#24613;&#21095;&#22686;&#21152;&#12290;&#36825;&#19968;&#36235;&#21183;&#20419;&#20351;&#30740;&#31350;&#32773;&#20204;&#24320;&#23637;&#20102;&#38024;&#23545;&#39640;&#25928;&#35757;&#32451;&#31639;&#27861;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#24555;&#22320;&#25913;&#21892;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19977;&#31867;&#36825;&#26679;&#30340;&#31639;&#27861;&#65306;&#21160;&#24577;&#26550;&#26500;&#65288;&#23618;&#21472;&#12289;&#23618;&#20002;&#24323;&#65289;&#12289;&#25209;&#37327;&#36873;&#25321;&#65288;&#36873;&#25321;&#24615;&#21453;&#21521;&#20256;&#25773;&#12289;RHO&#25439;&#22833;&#65289;&#21644;&#39640;&#25928;&#20248;&#21270;&#22120;&#65288;Lion&#12289;Sophia&#65289;&#12290;&#24403;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#23545;BERT&#21644;T5&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#19979;&#28216;&#25910;&#30410;&#30456;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23436;&#20840;&#34928;&#20943;&#23398;&#20064;&#29575;&#30340;&#22522;&#32447;&#32780;&#35328;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#35780;&#20272;&#21327;&#35758;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#25152;&#26377;&#35745;&#31639;&#26102;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#31216;&#20026;&#21442;&#32771;&#31995;&#32479;&#26102;&#38388;&#30340;&#21442;&#32771;&#26426;&#22120;&#19978;&#65292;&#22312;&#20219;&#24847;&#26426;&#22120;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#65292;&#20197;&#40723;&#21169;&#23545;&#39640;&#25928;&#35757;&#32451;&#30340;&#20005;&#26684;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#22312;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.06439</link><description>&lt;p&gt;
&#20026;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#25552;&#21462;&#32780;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Biomedical Knowledge Extraction: A Case Study on Adverse Drug Events. (arXiv:2307.06439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#25552;&#21319;&#20102;&#22312;&#33647;&#29289;&#19981;&#33391;&#20107;&#20214;&#25552;&#21462;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21253;&#25324;&#20581;&#24247;&#24212;&#29992;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#25193;&#23637;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#25972;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#20855;&#22791;&#20102;&#32467;&#26500;&#21270;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#33391;&#22909;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#23558;&#20854;&#33976;&#39311;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#23398;&#29983;&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20855;&#22791;&#25104;&#26412;&#12289;&#25928;&#29575;&#21644;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#31561;&#39069;&#22806;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADE&#65289;&#25552;&#21462;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#25913;&#36827;&#21307;&#30103;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#22312;&#26631;&#20934;ADE&#25552;&#21462;&#35780;&#20272;&#20013;&#65292;&#32463;&#33976;&#39311;&#30340;GPT-3.5 PubMedBERT&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#19982;&#30417;&#30563;&#24335;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;&#23613;&#31649;&#20307;&#31215;&#32553;&#23567;&#20102;1000&#22810;&#20493;&#65292;&#20294;&#33976;&#39311;&#27169;&#22411;&#22312;F1&#25351;&#26631;&#19978;&#36229;&#36807;&#20102;&#20854;&#25945;&#24072;GPT-3.5&#32422;6&#20010;&#32477;&#23545;&#28857;&#65292;&#36229;&#36807;&#20102;GPT-4&#32422;5&#20010;&#32477;&#23545;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-4, have demonstrated remarkable capabilities across a wide range of tasks, including health applications. In this paper, we study how LLMs can be used to scale biomedical knowledge curation. We find that while LLMs already possess decent competency in structuring biomedical text, by distillation into a task-specific student model through self-supervised learning, substantial gains can be attained over out-of-box LLMs, with additional advantages such as cost, efficiency, and white-box model access.  We conduct a case study on adverse drug event (ADE) extraction, which is an important area for improving care. On standard ADE extraction evaluation, a GPT-3.5 distilled PubMedBERT model attained comparable accuracy as supervised state-of-the-art models without using any labeled data. Despite being over 1,000 times smaller, the distilled model outperformed its teacher GPT-3.5 by over 6 absolute points in F1 and GPT-4 by over 5 absolute points.  Ablat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#20123;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.06399</link><description>&lt;p&gt;
&#20174;&#38754;&#21521;&#30446;&#26631;&#30340;LTLf&#20844;&#24335;&#35774;&#35745;&#34892;&#20026;&#26641;
&lt;/p&gt;
&lt;p&gt;
Designing Behavior Trees from Goal-Oriented LTLf Formulas. (arXiv:2307.06399v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#19968;&#20123;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36923;&#36753;&#21487;&#20197;&#29992;&#20110;&#24418;&#24335;&#21270;&#25351;&#23450;&#33258;&#20027;&#20195;&#29702;&#30446;&#26631;&#65292;&#20294;&#21512;&#25104;&#33021;&#22815;&#20445;&#35777;&#30446;&#26631;&#28385;&#36275;&#30340;&#35745;&#21010;&#32773;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#19978;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20351;&#29992;&#26377;&#38480;&#35775;&#38382;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#30340;&#23376;&#38598;&#25351;&#23450;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20445;&#35777;&#25104;&#21151;&#36335;&#24452;&#28385;&#36275;LTL&#30446;&#26631;&#30340;&#34892;&#20026;&#26641;&#65288;BT&#65289;&#12290;&#21487;&#20197;&#20351;&#29992;&#20197;&#23454;&#29616;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#20219;&#21153;&#20219;&#21153;&#35821;&#27861;&#26469;&#25552;&#21462;&#26377;&#29992;&#30340;LTL&#20844;&#24335;&#65292;&#20854;&#20013;&#30001;LTL&#36816;&#31639;&#31526;&#32452;&#21512;&#30340;&#20219;&#21153;&#26500;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;LTL&#20844;&#24335;&#26500;&#24314;BT&#23548;&#33268;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#34892;&#20026;&#21512;&#25104;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#21508;&#31181;&#35745;&#21010;&#32773;&#21487;&#20197;&#23454;&#29616;BT&#20013;&#30340;&#21160;&#20316;&#33410;&#28857;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35745;&#21010;&#32773;&#23548;&#33268;&#30340;&#20219;&#20309;&#25104;&#21151;&#36335;&#24452;&#37117;&#28385;&#36275;&#30456;&#24212;&#30340;LTL&#20844;&#24335;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;a) &#25506;&#32034;&#20004;&#20010;&#35745;&#21010;&#32773;&#21644;LTL&#30446;&#26631;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;b) &#35299;&#20915;Fetch&#26426;&#22120;&#20154;&#30340;&#39034;&#24207;&#38376;&#38145;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal logic can be used to formally specify autonomous agent goals, but synthesizing planners that guarantee goal satisfaction can be computationally prohibitive. This paper shows how to turn goals specified using a subset of finite trace Linear Temporal Logic (LTL) into a behavior tree (BT) that guarantees that successful traces satisfy the LTL goal. Useful LTL formulas for achievement goals can be derived using achievement-oriented task mission grammars, leading to missions made up of tasks combined using LTL operators. Constructing BTs from LTL formulas leads to a relaxed behavior synthesis problem in which a wide range of planners can implement the action nodes in the BT. Importantly, any successful trace induced by the planners satisfies the corresponding LTL formula. The usefulness of the approach is demonstrated in two ways: a) exploring the alignment between two planners and LTL goals, and b) solving a sequential key-door problem for a Fetch robot.
&lt;/p&gt;</description></item><item><title>&#35813;&#20070;&#20171;&#32461;&#20102;&#29992;&#39068;&#33394;&#21644;&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#38754;&#37096;&#24494;&#34920;&#24773;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;MATLAB&#32534;&#31243;&#23454;&#29616;&#12290;&#23427;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21040;&#19987;&#19994;&#35835;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#25551;&#36848;&#21644;&#21487;&#37325;&#29616;&#30340;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.06396</link><description>&lt;p&gt;
&#29992;&#39068;&#33394;&#21644;&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#38754;&#37096;&#24494;&#34920;&#24773;&#20998;&#26512;&#30340;&#20171;&#32461;&#65306;Matlab&#32534;&#31243;&#26041;&#27861;&#65288;&#31532;&#20108;&#29256;&#65292;2023&#65289;
&lt;/p&gt;
&lt;p&gt;
Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023). (arXiv:2307.06396v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#20070;&#20171;&#32461;&#20102;&#29992;&#39068;&#33394;&#21644;&#28145;&#24230;&#22270;&#20687;&#36827;&#34892;&#38754;&#37096;&#24494;&#34920;&#24773;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;MATLAB&#32534;&#31243;&#23454;&#29616;&#12290;&#23427;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#21040;&#19987;&#19994;&#35835;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#25551;&#36848;&#21644;&#21487;&#37325;&#29616;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#20070;&#35797;&#22270;&#36890;&#36807;MATLAB&#32534;&#31243;&#29615;&#22659;&#24341;&#20837;&#39068;&#33394;&#21644;&#28145;&#24230;&#22270;&#20687;&#22312;&#38754;&#37096;&#24494;&#34920;&#24773;&#35782;&#21035;&#65288;FMER&#65289;&#39046;&#22495;&#30340;&#28201;&#21644;&#20171;&#32461;&#12290;FMER&#26159;&#22270;&#20687;&#22788;&#29702;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#20998;&#26512;&#20027;&#39064;&#12290;&#22240;&#27492;&#65292;&#23427;&#38656;&#35201;&#29087;&#24713;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20854;&#20182;&#20027;&#39064;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#23383;&#22270;&#20687;&#22788;&#29702;&#12289;&#24515;&#29702;&#23398;&#31561;&#12290;&#25152;&#20197;&#65292;&#20026;AI&#39046;&#22495;&#30340;&#21021;&#23398;&#32773;&#21040;&#19987;&#19994;&#35835;&#32773;&#32534;&#20889;&#19968;&#26412;&#28085;&#30422;&#25152;&#26377;&#36825;&#20123;&#20027;&#39064;&#30340;&#20070;&#31821;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#26426;&#20250;&#65292;&#29978;&#33267;&#27809;&#26377;AI&#32972;&#26223;&#30340;&#35835;&#32773;&#20063;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#26412;&#29420;&#31435;&#30340;&#20171;&#32461;&#24615;&#20070;&#31821;&#65292;&#20197;&#29702;&#35770;&#25551;&#36848;&#30340;&#24418;&#24335;&#20171;&#32461;MFER&#20998;&#26512;&#39046;&#22495;&#65292;&#20379;&#27809;&#26377;&#22270;&#20687;&#22788;&#29702;&#32972;&#26223;&#30340;&#35835;&#32773;&#21442;&#32771;&#65292;&#24182;&#38468;&#26377;&#21487;&#37325;&#29616;&#30340;Matlab&#23454;&#38469;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;FMER&#20998;&#26512;&#21644;&#22312;&#25991;&#26412;&#20013;&#20351;&#29992;&#30340;MATLAB&#24211;&#30340;&#20219;&#20309;&#22522;&#26412;&#23450;&#20041;&#65292;&#20197;&#24110;&#21161;&#35835;&#32773;&#23558;&#23454;&#39564;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The book attempts to introduce a gentle introduction to the field of Facial Micro Expressions Recognition (FMER) using Color and Depth images, with the aid of MATLAB programming environment. FMER is a subset of image processing and it is a multidisciplinary topic to analysis. So, it requires familiarity with other topics of Artifactual Intelligence (AI) such as machine learning, digital image processing, psychology and more. So, it is a great opportunity to write a book which covers all of these topics for beginner to professional readers in the field of AI and even without having background of AI. Our goal is to provide a standalone introduction in the field of MFER analysis in the form of theorical descriptions for readers with no background in image processing with reproducible Matlab practical examples. Also, we describe any basic definitions for FMER analysis and MATLAB library which is used in the text, that helps final reader to apply the experiments in the real-world applicatio
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#36827;&#34892;&#20390;&#23519;&#38454;&#27573;&#30340;&#28183;&#36879;&#27979;&#35797;&#65292;&#33021;&#22815;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#24773;&#25253;&#25968;&#25454;&#65292;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.06391</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#30340;&#26377;&#25928;&#20390;&#23519;&#25216;&#26415;&#26368;&#22823;&#21270;&#28183;&#36879;&#27979;&#35797;&#30340;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;
Maximizing Penetration Testing Success with Effective Reconnaissance Techniques using ChatGPT. (arXiv:2307.06391v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06391
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#20390;&#23519;&#38454;&#27573;&#30340;&#28183;&#36879;&#27979;&#35797;&#65292;&#33021;&#22815;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#24773;&#25253;&#25968;&#25454;&#65292;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#21508;&#31181;&#38382;&#39064;&#25552;&#20379;&#38750;&#24120;&#35814;&#32454;&#30340;&#22238;&#31572;&#12290;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#29616;&#20195;&#30340;&#29616;&#35937;&#65292;&#36825;&#20010;&#24037;&#20855;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#30340;&#29992;&#20363;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;ChatGPT&#22312;&#24191;&#27867;&#30340;&#28508;&#22312;&#20027;&#39064;&#19978;&#20855;&#26377;&#22823;&#37327;&#20449;&#24687;&#65292;&#21487;&#20197;&#20026;&#35768;&#22810;&#20449;&#24687;&#23433;&#20840;&#29992;&#20363;&#22686;&#21152;&#20215;&#20540;&#65292;&#26080;&#35770;&#26159;&#20174;&#25928;&#29575;&#35282;&#24230;&#36824;&#26159;&#25552;&#20379;&#21478;&#19968;&#31181;&#23433;&#20840;&#20449;&#24687;&#26469;&#28304;&#65292;&#21487;&#29992;&#20110;&#24110;&#21161;&#20445;&#25252;&#32452;&#32455;&#30340;&#20114;&#32852;&#32593;&#21487;&#35775;&#38382;&#36164;&#20135;&#12290;&#20390;&#23519;&#38454;&#27573;&#26159;&#21463;&#30410;&#20110;ChatGPT&#30340;&#20449;&#24687;&#23433;&#20840;&#23454;&#36341;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#25506;&#32034;&#21644;&#35843;&#26597;ChatGPT&#22312;&#33719;&#24471;&#26377;&#20215;&#20540;&#30340;&#20390;&#23519;&#25968;&#25454;&#26041;&#38754;&#30340;&#29992;&#36884;&#12290;ChatGPT&#33021;&#22815;&#25552;&#20379;&#35768;&#22810;&#31867;&#22411;&#30340;&#26377;&#20851;&#30446;&#26631;&#23646;&#24615;&#30340;&#24773;&#25253;&#65292;&#21253;&#25324;&#20114;&#32852;&#32593;&#32593;&#32476;&#25299;&#25169;&#12289;&#32593;&#32476;&#26381;&#21153;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a generative pretrained transformer language model created using artificial intelligence implemented as chatbot which can provide very detailed responses to a wide variety of questions. As a very contemporary phenomenon, this tool has a wide variety of potential use cases that have yet to be explored. With the significant extent of information on a broad assortment of potential topics, ChatGPT could add value to many information security uses cases both from an efficiency perspective as well as to offer another source of security information that could be used to assist with securing Internet accessible assets of organizations. One information security practice that could benefit from ChatGPT is the reconnaissance phase of penetration testing. This research uses a case study methodology to explore and investigate the uses of ChatGPT in obtaining valuable reconnaissance data. ChatGPT is able to provide many types of intel regarding targeted properties which includes Internet 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#26495;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#21629;&#21517;&#32422;&#23450;&#65292;&#24378;&#21046;&#26576;&#20123;&#35859;&#35789;&#30340;&#23616;&#37096;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#27169;&#26495;&#39044;&#26399;&#32467;&#26524;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#21517;&#31216;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2307.06382</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#26495;
&lt;/p&gt;
&lt;p&gt;
Rethinking Answer Set Programming Templates. (arXiv:2307.06382v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#26495;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#21629;&#21517;&#32422;&#23450;&#65292;&#24378;&#21046;&#26576;&#20123;&#35859;&#35789;&#30340;&#23616;&#37096;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#27169;&#26495;&#39044;&#26399;&#32467;&#26524;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#21517;&#31216;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21629;&#20196;&#24335;&#32534;&#31243;&#20013;&#65292;&#39046;&#22495;&#39537;&#21160;&#35774;&#35745;&#26041;&#27861;&#35770;&#36890;&#36807;&#23558;&#39046;&#22495;&#20013;&#30340;&#19981;&#21464;&#37327;&#20307;&#29616;&#22312;&#20195;&#30721;&#20013;&#65292;&#24110;&#21161;&#24212;&#23545;&#36719;&#20214;&#24320;&#21457;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#30721;&#26356;&#28165;&#26224;&#12289;&#26356;&#23433;&#20840;&#65292;&#22240;&#20026;&#20219;&#20309;&#38544;&#21547;&#30340;&#20551;&#35774;&#37117;&#34987;&#31227;&#38500;&#65292;&#36716;&#32780;&#20351;&#29992;&#19981;&#21464;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#22833;&#36133;&#30340;&#24605;&#32500;&#26041;&#24335;&#24182;&#31435;&#21363;&#25253;&#21578;&#26410;&#39044;&#26399;&#30340;&#26465;&#20214;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31572;&#26696;&#38598;&#32534;&#31243;&#27169;&#26495;&#30340;&#27010;&#24565;&#65292;&#38500;&#20102;&#36981;&#24490;&#19981;&#37325;&#22797;&#33258;&#24049;&#21407;&#21017;&#22806;&#65292;&#36824;&#36890;&#36807;&#31616;&#21333;&#30340;&#21629;&#21517;&#32422;&#23450;&#24378;&#21046;&#26576;&#20123;&#35859;&#35789;&#30340;&#23616;&#37096;&#24615;&#12290;&#23616;&#37096;&#35859;&#35789;&#34987;&#26144;&#23556;&#21040;&#20027;&#27969;&#24341;&#25806;&#37319;&#29992;&#30340;&#36890;&#29992;&#20840;&#23616;&#21629;&#21517;&#31354;&#38388;&#65292;&#20351;&#29992;&#20840;&#23616;&#21807;&#19968;&#26631;&#35782;&#31526;&#26469;&#36991;&#20813;&#21517;&#31216;&#20914;&#31361;&#12290;&#36825;&#31181;&#26041;&#24335;&#65292;&#23616;&#37096;&#35859;&#35789;&#21487;&#20197;&#29992;&#20110;&#22312;&#21487;&#33021;&#20026;&#31354;&#30340;&#24212;&#29992;&#19978;&#19979;&#25991;&#20013;&#24378;&#21046;&#25191;&#34892;&#23545;&#27169;&#26495;&#39044;&#26399;&#32467;&#26524;&#30340;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#21463;&#20854;&#20182;&#21487;&#20197;&#28155;&#21152;&#21040;&#35813;&#19978;&#19979;&#25991;&#30340;&#35268;&#21017;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#24335;&#36716;&#25442;&#30340;&#27169;&#26495;&#24212;&#29992;&#21487;&#20197;&#34987;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In imperative programming, the Domain-Driven Design methodology helps in coping with the complexity of software development by materializing in code the invariants of a domain of interest. Code is cleaner and more secure because any implicit assumption is removed in favor of invariants, thus enabling a fail fast mindset and the immediate reporting of unexpected conditions. This article introduces a notion of template for Answer Set Programming that, in addition to the don't repeat yourself principle, enforces locality of some predicates by means of a simple naming convention. Local predicates are mapped to the usual global namespace adopted by mainstream engines, using universally unique identifiers to avoid name clashes. This way, local predicates can be used to enforce invariants on the expected outcome of a template in a possibly empty context of application, independently by other rules that can be added to such a context. Template applications transpiled this way can be processed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26080;&#27861;&#25512;&#27979;&#38271;&#26399;&#38477;&#35299;&#65307;&#19982;&#27492;&#30456;&#21453;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20934;&#30830;&#24230;&#31245;&#20302;&#65292;&#20294;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;</title><link>http://arxiv.org/abs/2307.06341</link><description>&lt;p&gt;
&#35780;&#20272;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessment of the suitability of degradation models for the planning of CCTV inspections of sewer pipes. (arXiv:2307.06341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06341
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#38477;&#35299;&#27169;&#22411;&#22312;&#25490;&#27745;&#31649;&#36947;CCTV&#26816;&#26597;&#35268;&#21010;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26080;&#27861;&#25512;&#27979;&#38271;&#26399;&#38477;&#35299;&#65307;&#19982;&#27492;&#30456;&#21453;&#65292;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20934;&#30830;&#24230;&#31245;&#20302;&#65292;&#20294;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#24378;&#19988;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#27745;&#31649;&#36947;&#30340;&#38477;&#35299;&#24341;&#36215;&#20102;&#37325;&#22823;&#30340;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#20581;&#24247;&#38382;&#39064;&#12290;&#36825;&#20123;&#36164;&#20135;&#30340;&#32500;&#25252;&#38656;&#35201;&#26377;&#32467;&#26500;&#21270;&#30340;&#35745;&#21010;&#26469;&#36827;&#34892;&#26816;&#26597;&#65292;&#32771;&#34385;&#21040;&#32467;&#26500;&#21644;&#29615;&#22659;&#29305;&#24449;&#20197;&#21450;&#20197;&#21069;&#26816;&#26597;&#25253;&#21578;&#30340;&#32467;&#26524;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#38477;&#35299;&#27169;&#22411;&#22312;&#35268;&#21010;&#26816;&#26597;&#26102;&#30340;&#36866;&#29992;&#24615;&#65292;&#32771;&#34385;&#20102;&#19977;&#20010;&#32500;&#24230;&#65306;&#20934;&#30830;&#24230;&#25351;&#26631;&#12289;&#33021;&#22815;&#20135;&#29983;&#38271;&#26399;&#38477;&#35299;&#26354;&#32447;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#38598;&#25104;&#27169;&#22411;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#25512;&#27979;&#31649;&#36947;&#30340;&#38271;&#26399;&#38477;&#35299;&#65292;&#32780;&#36923;&#36753;&#22238;&#24402;&#21017;&#25552;&#20379;&#20102;&#30053;&#20302;&#20934;&#30830;&#24230;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#24456;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#33268;&#30340;&#38477;&#35299;&#26354;&#32447;&#12290;&#36890;&#36807;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#35813;&#26041;&#27861;&#21450;&#20854;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
The degradation of sewer pipes poses significant economical, environmental and health concerns. The maintenance of such assets requires structured plans to perform inspections, which are more efficient when structural and environmental features are considered along with the results of previous inspection reports. The development of such plans requires degradation models that can be based on statistical and machine learning methods. This work proposes a methodology to assess their suitability to plan inspections considering three dimensions: accuracy metrics, ability to produce long-term degradation curves and explainability. Results suggest that although ensemble models yield the highest accuracy, they are unable to infer the long-term degradation of the pipes, whereas the Logistic Regression offers a slightly less accurate model that is able to produce consistent degradation curves with a high explainability. A use case is presented to demonstrate this methodology and the efficiency o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05921</link><description>&lt;p&gt;
&#38405;&#35835;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#26041;&#24335;&#65292;&#23601;&#20687;&#25918;&#23556;&#31185;&#21307;&#29983;&#19968;&#26679;
&lt;/p&gt;
&lt;p&gt;
Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#32454;&#24494;&#24046;&#24322;&#20851;&#27880;&#12289;&#25968;&#25454;&#20559;&#24046;&#21644;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#21253;&#21547;&#25918;&#23556;&#23398;&#25104;&#20687;&#30340;&#20016;&#23500;&#12289;&#31934;&#32454;&#25551;&#36848;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#19982;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#25551;&#36848;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#38750;&#24120;&#30456;&#20284;&#65292;&#20165;&#22312;&#30142;&#30149;&#21457;&#29983;&#30340;&#32454;&#24494;&#24046;&#24322;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#37492;&#20110;&#36825;&#20123;&#32454;&#24494;&#24046;&#24322;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#40723;&#21169;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#30142;&#30149;&#21457;&#29983;&#30340;&#24494;&#22937;&#21306;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#27425;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#20559;&#24046;&#30340;&#38382;&#39064;&#24456;&#20005;&#37325;&#12290;&#19981;&#20165;&#27491;&#24120;&#30149;&#20363;&#21344;&#25968;&#25454;&#38598;&#30340;&#22823;&#37096;&#20998;&#65292;&#36824;&#25551;&#32472;&#26377;&#30149;&#21464;&#21306;&#22495;&#30340;&#21477;&#23376;&#21482;&#21344;&#27573;&#33853;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26368;&#21518;&#65292;&#29983;&#25104;&#21307;&#23398;&#22270;&#20687;&#25253;&#21578;&#28041;&#21450;&#21040;&#38271;&#25991;&#26412;&#30340;&#29983;&#25104;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#21307;&#23398;&#30693;&#35782;&#30340;&#19987;&#19994;&#24615;&#21644;&#32463;&#39564;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#27492;&#31867;&#25253;&#21578;&#30340;&#38590;&#24230;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#30142;&#30149;&#20026;&#23548;&#21521;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated radiology report generation aims to generate radiology reports that contain rich, fine-grained descriptions of radiology imaging. Compared with image captioning in the natural image domain, medical images are very similar to each other, with only minor differences in the occurrence of diseases. Given the importance of these minor differences in the radiology report, it is crucial to encourage the model to focus more on the subtle regions of disease occurrence. Secondly, the problem of visual and textual data biases is serious. Not only do normal cases make up the majority of the dataset, but sentences describing areas with pathological changes also constitute only a small part of the paragraph. Lastly, generating medical image reports involves the challenge of long text generation, which requires more expertise and empirical training in medical knowledge. As a result, the difficulty of generating such reports is increased. To address these challenges, we propose a disease-ori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05766</link><description>&lt;p&gt;
Rad-ReStruct: &#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;VQA&#22522;&#20934;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Rad-ReStruct&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;hi-VQA&#65292;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#26469;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#23454;&#39564;&#35777;&#26126;hi-VQA&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#26159;&#25918;&#23556;&#31185;&#21307;&#29983;&#19982;&#20854;&#20182;&#21307;&#21153;&#20154;&#21592;&#20043;&#38388;&#27807;&#36890;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#20294;&#20854;&#21487;&#33021;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20854;&#20013;&#19968;&#31181;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#30340;&#26041;&#27861;&#26159;&#32467;&#26500;&#21270;&#25253;&#21578;&#65292;&#23427;&#27604;&#33258;&#30001;&#25991;&#26412;&#25253;&#21578;&#26356;&#33410;&#32422;&#26102;&#38388;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25253;&#21578;&#30340;&#30740;&#31350;&#26377;&#38480;&#65292;&#24182;&#19988;&#30446;&#21069;&#27809;&#26377;&#20844;&#24320;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Rad-ReStruct&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#12289;&#25353;&#23618;&#27425;&#25490;&#24207;&#30340;X&#20809;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#24418;&#24335;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#23558;&#32467;&#26500;&#21270;&#25253;&#21578;&#20219;&#21153;&#24314;&#27169;&#20026;&#20998;&#23618;&#35270;&#35273;&#38382;&#31572;(VQA)&#65292;&#24182;&#25552;&#20986;&#20102;hi-VQA&#65292;&#19968;&#31181;&#32771;&#34385;&#20808;&#21069;&#25552;&#38382;&#21644;&#22238;&#31572;&#30340;&#19978;&#19979;&#25991;&#20197;&#22635;&#20805;&#32467;&#26500;&#21270;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;hi-VQA&#22312;&#21307;&#23398;VQA&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology reporting is a crucial part of the communication between radiologists and other medical professionals, but it can be time-consuming and error-prone. One approach to alleviate this is structured reporting, which saves time and enables a more accurate evaluation than free-text reports. However, there is limited research on automating structured reporting, and no public benchmark is available for evaluating and comparing different methods. To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that provides fine-grained, hierarchically ordered annotations in the form of structured reports for X-Ray images. We model the structured reporting task as hierarchical visual question answering (VQA) and propose hi-VQA, a novel method that considers prior context in the form of previously asked questions and answers for populating a structured radiology report. Our experiments show that hi-VQA achieves competitive performance to the state-of-the-art on the medical VQA benc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03875</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20379;&#24212;&#38142;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Supply Chain Optimization. (arXiv:2307.03875v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24110;&#21161;&#35299;&#37322;&#21644;&#35299;&#35835;&#20379;&#24212;&#38142;&#20248;&#21270;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#20182;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#36890;&#36807;&#23450;&#37327;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#20225;&#19994;&#36816;&#33829;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#20449;&#20219;&#20248;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20379;&#24212;&#38142;&#25805;&#20316;&#28041;&#21450;&#21508;&#31181;&#22797;&#26434;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20379;&#24212;&#38142;&#21463;&#30410;&#20110;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20174;&#25163;&#21160;&#22788;&#29702;&#36807;&#28193;&#21040;&#33258;&#21160;&#21270;&#21644;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20225;&#19994;&#36816;&#33829;&#32773;&#20173;&#28982;&#38656;&#35201;&#33457;&#36153;&#22823;&#37327;&#31934;&#21147;&#26469;&#35299;&#37322;&#21644;&#35299;&#35835;&#20248;&#21270;&#32467;&#26524;&#32473;&#30456;&#20851;&#20154;&#22763;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#39072;&#35206;&#24615;&#25216;&#26415;&#22914;&#20309;&#24110;&#21161;&#24357;&#21512;&#20379;&#24212;&#38142;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#29702;&#35299;&#19982;&#20449;&#20219;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;\name{}&#30340;&#26694;&#26550;&#65292;&#23427;&#25509;&#21463;&#26222;&#36890;&#25991;&#26412;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20851;&#20110;&#24213;&#23618;&#20248;&#21270;&#32467;&#26524;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24182;&#27809;&#26377;&#25918;&#24323;&#26368;&#20808;&#36827;&#30340;&#32452;&#21512;&#20248;&#21270;&#25216;&#26415;&#65292;&#32780;&#26159;&#21033;&#29992;&#23427;&#26469;&#23450;&#37327;&#22320;&#22238;&#31572;&#20551;&#35774;&#24773;&#20917;&#65288;&#20363;&#22914;&#65292;&#22914;&#26524;&#25105;&#20204;&#20351;&#29992;&#20379;&#24212;&#21830;B&#32780;&#19981;&#26159;&#20379;&#24212;&#21830;A&#65292;&#25104;&#26412;&#20250;&#22914;&#20309;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain operations traditionally involve a variety of complex decision making problems. Over the last few decades, supply chains greatly benefited from advances in computation, which allowed the transition from manual processing to automation and cost-effective optimization. Nonetheless, business operators still need to spend substantial efforts in \emph{explaining} and interpreting the optimization outcomes to stakeholders. Motivated by the recent advances in Large Language Models (LLMs), we study how this disruptive technology can help bridge the gap between supply chain automation and human comprehension and trust thereof. We design \name{} -- a framework that accepts as input queries in plain text, and outputs insights about the underlying optimization outcomes. Our framework does not forgo the state-of-the-art combinatorial optimization technology, but rather leverages it to quantitatively answer what-if scenarios (e.g., how would the cost change if we used supplier B instead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;&#65292;&#28041;&#21450;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#12290;&#36890;&#36807;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#21046;&#23450;AI&#30456;&#20851;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.03198</link><description>&lt;p&gt;
&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A multilevel framework for AI governance. (arXiv:2307.03198v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#30340;AI&#27835;&#29702;&#26694;&#26550;&#65292;&#28041;&#21450;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#12290;&#36890;&#36807;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#21046;&#23450;AI&#30456;&#20851;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#30410;&#22788;&#24182;&#20943;&#36731;&#21487;&#33021;&#30340;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#20010;&#31526;&#21512;&#20262;&#29702;&#21644;&#22522;&#26412;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#27835;&#29702;&#26694;&#26550;&#12290;&#23613;&#31649;&#19968;&#20123;&#32452;&#32455;&#24050;&#32463;&#21457;&#24067;&#20102;&#21487;&#20449;AI&#30340;&#25351;&#21335;&#21644;&#20262;&#29702;&#26694;&#26550;&#65292;&#20294;&#22914;&#26524;&#27809;&#26377;&#19968;&#20010;&#35843;&#33410;&#30340;&#27835;&#29702;&#32467;&#26500;&#65292;&#36825;&#20123;&#20262;&#29702;&#21407;&#21017;&#23558;&#26080;&#27861;&#36716;&#21270;&#20026;&#23454;&#36341;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#27835;&#29702;&#26041;&#27861;&#65292;&#28041;&#21450;&#19977;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#32676;&#20307;&#65306;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#20844;&#27665;&#12290;&#25105;&#20204;&#36890;&#36807;&#20449;&#20219;&#30340;&#32500;&#24230;&#65288;&#22914;&#33021;&#21147;&#12289;&#35802;&#20449;&#21644;&#21892;&#24847;&#65289;&#26469;&#30740;&#31350;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#32467;&#21512;AI&#30340;&#27835;&#29702;&#27700;&#24179;&#21644;&#20449;&#20219;&#32500;&#24230;&#65292;&#25552;&#20379;&#20102;&#21487;&#29992;&#20110;&#36827;&#19968;&#27493;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#24182;&#20026;&#19982;AI&#30456;&#20851;&#30340;&#20844;&#20849;&#25919;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize the potential benefits and mitigate potential risks of AI, it is necessary to develop a framework of governance that conforms to ethics and fundamental human values. Although several organizations have issued guidelines and ethical frameworks for trustworthy AI, without a mediating governance structure, these ethical principles will not translate into practice. In this paper, we propose a multilevel governance approach that involves three groups of interdependent stakeholders: governments, corporations, and citizens. We examine their interrelationships through dimensions of trust, such as competence, integrity, and benevolence. The levels of governance combined with the dimensions of trust in AI provide practical insights that can be used to further enhance user experiences and inform public policy related to AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00470</link><description>&lt;p&gt;
PatternGPT: &#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation. (arXiv:2307.00470v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00470
&lt;/p&gt;
&lt;p&gt;
PatternGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#23454;&#29616;&#27169;&#24335;&#20849;&#20139;&#65292;&#26368;&#32456;&#36890;&#36807;&#25628;&#32034;&#39640;&#36136;&#37327;&#27169;&#24335;&#25351;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#29983;&#25104;&#27969;&#30021;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20851;&#38190;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PatternGPT&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#39537;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21462;&#33021;&#21147;&#29983;&#25104;&#20016;&#23500;&#22810;&#26679;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#20511;&#37492;&#32852;&#37030;&#23398;&#20064;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#23454;&#29616;&#20849;&#20139;&#20197;&#33719;&#21462;&#26356;&#22810;&#26679;&#30340;&#27169;&#24335;&#12290;&#26368;&#21518;&#65292;&#23427;&#20351;&#29992;&#21028;&#26029;&#26631;&#20934;&#21644;&#20248;&#21270;&#31639;&#27861;&#25628;&#32034;&#39640;&#36136;&#37327;&#30340;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#25628;&#32034;&#21040;&#30340;&#27169;&#24335;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12289;&#32467;&#21512;&#22806;&#37096;&#30693;&#35782;&#31561;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models(LLMS) have shown excellent text generation capabilities,capable of generating fluent responses for many downstream tasks. However,applying large language models to real-world critical tasks remains challenging due to their susceptibility to hallucinations and inability to directly use external knowledge. To address the above challenges,this paper proposes PatternGPT, a pattern-driven text generation framework for large language models. First,the framework utilizes the extraction capabilities of large language models to generate rich and diverse patterns and later draws on the idea of federated learning. Using multiple agents to achieve sharing to obtain more diverse patterns. Finally, it searches for high-quality patterns using judgment criteria and optimization algorithms and uses the searched patterns to guide the model for generation. This framework has the advantages of generating diversified patterns, protecting data privacy,combining external knowledge, and 
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2305.05222</link><description>&lt;p&gt;
FishRecGAN&#65306;&#29992;&#20110;&#40060;&#30524;&#22270;&#20687;&#30699;&#27491;&#21644;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#26631;&#23450;&#30340;&#31471;&#21040;&#31471;GAN&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05222
&lt;/p&gt;
&lt;p&gt;
FishRecGAN&#25552;&#20379;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#20854;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#65292;&#24182;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#30699;&#27491;&#40060;&#30524;&#22270;&#20687;&#24182;&#21516;&#26102;&#26657;&#20934;&#30456;&#26426;&#20869;&#21442;&#21644;&#30072;&#21464;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;&#20351;&#29992;Pix2Pix GAN&#21644;Wasserstein GAN&#65288;W-Pix2PixGAN&#65289;&#24320;&#21457;&#30340;Quick Image Rectification&#27169;&#22359;&#65292;&#20197;&#21450;&#20351;&#29992;CNN&#26550;&#26500;&#30340;Calibration&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#24555;&#36895;&#26657;&#27491;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#20998;&#36776;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#25668;&#20687;&#26426;&#22411;&#30417;&#25511;&#35774;&#22791;&#20013;&#30340;&#24658;&#23450;&#26631;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26631;&#23450;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Quick Image Rectification&#27169;&#22359;&#20013;&#36755;&#20986;&#30340;&#30452;&#32447;&#29305;&#24449;&#20316;&#20026;&#25351;&#23548;&#26679;&#26412;&#20256;&#36882;&#32473;Calibration&#27169;&#22359;&#65292;&#20197;&#23398;&#20064;&#26657;&#27491;&#21069;&#21518;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21644;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36879;&#35270;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#34920;&#29616;&#20986;&#20102;&#39640;&#20998;&#36776;&#29575;&#30340;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end deep learning approach to rectify fisheye images and simultaneously calibrate camera intrinsic and distortion parameters. Our method consists of two parts: a Quick Image Rectification Module developed with a Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a CNN architecture. Our Quick Rectification Network performs robust rectification with good resolution, making it suitable for constant calibration in camera-based surveillance equipment. To achieve high-quality calibration, we use the straightened output from the Quick Rectification Module as a guidance-like semantic feature map for the Calibration Module to learn the geometric relationship between the straightened feature and the distorted feature. We train and validate our method with a large synthesized dataset labeled with well-simulated parameters applied to a perspective image dataset. Our solution has achieved robust performance in high-resolution with a significant PSNR v
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FR3D&#30340;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20849;&#24418;&#26144;&#23556;&#23454;&#29616;&#23545;&#19981;&#21516;&#27178;&#25130;&#38754;&#30340;&#25289;&#20986;&#24335;&#19977;&#32500;&#29289;&#20307;&#21608;&#22260;&#30340;&#19977;&#32500;&#27969;&#22330;&#36827;&#34892;&#37325;&#26500;&#21644;&#21147;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2302.01802</link><description>&lt;p&gt;
FR3D: &#36890;&#36807;&#21327;&#21161;&#20849;&#24418;&#26144;&#23556;&#30340;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#19977;&#32500;&#27969;&#22330;&#37325;&#26500;&#21644;&#21147;&#20272;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FR3D: Three-dimensional Flow Reconstruction and Force Estimation for Unsteady Flows Around Extruded Bluff Bodies via Conformal Mapping Aided Convolutional Autoencoders. (arXiv:2302.01802v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FR3D&#30340;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20849;&#24418;&#26144;&#23556;&#23454;&#29616;&#23545;&#19981;&#21516;&#27178;&#25130;&#38754;&#30340;&#25289;&#20986;&#24335;&#19977;&#32500;&#29289;&#20307;&#21608;&#22260;&#30340;&#19977;&#32500;&#27969;&#22330;&#36827;&#34892;&#37325;&#26500;&#21644;&#21147;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#27969;&#20307;&#21160;&#21147;&#23398;&#23454;&#39564;&#20013;&#65292;&#21482;&#33021;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#20256;&#24863;&#22120;&#20301;&#32622;&#27979;&#37327;&#36895;&#24230;&#21644;&#21387;&#21147;&#31561;&#21464;&#37327;&#65292;&#23545;&#20110;&#19968;&#20123;&#20108;&#32500;&#24179;&#38754;&#25110;&#23567;&#22411;&#19977;&#32500;&#27969;&#22330;&#65292;&#30693;&#36947;&#23436;&#25972;&#30340;&#22330;&#26159;&#29702;&#35299;&#35768;&#22810;&#27969;&#21160;&#21160;&#21147;&#23398;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28145;&#24230;&#23398;&#20064;&#37325;&#26500;&#31232;&#30095;&#27979;&#37327;&#24471;&#20986;&#30340;&#23436;&#25972;&#27969;&#22330;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#37325;&#35201;&#30740;&#31350;&#20852;&#36259;&#65292;&#20316;&#20026;&#20811;&#26381;&#36825;&#31181;&#38480;&#21046;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#36825;&#19968;&#20219;&#21153;&#34987;&#31216;&#20026;&#27969;&#22330;&#37325;&#26500;&#65288;FR&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;FR3D&#65292;&#33021;&#22815;&#23545;&#19981;&#21516;&#27178;&#25130;&#38754;&#30340;&#25289;&#20986;&#24335;&#19977;&#32500;&#29289;&#20307;&#21608;&#22260;&#30340;&#19977;&#32500;&#27969;&#22330;&#36827;&#34892;FR&#12290;&#21019;&#26032;&#30340;&#26144;&#23556;&#26041;&#27861;&#23558;&#22810;&#20010;&#27969;&#20307;&#22495;&#26144;&#23556;&#21040;&#19968;&#20010;&#29615;&#29366;&#21306;&#22495;&#65292;&#20351;&#24471;FR3D&#33021;&#22815;&#23558;&#20854;&#24615;&#33021;&#25512;&#24191;&#21040;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#36935;&#21040;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#26126;&#30830;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical fluid dynamics experiments, measuring variables such as velocity and pressure is possible only at a limited number of sensor locations, \textcolor{black}{for a few two-dimensional planes, or for a small 3D domain in the flow}. However, knowledge of the full fields is necessary to understand the dynamics of many flows. Deep learning reconstruction of full flow fields from sparse measurements has recently garnered significant research interest, as a way of overcoming this limitation. This task is referred to as the flow reconstruction (FR) task. In the present study, we propose a convolutional autoencoder based neural network model, dubbed FR3D, which enables FR to be carried out for three-dimensional flows around extruded 3D objects with different cross-sections. An innovative mapping approach, whereby multiple fluid domains are mapped to an annulus, enables FR3D to generalize its performance to objects not encountered during training. We conclusively demonstrate this 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2301.03044</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;Transformers&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformers in Reinforcement Learning. (arXiv:2301.03044v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#12289;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#30340;&#20027;&#23548;&#31070;&#32463;&#26550;&#26500;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39046;&#22495;&#20013;&#20063;&#20986;&#29616;&#20102;&#31867;&#20284;&#30340;&#20351;&#29992;Transformers&#30340;&#28526;&#27969;&#65292;&#20294;&#38754;&#20020;&#30528;RL&#30340;&#29305;&#27530;&#35774;&#35745;&#36873;&#25321;&#21644;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;Transformers&#22312;RL&#20013;&#30340;&#21457;&#23637;&#23578;&#26410;&#34987;&#20805;&#20998;&#25581;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#22312;RL&#20013;&#20351;&#29992;Transformers&#30340;&#21160;&#26426;&#21644;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#35752;&#35770;&#20102;&#27599;&#20010;&#23376;&#39046;&#22495;&#65292;&#24182;&#24635;&#32467;&#20102;&#26410;&#26469;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2212.00679</link><description>&lt;p&gt;
&#26410;&#30693;&#21160;&#24577;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#25511;&#21046;&#22120;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Formal Controller Synthesis for Markov Jump Linear Systems with Uncertain Dynamics. (arXiv:2212.00679v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#65292;&#23545;&#20110;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#25511;&#21046;&#22120;&#30340;&#33258;&#21160;&#21270;&#21512;&#25104;&#21487;&#20197;&#30830;&#20445;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#29305;&#24615;&#21644;&#38543;&#26426;&#25110;&#26410;&#30693;&#30340;&#34892;&#20026;&#20351;&#24471;&#21512;&#25104;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#39532;&#23572;&#21487;&#22827;&#36339;&#21464;&#32447;&#24615;&#31995;&#32479;&#65288;MJLS&#65289;&#30340;&#25511;&#21046;&#22120;&#65292;&#36825;&#26159;&#19968;&#31867;&#31163;&#25955;&#26102;&#38047;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#28385;&#36275;&#27010;&#29575;&#35745;&#31639;&#26641;&#36923;&#36753;&#65288;PCTL&#65289;&#20844;&#24335;&#12290;&#19968;&#20010;MJLS&#30001;&#19968;&#32452;&#26377;&#38480;&#30340;&#38543;&#26426;&#32447;&#24615;&#21160;&#24577;&#21644;&#36825;&#20123;&#21160;&#24577;&#20043;&#38388;&#30340;&#31163;&#25955;&#36339;&#21464;&#32452;&#25104;&#65292;&#36825;&#20123;&#36339;&#21464;&#30001;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26469;&#31649;&#29702;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#26410;&#30693;&#25110;&#24050;&#30693;&#20294;&#23384;&#22312;&#19968;&#23450;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#26377;&#38480;&#29366;&#24577;&#25277;&#35937;&#65292;&#25429;&#25417;&#20102;MJLS&#30340;&#31163;&#25955;&#65288;&#27169;&#24335;&#36339;&#36291;&#65289;&#21644;&#36830;&#32493;&#65288;&#38543;&#26426;&#32447;&#24615;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25277;&#35937;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21306;&#38388;MDP&#65288;iMDP&#65289;&#65292;&#28982;&#21518;&#35745;&#31639;&#20102;&#29366;&#24577;&#36716;&#31227;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated synthesis of provably correct controllers for cyber-physical systems is crucial for deployment in safety-critical scenarios. However, hybrid features and stochastic or unknown behaviours make this problem challenging. We propose a method for synthesising controllers for Markov jump linear systems (MJLSs), a class of discrete-time models for cyber-physical systems, so that they certifiably satisfy probabilistic computation tree logic (PCTL) formulae. An MJLS consists of a finite set of stochastic linear dynamics and discrete jumps between these dynamics that are governed by a Markov decision process (MDP). We consider the cases where the transition probabilities of this MDP are either known up to an interval or completely unknown. Our approach is based on a finite-state abstraction that captures both the discrete (mode-jumping) and continuous (stochastic linear) behaviour of the MJLS. We formalise this abstraction as an interval MDP (iMDP) for which we compute intervals of tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;Continual-Dreamer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15944</link><description>&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#19990;&#30028;&#27169;&#22411;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;Continual-Dreamer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20123;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#26234;&#33021;&#20307;&#38754;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#24773;&#20917;&#12290;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#36830;&#32493;&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#23545;&#24615;&#33021;&#12289;&#36951;&#24536;&#21644;&#36801;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#30340;&#21508;&#31181;&#24314;&#27169;&#36873;&#39033;&#30340;&#24314;&#35758;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#31216;&#20026;Continual-Dreamer&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#25506;&#32034;&#12290;Continual-Dreamer&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;Minigrid&#21644;Minihack&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.
&lt;/p&gt;</description></item><item><title>Control Transformer&#26159;&#19968;&#31181;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.06407</link><description>&lt;p&gt;
&#25511;&#21046;&#21464;&#21387;&#22120;&#65306;&#36890;&#36807;PRM-Guided Return-Conditioned&#24207;&#21015;&#24314;&#27169;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling. (arXiv:2211.06407v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06407
&lt;/p&gt;
&lt;p&gt;
Control Transformer&#26159;&#19968;&#31181;&#36890;&#36807;&#37319;&#26679;&#35268;&#21010;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#25104;&#21151;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#20219;&#21153;&#65292;&#22914;&#23548;&#33322;&#65292;&#23545;&#20110;&#25104;&#21151;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#25552;&#20986;&#20102;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#22312;&#24050;&#30693;&#29615;&#22659;&#19979;&#65292;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#21487;&#20197;&#22312;&#19981;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#31283;&#20581;&#22320;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Control Transformer&#65292;&#36890;&#36807;&#30001;&#22522;&#20110;&#37319;&#26679;&#30340;&#27010;&#29575;&#22320;&#22270;&#65288;PRM&#65289;&#35268;&#21010;&#22120;&#24341;&#23548;&#30340;&#20302;&#23618;&#31574;&#30053;&#24314;&#27169;&#36820;&#22238;&#26465;&#20214;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#20165;&#23616;&#37096;&#20449;&#24687;&#35299;&#20915;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#23548;&#33322;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#37096;&#20998;&#35266;&#23519;&#30340;&#36855;&#23467;&#23548;&#33322;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Ant&#65292;Point&#21644;Humanoid&#30340;MuJoCo&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Control Transformer&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#36855;&#23467;&#20013;&#23548;&#33322;&#24182;&#36716;&#31227;&#21040;&#26410;&#30693;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#24046;&#20998;&#39537;&#21160;&#26426;&#22120;&#20154;&#65288;Turtlebot3&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#35266;&#27979;&#19979;&#30340;&#38646;&#26679;&#26412;sim2real&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning long-horizon tasks such as navigation has presented difficult challenges for successfully applying reinforcement learning to robotics. From another perspective, under known environments, sampling-based planning can robustly find collision-free paths in environments without learning. In this work, we propose Control Transformer that models return-conditioned sequences from low-level policies guided by a sampling-based Probabilistic Roadmap (PRM) planner. We demonstrate that our framework can solve long-horizon navigation tasks using only local information. We evaluate our approach on partially-observed maze navigation with MuJoCo robots, including Ant, Point, and Humanoid. We show that Control Transformer can successfully navigate through mazes and transfer to unknown environments. Additionally, we apply our method to a differential drive robot (Turtlebot3) and show zero-shot sim2real transfer under noisy observations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#24615;&#31574;&#30053;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#25112;&#32988;&#20102;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI KataGo&#65292;&#25581;&#31034;&#20102;&#20854;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#36229;&#32423;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2211.00241</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#31574;&#30053;&#25112;&#32988;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI
&lt;/p&gt;
&lt;p&gt;
Adversarial Policies Beat Superhuman Go AIs. (arXiv:2211.00241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00241
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#31574;&#30053;&#25915;&#20987;&#65292;&#25105;&#20204;&#25104;&#21151;&#25112;&#32988;&#20102;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI KataGo&#65292;&#25581;&#31034;&#20102;&#20854;&#26680;&#24515;&#24369;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#36229;&#32423;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#24615;&#31574;&#30053;&#26469;&#25915;&#20987;&#26368;&#20808;&#36827;&#30340;&#22260;&#26827;AI&#31995;&#32479;KataGo&#65292;&#22312;&#36229;&#20154;&#31867;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#36229;&#36807;97%&#30340;&#32988;&#29575;&#12290;&#25105;&#20204;&#30340;&#23545;&#25163;&#24182;&#19981;&#26159;&#36890;&#36807;&#20986;&#33394;&#22320;&#19979;&#22260;&#26827;&#26469;&#33719;&#32988;&#65292;&#32780;&#26159;&#36890;&#36807;&#35825;&#20351;KataGo&#29359;&#19979;&#20005;&#37325;&#22833;&#35823;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#38646;&#25439;&#32791;&#22320;&#20256;&#36755;&#32473;&#20854;&#20182;&#36229;&#32423;&#20154;&#31867;&#32423;&#22260;&#26827;AI&#65292;&#24182;&#19988;&#23545;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#65292;&#20182;&#20204;&#21487;&#20197;&#22312;&#27809;&#26377;&#31639;&#27861;&#36741;&#21161;&#30340;&#24773;&#20917;&#19979;&#23454;&#26045;&#36825;&#31181;&#25915;&#20987;&#26469;&#25345;&#32493;&#25112;&#32988;&#36229;&#32423;&#20154;&#31867;&#32423;AI&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#25581;&#31034;&#20102;KataGo&#30340;&#26680;&#24515;&#24369;&#28857;&#65292;&#21363;&#20351;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;KataGo&#20195;&#29702;&#20063;&#26080;&#27861;&#38450;&#24481;&#25105;&#20204;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#36229;&#32423;&#20154;&#31867;&#32423;&#30340;AI&#31995;&#32479;&#20063;&#21487;&#33021;&#23384;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We attack the state-of-the-art Go-playing AI system KataGo by training adversarial policies against it, achieving a &gt;97% win rate against KataGo running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick KataGo into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing AIs, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman AIs. The core vulnerability uncovered by our attack persists even in KataGo agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman AI systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/.
&lt;/p&gt;</description></item><item><title>RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.14905</link><description>&lt;p&gt;
RulE: &#20351;&#29992;&#35268;&#21017;&#23884;&#20837;&#30340;&#31070;&#32463;-&#31526;&#21495;&#30693;&#35782;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
RulE: Neural-Symbolic Knowledge Graph Reasoning with Rule Embedding. (arXiv:2210.14905v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14905
&lt;/p&gt;
&lt;p&gt;
RulE&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#32479;&#19968;&#34920;&#31034;&#22312;&#19968;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#25552;&#21319;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#21516;&#26102;&#65292;RulE&#27880;&#20837;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#65292;&#20351;&#24471;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20063;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25512;&#29702;&#23545;&#20110;&#30693;&#35782;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#26377;&#21407;&#21017;&#23450;&#20301;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;RulE&#65288;&#20195;&#34920;&#35268;&#21017;&#23884;&#20837;&#65289;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#22686;&#24378;KG&#25512;&#29702;&#12290;&#19982;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;RulE&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#32852;&#21512;&#34920;&#31034;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#29616;&#26377;&#19977;&#20803;&#32452;&#21644;&#19968;&#38454;&#35268;&#21017;&#20013;&#23398;&#20064;&#35268;&#21017;&#23884;&#20837;&#12290;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#35268;&#21017;&#23884;&#20837;&#65292;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#35268;&#21017;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#21453;&#26144;&#20854;&#19982;&#35266;&#23519;&#21040;&#30340;&#19977;&#20803;&#32452;&#30340;&#19968;&#33268;&#24615;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20197;&#36719;&#26041;&#24335;&#36827;&#34892;&#36923;&#36753;&#35268;&#21017;&#25512;&#29702;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36923;&#36753;&#30340;&#33030;&#24369;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;RulE&#23558;&#20808;&#21069;&#30340;&#36923;&#36753;&#35268;&#21017;&#20449;&#24687;&#27880;&#20837;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20016;&#23500;&#21644;&#35268;&#33539;&#21270;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#12290;&#36825;&#20063;&#20351;&#24471;&#20165;&#20351;&#29992;KGE&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;RulE&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#19988;&#22312;&#23454;&#39564;&#19978;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \textbf{entities}, \textbf{relations} and \textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10081</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#25955;&#22411;Soft Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#65288;SAC&#65289;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;SAC&#26041;&#27861;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#20102;&#22312;&#31163;&#25955;&#35774;&#32622;&#19979;&#20854;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29109;&#24809;&#32602;&#21644;&#20855;&#26377;Q-clip&#30340;&#21452;&#24179;&#22343;Q-learning&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;Atari&#28216;&#25103;&#21644;&#19968;&#20010;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#22312;&#20869;&#30340;&#20856;&#22411;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;
&lt;p&gt;
We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>TRUST-LAPSE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#31354;&#38388;&#23884;&#20837;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#36317;&#31163;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#20197;&#21450;&#39034;&#24207;&#30456;&#20851;&#24615;&#20559;&#24046;&#26469;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;</title><link>http://arxiv.org/abs/2207.11290</link><description>&lt;p&gt;
TRUST-LAPSE&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#30417;&#25511;&#19981;&#20449;&#20219;&#35780;&#20998;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRUST-LAPSE: An Explainable and Actionable Mistrust Scoring Framework for Model Monitoring. (arXiv:2207.11290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11290
&lt;/p&gt;
&lt;p&gt;
TRUST-LAPSE&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#28508;&#31354;&#38388;&#23884;&#20837;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#36317;&#31163;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#20197;&#21450;&#39034;&#24207;&#30456;&#20851;&#24615;&#20559;&#24046;&#26469;&#23454;&#29616;&#23545;&#27169;&#22411;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30417;&#27979;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30830;&#23450;&#20309;&#26102;&#24212;&#35813;&#20449;&#20219;&#23427;&#20204;&#30340;&#39044;&#27979;&#21644;&#20309;&#26102;&#19981;&#24212;&#35813;&#20449;&#20219;&#23427;&#20204;&#30340;&#39044;&#27979;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#26159;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRUST-LAPSE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#27169;&#22411;&#30417;&#25511;&#30340;&#8220;&#19981;&#20449;&#20219;&#8221;&#35780;&#20998;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#28508;&#31354;&#38388;&#23884;&#20837;&#26469;&#35780;&#20272;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;a&#65289;&#25105;&#20204;&#30340;&#28508;&#31354;&#38388;&#19981;&#20449;&#20219;&#35780;&#20998;&#20351;&#29992;&#28508;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#65288;&#39532;&#27663;&#36317;&#31163;&#65289;&#21644;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#26469;&#20272;&#35745;&#19981;&#20449;&#20219;&#24230;&#65292;&#65288;b&#65289;&#25105;&#20204;&#30340;&#39034;&#24207;&#19981;&#20449;&#20219;&#35780;&#20998;&#20351;&#29992;&#38750;&#21442;&#25968;&#12289;&#28369;&#21160;&#31383;&#21475;&#31639;&#27861;&#26469;&#30830;&#23450;&#36807;&#21435;&#36755;&#20837;&#34920;&#31034;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#24615;&#20559;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#21487;&#25805;&#20316;&#30340;&#36830;&#32493;&#30417;&#25511;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#23545;TRUST-LAPSE&#36827;&#34892;&#35780;&#20272;&#65306;&#65288;1&#65289;&#20998;&#24067;&#20559;&#31227;&#36755;&#20837;&#26816;&#27979;&#65292;&#65288;2&#65289;&#25968;&#25454;&#28418;&#31227;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;-&#38899;&#39057;...
&lt;/p&gt;
&lt;p&gt;
Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a "mistrust" scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection, and (2) data drift detection. We evaluate across diverse domains - audio 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.04827</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#31867;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24518;&#36215;&#22810;&#24180;&#26410;&#35265;&#30340;&#26379;&#21451;&#30340;&#38754;&#23380;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20320;&#20204;&#20598;&#28982;&#30456;&#36935;&#65292;&#20320;&#20204;&#20250;&#36731;&#26131;&#22320;&#35748;&#20986;&#24444;&#27492;&#12290;&#29983;&#29289;&#35760;&#24518;&#37197;&#22791;&#20102;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#23384;&#20648;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#26029;&#32454;&#33410;&#20197;&#21305;&#37197;&#24863;&#30693;&#12290;Willshaw Memory&#26159;&#19968;&#31181;&#29992;&#20110;&#30382;&#23618;&#35745;&#31639;&#30340;&#31616;&#21333;&#25277;&#35937;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#35760;&#24518;&#30340;&#26426;&#21046;&#12290;&#20351;&#29992;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35270;&#35273;&#27169;&#24335;&#30340;&#31232;&#30095;&#32534;&#30721;&#35268;&#21017;[34]&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#26694;&#26550;&#25193;&#23637;&#20102;&#22522;&#26412;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#35760;&#24518;&#21516;&#26102;&#23384;&#20648;&#27599;&#20010;&#27169;&#24335;&#30340;&#20960;&#31181;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#25110;&#25991;&#26412;&#65289;&#12290;&#35757;&#32451;&#21518;&#65292;&#24403;&#21482;&#24863;&#30693;&#21040;&#23376;&#38598;&#26102;&#65292;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#32534;&#30721;&#22120;-&#35760;&#24518;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#35774;&#35745;&#28385;&#36275;&#21311;&#21517;&#24615;&#12289;&#20013;&#31435;&#24615;&#21644;&#21487;&#35299;&#20915;&#24615;&#30340;&#26368;&#20339;&#25237;&#31080;&#35268;&#21017;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20844;&#24179;&#25913;&#36827;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#23545;&#28385;&#36275;&#20004;&#20010;&#20844;&#29702;&#30340;&#20219;&#20309;&#19981;&#30830;&#23450;&#35268;&#21017;&#37117;&#33021;&#26368;&#20248;&#22320;&#20445;&#25345;&#21311;&#21517;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14838</link><description>&lt;p&gt;
&#26368;&#20844;&#24179;&#30340;&#25237;&#31080;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Most Equitable Voting Rules. (arXiv:2205.14838v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14838
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#35774;&#35745;&#28385;&#36275;&#21311;&#21517;&#24615;&#12289;&#20013;&#31435;&#24615;&#21644;&#21487;&#35299;&#20915;&#24615;&#30340;&#26368;&#20339;&#25237;&#31080;&#35268;&#21017;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20844;&#24179;&#25913;&#36827;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#23545;&#28385;&#36275;&#20004;&#20010;&#20844;&#29702;&#30340;&#20219;&#20309;&#19981;&#30830;&#23450;&#35268;&#21017;&#37117;&#33021;&#26368;&#20248;&#22320;&#20445;&#25345;&#21311;&#21517;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#65292;&#21311;&#21517;&#24615;&#65288;&#25152;&#26377;&#20195;&#29702;&#20154;&#34987;&#24179;&#31561;&#23545;&#24453;&#65289;&#21644;&#20013;&#31435;&#24615;&#65288;&#25152;&#26377;&#26367;&#20195;&#26041;&#26696;&#34987;&#24179;&#31561;&#23545;&#24453;&#65289;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#20844;&#24179;&#24615;&#21644;&#20844;&#27491;&#24615;&#30340;&#8220;&#26368;&#20302;&#35201;&#27714;&#8221;&#21644;&#8220;&#26080;&#20105;&#35758;&#8221;&#30340;&#20844;&#29702;&#12290;&#28982;&#32780;&#65292;ANR&#19981;&#21487;&#33021;&#24615;&#8212;&#8212;&#27809;&#26377;&#28385;&#36275;&#21311;&#21517;&#24615;&#12289;&#20013;&#31435;&#24615;&#21644;&#21487;&#35299;&#20915;&#24615;&#65288;&#24635;&#26159;&#36873;&#25321;&#19968;&#20010;&#33719;&#32988;&#32773;&#65289;&#30340;&#25237;&#31080;&#35268;&#21017;&#8212;&#8212;&#29978;&#33267;&#22312;&#21482;&#26377;&#20004;&#20010;&#26367;&#20195;&#26041;&#26696;&#21644;&#20004;&#20010;&#20195;&#29702;&#20154;&#30340;&#31616;&#21333;&#24773;&#20917;&#19979;&#20063;&#25104;&#31435;&#12290;&#22914;&#20309;&#35774;&#35745;&#28385;&#36275;&#21311;&#21517;&#24615;&#12289;&#20013;&#31435;&#24615;&#21644;&#21487;&#35299;&#20915;&#24615;&#30340;&#26368;&#20339;&#25237;&#31080;&#35268;&#21017;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#31181;&#24191;&#27867;&#30340;&#20559;&#22909;&#21644;&#20915;&#31574;&#30340;&#26368;&#20339;&#35774;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#25490;&#21517;&#21015;&#34920;&#21644;&#22996;&#21592;&#20250;&#12290;&#25105;&#20204;&#30340;&#27010;&#24565;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#30340;&#21644;&#24378;&#22823;&#30340;&#26368;&#20844;&#24179;&#25913;&#36827;&#30340;&#27010;&#24565;&#65292;&#23427;&#23545;&#28385;&#36275;&#20004;&#20010;&#20844;&#29702;&#30340;&#20219;&#20309;&#19981;&#30830;&#23450;&#35268;&#21017;&#37117;&#33021;&#26368;&#20248;&#22320;&#20445;&#25345;&#21311;&#21517;&#24615;&#21644;&#20013;&#31435;&#24615;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;ANR&#19981;&#21487;&#33021;&#24615;&#22312;&#26222;&#36941;&#24773;&#20917;&#19979;&#25104;&#31435;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In social choice theory, anonymity (all agents being treated equally) and neutrality (all alternatives being treated equally) are widely regarded as ``minimal demands'' and ``uncontroversial'' axioms of equity and fairness. However, the ANR impossibility -- there is no voting rule that satisfies anonymity, neutrality, and resolvability (always choosing one winner) -- holds even in the simple setting of two alternatives and two agents. How to design voting rules that optimally satisfy anonymity, neutrality, and resolvability remains an open question.  We address the optimal design question for a wide range of preferences and decisions that include ranked lists and committees. Our conceptual contribution is a novel and strong notion of most equitable refinements that optimally preserves anonymity and neutrality for any irresolute rule that satisfies the two axioms. Our technical contributions are twofold. First, we characterize the conditions for the ANR impossibility to hold under gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#27927;&#29260;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22359;&#24182;&#24212;&#29992;&#27927;&#29260;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26435;&#37325;&#20849;&#20139;&#21644;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.02819</link><description>&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#30340;&#22359;&#27927;&#29260;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Block shuffling learning for Deepfake Detection. (arXiv:2202.02819v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#27927;&#29260;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#20998;&#20026;&#22359;&#24182;&#24212;&#29992;&#27927;&#29260;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26435;&#37325;&#20849;&#20139;&#21644;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26410;&#30693;&#30340;&#20266;&#36896;&#26041;&#27861;&#21644;&#24120;&#35265;&#30340;&#36716;&#25442;&#65288;&#22914;&#35843;&#25972;&#22823;&#23567;&#21644;&#27169;&#31946;&#65289;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#36935;&#21040;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#27979;&#35797;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#20559;&#24046;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#36807;&#25311;&#21512;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22359;&#27927;&#29260;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#21010;&#20998;&#20026;&#22359;&#24182;&#24212;&#29992;&#22359;&#20869;&#21644;&#22359;&#38388;&#27927;&#29260;&#25216;&#26415;&#65292;&#38388;&#25509;&#23454;&#29616;&#20102;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#24615;&#25439;&#22833;&#31639;&#27861;&#26469;&#20943;&#36731;&#27927;&#29260;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24674;&#22797;&#22359;&#30340;&#31354;&#38388;&#24067;&#23616;&#65292;&#20197;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfake detection methods based on convolutional neural networks (CNN) have demonstrated high accuracy. \textcolor{black}{However, these methods often suffer from decreased performance when faced with unknown forgery methods and common transformations such as resizing and blurring, resulting in deviations between training and testing domains.} This phenomenon, known as overfitting, poses a significant challenge. To address this issue, we propose a novel block shuffling regularization method. Firstly, our approach involves dividing the images into blocks and applying both intra-block and inter-block shuffling techniques. This process indirectly achieves weight-sharing across different dimensions. Secondly, we introduce an adversarial loss algorithm to mitigate the overfitting problem induced by the shuffling noise. Finally, we restore the spatial layout of the blocks to capture the semantic associations among them. Extensive experiments validate the effectiveness of our proposed method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#24182;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.07372</link><description>&lt;p&gt;
&#26410;&#26469;&#23398;&#20064;&#65306;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#21512;&#29702;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Prospective Learning: Principled Extrapolation to the Future. (arXiv:2201.07372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26410;&#26469;&#20449;&#24687;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#24182;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26159;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#26356;&#26032;&#20915;&#31574;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#26410;&#26469;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#24120;&#24120;&#22312;&#20551;&#35774;&#26410;&#26469;&#19982;&#36807;&#21435;&#30340;&#20998;&#24067;&#30456;&#21516;&#25110;&#20250;&#20197;&#23545;&#25239;&#30340;&#26041;&#24335;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;&#20294;&#26159;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35768;&#22810;&#38382;&#39064;&#26469;&#35828;&#65292;&#21487;&#33021;&#36807;&#20110;&#20048;&#35266;&#25110;&#24754;&#35266;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#22312;&#22810;&#20010;&#26102;&#31354;&#23610;&#24230;&#19978;&#28436;&#21464;&#65292;&#20855;&#26377;&#37096;&#20998;&#21487;&#39044;&#27979;&#30340;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#23398;&#20064;&#38382;&#39064;&#65292;&#23558;&#20854;&#32858;&#28966;&#20110;&#21160;&#24577;&#26410;&#26469;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#26410;&#26469;&#26159;&#37096;&#20998;&#21487;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#25512;&#27979;&#26576;&#20123;&#20219;&#21153;&#24207;&#21015;&#22312;&#22238;&#39038;&#24615;&#23398;&#20064;&#20013;&#19981;&#21487;&#23398;&#20064;&#65288;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#22266;&#23450;&#65289;&#65292;&#20294;&#22312;&#21069;&#30651;&#24615;&#23398;&#20064;&#20013;&#21487;&#23398;&#20064;&#65288;&#20854;&#20013;&#20998;&#24067;&#21487;&#33021;&#21160;&#24577;&#65289;&#65292;&#36825;&#34920;&#26126;&#21069;&#30651;&#24615;&#23398;&#20064;&#22312;&#26412;&#36136;&#19978;&#27604;&#22238;&#39038;&#24615;&#23398;&#20064;&#26356;&#22256;&#38590;&#12290;&#25105;&#20204;&#35748;&#20026;&#21069;&#30651;&#24615;&#23398;&#20064;&#26356;&#20934;&#30830;&#22320;&#25551;&#36848;&#20102;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning is a process which can update decision rules, based on past experience, such that future performance improves. Traditionally, machine learning is often evaluated under the assumption that the future will be identical to the past in distribution or change adversarially. But these assumptions can be either too optimistic or pessimistic for many problems in the real world. Real world scenarios evolve over multiple spatiotemporal scales with partially predictable dynamics. Here we reformulate the learning problem to one that centers around this idea of dynamic futures that are partially learnable. We conjecture that certain sequences of tasks are not retrospectively learnable (in which the data distribution is fixed), but are prospectively learnable (in which distributions may be dynamic), suggesting that prospective learning is more difficult in kind than retrospective learning. We argue that prospective learning more accurately characterizes many real world problems that (1) cur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#36807;&#20256;&#25773;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#23545;&#35937;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;2D&#26059;&#36716;&#30340;&#29087;&#24713;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2109.13445</link><description>&lt;p&gt;
&#26032;&#39062;&#26041;&#21521;&#19978;&#23545;&#35937;&#27867;&#21270;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations. (arXiv:2109.13445v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.13445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#36890;&#36807;&#20256;&#25773;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#29087;&#24713;&#23545;&#35937;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;2D&#26059;&#36716;&#30340;&#29087;&#24713;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#30340;&#33021;&#21147;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;DNNs&#33021;&#22815;&#36890;&#36807;&#20256;&#25773;&#20174;&#22810;&#20010;&#35270;&#28857;&#35266;&#23519;&#21040;&#30340;&#29087;&#24713;&#23545;&#35937;&#33719;&#24471;&#30340;&#26041;&#21521;&#19981;&#21464;&#24615;&#26469;&#27867;&#21270;&#21040;&#26032;&#39062;&#26041;&#21521;&#19978;&#30340;&#23545;&#35937;&#12290;&#36825;&#31181;&#33021;&#21147;&#22312;&#35757;&#32451;DNN&#26102;&#20351;&#29992;&#36234;&#26469;&#36234;&#22810;&#30340;&#29087;&#24713;&#23545;&#35937;&#26102;&#20250;&#22686;&#24378;&#65292;&#20294;&#20165;&#38480;&#20110;&#28041;&#21450;&#21040;&#29087;&#24713;&#26041;&#21521;&#30340;2D&#26059;&#36716;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20256;&#25773;&#26159;&#36890;&#36807;&#35843;&#25972;&#21040;&#29087;&#24713;&#21644;&#19981;&#29087;&#24713;&#23545;&#35937;&#20043;&#38388;&#20849;&#21516;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#23454;&#29616;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#31867;&#33041;&#31070;&#32463;&#26426;&#21046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of Deep Neural Networks (DNNs) to recognize objects in orientations outside the distribution of the training data is not well understood. We present evidence that DNNs are capable of generalizing to objects in novel orientations by disseminating orientation-invariance obtained from familiar objects seen from many viewpoints. This capability strengthens when training the DNN with an increasing number of familiar objects, but only in orientations that involve 2D rotations of familiar orientations. We show that this dissemination is achieved via neurons tuned to common features between familiar and unfamiliar objects. These results implicate brain-like neural mechanisms for generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#20027;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#29992;&#19977;&#23618;&#35268;&#21010;&#22120;&#21644;&#21487;&#21464;&#27493;&#34892;&#39640;&#24230;&#25511;&#21046;&#22120;&#65292;&#20351;&#21452;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#23433;&#20840;&#22320;&#25506;&#32034;&#39640;&#24230;&#21463;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2109.05714</link><description>&lt;p&gt;
&#22312;&#39640;&#24230;&#21463;&#38480;&#29615;&#22659;&#20013;&#33258;&#20027;&#23548;&#33322;&#27424;&#39537;&#21160;&#21452;&#36275;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Autonomous Navigation of Underactuated Bipedal Robots in Height-Constrained Environments. (arXiv:2109.05714v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#20027;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#29992;&#19977;&#23618;&#35268;&#21010;&#22120;&#21644;&#21487;&#21464;&#27493;&#34892;&#39640;&#24230;&#25511;&#21046;&#22120;&#65292;&#20351;&#21452;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#23433;&#20840;&#22320;&#25506;&#32034;&#39640;&#24230;&#21463;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#30693;&#19988;&#26434;&#20081;&#30340;&#39640;&#24230;&#21463;&#38480;&#29615;&#22659;&#20013;&#23548;&#33322;&#22823;&#22411;&#26426;&#22120;&#20154;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19981;&#20165;&#38656;&#35201;&#24555;&#36895;&#21487;&#38752;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#32469;&#36807;&#38556;&#30861;&#29289;&#65292;&#26426;&#22120;&#20154;&#36824;&#24212;&#33021;&#22815;&#36890;&#36807;&#36466;&#19979;&#25913;&#21464;&#20854;&#20869;&#31104;&#32500;&#24230;&#65292;&#20197;&#20415;&#22312;&#39640;&#24230;&#21463;&#38480;&#21306;&#22495;&#19979;&#34892;&#36827;&#12290;&#30446;&#21069;&#24456;&#23569;&#26377;&#33021;&#22815;&#22788;&#29702;&#27492;&#31867;&#25361;&#25112;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#32780;&#21452;&#36275;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21452;&#36275;&#26426;&#22120;&#20154;&#20855;&#26377;&#38750;&#32447;&#24615;&#21644;&#28151;&#21512;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#22312;&#36825;&#20123;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#21160;&#24577;&#21487;&#34892;&#24615;&#21644;&#23433;&#20840;&#30340;&#36712;&#36857;&#35268;&#21010;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#20027;&#23548;&#33322;&#26694;&#26550;&#65292;&#21033;&#29992;&#19977;&#23618;&#35268;&#21010;&#22120;&#21644;&#21487;&#21464;&#27493;&#34892;&#39640;&#24230;&#25511;&#21046;&#22120;&#65292;&#20351;&#21452;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#23433;&#20840;&#22320;&#25506;&#32034;&#39640;&#24230;&#21463;&#38480;&#30340;&#29615;&#22659;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#22402;&#30452;&#39537;&#21160;&#30340;&#24377;&#31783;&#36127;&#36733;&#20498;&#31435;&#25670;&#65288;vSLIP&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#26426;&#22120;&#20154;&#24179;&#38754;&#34892;&#36208;&#21644;&#22402;&#30452;&#34892;&#36208;&#39640;&#24230;&#30340;&#32806;&#21512;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Navigating a large-scaled robot in unknown and cluttered height-constrained environments is challenging. Not only is a fast and reliable planning algorithm required to go around obstacles, the robot should also be able to change its intrinsic dimension by crouching in order to travel underneath height-constrained regions. There are few mobile robots that are capable of handling such a challenge, and bipedal robots provide a solution. However, as bipedal robots have nonlinear and hybrid dynamics, trajectory planning while ensuring dynamic feasibility and safety on these robots is challenging. This paper presents an end-to-end autonomous navigation framework which leverages three layers of planners and a variable walking height controller to enable bipedal robots to safely explore height-constrained environments. A vertically-actuated Spring-Loaded Inverted Pendulum (vSLIP) model is introduced to capture the robot's coupled dynamics of planar walking and vertical walking height. This red
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38646;&#30693;&#35782;&#21327;&#35843;&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#31639;&#27861;&#20316;&#20026;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.06613</link><description>&lt;p&gt;
&#19968;&#31181;&#38646;&#30693;&#35782;&#21327;&#35843;&#30340;&#26032;&#24418;&#24335;&#12289;&#26041;&#27861;&#21644;&#26410;&#35299;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
A New Formalism, Method and Open Issues for Zero-Shot Coordination. (arXiv:2106.06613v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38646;&#30693;&#35782;&#21327;&#35843;&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#31639;&#27861;&#20316;&#20026;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21327;&#35843;&#38382;&#39064;&#20013;&#65292;&#29420;&#31435;&#25512;&#29702;&#30340;&#20154;&#31867;&#33021;&#22815;&#21457;&#29616;&#20114;&#30456;&#20860;&#23481;&#30340;&#31574;&#30053;&#12290;&#30456;&#21453;&#65292;&#29420;&#31435;&#35757;&#32451;&#30340;&#33258;&#21338;&#24328;&#31574;&#30053;&#36890;&#24120;&#26159;&#20114;&#19981;&#20860;&#23481;&#30340;&#12290;&#38646;&#30693;&#35782;&#21327;&#35843;&#65288;ZSC&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26032;&#21069;&#27839;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#22522;&#26412;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20551;&#35774;&#29609;&#23478;&#21487;&#20197;&#22312;&#23398;&#20064;&#31639;&#27861;&#19978;&#36798;&#25104;&#19968;&#33268;&#65292;&#20294;&#22312;&#34892;&#21160;&#21644;&#35266;&#27979;&#26631;&#31614;&#19978;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#65292;&#25552;&#20986;&#20102;&#20854;&#20182;&#23545;&#23616;&#20316;&#20026;&#19968;&#31181;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#36825;&#20010;&#8220;&#26080;&#26631;&#31614;&#8221;&#38382;&#39064;&#21482;&#26159;&#34987;&#38750;&#27491;&#24335;&#22320;&#23450;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#35774;&#32622;&#24418;&#24335;&#21270;&#20026;&#26080;&#26631;&#31614;&#21327;&#35843;&#65288;LFC&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#26080;&#26631;&#31614;&#21327;&#35843;&#21338;&#24328;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#20182;&#23545;&#23616;&#19981;&#33021;&#25104;&#20026;LFC&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#20026;&#23427;&#26410;&#33021;&#22312;&#19981;&#20860;&#23481;&#30340;&#26368;&#22823;&#21270;&#32773;&#20043;&#38388;&#19968;&#33268;&#22320;&#25171;&#30772;&#24179;&#23616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#21363;&#24102;&#26377;&#25171;&#30772;&#24179;&#23616;&#30340;&#20854;&#20182;&#23545;&#23616;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#23427;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many coordination problems, independently reasoning humans are able to discover mutually compatible policies. In contrast, independently trained self-play policies are often mutually incompatible. Zero-shot coordination (ZSC) has recently been proposed as a new frontier in multi-agent reinforcement learning to address this fundamental issue. Prior work approaches the ZSC problem by assuming players can agree on a shared learning algorithm but not on labels for actions and observations, and proposes other-play as an optimal solution. However, until now, this "label-free" problem has only been informally defined. We formalize this setting as the label-free coordination (LFC) problem by defining the label-free coordination game. We show that other-play is not an optimal solution to the LFC problem as it fails to consistently break ties between incompatible maximizers of the other-play objective. We introduce an extension of the algorithm, other-play with tie-breaking, and prove that it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>