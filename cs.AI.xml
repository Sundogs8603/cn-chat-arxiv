<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11854</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#20196;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577; Web &#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;&#35757;&#32451;&#30340; Web &#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;WebGUM&#65292;&#23558;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027; Web &#23548;&#33322;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20381;&#36182;&#25968;&#21313;&#20159;&#27425;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;&#24615;&#20132;&#20114;&#21644;&#20855;&#26377;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#26469;&#33258;&#20016;&#23500;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#33073;&#26426;&#35757;&#32451;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340; Web &#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#22810;&#27169;&#24577;&#20195;&#29702;&#65292; WebGUM&#65292;&#23427;&#35266;&#23519;&#20102;&#32593;&#39029;&#25130;&#22270;&#21644; HTML &#39029;&#38754;&#65292;&#24182;&#36755;&#20986; Web &#23548;&#33322;&#25805;&#20316;&#65292;&#22914;&#21333;&#20987;&#21644;&#36755;&#20837;&#12290;WebGUM &#26159;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#22312;&#22823;&#37327;&#30340;&#28436;&#31034;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20195;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#24863;&#30693;&#12289;HTML &#29702;&#35299;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#22312; MiniWoB &#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20043;&#21069;&#26368;&#20339;&#33073;&#26426;&#26041;&#27861; 31.9% &#20197;&#19978;&#65292;&#25509;&#36817;&#23454;&#29616;&#22312;&#32447;&#20132;&#20114;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
&lt;/p&gt;</description></item><item><title>RxnScribe&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#21270;&#23398;&#25991;&#29486;&#20013;&#22797;&#26434;&#30340;&#21453;&#24212;&#22270;&#65292;&#24182;&#22312;&#20132;&#21449;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.11845</link><description>&lt;p&gt;
RxnScribe&#65306;&#21453;&#24212;&#22270;&#35299;&#26512;&#30340;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. (arXiv:2305.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11845
&lt;/p&gt;
&lt;p&gt;
RxnScribe&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#21270;&#23398;&#25991;&#29486;&#20013;&#22797;&#26434;&#30340;&#21453;&#24212;&#22270;&#65292;&#24182;&#22312;&#20132;&#21449;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24212;&#22270;&#35299;&#26512;&#26159;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#21453;&#24212;&#26041;&#26696;&#30340;&#20219;&#21153;&#12290;&#21453;&#24212;&#22270;&#21487;&#20197;&#26497;&#20854;&#22797;&#26434;&#65292;&#22240;&#27492;&#23558;&#20854;&#31283;&#20581;&#22320;&#35299;&#26512;&#25104;&#32467;&#26500;&#21270;&#25968;&#25454;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RxnScribe&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#26512;&#19981;&#21516;&#39118;&#26684;&#21453;&#24212;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#24207;&#21015;&#29983;&#25104;&#26041;&#27861;&#26469;&#21046;&#23450;&#36825;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#65292;&#23558;&#20256;&#32479;&#30340;&#31649;&#36947;&#27969;&#31243;&#21387;&#32553;&#20026;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;1,378&#20010;&#22270;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;RxnScribe&#65292;&#24182;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;80.0%&#30340;&#36719;&#21305;&#37197;F1&#20998;&#25968;&#65292;&#19982;&#20197;&#21069;&#30340;&#27169;&#22411;&#30456;&#27604;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://github.com/thomas0809/RxnScribe &#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reaction diagram parsing is the task of extracting reaction schemes from a diagram in the chemistry literature. The reaction diagrams can be arbitrarily complex, thus robustly parsing them into structured data is an open challenge. In this paper, we present RxnScribe, a machine learning model for parsing reaction diagrams of varying styles. We formulate this structured prediction task with a sequence generation approach, which condenses the traditional pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378 diagrams and evaluate it with cross validation, achieving an 80.0% soft match F1 score, with significant improvements over previous models. Our code and data are publicly available at https://github.com/thomas0809/RxnScribe.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20110;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#22806;&#26469;&#35270;&#35282;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#65292;&#23545;T2I&#30340;&#38480;&#21046;&#36827;&#34892;&#30740;&#31350;&#65292;&#28145;&#20837;&#20102;&#35299;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24314;&#35758;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11844</link><description>&lt;p&gt;
AI &#30340;&#34920;&#29616;&#24418;&#24335;&#65306;&#21335;&#20122;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#31038;&#21306;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia. (arXiv:2305.11844v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20110;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#22806;&#26469;&#35270;&#35282;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#65292;&#23545;T2I&#30340;&#38480;&#21046;&#36827;&#34892;&#30740;&#31350;&#65292;&#28145;&#20837;&#20102;&#35299;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#24314;&#35758;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#21306;&#20013;&#24515;&#30340;&#30740;&#31350;&#26041;&#27861;&#65292;&#26469;&#25506;&#35752;&#21335;&#20122;&#25991;&#21270;&#32972;&#26223;&#19979;&#25991;&#23383;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#25991;&#21270;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#25903;&#37197;&#24615;&#30340;&#23186;&#20307;&#34920;&#29616;&#24418;&#24335;&#30340;&#23398;&#26415;&#29702;&#35770;&#26469;&#38416;&#36848;&#36825;&#20123;&#22833;&#36133;&#65292;&#24182;&#23558;&#23427;&#20204;&#23450;&#20301;&#20110;&#21442;&#19982;&#32773;&#23545;&#20182;&#20204;&#29616;&#26377;&#31038;&#20250;&#36793;&#32536;&#21270;&#30340;&#25253;&#21578;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;AI&#20250;&#20877;&#29616;&#19968;&#31181;&#22806;&#37096;&#20154;&#30524;&#20013;&#30475;&#24453;&#21335;&#20122;&#25991;&#21270;&#30340;&#35270;&#35282;&#65292;&#36825;&#31181;&#35270;&#35282;&#26159;&#30001;&#20840;&#29699;&#21644;&#22320;&#21306;&#26435;&#21147;&#19981;&#24179;&#31561;&#25152;&#22609;&#36896;&#30340;&#12290;&#36890;&#36807;&#23558;&#31038;&#21306;&#35270;&#20026;&#19987;&#23478;&#24182;&#24449;&#27714;&#20182;&#20204;&#23545;T2I&#38480;&#21046;&#30340;&#30475;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;&#35780;&#20272;&#26694;&#26550;&#22686;&#28155;&#20102;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#25991;&#21270;&#29305;&#23450;&#30340;AI&#25216;&#26415;&#22312;&#38750;&#35199;&#26041;&#21644;&#21335;&#29699;&#22320;&#21306;&#19978;&#22833;&#36133;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24635;&#32467;&#20986;&#20102;&#36127;&#36131;&#20219;&#22320;&#24320;&#21457;T2I&#27169;&#22411;&#30340;&#25945;&#35757;&#65292;&#25512;&#33616;&#20855;&#20307;&#30340;&#21069;&#36827;&#36335;&#24452;&#65292;&#20197;&#20801;&#35768;&#23545;&#32467;&#26500;&#24615;&#19981;&#24179;&#31561;&#24615;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants' reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;ChatGPT&#19982;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#27604;&#36739;&#65292;&#35748;&#20026;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#20105;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.11837</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#27604;&#36739;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#65306;&#19968;&#39033;&#23454;&#35777;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Comparing Software Developers with ChatGPT: An Empirical Investigation. (arXiv:2305.11837v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;ChatGPT&#19982;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#27604;&#36739;&#65292;&#35748;&#20026;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#20105;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#22312;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20013;&#30340;&#24212;&#29992;&#24050;&#20174;&#29702;&#35770;&#21040;&#29616;&#23454;&#20013;&#36716;&#21464;&#12290;&#35768;&#22810;&#23398;&#26415;&#25991;&#31456;&#35760;&#24405;&#20102;&#20154;&#24037;&#26234;&#33021;&#25104;&#21151;&#24212;&#29992;&#20110;&#39033;&#30446;&#31649;&#29702;&#12289;&#24314;&#27169;&#12289;&#27979;&#35797;&#21644;&#24320;&#21457;&#31561;&#39046;&#22495;&#30340;&#26696;&#20363;&#12290;&#26368;&#36817;&#30340;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;ChatGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#34987;&#21561;&#25447;&#20026;&#33021;&#22815;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#21644;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#36719;&#20214;&#27979;&#35797;&#31574;&#30053;&#30340;&#26426;&#22120;&#23398;&#20064;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#34429;&#28982;&#26377;&#29468;&#27979;&#35748;&#20026;&#22522;&#20110;AI&#30340;&#35745;&#31639;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#21147;&#29978;&#33267;&#21462;&#20195;&#36719;&#20214;&#24037;&#31243;&#24072;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#26469;&#39564;&#35777;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;AI&#31995;&#32479;&#30340;&#20027;&#35201;&#37325;&#28857;&#22312;&#20110;&#22686;&#24378;&#20934;&#30830;&#24615;&#65292;&#20294;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21253;&#25324;&#33021;&#28304;&#25928;&#29575;&#12289;&#28431;&#27934;&#12289;&#20844;&#24179;&#24615;&#65288;&#21363;&#20154;&#20026;&#20559;&#35265;&#65289;&#21644;&#23433;&#20840;&#24615;&#32463;&#24120;&#21463;&#21040;&#19981;&#36275;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of automation in particular Software Engineering (SE) tasks has transitioned from theory to reality. Numerous scholarly articles have documented the successful application of Artificial Intelligence to address issues in areas such as project management, modeling, testing, and development. A recent innovation is the introduction of ChatGPT, an ML-infused chatbot, touted as a resource proficient in generating programming codes and formulating software testing strategies for developers and testers respectively. Although there is speculation that AI-based computation can increase productivity and even substitute software engineers in software development, there is currently a lack of empirical evidence to verify this. Moreover, despite the primary focus on enhancing the accuracy of AI systems, non-functional requirements including energy efficiency, vulnerability, fairness (i.e., human bias), and safety frequently receive insufficient attention. This paper posits that a comprehe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#19982;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30456;&#20851;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#26102;&#31639;&#27861;&#21487;&#35299;&#24615;&#23384;&#22312;&#30097;&#38382;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11833</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#21450;ETR: &#26377;&#25928;&#36830;&#32493;&#20989;&#25968;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions. (arXiv:2305.11833v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#19982;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30456;&#20851;&#65292;&#36825;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#26102;&#31639;&#27861;&#21487;&#35299;&#24615;&#23384;&#22312;&#30097;&#38382;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#23450;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;&#23545;&#20110;&#32447;&#24615;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#32780;&#35328;&#65292;&#24050;&#30693;&#35757;&#32451;&#38382;&#39064;&#26159;&#23384;&#22312;R-&#23436;&#22791;&#30340;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#21644;&#20854;&#20182;&#26377;&#25928;&#36830;&#32493;&#20989;&#25968;&#30340;&#38382;&#39064;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#35757;&#32451;&#38382;&#39064;&#21487;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340; many-one &#21487;&#36824;&#21407;&#21040;&#20851;&#20110;&#30456;&#24212;&#28608;&#27963;&#20989;&#25968;&#25299;&#23637;&#30340;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#19978;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;sigmoid&#28608;&#27963;&#20989;&#25968;&#23548;&#33268;&#20102;&#24102;&#26377;&#25351;&#25968;&#20989;&#25968;&#30340;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;sigmoid&#28608;&#27963;&#20989;&#25968;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#26377;&#31639;&#27861;&#21487;&#35299;&#24615;&#20197;&#21450;&#23454;&#25968;&#23384;&#22312;&#24615;&#29702;&#35770;&#21644;&#25351;&#25968;&#20989;&#25968;&#30340;&#21487;&#21028;&#23450;&#24615;&#31561;&#38382;&#39064;&#37117;&#26159;&#24320;&#25918;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#20351;&#29992;&#27491;&#24358;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#38382;&#39064;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11831</link><description>&lt;p&gt;
&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment. (arXiv:2305.11831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#21017;&#21270;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#22686;&#21152;&#20102;&#23545;&#21407;&#29702;&#30340;&#26126;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#28201;&#24230;&#35843;&#25972;&#30340;&#36719;&#24615;&#28436;&#21592;&#35780;&#35770;&#65288;SAC&#65289;&#31639;&#27861;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#23545;&#31574;&#30053;&#35780;&#20272;&#12289;&#31574;&#30053;&#25913;&#36827;&#21644;&#28201;&#24230;&#35843;&#25972;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#21644;&#20462;&#25913;&#65292;&#20197;&#26356;&#21152;&#26126;&#30830;&#22320;&#38416;&#36848;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11826</link><description>&lt;p&gt;
STOAT: &#32467;&#26500;&#21270;&#25968;&#25454;&#25511;&#21046;&#24615;&#20998;&#26512;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11826
&lt;/p&gt;
&lt;p&gt;
STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#20197;&#29983;&#25104;&#25551;&#36848;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#20998;&#26512;&#25991;&#26412;&#12290;&#22312;&#65288;Gupta et al.,2020&#65289;&#25552;&#20986;&#30340;&#20998;&#31867;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#25512;&#29702;&#31867;&#21035;&#30340;&#21487;&#25511;&#21046;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#65306;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STOAT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#23558;&#32473;&#23450;&#30340;&#25512;&#29702;&#31867;&#21035;&#27880;&#20837;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20998;&#26512;&#21477;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;iToTTo&#21644;Infotabs&#30340;PARENT&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#20379;&#20102;10.19&#65285;&#21644;1.13&#65285;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#26356;&#21152;&#20934;&#30830;&#21644;&#20998;&#26512;&#65292;&#20154;&#31867;&#35780;&#20272;&#20013;&#22686;&#21152;&#20102;15.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#20197;&#21450;&#27604;&#36187;&#21382;&#21490;&#65292;&#32473;&#20986;&#20102;&#32452;&#32455;AI&#27604;&#36187;&#30340;&#24314;&#35758;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11814</link><description>&lt;p&gt;
&#25112;&#30053;&#21345;&#29260;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Summarizing Strategy Card Game AI Competition. (arXiv:2305.11814v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#20197;&#21450;&#27604;&#36187;&#21382;&#21490;&#65292;&#32473;&#20986;&#20102;&#32452;&#32455;AI&#27604;&#36187;&#30340;&#24314;&#35758;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22522;&#20110;&#12298;&#39764;&#27861;&#19982;&#20195;&#30721;&#33521;&#38596;&#12299;&#65288;LOCM&#65289;&#30340;&#20116;&#24180;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25903;&#25345;&#30740;&#31350;&#21644;&#31639;&#27861;&#24320;&#21457;&#30340;&#23567;&#22411;&#25910;&#38598;&#21345;&#29260;&#28216;&#25103;&#65288;CCG&#65289;&#12290;&#35813;&#28216;&#25103;&#34987;&#29992;&#20110;&#22810;&#20010;&#20107;&#20214;&#20013;&#65292;&#21253;&#25324;CodinGame&#24179;&#21488;&#19978;&#30340;&#31038;&#21306;&#27604;&#36187;&#65292;&#20197;&#21450;IEEE&#36827;&#21270;&#35745;&#31639;&#22823;&#20250;&#21644;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#25112;&#30053;&#21345;&#29260;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#12290;LOCM&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19982;&#28216;&#25103;&#26641;&#25628;&#32034;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#35780;&#20272;&#20989;&#25968;&#21644;CCG&#21345;&#32452;&#26500;&#24314;&#31561;&#30456;&#20851;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28216;&#25103;&#35268;&#21017;&#12289;&#32452;&#32455;&#27604;&#36187;&#30340;&#21382;&#21490;&#20197;&#21450;&#21442;&#36187;&#32773;&#21644;&#20182;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20026;&#30740;&#31350;&#31038;&#21306;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#27604;&#36187;&#30340;&#19968;&#20123;&#24120;&#35268;&#24314;&#35758;&#12290;&#23613;&#31649;COG 2022&#29256;&#23459;&#24067;&#26159;&#26368;&#21518;&#19968;&#29256;&#65292;&#20294;&#28216;&#25103;&#20173;&#28982;&#21487;&#29992;&#65292;&#24182;&#21487;&#22312;&#22312;&#32447;&#25490;&#34892;&#27036;&#31454;&#25216;&#22330;&#19978;&#36827;&#34892;&#28216;&#29609;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concludes five years of AI competitions based on Legends of Code and Magic (LOCM), a small Collectible Card Game (CCG), designed with the goal of supporting research and algorithm development. The game was used in a number of events, including Community Contests on the CodinGame platform, and Strategy Card Game AI Competition at the IEEE Congress on Evolutionary Computation and IEEE Conference on Games. LOCM has been used in a number of publications related to areas such as game tree search algorithms, neural networks, evaluation functions, and CCG deckbuilding. We present the rules of the game, the history of organized competitions, and a listing of the participant and their approaches, as well as some general advice on organizing AI competitions for the research community. Although the COG 2022 edition was announced to be the last one, the game remains available and can be played using an online leaderboard arena.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;Dec-POMDP&#39046;&#22495;&#20351;&#29992;&#20102;Monte Carlo&#25628;&#32034;&#31639;&#27861;&#26469;&#23547;&#25214;&#32435;&#20160;&#22343;&#34913;&#65292;&#26368;&#32456;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;MC-JESP&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25511;&#21046;&#22120;&#38382;&#39064;&#24182;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11811</link><description>&lt;p&gt;
Dec-POMDP&#20013;&#30340;Monte Carlo&#25628;&#32034;&#31639;&#27861;&#23547;&#25214;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Monte-Carlo Search for an Equilibrium in Dec-POMDPs. (arXiv:2305.11811v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Dec-POMDP&#39046;&#22495;&#20351;&#29992;&#20102;Monte Carlo&#25628;&#32034;&#31639;&#27861;&#26469;&#23547;&#25214;&#32435;&#20160;&#22343;&#34913;&#65292;&#26368;&#32456;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;MC-JESP&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25511;&#21046;&#22120;&#38382;&#39064;&#24182;&#36798;&#21040;&#39044;&#26399;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Dec-POMDPs&#65289;&#27491;&#24335;&#34920;&#36848;&#20102;&#22312;&#38543;&#26426;&#21160;&#24577;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#20026;&#19968;&#32452;&#21327;&#20316;&#20195;&#29702;&#35774;&#35745;&#21508;&#33258;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#23547;&#27714;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#22256;&#38590;&#30340;&#65288;NEXP&#23436;&#20840;&#65289;&#65292;&#20294;&#23547;&#27714;&#32435;&#20160;&#22343;&#34913; - &#27599;&#20010;&#20195;&#29702;&#31574;&#30053;&#37117;&#26159;&#23545;&#20854;&#20182;&#20195;&#29702;&#30340;&#26368;&#20339;&#21709;&#24212;&#38382;&#39064; - &#26356;&#23481;&#26131;&#65292;&#21516;&#26102;&#20801;&#35768;&#20197;&#26377;&#38480;&#29366;&#24577;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#35299;&#20915;&#26080;&#38480;&#22320;&#24179;&#32447;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20165;&#21487;&#29992;Dec-POMDP&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#27169;&#25311;&#22120;&#65289;&#30340;&#24773;&#20917;&#12290;&#36825;&#38656;&#35201;&#20381;&#38752;&#22522;&#20110;&#27169;&#25311;&#30340;POMDP&#27714;&#35299;&#22120;&#36880;&#20010;&#33410;&#28857;&#22320;&#26500;&#24314;&#20195;&#29702;&#30340;FSC&#33410;&#28857;&#12290;&#30456;&#20851;&#36807;&#31243;&#29992;&#20110;&#21551;&#21457;&#24335;&#22320;&#23548;&#20986;&#21021;&#22987;&#30340;FSC&#12290;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#65292;MC-JESP&#19982;&#29616;&#26377;&#30340;Dec-POMDP&#27714;&#35299;&#22120;&#31454;&#20105;&#21147;&#24378;&#65292;&#29978;&#33267;&#27604;&#20351;&#29992;&#26174;&#24335;&#27169;&#22411;&#30340;&#35768;&#22810;&#31163;&#32447;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized partially observable Markov decision processes (Dec-POMDPs) formalize the problem of designing individual controllers for a group of collaborative agents under stochastic dynamics and partial observability. Seeking a global optimum is difficult (NEXP complete), but seeking a Nash equilibrium -- each agent policy being a best response to the other agents -is more accessible, and allowed addressing infinite-horizon problems with solutions in the form of finite state controllers. In this paper, we show that this approach can be adapted to cases where only a generative model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based POMDP solver to construct an agent's FSC node by node. A related process is used to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers, even better than many offline methods using explicit models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#27169;&#22411;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#24314;&#35758;&#22312;PATE&#30340;&#24212;&#29992;&#20013;&#21152;&#20837;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#20197;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.11807</link><description>&lt;p&gt;
&#31169;&#26377;&#38598;&#25104;&#27169;&#22411;&#30340;&#20844;&#24179;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fairness Impacts of Private Ensembles Models. (arXiv:2305.11807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#27169;&#22411;&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#24314;&#35758;&#22312;PATE&#30340;&#24212;&#29992;&#20013;&#21152;&#20837;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#20197;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;&#25945;&#24072;&#38598;&#25104;&#65288;PATE&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#20010;&#8220;&#25945;&#24072;&#8221;&#27169;&#22411;&#21644;&#19968;&#20010;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#30340;&#32452;&#21512;&#26469;&#21019;&#24314;&#31169;&#26377;&#27169;&#22411;&#12290;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#39044;&#27979;&#22522;&#20110;&#25945;&#24072;&#30340;&#25237;&#31080;&#30340;&#36755;&#20986;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#12290;&#24050;&#32463;&#35777;&#26126;PATE&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#25110;&#20445;&#25252;&#25968;&#25454;&#26631;&#31614;&#26159;&#20248;&#20808;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#31169;&#26377;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;PATE&#26159;&#21542;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#31639;&#27861;&#21644;&#25968;&#25454;&#23646;&#24615;&#23545;&#36825;&#20123;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#20123;&#26041;&#38754;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#19981;&#21516;&#30340;&#32676;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#36825;&#20123;&#24433;&#21709;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Private Aggregation of Teacher Ensembles (PATE) is a machine learning framework that enables the creation of private models through the combination of multiple "teacher" models and a "student" model. The student model learns to predict an output based on the voting of the teachers, and the resulting model satisfies differential privacy. PATE has been shown to be effective in creating private models in semi-supervised settings or when protecting data labels is a priority. This paper explores whether the use of PATE can result in unfairness, and demonstrates that it can lead to accuracy disparities among groups of individuals. The paper also analyzes the algorithmic and data properties that contribute to these disproportionate impacts, why these aspects are affecting different groups disproportionately, and offers recommendations for mitigating these effects
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11792</link><description>&lt;p&gt;
LLM&#30340;&#24605;&#36335;&#38142;&#32034;&#24341;&#29992;&#20110;&#22238;&#31572;&#28145;&#20837;&#23545;&#35805;&#38382;&#39064;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting for responding to in-depth dialogue questions with LLM. (arXiv:2305.11792v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#20570;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#25552;&#38382;&#30340;&#26041;&#24335;&#21644;&#20869;&#23481;&#21487;&#20197;&#27934;&#23519;&#20182;&#20204;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24605;&#36335;&#38142;&#32034;&#24341;&#30340;&#26041;&#24335;&#26469;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#21644;&#35268;&#21010;&#65292;&#20197;&#21709;&#24212;&#29992;&#25143;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;6&#20010;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#23545;&#35805;&#25110;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29992;&#25143;&#29366;&#24577;&#30340;3&#20010;&#19981;&#21516;&#26041;&#38754;&#65288;&#21253;&#25324;&#20154;&#26684;&#12289;&#24773;&#24863;&#21644;&#24515;&#29702;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#29992;&#25143;&#29366;&#24577;&#30340;&#21709;&#24212;&#20316;&#20026;&#20013;&#38388;&#25512;&#29702;&#22788;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20013;&#38388;&#25512;&#29702;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#32780;&#38750;&#27979;&#35797;&#26597;&#35810;&#30340;&#26032;&#39062;&#28436;&#31034;&#36873;&#25321;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we first construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status (\textit{including} \textit{personality}, \textit{emotion}, and \textit{psychology}). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive e
&lt;/p&gt;</description></item><item><title>DMDD&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20219;&#21153;&#65292;&#21253;&#21547;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#21644;&#36229;&#36807;449,000&#20010;&#24369;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#35813;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11779</link><description>&lt;p&gt;
DMDD&#65306;&#38754;&#21521;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DMDD: A Large-Scale Dataset for Dataset Mentions Detection. (arXiv:2305.11779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11779
&lt;/p&gt;
&lt;p&gt;
DMDD&#26159;&#19968;&#20010;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20219;&#21153;&#65292;&#21253;&#21547;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#21644;&#36229;&#36807;449,000&#20010;&#24369;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#35813;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#30830;&#23450;&#20102;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21517;&#31216;&#35782;&#21035;&#26159;&#31185;&#23398;&#25991;&#29486;&#33258;&#21160;&#20449;&#24687;&#25552;&#21462;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#29702;&#35299;&#21644;&#35782;&#21035;&#30740;&#31350;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#22312;&#22823;&#23567;&#21644;&#21629;&#21517;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#25968;&#25454;&#38598;&#65288;DMDD&#65289;&#65292;&#36825;&#26159;&#24403;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22788;&#29702;&#36825;&#39033;&#20219;&#21153;&#12290;DMDD&#30001;DMDD&#20027;&#35201;&#35821;&#26009;&#24211;&#21644;&#35780;&#20272;&#38598;&#32452;&#25104;&#65292;&#20854;&#20013;DMDD&#20027;&#35201;&#35821;&#26009;&#24211;&#21253;&#25324;31,219&#31687;&#31185;&#23398;&#25991;&#31456;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;449,000&#20010;&#25968;&#25454;&#38598;&#25552;&#21450;&#24369;&#27880;&#37322;&#26684;&#24335;&#30340;&#25991;&#26412;&#27573;&#65292;&#35780;&#20272;&#38598;&#21253;&#25324;450&#31687;&#25163;&#21160;&#27880;&#37322;&#30340;&#31185;&#23398;&#25991;&#31456;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;DMDD&#24314;&#31435;&#20102;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#21644;&#38142;&#25509;&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#36890;&#36807;&#20998;&#26512;&#21508;&#31181;&#27169;&#22411;&#22312;DMDD&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36992;&#35831;&#31038;&#21306;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20316;&#20026;&#25361;&#25112;&#65292;&#24320;&#21457;&#26032;&#30340;&#25968;&#25454;&#38598;&#25552;&#21450;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recognition of dataset names is a critical task for automatic information extraction in scientific literature, enabling researchers to understand and identify research opportunities. However, existing corpora for dataset mention detection are limited in size and naming diversity. In this paper, we introduce the Dataset Mentions Detection Dataset (DMDD), the largest publicly available corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219 scientific articles with over 449,000 dataset mentions weakly annotated in the format of in-text spans, and an evaluation set, which comprises of 450 scientific articles manually annotated for evaluation purposes. We use DMDD to establish baseline performance for dataset mention detection and linking. By analyzing the performance of various models on DMDD, we are able to identify open problems in dataset mention detection. We invite the community to use our dataset as a challenge to develop novel dataset mention detection mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20154;&#31867;&#21644;&#21160;&#29289;&#22914;&#20309;&#25512;&#26029;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#20197;&#21450;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.11772</link><description>&lt;p&gt;
&#31070;&#32463;&#22522;&#30784;&#20013;&#30340;&#24515;&#29702;&#27169;&#25311;&#65306;&#39044;&#27979;&#21160;&#24577;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes. (arXiv:2305.11772v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20154;&#31867;&#21644;&#21160;&#29289;&#22914;&#20309;&#25512;&#26029;&#29289;&#29702;&#19990;&#30028;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#20197;&#21450;&#22914;&#20309;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21644;&#21160;&#29289;&#23545;&#29289;&#29702;&#19990;&#30028;&#26377;&#30528;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#20107;&#20214;&#30340;&#22522;&#26412;&#21160;&#24577;&#36712;&#36857;&#65292;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#35268;&#21010;&#21644;&#39044;&#27979;&#34892;&#20026;&#30340;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35745;&#31639;&#32972;&#21518;&#30340;&#31070;&#32463;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#30446;&#26631;&#39537;&#21160;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#32467;&#21512;&#23494;&#38598;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#25968;&#25454;&#21644;&#39640;&#36890;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#36755;&#20986;&#26469;&#25506;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#20960;&#31867;&#24863;&#30693;-&#35748;&#30693;&#32593;&#32476;&#26469;&#39044;&#27979;&#20016;&#23500;&#12289;&#20855;&#26377;&#34892;&#20026;&#23398;&#24847;&#20041;&#30340;&#29615;&#22659;&#30340;&#26410;&#26469;&#29366;&#24577;&#65292;&#20174;&#20687;&#32032;&#25110;&#38754;&#21521;&#23545;&#35937;&#30446;&#26631;&#30340;&#33258;&#20027;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#21040;&#23558;&#32431;&#38745;&#24577;&#22522;&#20110;&#22270;&#20687;&#25110;&#21160;&#24577;&#35270;&#39057;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#26410;&#26469;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#31867;&#21035;&#22312;&#20854;&#39044;&#27979;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#30340;&#33021;&#21147;&#19978;&#26377;&#24456;&#24378;&#30340;&#24046;&#24322;&#65292;&#26080;&#35770;&#22312;&#20854;&#22521;&#35757;&#39046;&#22495;&#20869;&#25110;&#22806;&#65292;&#36825;&#31181;&#24046;&#24322;&#21453;&#26144;&#20102;&#25928;&#29575;&#12289;&#26222;&#36941;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-centric objectives, to models that future predict in the latent space of purely static image-based or dynamic video-based pretrained foundation models. We find strong differentiation across these model classes in their ability to predict neural and behavioral data both w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.11769</link><description>&lt;p&gt;
&#25552;&#21319;&#32852;&#21512;&#23398;&#20064;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;&#22522;&#20110;&#32852;&#21512;&#23398;&#20064;&#30340;&#38382;&#31572;&#19982;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner. (arXiv:2305.11769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JADE&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#36827;&#34892;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#21253;&#25324;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20316;&#20026;&#39044;&#20808;&#35757;&#32451;&#30340;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#38656;&#35201;&#32454;&#31890;&#24230;&#29305;&#24449;&#23545;&#40784;&#30340;&#38656;&#27714;&#65292;&#36825;&#38656;&#35201;&#23545;&#22270;&#20687;&#21644;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#35814;&#32454;&#30340;&#29702;&#35299;&#12290;&#23558;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#38598;&#25104;&#21040;&#39044;&#20808;&#35757;&#32451;&#20013;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#33719;&#21462;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#20197;&#21450;&#22270;&#20687;-&#20301;&#32622;-&#23383;&#24149;&#19977;&#20803;&#32452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#25968;&#25454;&#38598;&#36890;&#24120;&#30001;&#20110;&#25163;&#21160;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#32780;&#35268;&#27169;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#21512;&#38382;&#31572;&#21644;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#65288;JADE&#65289;&#65292;&#23427;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#26131;&#20110;&#33719;&#21462;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained multimodal models have demonstrated significant success in a range of downstream tasks, including image captioning, image-text retrieval, visual question answering (VQA), etc. However, many of these methods rely on image-text pairs collected from the web as pre-training data and unfortunately overlook the need for fine-grained feature alignment between vision and language modalities, which requires detailed understanding of images and language expressions. While integrating VQA and dense captioning (DC) into pre-training can address this issue, acquiring image-question-answer as well as image-location-caption triplets is challenging and time-consuming. Additionally, publicly available datasets for VQA and dense captioning are typically limited in scale due to manual data collection and labeling efforts. In this paper, we propose a novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;Prompt-Tuning&#31574;&#30053;&#26469;&#25511;&#21046;&#20174;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;GPT-Neo&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25915;&#20987;&#31574;&#30053;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3%&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#65292;&#32780;&#38450;&#24481;&#31574;&#30053;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#20294;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;</title><link>http://arxiv.org/abs/2305.11759</link><description>&lt;p&gt;
&#36890;&#36807;Prompt-Tuning&#25511;&#21046;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. (arXiv:2305.11759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;Prompt-Tuning&#31574;&#30053;&#26469;&#25511;&#21046;&#20174;&#20013;&#25552;&#21462;&#35760;&#24518;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#20004;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;GPT-Neo&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25915;&#20987;&#31574;&#30053;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3%&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#65292;&#32780;&#38450;&#24481;&#31574;&#30053;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#20294;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#30693;&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#19988;&#31616;&#21333;&#26597;&#35810;&#27169;&#22411;&#21363;&#21487;&#25552;&#21462;&#35760;&#24518;&#30340;&#20869;&#23481;&#65292;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;Prompt-Tuning&#26469;&#25511;&#21046;LLMs&#20013;&#35760;&#24518;&#20869;&#23481;&#30340;&#25552;&#21462;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;Prompt&#35757;&#32451;&#31574;&#30053;&#26469;&#22686;&#21152;&#21644;&#20943;&#23569;&#25552;&#21462;&#29575;&#65292;&#20998;&#21035;&#23545;&#24212;&#25915;&#20987;&#21644;&#38450;&#24481;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;GPT-Neo&#31995;&#21015;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;1.3B&#21442;&#25968;&#30340;GPT-Neo&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#30456;&#23545;&#20110;&#22522;&#32447;&#20135;&#29983;&#20102;9.3&#20010;&#30334;&#20998;&#28857;&#30340;&#25552;&#21462;&#29575;&#22686;&#21152;&#12290;&#36890;&#36807;&#29992;&#25143;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#21487;&#20197;&#35843;&#25972;&#20197;&#23454;&#29616;&#19981;&#21516;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#25105;&#20204;&#30456;&#23545;&#20110;&#22522;&#32447;&#23454;&#29616;&#20102;&#26368;&#22810;97.7%&#30340;&#25552;&#21462;&#29575;&#20943;&#23569;&#65292;&#21516;&#26102;&#22256;&#24785;&#24230;&#22686;&#21152;&#20102;16.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2305.11755</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#65306;&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Visualization for Recommendation Explainability: A Survey and New Perspectives. (arXiv:2305.11755v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#25512;&#33616;&#25552;&#20379;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#26159;&#23454;&#29616;&#36879;&#26126;&#19988;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#20026;&#36755;&#20986;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#30784;&#12290;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#35299;&#37322;&#30446;&#26631;&#12289;&#35299;&#37322;&#33539;&#22260;&#12289;&#35299;&#37322;&#26679;&#24335;&#21644;&#35299;&#37322;&#26684;&#24335;&#36825;&#22235;&#20010;&#32500;&#24230;&#31995;&#32479;&#22320;&#23457;&#26597;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#20851;&#35299;&#37322;&#30340;&#25991;&#29486;&#12290;&#35748;&#35782;&#21040;&#21487;&#35270;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#20174;&#35299;&#37322;&#24615;&#35270;&#35273;&#26041;&#24335;&#30340;&#35282;&#24230;&#36884;&#24452;&#25512;&#33616;&#31995;&#32479;&#25991;&#29486;&#65292;&#21363;&#20351;&#29992;&#21487;&#35270;&#21270;&#20316;&#20026;&#35299;&#37322;&#30340;&#26174;&#31034;&#26679;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#32452;&#21487;&#33021;&#26377;&#30410;&#20110;&#35774;&#35745;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11742</link><description>&lt;p&gt;
MedLens: &#36890;&#36807;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#26469;&#25552;&#39640;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MedLens: Improve mortality prediction via medical signs selecting and regression interpolation. (arXiv:2305.11742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#21307;&#23398;&#20307;&#24449;&#21644;&#22238;&#24402;&#25554;&#20540;&#30340;&#26041;&#27861;&#65288;MedLens&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30005;&#23376;&#30149;&#21382;&#20013;&#21307;&#23398;&#20307;&#24449;&#25968;&#25454;&#32570;&#22833;&#29575;&#36807;&#39640;&#23548;&#33268;&#39044;&#27979;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#24182;&#25552;&#21069;&#39044;&#27979;&#27515;&#20129;&#29575;&#23545;&#21450;&#26102;&#25552;&#20379;&#24739;&#32773;&#25252;&#29702;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#22823;&#37327;&#21307;&#23398;&#20307;&#24449;&#34987;&#29992;&#20110;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#20020;&#24202;&#20307;&#24449;&#30340;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#34987;&#36739;&#23569;&#35752;&#35770;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#21307;&#23398;&#20307;&#24449;&#21644;&#22823;&#37327;&#24739;&#32773;&#20303;&#38498;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#29575;&#21644;&#30456;&#20851;&#20998;&#25968;&#36827;&#34892;&#28145;&#20837;&#27979;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#32508;&#21512;&#32570;&#22833;&#29575;&#38750;&#24120;&#39640;&#65292;&#22823;&#37327;&#26080;&#29992;&#30340;&#20307;&#24449;&#21487;&#33021;&#20250;&#25439;&#23475;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#21482;&#26377;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#25165;&#33021;&#25552;&#39640;&#19981;&#21516;&#39044;&#27979;&#31639;&#27861;&#30340;&#22522;&#32447;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;MedLens&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#21160;&#36873;&#25321;&#37325;&#35201;&#21307;&#23398;&#20307;&#24449;&#65292;&#24182;&#20351;&#29992;&#28789;&#27963;&#30340;&#25554;&#20540;&#26041;&#27861;&#22788;&#29702;&#39640;&#32570;&#22833;&#29575;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring the health status of patients and predicting mortality in advance is vital for providing patients with timely care and treatment. Massive medical signs in electronic health records (EHR) are fitted into advanced machine learning models to make predictions. However, the data-quality problem of original clinical signs is less discussed in the literature. Based on an in-depth measurement of the missing rate and correlation score across various medical signs and a large amount of patient hospital admission records, we discovered the comprehensive missing rate is extremely high, and a large number of useless signs could hurt the performance of prediction models. Then we concluded that only improving data-quality could improve the baseline accuracy of different prediction algorithms. We designed MEDLENS, with an automatic vital medical signs selection approach via statistics and a flexible interpolation approach for high missing rate time series. After augmenting the data-quality 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11707</link><description>&lt;p&gt;
&#19979;&#19968;&#20010;&#20250;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
What Comes Next? Evaluating Uncertainty in Neural Text Generators Against Human Production Variability. (arXiv:2305.11707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#22120;&#19982;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#25506;&#27979;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#31354;&#38388;&#26469;&#27979;&#37327;&#20854;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#65292;&#24182;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#65292;&#38024;&#23545;&#20219;&#20309;&#36755;&#20837;&#65292;&#23384;&#22312;&#22810;&#20010;&#21487;&#34892;&#30340;&#20132;&#38469;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#22810;&#31181;&#26041;&#24335;&#23558;&#20219;&#20309;&#30446;&#26631;&#29992;&#35821;&#35328;&#34920;&#36798;&#20986;&#26469;&#25110;&#36827;&#34892;&#29983;&#20135;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#20154;&#31867;&#29983;&#20135;&#22312;&#22235;&#20010;NLG&#20219;&#21153;&#20013;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#21464;&#24322;&#31243;&#24230;&#65292;&#24182;&#23558;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#19982;&#19981;&#30830;&#23450;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#29983;&#25104;&#31995;&#32479;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;&#35299;&#30721;&#31639;&#27861;&#25152;&#24418;&#25104;&#30340;&#36755;&#20986;&#23383;&#31526;&#20018;&#31354;&#38388;&#65292;&#20197;&#25506;&#31350;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#38024;&#23545;&#27599;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#29983;&#25104;&#22120;&#23545;&#20154;&#31867;&#29983;&#20135;&#21464;&#24322;&#24615;&#30340;&#26657;&#20934;&#31243;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#22522;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;NLG&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#35777;&#26126;&#29992;&#22810;&#20010;&#26679;&#26412;&#21644;&#22810;&#20010;&#21442;&#32771;&#23545;&#29983;&#25104;&#22120;&#36827;&#34892;&#25506;&#27979;&#65292;&#25552;&#20379;&#20102;&#29702;&#35299;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#25152;&#24517;&#38656;&#30340;&#35814;&#32454;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Generation (NLG) tasks, for any input, multiple communicative goals are plausible, and any goal can be put into words, or produced, in multiple ways. We characterise the extent to which human production varies lexically, syntactically, and semantically across four NLG tasks, connecting human production variability to aleatoric or data uncertainty. We then inspect the space of output strings shaped by a generation system's predicted probability distribution and decoding algorithm to probe its uncertainty. For each test input, we measure the generator's calibration to human production variability. Following this instance-level approach, we analyse NLG models and decoding strategies, demonstrating that probing a generator with multiple samples and, when possible, multiple references, provides the level of detail necessary to gain understanding of a model's representation of uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11699</link><description>&lt;p&gt;
RGCVAE&#65306;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
RGCVAE: Relational Graph Conditioned Variational Autoencoder for Molecule Design. (arXiv:2305.11699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RGCVAE&#65292;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#22270;&#26465;&#20214;&#21270;&#30340;&#21487;&#21464;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#26377;&#25928;&#12289;&#39640;&#25928;&#22320;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;&#65292;&#24182;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20808;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#34920;&#29616;&#20986;&#26576;&#20123;&#39044;&#23450;&#29305;&#24615;&#30340;&#20998;&#23376;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#12290;&#28145;&#24230;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#26368;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20043;&#19968;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#20542;&#21521;&#20110;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#22270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;RGCVAE&#65306;&#65288;i&#65289;&#21033;&#29992;&#20840;&#26032;&#30340;&#24378;&#22823;&#20851;&#31995;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#32534;&#30721;&#32593;&#32476;&#65307;&#65288;ii&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35299;&#30721;&#32452;&#20214;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#25968;&#31181;&#26368;&#20808;&#36827;&#30340;VAE&#26041;&#27861;&#30456;&#27604;&#65292;RGCVAE&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#20998;&#23376;&#29983;&#25104;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying molecules that exhibit some pre-specified properties is a difficult problem to solve. In the last few years, deep generative models have been used for molecule generation. Deep Graph Variational Autoencoders are among the most powerful machine learning tools with which it is possible to address this problem. However, existing methods struggle in capturing the true data distribution and tend to be computationally expensive. In this work, we propose RGCVAE, an efficient and effective Graph Variational Autoencoder based on: (i) an encoding network exploiting a new powerful Relational Graph Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE shows state-of-the-art molecule generation performance while being significantly faster to train.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11692</link><description>&lt;p&gt;
&#24102;&#26377;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30340;Transformer&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#30340;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2305.11692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Surgical-VQLA&#26041;&#27861;&#65292;&#32467;&#21512;Transformer&#27169;&#22411;&#21644;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;VQA&#20013;&#23545;&#35937;&#26816;&#27979;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#23450;&#20301;&#31572;&#26696;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#30528;&#35745;&#31639;&#26426;&#36741;&#21161;&#27169;&#25311;&#22120;&#21644;&#25163;&#26415;&#36807;&#31243;&#30340;&#24405;&#21046;&#35270;&#39057;&#65292;&#20294;&#21021;&#32423;&#20303;&#38498;&#21307;&#24072;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#19987;&#23478;&#26469;&#22238;&#31572;&#20182;&#20204;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19987;&#23478;&#22806;&#31185;&#21307;&#29983;&#36890;&#24120;&#25215;&#25285;&#30528;&#20020;&#24202;&#21644;&#23398;&#26415;&#24037;&#20316;&#65292;&#38480;&#21046;&#20102;&#20182;&#20204;&#22238;&#31572;&#38382;&#39064;&#30340;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25163;&#26415;&#38382;&#31572;&#31995;&#32479;&#65292;&#20197;&#20415;&#20174;&#24405;&#21046;&#30340;&#35270;&#39057;&#20013;&#20419;&#36827;&#26426;&#22120;&#20154;&#36741;&#21161;&#25163;&#26415;&#22330;&#26223;&#21644;&#27963;&#21160;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;Transformer&#27169;&#22411;&#19982;&#38376;&#25511;&#35270;&#35273;-&#35821;&#35328;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#26415;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#31232;&#32570;&#12289;&#24322;&#26500;&#27169;&#24577;&#34701;&#21512;&#31574;&#30053;&#19981;&#36275;&#12289;&#32570;&#22833;&#23450;&#20301;&#31572;&#26696;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#22522;&#20934;&#25163;&#26415;VQA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11662</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#35780;&#20272;&#20219;&#21153;&#29702;&#35299;&#65306;ChatGPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating task understanding through multilingual consistency: A ChatGPT case study. (arXiv:2305.11662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#33021;&#21147;&#30340;&#33539;&#20363;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#19981;&#21516;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21151;&#33021;&#30340;&#24778;&#20154;&#25552;&#21319;&#65292;&#21019;&#24314;&#26410;&#26469;&#21487;&#25345;&#32493;&#30340;&#35780;&#20272;&#38598;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#29702;&#35299;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;LLM&#30340;&#33539;&#20363;&#65292;&#35813;&#33539;&#20363;&#21033;&#29992;&#20102;&#27491;&#30830;&#30340;&#19990;&#30028;&#29702;&#35299;&#24212;&#35813;&#22312;&#30456;&#21516;&#21547;&#20041;&#30340;&#19981;&#21516;&#65288;&#24343;&#38647;&#26684;&#65289;&#24847;&#20041;&#19978;&#20445;&#25345;&#19968;&#33268;&#30340;&#24605;&#24819;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#36890;&#36807;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#29702;&#35299;&#65292;&#32780;&#26159;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#33258;&#36523;&#29983;&#25104;&#30340;&#22810;&#20010;&#24847;&#20041;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#21270;&#19968;&#20010;&#27979;&#35797;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#30340;&#24847;&#20041;&#26159;&#19981;&#21516;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#23558;&#22810;&#35821;&#35328;&#33258;&#25105;&#19968;&#33268;&#24615;&#20316;&#20026;&#27169;&#22411;&#29702;&#35299;&#30340;&#26816;&#39564;&#24182;&#21516;&#26102;&#35299;&#20915;&#22810;&#35821;&#35328;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#25105;&#20204;&#20197;&#26368;&#26032;&#29256;&#26412;&#30340;ChatGPT&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#35937;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#35780;&#20272;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;ChatGPT&#22312;&#22810;&#35821;&#35328;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the staggering pace with which the capabilities of large language models (LLMs) are increasing, creating future-proof evaluation sets to assess their understanding becomes more and more challenging. In this paper, we propose a novel paradigm for evaluating LLMs which leverages the idea that correct world understanding should be consistent across different (Fregean) senses of the same meaning. Accordingly, we measure understanding not in terms of correctness but by evaluating consistency across multiple senses that are generated by the model itself. We showcase our approach by instantiating a test where the different senses are different languages, hence using multilingual self-consistency as a litmus test for the model's understanding and simultaneously addressing the important topic of multilingualism. Taking one of the latest versions of ChatGPT as our object of study, we evaluate multilingual consistency for two different tasks across three different languages. We show that its m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110; Ising &#26426;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27714;&#35299;&#22810;&#30446;&#26631; QUBOs &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25506;&#32034; Pareto &#21069;&#27839;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.11648</link><description>&lt;p&gt;
&#23558; Ising &#26426;&#24212;&#29992;&#20110;&#22810;&#30446;&#26631; QUBOs
&lt;/p&gt;
&lt;p&gt;
Applying Ising Machines to Multi-objective QUBOs. (arXiv:2305.11648v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110; Ising &#26426;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27714;&#35299;&#22810;&#30446;&#26631; QUBOs &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25506;&#32034; Pareto &#21069;&#27839;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#28041;&#21450;&#22312;&#22810;&#20010;&#19988;&#24120;&#24120;&#26159;&#20914;&#31361;&#30340;&#30446;&#26631;&#20043;&#38388;&#25214;&#21040;&#20855;&#26377;&#19981;&#21516;&#26435;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; Ising &#26426;&#26159;&#26088;&#22312;&#25214;&#21040; Ising &#27169;&#22411;&#30340;&#32477;&#23545;&#25110;&#36817;&#20284;&#22522;&#24577;&#30340;&#29289;&#29702;&#35774;&#22791;&#12290;&#20026;&#20102;&#23558; Ising &#26426;&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#38382;&#39064;&#65292;&#20351;&#29992;&#21152;&#26435;&#27714;&#21644;&#30446;&#26631;&#20989;&#25968;&#23558;&#22810;&#30446;&#26631;&#36716;&#25442;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23548;&#20986;&#22343;&#21248;&#20998;&#24067;&#20110; Pareto &#21069;&#27839;&#30340;&#26631;&#37327;&#21270;&#26435;&#37325;&#24182;&#19981;&#23481;&#26131;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22522;&#20110;&#21452;&#20998;&#21106;&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#21644;&#22522;&#20110;&#20808;&#21069;&#25506;&#32034;&#26435;&#37325;&#30340;&#24179;&#22343;&#20540;&#30340;&#26435;&#37325;&#21487;&#20197;&#27604;&#22343;&#21248;&#29983;&#25104;&#30340;&#26435;&#37325;&#26356;&#24555;&#22320;&#25506;&#32034; Pareto &#21069;&#27839;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33258;&#36866;&#24212;&#26041;&#27861;&#36807;&#21435;&#21482;&#29992;&#20110;&#21452;&#30446;&#26631;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#20004;&#31181;&#26041;&#24335;&#25193;&#23637;&#20102;&#22522;&#20110;&#24179;&#22343;&#20540;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25193;&#23637;&#20102;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#20004;&#20010;&#25110;&#22810;&#20010;&#30446;&#26631;&#30340;&#26631;&#37327;&#21270;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#23450;&#20041;&#25910;&#25947;&#26631;&#20934;&#30340;&#26041;&#24335;&#26469;&#32456;&#27490;&#26435;&#37325;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22343;&#21248;&#29983;&#25104;&#21644;&#22522;&#20110;&#21452;&#20998;&#21106;&#25628;&#32034;&#30340;&#26435;&#37325;&#22312;&#20004;&#20010;&#21644;&#19977;&#20010;&#30446;&#26631;&#30340; QUBO &#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#24179;&#22343;&#20540;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20248;&#20110;&#22343;&#21248;&#29983;&#25104;&#21644;&#22522;&#20110;&#21452;&#20998;&#21106;&#25628;&#32034;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-objective optimisation problems involve finding solutions with varying trade-offs between multiple and often conflicting objectives. Ising machines are physical devices that aim to find the absolute or approximate ground states of an Ising model. To apply Ising machines to multi-objective problems, a weighted sum objective function is used to convert multi-objective into single-objective problems. However, deriving scalarisation weights that archives evenly distributed solutions across the Pareto front is not trivial. Previous work has shown that adaptive weights based on dichotomic search, and one based on averages of previously explored weights can explore the Pareto front quicker than uniformly generated weights. However, these adaptive methods have only been applied to bi-objective problems in the past. In this work, we extend the adaptive method based on averages in two ways: (i)~we extend the adaptive method of deriving scalarisation weights for problems with two or more ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#20415;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26082;&#33021;&#20445;&#25345;&#31283;&#23450;&#24615;&#21448;&#33021;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11624</link><description>&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#32780;&#35843;&#25972;&#27169;&#24335;&#30340;ConvBN&#22359;
&lt;/p&gt;
&lt;p&gt;
Tune-Mode ConvBN Blocks For Efficient Transfer Learning. (arXiv:2305.11624v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#20415;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26082;&#33021;&#20445;&#25345;&#31283;&#23450;&#24615;&#21448;&#33021;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;-&#25209;&#24402;&#19968;&#21270;&#65288;ConvBN&#65289;&#22359;&#26159;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;ConvBN&#22359;&#21487;&#20197;&#22312;&#19977;&#31181;&#27169;&#24335;&#19979;&#36816;&#34892;&#65306;Train&#12289;Eval&#21644;Deploy&#12290;&#34429;&#28982;Train&#27169;&#24335;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;Eval&#27169;&#24335;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#27169;&#22411;&#39564;&#35777;&#65292;&#32780;Deploy&#27169;&#24335;&#21017;&#36866;&#29992;&#20110;&#27169;&#22411;&#37096;&#32626;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;ConvBN&#22359;&#20013;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;Deploy&#27169;&#24335;&#25928;&#29575;&#39640;&#20294;&#35757;&#32451;&#19981;&#31283;&#23450;&#65307;Eval&#27169;&#24335;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#32570;&#20047;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;Deploy&#27169;&#24335;&#19979;&#31283;&#23450;&#24615;&#19979;&#38477;&#30340;&#21407;&#22240;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Tune&#27169;&#24335;&#65292;&#20197;&#24357;&#21512;Eval&#27169;&#24335;&#21644;Deploy&#27169;&#24335;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25152;&#25552;&#20986;&#30340;Tune&#27169;&#24335;&#19982;Eval&#27169;&#24335;&#19968;&#26679;&#31283;&#23450;&#65292;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#65292;&#32780;&#20854;&#35745;&#31639;&#25928;&#29575;&#19982;Deploy&#27169;&#24335;&#38750;&#24120;&#25509;&#36817;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Tune&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolution-BatchNorm (ConvBN) blocks are integral components in various computer vision tasks and other domains. A ConvBN block can operate in three modes: Train, Eval, and Deploy. While the Train mode is indispensable for training models from scratch, the Eval mode is suitable for transfer learning and model validation, and the Deploy mode is designed for the deployment of models. This paper focuses on the trade-off between stability and efficiency in ConvBN blocks: Deploy mode is efficient but suffers from training instability; Eval mode is widely used in transfer learning but lacks efficiency. To solve the dilemma, we theoretically reveal the reason behind the diminished training stability observed in the Deploy mode. Subsequently, we propose a novel Tune mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune mode is as stable as Eval mode for transfer learning, and its computational efficiency closely matches that of the Deploy mode. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#34892;&#20026;&#39537;&#21160;&#24320;&#21457;&#27979;&#35797;&#35268;&#33539;&#65292;&#32467;&#21512;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#29983;&#25104; Angular &#26694;&#26550;&#30340;&#21069;&#31471;&#32452;&#20214;&#20195;&#30721;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24320;&#21457;&#26102;&#38388;&#65292;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#65292;&#24182;&#20026;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30740;&#31350;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.11619</link><description>&lt;p&gt;
&#20174;BDD&#27979;&#35797;&#29992;&#20363;&#35268;&#33539;&#21040;&#20195;&#30721;&#29983;&#25104;&#65306;&#19968;&#20010;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Code Generation from BDD Test Case Specifications: A Vision. (arXiv:2305.11619v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#34892;&#20026;&#39537;&#21160;&#24320;&#21457;&#27979;&#35797;&#35268;&#33539;&#65292;&#32467;&#21512;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#29983;&#25104; Angular &#26694;&#26550;&#30340;&#21069;&#31471;&#32452;&#20214;&#20195;&#30721;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24320;&#21457;&#26102;&#38388;&#65292;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#65292;&#24182;&#20026;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#30740;&#31350;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#27491;&#22312;&#25104;&#20026;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#27491;&#22312;&#20197;&#26377;&#25928;&#21644;&#21019;&#26032;&#30340;&#26041;&#24335;&#22686;&#21152;&#20154;&#31867;&#21644;&#36719;&#20214;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#21457;&#23637;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#27969;&#34892;&#30340;Angular&#26694;&#26550;&#30340;&#21069;&#31471;&#32452;&#20214;&#20195;&#30721;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#34892;&#20026;&#39537;&#21160;&#24320;&#21457;&#27979;&#35797;&#35268;&#33539;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#22823;&#22823;&#32553;&#30701;Web&#24212;&#29992;&#31243;&#24207;&#25152;&#38656;&#30340;&#24320;&#21457;&#26102;&#38388;&#65292;&#21516;&#26102;&#21487;&#33021;&#25552;&#39640;&#36719;&#20214;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#26032;&#30340;&#30740;&#31350;&#24605;&#36335;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic code generation has recently attracted large attention and is becoming more significant to the software development process. Solutions based on Machine Learning and Artificial Intelligence are being used to increase human and software efficiency in potent and innovative ways. In this paper, we aim to leverage these developments and introduce a novel approach to generating frontend component code for the popular Angular framework. We propose to do this using behavior-driven development test specifications as input to a transformer-based machine learning model. Our approach aims to drastically reduce the development time needed for web applications while potentially increasing software quality and introducing new research ideas toward automatic code generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>MIDI-Draw&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30011;&#26354;&#32447;&#26469;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#26356;&#24555;&#36895;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.11605</link><description>&lt;p&gt;
MIDI-Draw: &#29992;&#32472;&#30011;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MIDI-Draw: Sketching to Control Melody Generation. (arXiv:2305.11605v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11605
&lt;/p&gt;
&lt;p&gt;
MIDI-Draw&#26159;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#30011;&#26354;&#32447;&#26469;&#25511;&#21046;&#26059;&#24459;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35753;&#29992;&#25143;&#26356;&#24555;&#36895;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#23454;&#29616;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#26059;&#24459;&#36718;&#24275;&#23558;&#38899;&#31526;&#32423;&#21035;&#30340;&#36755;&#20837;&#34920;&#31034;&#25277;&#35937;&#21270;&#12290;&#20854;&#30446;&#30340;&#26159;&#20801;&#35768;&#29992;&#25143;&#34920;&#36798;&#20182;&#20204;&#30340;&#38899;&#20048;&#24847;&#22270;&#65292;&#32780;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#38899;&#31526;&#22914;&#20309;&#21644;&#35856;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#30446;&#21069;&#65292;&#21487;&#25511;&#26059;&#24459;&#29983;&#25104;&#30340;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#29992;&#25143;&#36873;&#25321;&#25972;&#20010;&#24207;&#21015;&#20013;&#38745;&#24577;&#30340;&#21442;&#25968;&#65292;&#36890;&#36807;&#25353;&#38062;&#25110;&#28369;&#22359;&#36827;&#34892;&#36873;&#25321;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#32472;&#21046;&#36718;&#24275;&#24555;&#36895;&#25351;&#23450;&#21442;&#25968;&#22914;&#20309;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#25216;&#24039;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20869;&#30465;&#36712;&#36857;&#29983;&#25104;&#31616;&#27905;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#65292;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#23601;&#33021;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11598</link><description>&lt;p&gt;
&#20869;&#30465;&#25216;&#24039;&#65306;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#19979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Introspective Tips: Large Language Model for In-Context Decision Making. (arXiv:2305.11598v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#25216;&#24039;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20869;&#30465;&#36712;&#36857;&#29983;&#25104;&#31616;&#27905;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#65292;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#23601;&#33021;&#25552;&#39640;&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20135;&#29983;&#20102;&#24040;&#22823;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#8220;&#20869;&#30465;&#25216;&#24039;&#8221;&#26469;&#20419;&#36827;LLMs&#22312;&#33258;&#25105;&#20248;&#21270;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#36890;&#36807;&#20869;&#30465;&#22320;&#26816;&#26597;&#36712;&#36857;&#65292;LLM&#29983;&#25104;&#31616;&#27905;&#32780;&#26377;&#20215;&#20540;&#30340;&#25552;&#31034;&#26469;&#25913;&#21892;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#19977;&#31181;&#37325;&#35201;&#24773;&#22659;&#26469;&#25552;&#39640;&#20195;&#29702;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65306;&#20174;&#20195;&#29702;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#12289;&#25972;&#21512;&#19987;&#23478;&#28436;&#31034;&#21644;&#22312;&#19981;&#21516;&#28216;&#25103;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19981;&#35843;&#25972;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#35843;&#25972;&#25552;&#31034;&#26469;&#20174;&#36825;&#19977;&#31181;&#24773;&#20917;&#20013;&#27010;&#25324;&#35265;&#35299;&#26469;&#23454;&#29616;&#36825;&#20123;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#25903;&#25345;&#32780;&#19988;&#24378;&#35843;&#20102;&#22312;&#19978;&#19979;&#25991;&#20915;&#31574;&#21046;&#23450;&#20013;&#37319;&#29992;LLM&#30340;&#20248;&#21183;&#12290;&#22312;TextWorld&#20013;&#28041;&#21450;&#36229;&#36807;100&#20010;&#28216;&#25103;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21046;&#36896;&#19994;&#32972;&#26223;&#19979;&#65292;&#21487;&#20449;&#12289;&#36127;&#36131;&#12289;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#35752;&#35770;&#20102;&#27599;&#19968;&#27493;&#21487;&#33021;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11581</link><description>&lt;p&gt;
&#21046;&#36896;&#19994;&#21644;&#20379;&#24212;&#38142;&#20013;&#21487;&#20449;&#12289;&#36127;&#36131;&#12289;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#32508;&#36848;&#21450;&#26032;&#20852;&#30740;&#31350;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy, responsible, ethical AI in manufacturing and supply chains: synthesis and emerging research questions. (arXiv:2305.11581v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21046;&#36896;&#19994;&#32972;&#26223;&#19979;&#65292;&#21487;&#20449;&#12289;&#36127;&#36131;&#12289;&#36947;&#24503;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20351;&#29992;&#23454;&#20363;&#35752;&#35770;&#20102;&#27599;&#19968;&#27493;&#21487;&#33021;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21046;&#36896;&#19994;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#21487;&#33021;&#24341;&#21457;&#30340;&#39118;&#38505;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#21508;&#31181;&#39640;&#23618;&#26694;&#26550;&#21644;&#23450;&#20041;&#26469;&#24041;&#22266;&#28508;&#22312;&#39118;&#38505;&#65292;&#20294;&#20174;&#19994;&#32773;&#20173;&#38590;&#20197;&#29702;&#35299;&#21644;&#23454;&#26045;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35299;&#20351;&#24471;&#21046;&#36896;&#19994;&#38754;&#20020;&#30528;&#22810;&#31181;&#39118;&#38505;&#65292;&#21253;&#25324;&#32452;&#32455;&#12289;&#24037;&#20154;&#20197;&#21450;&#20379;&#24212;&#21830;&#21644;&#23458;&#25143;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#21644;&#35299;&#37322;&#20102;&#36127;&#36131;&#12289;&#36947;&#24503;&#21644;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21046;&#36896;&#19994;&#32972;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#36890;&#36807;&#23454;&#20363;&#35752;&#35770;&#20102;&#27599;&#19968;&#27493;&#21487;&#33021;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#20449;&#20219;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#21046;&#36896;&#19994;&#30740;&#31350;&#30028;&#24341;&#39046;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#20197;&#33719;&#24471;&#32463;&#27982;&#21644;&#31038;&#20250;&#30340;&#21452;&#37325;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the increased use of AI in the manufacturing sector has been widely noted, there is little understanding on the risks that it may raise in a manufacturing organisation. Although various high level frameworks and definitions have been proposed to consolidate potential risks, practitioners struggle with understanding and implementing them.  This lack of understanding exposes manufacturing to a multitude of risks, including the organisation, its workers, as well as suppliers and clients. In this paper, we explore and interpret the applicability of responsible, ethical, and trustworthy AI within the context of manufacturing. We then use a broadened adaptation of a machine learning lifecycle to discuss, through the use of illustrative examples, how each step may result in a given AI trustworthiness concern. We additionally propose a number of research questions to the manufacturing research community, in order to help guide future research so that the economic and societal benefits en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.11579</link><description>&lt;p&gt;
&#24102;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#29992;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Speech-Text Dialog Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment. (arXiv:2305.11579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SPECTRA&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24212;&#29992;&#20102;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#65292;&#21516;&#26102;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#65292;&#29992;&#20110;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#24120;&#38024;&#23545;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20102;&#23450;&#21046;&#65292;&#20294;&#26410;&#33021;&#24449;&#26381;&#21508;&#31181;&#35821;&#38899;&#25991;&#26412;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#35821;&#38899;&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#26410;&#33021;&#25506;&#32034;&#23545;&#35805;&#20013;&#30340;&#24773;&#22659;&#20449;&#24687;&#20197;&#20016;&#23500;&#35805;&#35821;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;SPECTRA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35821;&#38899;&#25991;&#26412;&#23545;&#35805;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#32771;&#34385;&#35821;&#38899;&#27169;&#24577;&#30340;&#26102;&#38388;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#30340;&#26102;&#38388;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#26469;&#25429;&#25417;&#35821;&#38899;&#25991;&#26412;&#23545;&#40784;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#26088;&#22312;&#39044;&#27979;&#30456;&#24212;&#35821;&#38899;&#27874;&#24418;&#20013;&#27599;&#20010;&#25991;&#26412;&#21333;&#35789;&#30340;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23398;&#20064;&#21475;&#35821;&#23545;&#35805;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#22238;&#31572;&#36873;&#25321;&#20219;&#21153;&#25512;&#24191;&#21040;&#26381;&#21153;&#20110;&#21475;&#35821;&#23545;&#35805;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11566</link><description>&lt;p&gt;
&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE
&lt;/p&gt;
&lt;p&gt;
StereoVAE: A lightweight stereo matching system through embedded GPUs. (arXiv:2305.11566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#22522;&#20110;VAE&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#65292;&#36798;&#21040;&#20102;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#21644;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23884;&#20837;&#24335;GPU&#23454;&#29616;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;-StereoVAE&#65292;&#23427;&#25171;&#30772;&#20102;&#31435;&#20307;&#21305;&#37197;&#20013;&#31934;&#24230;&#21644;&#22788;&#29702;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#33021;&#22815;&#22312;&#20445;&#35777;&#23454;&#26102;&#22788;&#29702;&#30340;&#21516;&#26102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21305;&#37197;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;&#20256;&#32479;&#21305;&#37197;&#26041;&#27861;&#29983;&#25104;&#30340;&#23567;&#23610;&#23544;&#31895;&#31961;&#35270;&#24046;&#22270;&#36827;&#34892;&#19978;&#37319;&#26679;&#19982;&#32454;&#21270;&#12290;&#36825;&#31181;&#28151;&#21512;&#32467;&#26500;&#19981;&#20165;&#21487;&#20197;&#24102;&#26469;&#20256;&#32479;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36824;&#21487;&#20197;&#20445;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#19979;&#30340;&#21305;&#37197;&#31934;&#24230;&#12290;&#23545;KITTI 2015&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#31435;&#20307;&#21305;&#37197;&#31995;&#32479;&#22312;&#25552;&#39640;&#30001;&#19981;&#21516;&#31639;&#27861;&#29983;&#25104;&#30340;&#31895;&#31961;&#35270;&#24046;&#22270;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#23884;&#20837;&#24335;GPU&#19978;&#23454;&#26102;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.11564</link><description>&lt;p&gt;
&#35299;&#32806;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#65306;&#21487;&#25554;&#25300;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decouple knowledge from paramters for plug-and-play language modeling. (arXiv:2305.11564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20214;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#19982;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#30693;&#35782;&#23384;&#20648;&#20998;&#31163;&#65292;&#37319;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#65292;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#21508;&#31181;&#30693;&#35782;&#12290; &#28982;&#32780;&#65292;&#23558;&#30693;&#35782;&#38544;&#21547;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#20855;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#12290; &#39318;&#20808;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#65292;&#26080;&#27861;&#32534;&#36753;&#25110;&#25193;&#23637;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#30693;&#35782;&#19981;&#26029;&#21457;&#23637;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290; &#20854;&#27425;&#65292;&#23427;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#24182;&#38459;&#27490;&#20154;&#20204;&#20102;&#35299;PLM&#22312;&#26576;&#20010;&#38382;&#39064;&#19978;&#25152;&#38656;&#30340;&#21738;&#20123;&#30693;&#35782;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;PlugLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21487;&#24494;&#20998;&#25554;&#20214;&#23384;&#20648;&#22120;&#65288;DPM&#65289;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290; &#20851;&#38190;&#30340;&#30452;&#35273;&#26159;&#20351;&#29992;&#21487;&#32534;&#36753;&#21644;&#21487;&#25193;&#23637;&#30340;&#38190;&#20540;&#23384;&#20648;&#22120;&#23558;&#30693;&#35782;&#23384;&#20648;&#19982;&#27169;&#22411;&#21442;&#25968;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;DPM&#20013;&#30340;&#30693;&#35782;&#26816;&#32034;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#21033;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#30340;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#35774;&#32622;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#35774;&#32622;&#38656;&#35201;&#21508;&#31181;&#24418;&#24335;&#30340;&#30693;&#35782;&#65306;&#65288;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;&#65288;2&#65289;&#26410;&#35265;&#23454;&#20307;&#21512;&#24182;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22312;&#19981;&#36951;&#24536;&#26087;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models(PLM) have made impressive results in various NLP tasks. It has been revealed that one of the key factors to their success is the parameters of these models implicitly learn all kinds of knowledge during pre-training. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents humans from understanding which knowledge PLM requires for a certain problem. In this paper, we introduce PlugLM, a pre-training model with differentiable plug-in memory(DPM). The key intuition is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the DPM. To justify this design choice, we conduct evaluations in three settings in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11541</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering. (arXiv:2305.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#34892;&#19994;&#20113;&#29305;&#23450;QA&#25968;&#25454;&#38598; MSQA&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#24191;&#27867;&#24212;&#29992;&#21644;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#20294;&#20854;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29305;&#23450;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#24456;&#24179;&#24248;&#65292;&#22240;&#20026;&#23427;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30456;&#20851;&#22522;&#20934;&#27979;&#35797;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MSQA&#30340;&#22522;&#20934;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#28041;&#21450;Microsoft&#20135;&#21697;&#21644;&#23458;&#25143;&#36935;&#21040;&#30340;IT&#25216;&#26415;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#34892;&#19994;&#20113;&#30340;&#29305;&#23450;QA&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#30340;LLM&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#35780;&#20272;&#26088;&#22312;&#25552;&#39640;LLM&#29305;&#23450;&#39046;&#22495;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#20132;&#20114;&#33539;&#24335;&#65292;&#21487;&#20197;&#20351;LLM&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#29305;&#23450;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36981;&#24490;&#25105;&#20204;&#30340;&#27169;&#22411;&#34701;&#21512;&#26694;&#26550;&#30340;&#26041;&#27861;&#27604;&#20351;&#29992;&#26816;&#32034;&#26041;&#27861;&#30340;&#24120;&#29992;LLM&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) has gained popularity and achieved remarkable results in open-domain tasks, but its performance in real industrial domain-specific scenarios is average since there is no specific knowledge in it. This issue has attracted widespread attention, but there are few relevant benchmarks available. In this paper, we provide a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers. This dataset contains industry cloud-specific QA knowledge, which is not available for general LLM, so it is well suited for evaluating methods aimed at improving domain-specific capabilities of LLM. In addition, we propose a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient. Extensive experiments demonstrate that the approach following our model fusion framework outperforms the commonly used LLM with retrieval methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#25903;&#26609;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#19982;&#38544;&#31169;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.11537</link><description>&lt;p&gt;
&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Federated Learning: A Survey. (arXiv:2305.11537v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#25903;&#26609;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#19982;&#38544;&#31169;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#30340;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#35299;&#20915;&#20854;&#21508;&#20010;&#26041;&#38754;&#30340;&#21487;&#20449;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#21487;&#20449;&#32852;&#37030;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#24577;&#30340;&#24191;&#27867;&#27010;&#36848;&#65292;&#25506;&#35752;&#20102;&#19982;&#21487;&#20449;&#24615;&#30456;&#20851;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#25903;&#26609;&#12290;&#23613;&#31649;&#22312;&#26377;&#20851;&#21487;&#20449;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;/&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#25991;&#29486;&#22686;&#38271;&#36805;&#36895;&#65292;&#20294;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#21162;&#21147;&#65292;&#20197;&#30830;&#23450;&#38024;&#23545;FL&#27169;&#22411;&#30340;&#21487;&#20449;&#25903;&#26609;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#21450;&#24320;&#21457;&#35745;&#31639;&#21487;&#20449;&#24230;&#27700;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28085;&#30422;&#19977;&#20010;&#20027;&#35201;&#25903;&#26609;&#30340;&#20998;&#31867;&#27861;&#65306;&#21487;&#35299;&#37322;&#24615;&#65292;&#20844;&#24179;&#24615;&#21644;&#23433;&#20840;&#19982;&#38544;&#31169;&#12290;&#27599;&#20010;&#25903;&#26609;&#20195;&#34920;&#19968;&#32500;&#20449;&#20219;&#65292;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#28085;&#30422;&#20102;&#19982;&#36825;&#19977;&#20010;&#25903;&#26609;&#30456;&#20851;&#30340;FL&#30340;&#21487;&#20449;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#25361;&#25112;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a significant advancement in the field of Artificial Intelligence (AI), enabling collaborative model training across distributed devices while maintaining data privacy. As the importance of FL increases, addressing trustworthiness issues in its various aspects becomes crucial. In this survey, we provide an extensive overview of the current state of Trustworthy FL, exploring existing solutions and well-defined pillars relevant to Trustworthy . Despite the growth in literature on trustworthy centralized Machine Learning (ML)/Deep Learning (DL), further efforts are necessary to identify trustworthiness pillars and evaluation metrics specific to FL models, as well as to develop solutions for computing trustworthiness levels. We propose a taxonomy that encompasses three main pillars: Interpretability, Fairness, and Security &amp; Privacy. Each pillar represents a dimension of trust, further broken down into different notions. Our survey covers trustworthin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#22806;&#37096;&#20195;&#26367;&#24037;&#20855;&#65292;&#25198;&#28436;&#20122;&#24403;&#183;&#26031;&#23494;&#30340;&#20844;&#27491;&#26049;&#35266;&#32773;&#35282;&#33394;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#36947;&#24503;&#35780;&#20272;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.11519</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#20122;&#24403;&#183;&#26031;&#23494;&#30340;&#20844;&#27491;&#26049;&#35266;&#32773;&#30340;&#36947;&#24503;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence moral agent as Adam Smith's impartial spectator. (arXiv:2305.11519v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#20316;&#20026;&#22806;&#37096;&#20195;&#26367;&#24037;&#20855;&#65292;&#25198;&#28436;&#20122;&#24403;&#183;&#26031;&#23494;&#30340;&#20844;&#27491;&#26049;&#35266;&#32773;&#35282;&#33394;&#30340;&#21487;&#33021;&#24615;&#65292;&#20197;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#36947;&#24503;&#35780;&#20272;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20122;&#24403;&#183;&#26031;&#23494;&#21457;&#23637;&#20102;&#19968;&#31181;&#36947;&#24503;&#21746;&#23398;&#65292;&#35748;&#20026;&#36890;&#36807;&#23457;&#38382;&#25105;&#20204;&#20869;&#22312;&#30340;&#20844;&#27491;&#26049;&#35266;&#32773;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#19968;&#20010;&#22806;&#37096;&#30340;&#12289;&#38750;&#22522;&#20110;&#20154;&#31867;&#30340;&#20195;&#26367;&#24037;&#20855;&#30340;&#21487;&#33021;&#24615;&#65292;&#23427;&#23558;&#22686;&#24378;&#25105;&#20204;&#30340;&#20869;&#22312;&#24605;&#32500;&#36807;&#31243;&#65292;&#25198;&#28436;&#20844;&#27491;&#26049;&#35266;&#32773;&#30340;&#35282;&#33394;&#12290;&#36825;&#31181;&#24037;&#20855;&#23558;&#25317;&#26377;&#26356;&#22810;&#30340;&#20851;&#20110;&#19990;&#30028;&#30340;&#30693;&#35782;&#65292;&#26356;&#20844;&#27491;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#36947;&#24503;&#35780;&#20272;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adam Smith developed a version of moral philosophy where better decisions are made by interrogating an impartial spectator within us. We discuss the possibility of using an external non-human-based substitute tool that would augment our internal mental processes and play the role of the impartial spectator. Such tool would have more knowledge about the world, be more impartial, and would provide a more encompassing perspective on moral assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DiffuSIA&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#12290;&#22312;&#36825;&#20010;&#26550;&#26500;&#20013;&#65292;&#26465;&#20214;&#20449;&#24687;&#21644;&#30446;&#26631;&#20449;&#24687;&#20250;&#20132;&#20114;&#25429;&#33719;&#65292;&#20197;&#25552;&#39640;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11517</link><description>&lt;p&gt;
DiffuSIA&#65306;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion. (arXiv:2305.11517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DiffuSIA&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65292;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#12290;&#22312;&#36825;&#20010;&#26550;&#26500;&#20013;&#65292;&#26465;&#20214;&#20449;&#24687;&#21644;&#30446;&#26631;&#20449;&#24687;&#20250;&#20132;&#20114;&#25429;&#33719;&#65292;&#20197;&#25552;&#39640;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#26032;&#19968;&#20195;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#20195;&#34920;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#23427;&#20204;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#37319;&#29992;&#21333;&#20010;&#32534;&#30721;&#22120;&#32467;&#26500;&#21644;&#37096;&#20998;&#22122;&#22768;&#36807;&#31243;&#29992;&#20110;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#26159;&#20854;&#23545;&#20110;&#26465;&#20214;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#23454;&#38469;&#19978;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23545;&#20110;&#23427;&#30340;&#21487;&#20998;&#31163;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22359;&#22825;&#28982;&#26356;&#21152;&#28789;&#27963;&#65292;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#26465;&#20214;&#30340;&#25991;&#26412;&#32534;&#30721;&#36807;&#31243;&#32570;&#20047;&#23545;&#30446;&#26631;&#25991;&#26412;&#30340;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#25991;&#26412;&#25193;&#25955;&#30340;&#34746;&#26059;&#20132;&#20114;&#26550;&#26500;&#65288;DiffuSIA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#20102;&#20174;&#32534;&#30721;&#22120;&#25429;&#33719;&#26465;&#20214;&#20449;&#24687;&#24182;&#34987;&#25193;&#25955;&#35299;&#30721;&#22120;&#25429;&#33719;&#65292;&#20197;&#21450;&#20174;&#35299;&#30721;&#22120;&#25429;&#33719;&#30446;&#26631;&#20449;&#24687;&#24182;&#34987;&#26465;&#20214;&#32534;&#30721;&#22120;&#25429;&#33719;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as the new state-of-the-art family of deep generative models, and their promising potentials for text generation have recently attracted increasing attention. Existing studies mostly adopt a single encoder architecture with partially noising processes for conditional text generation, but its degree of flexibility for conditional modeling is limited. In fact, the encoder-decoder architecture is naturally more flexible for its detachable encoder and decoder modules, which is extensible to multilingual and multimodal generation tasks for conditions and target texts. However, the encoding process of conditional texts lacks the understanding of target texts. To this end, a spiral interaction architecture for encoder-decoder text diffusion (DiffuSIA) is proposed. Concretely, the conditional information from encoder is designed to be captured by the diffusion decoder, while the target information from decoder is designed to be captured by the conditional encoder.
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#22810;&#26234;&#33021;&#20307;&#21462;&#36865;&#36135;&#38382;&#39064;&#20013;&#65292;&#36816;&#29992;Terraforming&#25216;&#26415;&#28789;&#27963;&#22320;&#37325;&#26032;&#23450;&#20301;&#36135;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#36335;&#24452;&#38271;&#24230;&#21644;&#25552;&#39640;&#36816;&#36755;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11510</link><description>&lt;p&gt;
Terraforming - &#22810;&#26234;&#33021;&#20307;&#21462;&#36865;&#36135;&#26399;&#38388;&#30340;&#29615;&#22659;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Terraforming -- Environment Manipulation during Disruptions for Multi-Agent Pickup and Delivery. (arXiv:2305.11510v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11510
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#20179;&#24211;&#30340;&#22810;&#26234;&#33021;&#20307;&#21462;&#36865;&#36135;&#38382;&#39064;&#20013;&#65292;&#36816;&#29992;Terraforming&#25216;&#26415;&#28789;&#27963;&#22320;&#37325;&#26032;&#23450;&#20301;&#36135;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#36335;&#24452;&#38271;&#24230;&#21644;&#25552;&#39640;&#36816;&#36755;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#20179;&#24211;&#20013;&#65292;&#30001;&#31227;&#21160;&#26426;&#22120;&#20154;&#32452;&#25104;&#30340;&#22242;&#38431;&#22312;&#22312;&#32039;&#23494;&#25490;&#21015;&#30340;&#24211;&#23384;&#36135;&#26550;&#20043;&#38388;&#31359;&#34892;&#65292;&#24182;&#23558;&#36135;&#26550;&#20256;&#36882;&#21040;&#25351;&#23450;&#30340;&#24037;&#20316;&#31449;&#20197;&#23436;&#25104;&#21253;&#35013;&#27969;&#31243;&#12290;&#35813;&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#21462;&#36865;&#36135;&#65288;MAPD&#65289;&#38382;&#39064;&#65292;&#28982;&#21518;&#36890;&#36807;&#22312;&#22266;&#23450;&#22270;&#24418;&#19978;&#20026;&#26426;&#22120;&#20154;&#35268;&#21010;&#26080;&#30896;&#25758;&#36335;&#24452;&#26469;&#35299;&#20915;&#65292;&#20363;&#22914;Rolling-Horizon&#30896;&#25758;&#35299;&#20915;&#65288;RHCR&#65289;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20316;&#20986;&#20102;&#19968;&#39033;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#65292;&#21363;&#35268;&#23450;&#26426;&#22120;&#20154;&#21482;&#33021;&#31227;&#21160;&#19982;&#20854;&#24403;&#21069;&#20219;&#21153;&#23545;&#24212;&#30340;&#36135;&#26550;&#65292;&#32780;&#23558;&#20854;&#20182;&#36135;&#26550;&#35270;&#20026;&#38745;&#27490;&#38556;&#30861;&#29289;&#65288;&#23613;&#31649;&#25152;&#26377;&#36135;&#26550;&#37117;&#26159;&#21487;&#31227;&#21160;&#30340;&#65289;&#12290;&#36825;&#31181;&#34892;&#20026;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#38271;&#36335;&#24452;&#65292;&#21542;&#21017;&#21487;&#20197;&#36890;&#36807;&#36135;&#26550;&#25805;&#20316;&#25171;&#24320;&#39069;&#22806;&#30340;&#36890;&#36947;&#26469;&#36991;&#20813;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20801;&#35768;&#26426;&#22120;&#20154;&#21160;&#24577;&#37325;&#26032;&#23450;&#20301;&#36135;&#26550;&#30340;&#28789;&#27963;&#24615;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26032;&#30340;&#38382;&#39064;Terraforming MAPD&#65288;tMAPD&#65289;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;RHCR&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In automated warehouses, teams of mobile robots fulfill the packaging process by transferring inventory pods to designated workstations while navigating narrow aisles formed by tightly packed pods. This problem is typically modeled as a Multi-Agent Pickup and Delivery (MAPD) problem, which is then solved by repeatedly planning collision-free paths for agents on a fixed graph, as in the Rolling-Horizon Collision Resolution (RHCR) algorithm. However, existing approaches make the limiting assumption that agents are only allowed to move pods that correspond to their current task, while considering the other pods as stationary obstacles (even though all pods are movable). This behavior can result in unnecessarily long paths which could otherwise be avoided by opening additional corridors via pod manipulation. To this end, we explore the implications of allowing agents the flexibility of dynamically relocating pods. We call this new problem Terraforming MAPD (tMAPD) and develop an RHCR-based
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.11508</link><description>&lt;p&gt;
&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#38382;&#31572;&#39046;&#22495;&#20855;&#26377;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20855;&#22791;&#20102;&#23545;&#24120;&#35782;&#30340;&#20016;&#23500;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35786;&#26029;&#31574;&#30053;&#65292;LLMs&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#35786;&#26029;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;&#21478;&#19968;&#31181;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#25554;&#20214;&#65292;&#36171;&#20104;LLMs&#25191;&#34892;&#21307;&#30103;&#23545;&#35805;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PlugMed&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#20010;&#27169;&#22359;&#20419;&#36827;&#20102;LLMs&#30340;&#24688;&#24403;&#23545;&#35805;&#21160;&#20316;&#65306;&#25552;&#31034;&#29983;&#25104;&#65288;PG&#65289;&#27169;&#22359;&#21644;&#22238;&#22797;&#25490;&#21517;&#65288;RR&#65289;&#27169;&#22359;&#12290;PG&#27169;&#22359;&#26088;&#22312;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#25429;&#33719;&#23545;&#35805;&#20449;&#24687;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#24230;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems aim to provide accurate answers to patients, necessitating specific domain knowledge. Recent advancements in Large Language Models (LLMs) have demonstrated their exceptional capabilities in the medical Q&amp;A domain, indicating a rich understanding of common sense. However, LLMs are insufficient for direct diagnosis due to the absence of diagnostic strategies. The conventional approach to address this challenge involves expensive fine-tuning of LLMs. Alternatively, a more appealing solution is the development of a plugin that empowers LLMs to perform medical conversation tasks. Drawing inspiration from in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue actions by LLMs through two modules: the prompt generation (PG) module and the response ranking (RR) module. The PG module is designed to capture dialogue information from both global and local perspectives. It selects suitable prompts by assessing 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11498</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#20107;&#20214;&#25277;&#21462;&#20013;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;
&lt;/p&gt;
&lt;p&gt;
Recouple Event Field via Probabilistic Bias for Event Extraction. (arXiv:2305.11498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20559;&#32622;&#30340;&#37325;&#26032;&#32806;&#21512;&#20107;&#20214;&#22330;&#27169;&#22411;&#65288;ProCE&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#65292;&#24182;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#20197;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#65288;EE&#65289;&#26088;&#22312;&#20174;&#20107;&#20214;&#25552;&#21450;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#20107;&#20214;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#65292;&#24050;&#32463;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#35302;&#21457;/&#21442;&#25968;&#23383;&#27573;&#30340;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20107;&#20214;&#27169;&#24335;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#37325;&#26032;&#32806;&#21512;&#27169;&#22411;&#22686;&#24378;&#20107;&#20214;&#25552;&#21462;&#26694;&#26550;&#65288;ProCE&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#35821;&#27861;&#30456;&#20851;&#30340;&#20107;&#20214;&#23383;&#27573;&#24314;&#27169;&#20026;&#27010;&#29575;&#20559;&#32622;&#65292;&#20197;&#28548;&#28165;&#26469;&#33258;&#27169;&#31946;&#32416;&#32544;&#30340;&#20107;&#20214;&#23383;&#27573;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;EE&#20013;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#27425;&#20986;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21516;&#19968;&#35302;&#21457;&#22120;/&#21442;&#25968;&#30340;&#22810;&#20010;&#23383;&#27573;&#20043;&#38388;&#30340;&#27010;&#29575;&#20132;&#20114;&#31574;&#30053;&#65292;&#20197;&#37325;&#26032;&#32806;&#21512;&#30456;&#24212;&#30340;&#28548;&#28165;&#20998;&#24067;&#24182;&#25429;&#33719;&#26356;&#22810;&#28508;&#22312;&#20449;&#24687;&#23383;&#27573;&#12290;&#22312;EE&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Extraction (EE), aiming to identify and classify event triggers and arguments from event mentions, has benefited from pre-trained language models (PLMs). However, existing PLM-based methods ignore the information of trigger/argument fields, which is crucial for understanding event schemas. To this end, we propose a Probabilistic reCoupling model enhanced Event extraction framework (ProCE). Specifically, we first model the syntactic-related event fields as probabilistic biases, to clarify the event fields from ambiguous entanglement. Furthermore, considering multiple occurrences of the same triggers/arguments in EE, we explore probabilistic interaction strategies among multiple fields of the same triggers/arguments, to recouple the corresponding clarified distributions and capture more latent information fields. Experiments on EE datasets demonstrate the effectiveness and generalization of our proposed approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#24050;&#32463;&#22312;&#35270;&#35273;&#23450;&#20301;&#39046;&#22495;&#21331;&#26377;&#25104;&#25928;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#19981;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#26041;&#27861;&#65292;&#21517;&#20026;TreePrompt&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#20998;&#35299;&#25104;&#26641;&#29366;&#32467;&#26500;&#36827;&#34892;&#36880;&#27493;&#25552;&#31034;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11497</link><description>&lt;p&gt;
TreePrompt&#65306;&#23398;&#20064;&#29983;&#25104;&#26641;&#24418;&#25552;&#31034;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding. (arXiv:2305.11497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#24050;&#32463;&#22312;&#35270;&#35273;&#23450;&#20301;&#39046;&#22495;&#21331;&#26377;&#25104;&#25928;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#24615;&#19981;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26500;&#24314;&#26041;&#27861;&#65292;&#21517;&#20026;TreePrompt&#65292;&#36890;&#36807;&#23558;&#21477;&#23376;&#20998;&#35299;&#25104;&#26641;&#29366;&#32467;&#26500;&#36827;&#34892;&#36880;&#27493;&#25552;&#31034;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#25552;&#31034;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#22312;&#23558;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#24050;&#32463;&#25903;&#37197;&#20102;&#35270;&#35273;&#23450;&#20301;&#65288;VG&#65289;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#37117;&#36973;&#21463;&#30528;&#21487;&#35299;&#37322;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20854;&#21487;&#35299;&#37322;&#24615;&#24046;&#26159;&#30001;&#20110;&#20840;&#23616;&#25552;&#31034;&#29983;&#25104;&#21644;&#25512;&#29702;&#36807;&#31243;&#36896;&#25104;&#30340;&#12290;&#36890;&#36807;&#8220;&#20840;&#23616;&#8221;&#65292;&#25105;&#20204;&#26159;&#25351;&#23427;&#20204;&#36890;&#24120;&#30452;&#25509;&#23398;&#20064;&#19968;&#32452;&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65288;&#21363;&#25552;&#31034;&#29983;&#25104;&#65289;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20840;&#23616;&#25552;&#31034;&#22686;&#24378;VG&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#65288;&#21363;&#25552;&#31034;&#25512;&#29702;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26174;&#24335;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#25552;&#31034;&#26500;&#24314;&#33539;&#20363;&#65292;&#31216;&#20026;TreePrompt&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22797;&#26434;&#30340;&#21477;&#23376;&#20998;&#35299;&#25104;&#19968;&#26869;&#19982;&#20154;&#31867;&#25512;&#29702;&#19968;&#33268;&#30340;&#26641;&#12290;&#28982;&#21518;&#65292;&#25353;&#29031;&#35821;&#27861;&#26641;&#65292;&#25105;&#20204;&#20174;&#24213;&#21521;&#19978;&#20197;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#24335;&#26500;&#25104;&#19968;&#20010;&#25552;&#31034;&#12290;&#30001;&#20110;&#36825;&#19968;&#27493;&#39588;&#19968;&#27493;&#19968;&#27493;&#30340;&#25552;&#31034;&#26500;&#24314;&#36807;&#31243;&#65292;e
&lt;/p&gt;
&lt;p&gt;
Prompt tuning has achieved great success in transferring the knowledge from large pretrained vision-language models into downstream tasks, and has dominated the performance on visual grounding (VG). However, almost all existing prompt tuning paradigms suffer from poor interpretability. In this paper, we argue that their poor interpretability is attributed to the holistic prompt generation and inference process. By "holistic", we mean that they usually directly learn a set of vectors as the prompt (i.e., prompt generation), and use the learned global prompt to augment the textual input for the VG model (i.e., prompt inference). To this end, we propose a new prompt construction paradigm with explicit explainable ability, named TreePrompt. Specifically, we first deconstruct a complex sentence into a tree, that is consistent with human reasoning. Then, following the syntax tree, we compose a structured prompt in a bottom-up manner. Thanks to this step-by-step prompt construction process, e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.11490</link><description>&lt;p&gt;
LLM&#33258;&#36523;&#21487;&#35835;&#21462;&#21644;&#29983;&#25104;CXR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11490
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#35757;&#32451;&#12289;&#25110;&#35757;&#32451;&#19987;&#38376;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#26469;&#35835;&#21462;&#21644;&#29983;&#25104;&#20687;&#25991;&#26412;&#19968;&#26679;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#20110;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#65292;&#20154;&#20204;&#27491;&#31215;&#26497;&#23581;&#35797;&#23558;LLMs&#30340;&#23454;&#29992;&#24615;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#24050;&#32463;&#26377;&#20154;&#23581;&#35797;&#36830;&#25509;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#19988;&#20063;&#22312;&#19981;&#26029;&#23581;&#35797;&#20026;LLMs&#28155;&#21152;&#35270;&#35273;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21482;&#20351;&#29992;LLMs&#20316;&#20026;&#22270;&#20687;&#35299;&#30721;&#22120;&#65292;&#27809;&#26377;&#23581;&#35797;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26469;&#29983;&#25104;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;VQ-GAN&#26694;&#26550;&#65292;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#34920;&#31034;&#35270;&#20026;&#19968;&#31181;&#25991;&#26412;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;LLM&#65292;&#20197;&#20687;&#25991;&#26412;&#19968;&#26679;&#35835;&#21462;&#21644;&#29983;&#25104;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32467;&#26500;&#26356;&#25913;&#12289;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#25110;&#35757;&#32451;&#19987;&#38376;&#30340;&#32593;&#32476;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;LLM&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#33016;&#37096;X&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#20449;&#24687;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#32763;&#35793;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11489</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#34917;&#20840;&#23454;&#29616;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Incomplete Multi-view Clustering via Diffusion Completion. (arXiv:2305.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#65292;&#20026;&#20102;&#23545;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26377;&#25928;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#38750;&#24120;&#35268;&#30340;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#37117;&#38656;&#35201;&#35299;&#20915;&#22914;&#20309;&#20943;&#23569;&#32570;&#22833;&#35270;&#35282;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#34917;&#20840;&#26041;&#27861;&#65292;&#23558;&#32570;&#22833;&#35270;&#35282;&#24674;&#22797;&#21040;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#20013;&#12290;&#22522;&#20110;&#21487;&#35266;&#23519;&#35270;&#35282;&#20449;&#24687;&#65292;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24674;&#22797;&#32570;&#22833;&#30340;&#35270;&#35282;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#22810;&#35270;&#35282;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#20197;&#25552;&#39640;&#22810;&#35270;&#35282;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#21487;&#33021;&#26159;&#23558;&#25193;&#25955;&#27169;&#22411;&#32435;&#20837;&#19981;&#23436;&#25972;&#22810;&#35270;&#35282;&#32858;&#31867;&#26694;&#26550;&#30340;&#39318;&#20010;&#24037;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#26041;&#27861;&#22312;&#24674;&#22797;&#32570;&#22833;&#30340;&#35270;&#35282;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete multi-view clustering is a challenging and non-trivial task to provide effective data analysis for large amounts of unlabeled data in the real world. All incomplete multi-view clustering methods need to address the problem of how to reduce the impact of missing views. To address this issue, we propose diffusion completion to recover the missing views integrated into an incomplete multi-view clustering framework. Based on the observable views information, the diffusion model is used to recover the missing views, and then the consistency information of the multi-view data is learned by contrastive learning to improve the performance of multi-view clustering. To the best of our knowledge, this may be the first work to incorporate diffusion models into an incomplete multi-view clustering framework. Experimental results show that the proposed method performs well in recovering the missing views while achieving superior clustering performance compared to state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35762;&#36848;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;Sensecape&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25903;&#25345;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#32455;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11483</link><description>&lt;p&gt;
Sensecape&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22810;&#23618;&#27425;&#25506;&#32034;&#21644;&#30693;&#35782;&#24314;&#26500;
&lt;/p&gt;
&lt;p&gt;
Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models. (arXiv:2305.11483v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11483
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35762;&#36848;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;Sensecape&#65292;&#23427;&#33021;&#22815;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25903;&#25345;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#26080;&#32541;&#20999;&#25442;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#32455;&#21644;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#23398;&#26415;&#30740;&#31350;&#25110;&#35745;&#21010;&#25644;&#21040;&#21478;&#19968;&#20010;&#22478;&#24066;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#24037;&#20316;&#26041;&#24335;&#65292;&#20363;&#22914;&#23558;&#20449;&#24687;&#22312;&#31354;&#38388;&#19978;&#36827;&#34892;&#25490;&#24067;&#20197;&#32452;&#32455;&#24182;&#29702;&#35299;&#23427;&#65292;&#20294;&#30446;&#21069;&#19982;LLM&#20132;&#20114;&#30340;&#30028;&#38754;&#36890;&#24120;&#26159;&#32447;&#24615;&#30340;&#65292;&#20197;&#25903;&#25345;&#23545;&#35805;&#24335;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#24182;&#25506;&#32034;&#22914;&#20309;&#25903;&#25345;LLM-powered&#30340;&#25506;&#32034;&#21644;&#30693;&#35782;&#24314;&#26500;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sensecape&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#25143;&#33021;&#22815;&#65288;1&#65289;&#36890;&#36807;&#22810;&#32423;&#25277;&#35937;&#26469;&#31649;&#29702;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#65292;&#65288;2&#65289;&#26080;&#32541;&#22320;&#22312;&#35268;&#21010;&#21644;&#30693;&#35782;&#24314;&#26500;&#20043;&#38388;&#20999;&#25442;&#65292;&#20197;&#25903;&#25345;LLM&#36827;&#34892;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#34987;&#35797;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;Sensecape&#20351;&#29992;&#25143;&#33021;&#22815;&#25506;&#32034;&#26356;&#22810;&#30340;&#20027;&#39064;&#24182;&#20197;&#20998;&#23618;&#32467;&#26500;&#32452;&#32455;&#20182;&#20204;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#20026;&#22522;&#20110;LLM&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#20449;&#24687;&#20219;&#21153;&#30028;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#21644;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city. However, while they often require working in a nonlinear manner - e.g., to arrange information spatially to organize and make sense of it, current interfaces for interacting with LLMs are generally linear to support conversational interaction. To address this limitation and explore how we can support LLM-powered exploration and sensemaking, we developed Sensecape, an interactive system designed to support complex information tasks with an LLM by enabling users to (1) manage the complexity of information through multilevel abstraction and (2) seamlessly switch between foraging and sensemaking. Our within-subject user study reveals that Sensecape empowers users to explore more topics and structure their knowledge hierarchically. We contribute implications for LLM-based workflows and interfaces for information tasks.
&lt;/p&gt;</description></item><item><title>CCGen&#26159;&#19968;&#20010;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#25490;&#21517;&#21015;&#34920;&#65292;&#24182;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11480</link><description>&lt;p&gt;
CCGen&#65306;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CCGen: Explainable Complementary Concept Generation in E-Commerce. (arXiv:2305.11480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11480
&lt;/p&gt;
&lt;p&gt;
CCGen&#26159;&#19968;&#20010;&#30005;&#23376;&#21830;&#21153;&#20013;&#21487;&#35299;&#37322;&#30340;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#25490;&#21517;&#21015;&#34920;&#65292;&#24182;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20114;&#34917;&#27010;&#24565;&#29983;&#25104;&#65288;CCGen&#65289;&#65306;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;&#8220;&#25968;&#30721;&#30456;&#26426;&#8221;&#65292;&#29983;&#25104;&#19968;&#31995;&#21015;&#20114;&#34917;&#30340;&#27010;&#24565;&#65292;&#20363;&#22914;1&#65289;&#30456;&#26426;&#38236;&#22836;2&#65289;&#30005;&#27744;3&#65289;&#30456;&#26426;&#30418;&#23376;4&#65289;&#23384;&#20648;&#21345;5&#65289;&#30005;&#27744;&#20805;&#30005;&#22120;&#12290;CCGen&#23545;&#20110;&#35832;&#22914;&#26597;&#35810;&#24314;&#35758;&#21644;&#29289;&#21697;&#25512;&#33616;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;CCGen&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25490;&#21517;&#21015;&#34920;&#12290;&#25105;&#20204;&#36824;&#25945;&#25480;&#27169;&#22411;&#29983;&#25104;&#35299;&#37322;&#65292;&#36890;&#36807;&#21152;&#20837;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#35299;&#37322;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20114;&#34917;&#27010;&#24565;&#65292;&#21516;&#26102;&#29983;&#25104;&#35299;&#37322;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and study Complementary Concept Generation (CCGen): given a concept of interest, e.g., "Digital Cameras", generating a list of complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4) Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications like query suggestion and item recommendation, especially in the e-commerce domain. To solve CCGen, we propose to train language models to generate ranked lists of concepts with a two-step training strategy. We also teach the models to generate explanations by incorporating explanations distilled from large teacher models. Extensive experiments and analysis demonstrate that our model can generate high-quality concepts complementary to the input concept while producing explanations to justify the predictions.
&lt;/p&gt;</description></item><item><title>RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11476</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#32676;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#22810;&#26679;&#39118;&#38505;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Risk Preferences in Population-based Self-play. (arXiv:2305.11476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11476
&lt;/p&gt;
&lt;p&gt;
RPPO&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#65292;&#20174;&#32780;&#22686;&#21152;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#39640;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#20013;&#65292;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#35299;&#20915;&#31454;&#20105;&#24615;&#28216;&#25103;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#24403;&#21069;&#30340;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#22312;&#20248;&#21270;&#20195;&#29702;&#31243;&#24207;&#20197;&#26368;&#22823;&#21270;&#39044;&#26399;&#32988;&#29575;&#26102;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#24182;&#20135;&#29983;&#21333;&#19968;&#21516;&#36136;&#21270;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#25171;&#30772;&#20725;&#23616;&#24182;&#22686;&#24378;&#20195;&#29702;&#31243;&#24207;&#38754;&#23545;&#19981;&#21516;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#35299;&#20915;&#26041;&#27861;&#21487;&#33021;&#22312;&#20110;&#22686;&#21152;&#31574;&#30053;&#30340;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#25105;&#23545;&#25239;&#31639;&#27861;&#20013;&#22686;&#21152;&#22810;&#26679;&#24615;&#24182;&#19981;&#26159;&#26131;&#22914;&#21453;&#25484;&#30340;&#12290;&#26412;&#25991;&#35797;&#22270;&#20174;&#20195;&#29702;&#31243;&#24207;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#21487;&#20197;&#20855;&#22791;&#22810;&#26679;&#30340;&#39118;&#38505;&#20559;&#22909;&#36825;&#19968;&#35270;&#35282;&#20986;&#21457;&#22686;&#21152;&#31574;&#30053;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#39118;&#38505;&#25935;&#24863;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(RPPO)&#65292;&#23427;&#22312;&#26368;&#22351;&#21644;&#26368;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#20043;&#38388;&#24179;&#28369;&#22320;&#25554;&#20540;&#65292;&#20801;&#35768;&#20855;&#26377;&#25152;&#38656;&#39118;&#38505;&#20559;&#22909;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the great successes of Reinforcement Learning (RL), self-play algorithms play an essential role in solving competitive games. Current self-play algorithms optimize the agent to maximize expected win-rates against its current or historical copies, making it often stuck in the local optimum and its strategy style simple and homogeneous. A possible solution is to improve the diversity of policies, which helps the agent break the stalemate and enhances its robustness when facing different opponents. However, enhancing diversity in the self-play algorithms is not trivial. In this paper, we aim to introduce diversity from the perspective that agents could have diverse risk preferences in the face of uncertainty. Specifically, we design a novel reinforcement learning algorithm called Risk-sensitive Proximal Policy Optimization (RPPO), which smoothly interpolates between worst-case and best-case policy learning and allows for policy learning with desired risk preferences. Seamlessly inte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11474</link><description>&lt;p&gt;
RAMiT&#65306;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration. (arXiv:2305.11474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36731;&#37327;&#32423;&#22270;&#20687;&#24674;&#22797;&#30340;&#20114;&#24800;&#24335;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#27880;&#24847;&#21147;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24674;&#22797;&#25928;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#35768;&#22810;&#24037;&#20316;&#22312;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#38754;&#20020;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;Transformer&#30340;IR&#26041;&#27861;&#21482;&#20381;&#38752;&#26412;&#22320;&#25110;&#20840;&#23616;&#29305;&#24449;&#65292;&#23548;&#33268;&#25509;&#21463;&#22495;&#26377;&#38480;&#25110;&#23384;&#22312;&#21442;&#25968;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;IR&#32593;&#32476;&#65306;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;RAMiT&#65289;&#12290;&#23427;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#32500;&#24230;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;Transformer&#65288;D-RAMiT&#65289;&#22359;&#65292;&#22312;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#22810;&#22836;&#24182;&#34892;&#35745;&#31639;&#21452;&#21521;&#65288;&#31354;&#38388;&#21644;&#36890;&#36947;&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#21452;&#21521;&#20851;&#27880;&#24110;&#21161;&#24444;&#27492;&#24357;&#34917;&#23545;&#26041;&#30340;&#32570;&#28857;&#65292;&#28982;&#21518;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#20114;&#24800;&#27880;&#24847;&#21147;&#28151;&#21512;&#65288;H-RAMi&#65289;&#23618;&#65292;&#23427;&#34917;&#20607;&#20687;&#32032;&#32423;&#20449;&#24687;&#20002;&#22833;&#24182;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#22312;IR&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31867;&#22411;&#8212;&#8212;&#24378;&#24230;&#25513;&#30721;&#65292;&#20197;&#25552;&#39640;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;IR&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#22122;&#12289;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;JPEG&#22270;&#20687;&#21435;&#22359;&#65292;&#25105;&#20204;&#30340;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22823;&#22823;&#20943;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although many recent works have made advancements in the image restoration (IR) field, they often suffer from an excessive number of parameters. Another issue is that most Transformer-based IR methods focus only on either local or global features, leading to limited receptive fields or deficient parameter issues. To address these problems, we propose a lightweight IR network, Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which compute bi-dimensional (spatial and channel) self-attentions in parallel with different numbers of multi-heads. The bi-dimensional attentions help each other to complement their counterpart's drawbacks and are then mixed. Additionally, we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that compensates for pixel-level information losses and utilizes semantic information while maintaining an efficient hierarchical structure. Furthermore, we revisit 
&lt;/p&gt;</description></item><item><title>Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2305.11473</link><description>&lt;p&gt;
Graphologue&#65306;&#29992;&#20132;&#20114;&#24335;&#22270;&#34920;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graphologue: Exploring Large Language Model Responses with Interactive Diagrams. (arXiv:2305.11473v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11473
&lt;/p&gt;
&lt;p&gt;
Graphologue&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#20197;&#22686;&#24378;&#20854;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#26131;&#20110;&#33719;&#21462;&#21644;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#26234;&#33021;&#32780;&#36817;&#26469;&#39118;&#38753;&#19968;&#26102;&#12290;&#28982;&#32780;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#22312;&#25903;&#25345;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#38480;&#21046;&#65292;&#21407;&#22240;&#26159;&#22522;&#20110;&#25991;&#26412;&#30340;&#23186;&#20171;&#21644;&#32447;&#24615;&#23545;&#35805;&#32467;&#26500;&#25552;&#20379;&#30340;&#21151;&#33021;&#19981;&#36275;&#12290;&#36890;&#36807;&#19982;&#21313;&#21517;&#21442;&#19982;&#32773;&#30340;&#24418;&#24335;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30028;&#38754;&#36890;&#24120;&#20250;&#21576;&#29616;&#20887;&#38271;&#30340;&#21709;&#24212;&#65292;&#20351;&#20154;&#20204;&#38590;&#20197;&#24555;&#36895;&#29702;&#35299;&#21644;&#28789;&#27963;&#22320;&#19982;&#21508;&#31181;&#20449;&#24687;&#36827;&#34892;&#20132;&#20114;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Graphologue&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#23558;LLM&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21709;&#24212;&#36716;&#25442;&#20026;&#22270;&#24418;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20110;&#20449;&#24687;&#26597;&#25214;&#21644;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#12290;Graphologue&#37319;&#29992;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#30028;&#38754;&#35774;&#35745;&#65292;&#20174;LLM&#21709;&#24212;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#65292;&#24182;&#23454;&#26102;&#26500;&#24314;&#33410;&#28857;&#38142;&#25509;&#22270;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#36873;&#25321;&#21644;&#31361;&#20986;&#26174;&#31034;&#29305;&#23450;&#33410;&#28857;&#21644;&#38142;&#25509;&#26469;&#19982;&#36825;&#20123;&#22270;&#34920;&#36827;&#34892;&#20132;&#20114;&#65292;&#20197;&#25506;&#32034;&#30456;&#20851;&#20449;&#24687;&#21644;&#36319;&#36827;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30028;&#38754;&#30456;&#27604;&#65292;Graphologue&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#22312;&#22797;&#26434;&#20449;&#24687;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#21644;&#28385;&#24847;&#24230;&#12290;Graphologue&#20026;&#22686;&#24378;LLM&#22312;&#21508;&#31181;&#24212;&#29992;&#21644;&#39046;&#22495;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently soared in popularity due to their ease of access and the unprecedented intelligence exhibited on diverse applications. However, LLMs like ChatGPT present significant limitations in supporting complex information tasks due to the insufficient affordances of the text-based medium and linear conversational structure. Through a formative study with ten participants, we found that LLM interfaces often present long-winded responses, making it difficult for people to quickly comprehend and interact flexibly with various pieces of information, particularly during more complex tasks. We present Graphologue, an interactive system that converts text-based responses from LLMs into graphical diagrams to facilitate information-seeking and question-answering tasks. Graphologue employs novel prompting strategies and interface designs to extract entities and relationships from LLM responses and constructs node-link diagrams in real-time. Further, users can int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20197;&#24448;&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#30340;&#19981;&#36275;&#21644;&#25152;&#25552;&#20986;&#30340;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#19968;&#31181;&#23436;&#21892;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#35813;&#27979;&#35797;&#21453;&#26144;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#25216;&#33021;&#20114;&#34917;&#24615;&#65292;&#33021;&#22815;&#26500;&#24314;&#26356;&#22810;&#21453;&#26144;&#26234;&#33021;&#21487;&#33021;&#30340;&#27010;&#24565;&#21644;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11472</link><description>&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Testing System Intelligence. (arXiv:2305.11472v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20197;&#24448;&#26234;&#33021;&#31995;&#32479;&#27979;&#35797;&#30340;&#19981;&#36275;&#21644;&#25152;&#25552;&#20986;&#30340;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#19968;&#31181;&#23436;&#21892;&#27979;&#35797;&#30340;&#33021;&#21147;&#65292;&#35813;&#27979;&#35797;&#21453;&#26144;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#25216;&#33021;&#20114;&#34917;&#24615;&#65292;&#33021;&#22815;&#26500;&#24314;&#26356;&#22810;&#21453;&#26144;&#26234;&#33021;&#21487;&#33021;&#30340;&#27010;&#24565;&#21644;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#38024;&#23545;&#26234;&#33021;&#31995;&#32479;&#30340;&#27979;&#35797;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#20854;&#23454;&#26045;&#24102;&#26469;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26367;&#25442;&#27979;&#35797;&#20316;&#20026;&#31995;&#32479;&#22312;&#32473;&#23450;&#29615;&#22659;&#19979;&#25104;&#21151;&#26367;&#25442;&#21478;&#19968;&#20010;&#25191;&#34892;&#20219;&#21153;&#30340;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#34920;&#24449;&#20154;&#31867;&#26234;&#33021;&#30340;&#26174;&#33879;&#26041;&#38754;&#65292;&#36825;&#20123;&#26041;&#38754;&#19981;&#33021;&#34987;&#22270;&#28789;&#27979;&#35797;&#32771;&#34385;&#21040;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26500;&#24314;&#36890;&#36807;&#26367;&#25442;&#27979;&#35797;&#30340;&#26234;&#33021;&#31995;&#32479;&#28041;&#21450;&#21040;&#19968;&#31995;&#21015;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#25152;&#26080;&#27861;&#35299;&#20915;&#30340;&#25216;&#26415;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23454;&#26045;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#24182;&#39564;&#35777;&#26234;&#33021;&#31995;&#32479;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26234;&#33021;&#31995;&#32479;&#39564;&#35777;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#24182;&#20513;&#23548;&#26032;&#30340;&#29702;&#35770;&#22522;&#30784;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#20005;&#26684;&#27979;&#35797;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#25216;&#33021;&#20114;&#34917;&#24615;&#30340;&#26367;&#25442;&#27979;&#35797;&#21487;&#20197;&#23548;&#33268;&#22810;&#31181;&#21453;&#26144;&#32467;&#21512;&#22522;&#20110;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#33021;&#21147;&#30340;&#26234;&#33021;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the adequacy of tests for intelligent systems and practical problems raised by their implementation. We propose the replacement test as the ability of a system to replace successfully another system performing a task in a given context. We show how it can characterize salient aspects of human intelligence that cannot be taken into account by the Turing test. We argue that building intelligent systems passing the replacement test involves a series of technical problems that are outside the scope of current AI. We present a framework for implementing the proposed test and validating the properties of the intelligent systems. We discuss the inherent limitations of intelligent system validation and advocate new theoretical foundations for extending existing rigorous test methods. We suggest that the replacement test, based on the complementarity of skills between human and machine, can lead to a multitude of intelligence concepts reflecting the ability to combine data-based and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11461</link><description>&lt;p&gt;
&#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#20174;&#35821;&#20041;&#32423;&#21035;&#21040;&#20195;&#30721;&#32423;&#21035;&#30340; SelfzCoT&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#22312;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;&#21516;&#26102;&#65292;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807; SelfzCoT &#33258;&#21160;&#33258;&#25105;&#29983;&#25104;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#26356;&#22909;&#22320;&#21033;&#29992;LLMs&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558; SelfzCoT &#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#65292;&#20854;&#20934;&#30830;&#24615;&#20174;GSM8K&#30340;40.50%&#25552;&#39640;&#33267;82.34%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;94.7%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;94.10%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;91.30%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;82.33%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;79.70%&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;&#21069;&#20004;&#20010;&#25345;&#20037;&#36335;&#24452;&#28608;&#27963;&#21040;LLM&#65292;&#29305;&#21035;&#26159;&#20195;&#30721;&#32423;&#21035;&#30340;&#33258;&#25105;&#25552;&#31034;&#65292;&#20351; SelfzCoT &#22312;&#25152;&#26377;&#20845;&#20010;&#38646;&#26679;&#26412;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#38646;&#26679;&#26412;&#32534;&#30721; MzCoT &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#22312;GSM8K&#20013;&#65292;MzCoT&#30340;&#20934;&#30830;&#24615;&#20174;40.50%&#25552;&#39640;&#33267;76.32%&#65292;MultiArith&#20174;79.3%&#25552;&#39640;&#33267;96.97%&#65292;ADDSUB&#20174;74.70%&#25552;&#39640;&#33267;92.39%&#65292;SingleEq&#20174;78.70%&#25552;&#39640;&#33267;94.60%&#65292;AQUA&#20174;31.90%&#25552;&#39640;&#33267;79.90%&#65292;SVAMP&#20174;63.70%&#25552;&#39640;&#33267;81.50%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper show a work on better use of LLMs with SelfzCoT a self-prompt zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%, with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP from 63.70% to 79.70%. Totally, using the first two lasting path activations to LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our modified zero-shot CoT (MzCoT) also achieves remarkable performance in the reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from 40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70% to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to 79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.11460</link><description>&lt;p&gt;
&#33258;&#25105;&#21327;&#35758;&#65306;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#22312;&#19981;&#21516;&#24847;&#35265;&#20043;&#38388;&#25214;&#21040;&#20849;&#35782;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions. (arXiv:2305.11460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#25214;&#21040;&#19981;&#21516;&#24847;&#35265;&#20043;&#38388;&#30340;&#20849;&#35782;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35805;&#39064;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#29702;&#35299;&#20154;&#31867;&#35266;&#28857;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#21327;&#35758;(Self-Agreement)&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#35843;LLMs&#20197;&#33258;&#20027;&#22320;&#25214;&#21040;&#20849;&#35782;&#65292;&#24182;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;3(GPT-3)&#20026;&#38382;&#39064;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#29983;&#25104;&#22810;&#20010;&#24847;&#35265;&#65292;&#24182;&#20026;&#36825;&#20123;&#24847;&#35265;&#21019;&#24314;&#22810;&#20010;&#20849;&#35782;&#20505;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;(BERT)&#30340;&#27169;&#22411;&#35780;&#20272;&#27599;&#20010;&#20849;&#35782;&#20505;&#36873;&#39033;&#30340;&#19968;&#33268;&#24615;&#24471;&#20998;&#65292;&#24182;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#20849;&#35782;&#20505;&#36873;&#39033;&#12290;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;-&#24847;&#35265;-&#20849;&#35782;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#24494;&#35843;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding an agreement among diverse opinions is a challenging topic in multiagent systems. Recently, large language models (LLMs) have shown great potential in addressing this challenge due to their remarkable capabilities in comprehending human opinions and generating human-like text. However, they typically rely on extensive human-annotated data. In this paper, we propose Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find agreement using data generated by LLM itself. Specifically, our approach employs the generative pre-trained transformer-3 (GPT-3) to generate multiple opinions for each question in a question dataset and create several agreement candidates among these opinions. Then, a bidirectional encoder representations from transformers (BERT)-based model evaluates the agreement score of each agreement candidate and selects the one with the highest agreement score. This process yields a dataset of question-opinion-agreements, which we use to fine-tune a p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.11455</link><description>&lt;p&gt;
&#31361;&#30772;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20248;&#21270;&#20855;&#26377;&#21253;&#23481;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models. (arXiv:2305.11455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24605;&#36335;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#20316;&#20026;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#65292;&#21487;&#20197;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65288;RLHF&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#26174;&#24335;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#32780;&#19981;&#26159;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20010;&#22870;&#21169;&#27169;&#22411;&#19982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#32806;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19982;&#26399;&#26395;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#21516;&#26102;&#26159;&#31574;&#30053;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#20989;&#25968;&#12290;&#36825;&#20010;&#35266;&#28857;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#65292;&#21487;&#20197;&#21516;&#26102;&#30452;&#25509;&#36827;&#34892;&#22870;&#21169;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#19979;&#28216;&#31574;&#30053;&#20248;&#21270;&#12290;&#34429;&#28982;&#36825;&#20010;&#35266;&#28857;&#30830;&#23454;&#25171;&#30772;&#20102;&#20256;&#32479;&#26234;&#33021;&#20307;-&#29615;&#22659;&#30028;&#38754;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#35748;&#20026;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#30340;&#20256;&#32479;&#31639;&#27861;&#27010;&#24565;&#36816;&#29992;&#20110;&#36825;&#31181;&#26041;&#27861;&#20013;&#21487;&#20197;&#24102;&#26469;&#24040;&#22823;&#30340;&#32479;&#35745;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning.
&lt;/p&gt;</description></item><item><title>Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#36879;&#26126;&#30340;&#30740;&#31350;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.11444</link><description>&lt;p&gt;
Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598; (arXiv:2305.11444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Arukikata Travelogue Dataset. (arXiv:2305.11444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11444
&lt;/p&gt;
&lt;p&gt;
Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#36879;&#26126;&#30340;&#30740;&#31350;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21019;&#24314;&#20102;Arukikata&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#20813;&#36153;&#25552;&#20379;&#32473;&#23398;&#26415;&#30740;&#31350;&#20351;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;3100&#19975;&#20010;&#26085;&#25991;&#21333;&#35789;&#65292;&#21253;&#25324;4672&#20010;&#26085;&#26412;&#22269;&#20869;&#28216;&#35760;&#21644;9607&#20010;&#28023;&#22806;&#28216;&#35760;&#12290;&#22312;&#25105;&#20204;&#25552;&#20379;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#24456;&#38590;&#33719;&#24471;&#21487;&#29992;&#20110;&#30740;&#31350;&#30340;&#24191;&#27867;&#26053;&#28216;&#28216;&#35760;&#25968;&#25454;&#65292;&#27599;&#20010;&#30740;&#31350;&#20154;&#21592;&#37117;&#24517;&#39035;&#20934;&#22791;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#36825;&#38459;&#30861;&#20102;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#22797;&#21046;&#20197;&#21450;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20844;&#27491;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20351;&#24471;&#20219;&#20309;&#30740;&#31350;&#20154;&#21592;&#37117;&#21487;&#20197;&#23545;&#30456;&#21516;&#30340;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#30830;&#20445;&#30740;&#31350;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#23398;&#26415;&#24847;&#20041;&#12289;&#29305;&#28857;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have constructed Arukikata Travelogue Dataset and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN&#65292;&#36890;&#36807;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#25968;&#25454;&#29615;&#22659;&#19979;&#25429;&#25417;&#26412;&#22320;&#25968;&#25454;&#24635;&#20307;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26694;&#26550;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#36890;&#20449;&#24320;&#38144;&#21644;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11437</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#30340;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN &#29992;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy. (arXiv:2305.11437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550; PS-FedGAN&#65292;&#36890;&#36807;&#37096;&#20998;&#20849;&#20139;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#24067;&#25968;&#25454;&#29615;&#22659;&#19979;&#25429;&#25417;&#26412;&#22320;&#25968;&#25454;&#24635;&#20307;&#29305;&#24449;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26694;&#26550;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12289;&#36890;&#20449;&#24320;&#38144;&#21644;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#30784;&#25968;&#25454;&#32479;&#35745;&#20449;&#24687;&#32780;&#25104;&#20026;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#26377;&#25928;&#23398;&#20064;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;FL&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#23454;&#38469;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;FL&#26694;&#26550;&#22312;&#25429;&#25417;&#23637;&#29616;&#19981;&#21516;&#20998;&#24067;&#30340;&#26412;&#22320;&#29992;&#25143;&#25968;&#25454;&#30340;&#24635;&#20307;&#29305;&#24449;&#24615;&#33021;&#19978;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#26694;&#26550;PS-FedGAN&#65292;&#21482;&#38656;&#35201;&#37096;&#20998;&#30340;GAN&#27169;&#22411;&#20849;&#20139;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#23616;&#37492;&#21035;&#32593;&#32476;&#21644;&#37096;&#20998;&#20849;&#20139;&#30340;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#20197;&#20197;&#37096;&#20998;&#21512;&#20316;&#30340;&#26041;&#24335;&#21327;&#20316;&#23398;&#20064;&#20840;&#23616;&#25968;&#25454;&#32479;&#35745;&#24182;&#22522;&#20110;&#37096;&#20998;&#20849;&#20139;GAN&#36827;&#34892;&#26412;&#22320;&#25968;&#25454;&#20877;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as an effective learning paradigm for distributed computation owing to its strong potential in capturing underlying data statistics while preserving data privacy. However, in cases of practical data heterogeneity among FL clients, existing FL frameworks still exhibit deficiency in capturing the overall feature properties of local client data that exhibit disparate distributions. In response, generative adversarial networks (GANs) have recently been exploited in FL to address data heterogeneity since GANs can be integrated for data regeneration without exposing original raw data. Despite some successes, existing GAN-related FL frameworks often incur heavy communication cost and also elicit other privacy concerns, which limit their applications in real scenarios. To this end, this work proposes a novel FL framework that requires only partial GAN model sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and training mechanism to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11435</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#35757;&#32451;&#30446;&#26631;&#35757;&#32451;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#34920;&#31034;&#38899;&#33410;&#30340;&#21333;&#20803;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#65288;HuBERT&#65289;&#65292;&#22312;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#27809;&#26377;&#34920;&#29616;&#20986;&#36825;&#31181;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#24341;&#23548;&#30446;&#26631;&#23548;&#33268;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#23558;&#30456;&#21516;&#30340;&#38899;&#33410;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35757;&#32451;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;4&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#21333;&#35789;&#20998;&#21106;&#20219;&#21153;&#27867;&#21270;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20987;&#36133;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBERT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11430</link><description>&lt;p&gt;
TELeR&#65306;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#22797;&#26434;&#20219;&#21153;&#30340;LLM&#25552;&#31034;&#30340;&#36890;&#29992;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks. (arXiv:2305.11430v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#26469;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;LLM&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#24615;&#33021;&#21464;&#24322;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLM&#22312;&#20256;&#32479;&#23545;&#35805;&#29615;&#22659;&#20013;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26102;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25191;&#34892;&#19981;&#26126;&#30830;&#30340;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#21463;&#21040;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31867;&#22411;/&#39118;&#26684;&#21644;&#25552;&#31034;&#25552;&#20379;&#30340;&#19981;&#21516;&#35814;&#32454;&#31243;&#24230;&#26102;LLM&#24615;&#33021;&#21464;&#21270;&#24040;&#22823;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#23558;&#20351;&#26410;&#26469;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#33021;&#22815;&#25253;&#21578;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25552;&#31034;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#19981;&#21516;&#30740;&#31350;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
While LLMs have shown great success in understanding and generating text in traditional conversational settings, their potential for performing ill-defined complex tasks is largely under-studied. Indeed, we are yet to conduct comprehensive benchmarking studies with multiple LLMs that are exclusively focused on a complex task. However, conducting such benchmarking studies is challenging because of the large variations in LLMs' performance when different prompt types/styles are used and different degrees of detail are provided in the prompts. To address this issue, the paper proposes a general taxonomy that can be used to design prompts with specific properties in order to perform a wide range of complex tasks. This taxonomy will allow future benchmarking studies to report the specific categories of prompts used as part of the study, enabling meaningful comparisons across different studies. Also, by establishing a common standard through this taxonomy, researchers will be able to draw mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11426</link><description>&lt;p&gt;
&#21518;&#39564;&#35299;&#37322;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Post Hoc Explanations of Language Models Can Improve Language Models. (arXiv:2305.11426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;AMPLIFY&#65292;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#33258;&#21160;&#21270;&#29983;&#25104;&#21407;&#22240;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#20154;&#31867;&#27880;&#37322;&#30340;&#21407;&#29702;&#65288;&#20363;&#22914;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#21407;&#29702;&#21152;&#20837;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#38656;&#35201;&#39640;&#24230;&#30340;&#20154;&#24037;&#21442;&#19982;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25918;&#22823;&#27169;&#22411;&#24615;&#33021;&#65292;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#31216;&#20026;&#23646;&#24615;&#20998;&#25968;&#65288;&#35299;&#37322;&#65289;&#30340;&#20540;&#65292;&#29992;&#20110;&#25429;&#33719;&#27599;&#20010;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33258;&#21160;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#21407;&#29702;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23646;&#24615;&#20998;&#25968;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMPLIFY&#21487;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of- Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11421</link><description>&lt;p&gt;
PastNet&#65306;&#24341;&#20837;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#29992;&#20110;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction. (arXiv:2305.11421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PastNet&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#21033;&#29992;&#20869;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#31354;&#35270;&#39057;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#26681;&#25454;&#21382;&#21490;&#25968;&#25454;&#27969;&#29983;&#25104;&#26410;&#26469;&#35270;&#39057;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#35821;&#20041;&#22320;&#22270;&#31561;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#35270;&#39057;&#39044;&#27979;&#65292;&#20294;&#24120;&#24120;&#24573;&#35270;&#35270;&#39057;&#20869;&#22266;&#26377;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21487;&#33021;&#20250;&#38459;&#30861;&#23545;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29289;&#29702;&#36741;&#21161;&#26102;&#31354;&#32593;&#32476;&#65288;PastNet&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;PastNet&#26680;&#24515;&#22312;&#20110;&#22312;&#20613;&#37324;&#21494;&#22495;&#20013;&#24341;&#20837;&#35889;&#21367;&#31215;&#31639;&#23376;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#24341;&#20837;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#30340;&#23384;&#20648;&#22120;&#24211;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#26102;&#31354;&#20449;&#21495;&#26102;&#31163;&#25955;&#21270;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs 
&lt;/p&gt;</description></item><item><title>JetSeg&#26159;&#19968;&#20010;&#19987;&#20026;GPU-&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#22411;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;JetBlock&#21644;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#30340;&#31574;&#30053;JetConv&#20197;&#21450;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11419</link><description>&lt;p&gt;
JetSeg: &#20302;&#21151;&#32791;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems. (arXiv:2305.11419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11419
&lt;/p&gt;
&lt;p&gt;
JetSeg&#26159;&#19968;&#20010;&#19987;&#20026;GPU-&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#30340;&#39640;&#25928;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#22411;&#30340;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;JetBlock&#21644;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#30340;&#31574;&#30053;JetConv&#20197;&#21450;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#31934;&#20934;&#21644;&#20855;&#26377;&#20302;&#25512;&#29702;&#26102;&#38388;&#30340;&#27169;&#22411;&#12290;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#30828;&#20214;&#33021;&#21147;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#20102;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JetSeg&#30340;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#39640;&#25928;&#27169;&#22411;&#65292;&#30001;&#19968;&#20010;&#31216;&#20026;JetNet&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25913;&#36827;&#30340;RegSeg&#35299;&#30721;&#22120;&#32452;&#25104;&#12290;JetNet&#19987;&#20026;GPU&#23884;&#20837;&#24335;&#31995;&#32479;&#35774;&#35745;&#65292;&#24182;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#31216;&#20026;JetBlock&#30340;&#26032;&#22411;&#36731;&#37327;&#32423;&#39640;&#25928;&#22359;&#65292;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#26469;&#26368;&#23567;&#21270;&#20869;&#23384;&#20351;&#29992;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#20934;&#30830;&#24615;&#65307;&#19968;&#31181;&#31216;&#20026;JetConv &#30340;&#26032;&#31574;&#30053;&#65292;&#23427;&#32467;&#21512;&#20102;&#19981;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21367;&#31215;&#12289;&#28145;&#24230;&#31354;&#27934;&#21367;&#31215;&#12289;&#36890;&#36947;&#28151;&#27927;&#25805;&#20316;&#12289;&#36731;&#37327;&#32423;&#28608;&#27963;&#20989;&#25968;&#21644;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#26041;&#20415;&#25968;&#37327;&#30340;&#32452;&#21367;&#31215;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;JetLoss&#65292;&#23427;&#38598;&#25104;&#20102;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JetSeg&#22312;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time semantic segmentation is a challenging task that requires high-accuracy models with low-inference times. Implementing these models on embedded systems is limited by hardware capability and memory usage, which produces bottlenecks. We propose an efficient model for real-time semantic segmentation called JetSeg, consisting of an encoder called JetNet, and an improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and includes two main components: a new light-weight efficient block called JetBlock, that reduces the number of parameters minimizing memory usage and inference time without sacrificing accuracy; a new strategy that involves the combination of asymmetric and non-asymmetric convolutions with depthwise-dilated convolutions called JetConv, a channel shuffle operation, light-weight activation functions, and a convenient number of group convolutions for embedded systems, and an innovative loss function named JetLoss, which integrates the Precision, Recall,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11414</link><description>&lt;p&gt;
&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65306;&#29992;&#20110;&#22823;&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models. (arXiv:2305.11414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22914;BERT&#12289;GPT&#12289;ViT&#21644;CLIP&#65292;&#20294;&#20854;&#20248;&#21270;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#25935;&#24863;&#25968;&#25454;&#65292;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#24182;&#38480;&#21046;&#20854;&#36866;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#65288;FFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21487;&#23454;&#29616;&#36328;&#22810;&#20010;&#26426;&#26500;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Models (FMs), such as BERT, GPT, ViT, and CLIP, have demonstrated remarkable success in a wide range of applications, driven by their ability to leverage vast amounts of data for pre-training. However, optimizing FMs often requires access to sensitive data, raising privacy concerns and limiting their applicability in certain domains. In this paper, we introduce the concept of Federated Foundation Models (FFMs), a novel approach that combines the benefits of FMs and Federated Learning (FL) to enable privacy-preserving and collaborative learning across multiple institutions. We discuss the potential benefits and challenges of integrating FL into the lifespan of FMs, covering pre-training, fine-tuning, and application. We further provide formal definitions of FFM tasks, including FFM pre-training, FFM fine-tuning, and federated prompt engineering, allowing for more personalized and context-aware models while maintaining data privacy. Moreover, we explore the possibility of cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#39640;&#25928;&#30340;&#20107;&#20214;&#31867;&#22411;&#26631;&#27880;&#31639;&#27861;&#65288;LATTE&#65289;&#65292;&#29992;&#20110;&#20934;&#30830;&#26631;&#27880;&#32437;&#21521;EHR&#25968;&#25454;&#20013;&#30340;&#20020;&#24202;&#20107;&#20214;&#26102;&#38388;&#65292;&#36890;&#36807;&#26102;&#38388;&#27169;&#24335;&#25366;&#25496;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#32437;&#21521;&#38134;&#26631;&#20934;&#20107;&#20214;&#26631;&#31614;&#65292;&#24182;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;7.57&#65285;&#21644;10.82&#65285;&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2305.11407</link><description>&lt;p&gt;
LATTE: &#20174;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#39640;&#25928;&#26631;&#27880;&#20107;&#20214;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
LATTE: Label-efficient Incident Phenotyping from Longitudinal Electronic Health Records. (arXiv:2305.11407v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#39640;&#25928;&#30340;&#20107;&#20214;&#31867;&#22411;&#26631;&#27880;&#31639;&#27861;&#65288;LATTE&#65289;&#65292;&#29992;&#20110;&#20934;&#30830;&#26631;&#27880;&#32437;&#21521;EHR&#25968;&#25454;&#20013;&#30340;&#20020;&#24202;&#20107;&#20214;&#26102;&#38388;&#65292;&#36890;&#36807;&#26102;&#38388;&#27169;&#24335;&#25366;&#25496;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#32437;&#21521;&#38134;&#26631;&#20934;&#20107;&#20214;&#26631;&#31614;&#65292;&#24182;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;7.57&#65285;&#21644;10.82&#65285;&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#25903;&#25345;&#23454;&#38469;&#35777;&#25454;&#65288;RWE&#65289;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23427;&#29983;&#25104;&#21487;&#38752;&#30340;RWE&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#32570;&#20047;&#26377;&#20851;&#20020;&#24202;&#20107;&#20214;&#30340;&#26102;&#38388;&#65288;&#20363;&#22914;&#24515;&#21147;&#34928;&#31469;&#21457;&#20316;&#26102;&#38388;&#65289;&#30340;&#31934;&#30830;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#39640;&#25928;&#30340;&#20107;&#20214;&#31867;&#22411;&#26631;&#27880;&#31639;&#27861;&#65288;LATTE&#65289;&#65292;&#20197;&#20934;&#30830;&#26631;&#27880;&#32437;&#21521;EHR&#25968;&#25454;&#20013;&#30340;&#20020;&#24202;&#20107;&#20214;&#26102;&#38388;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#23884;&#20837;&#21521;&#37327;&#65292;&#26681;&#25454;&#19982;&#30446;&#26631;&#20107;&#20214;&#30340;&#20851;&#31995;&#25366;&#25496;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;LATTE&#22312;&#27010;&#24565;&#37325;&#26032;&#21152;&#26435;&#27169;&#22359;&#20013;&#36873;&#25321;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#35775;&#38382;&#27880;&#24847;&#21147;&#23398;&#20064;&#32593;&#32476;&#23558;&#20854;&#20449;&#24687;&#21387;&#32553;&#20026;&#32437;&#21521;&#35775;&#38382;&#23884;&#20837;&#12290;LATTE&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#30446;&#26631;&#20107;&#20214;&#20043;&#21069;/&#20043;&#21518;&#30340;&#39034;&#24207;&#20381;&#36182;&#20851;&#31995;&#21644;&#35775;&#38382;&#23884;&#20837;&#12290;&#20026;&#20102;&#25552;&#39640;&#26631;&#31614;&#25928;&#29575;&#65292;LATTE&#36890;&#36807;&#26102;&#38388;&#27169;&#24335;&#25366;&#25496;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#32437;&#21521;&#38134;&#26631;&#20934;&#20107;&#20214;&#26631;&#31614;&#65288;SSEL&#65289;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#35266;&#27979;&#25968;&#25454;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;MIMIC-III&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LATTE&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#24613;&#24615;&#32958;&#25439;&#20260;&#21457;&#20316;&#26041;&#38754;&#65292;&#22312;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#20998;&#21035;&#23454;&#29616;&#20102;7.57&#65285;&#21644;10.82&#65285;&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health record (EHR) data are increasingly used to support real-world evidence (RWE) studies. Yet its ability to generate reliable RWE is limited by the lack of readily available precise information on the timing of clinical events such as the onset time of heart failure. We propose a LAbel-efficienT incidenT phEnotyping (LATTE) algorithm to accurately annotate the timing of clinical events from longitudinal EHR data. By leveraging the pre-trained semantic embedding vectors from large-scale EHR data as prior knowledge, LATTE selects predictive EHR features in a concept re-weighting module by mining their relationship to the target event and compresses their information into longitudinal visit embeddings through a visit attention learning network. LATTE employs a recurrent neural network to capture the sequential dependency between the target event and visit embeddings before/after it. To improve label efficiency, LATTE constructs highly informative longitudinal silver-standar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.11391</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20026;&#32456;&#31471;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#21644;&#26377;&#26465;&#29702;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;AI&#30340;&#19968;&#27874;&#26032;&#28909;&#28526;&#12290;&#20026;&#20102;&#24212;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#26412;&#27425;&#35843;&#26597;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;LLM&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23558;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20256;&#32479;&#36719;&#20214;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65288;V&#65286;V&#65289;&#25216;&#26415;&#65292;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;LLM&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#65292;&#20197;&#25552;&#20379;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#30830;&#20445;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#65306;&#34394;&#20551;&#24615;&#21644;&#35780;&#20272;&#12289;&#39564;&#35777;&#12289;&#36816;&#34892;&#26102;&#30417;&#35270;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#32771;&#34385;&#21040;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&amp;V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20154;&#21147;&#21644;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24314;&#27169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.11390</link><description>&lt;p&gt;
ALT: &#19968;&#31181;&#29992;&#20110;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ALT: An Automatic System for Long Tail Scenario Modeling. (arXiv:2305.11390v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#20154;&#21147;&#21644;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24314;&#27169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#38271;&#23614;&#22330;&#26223;&#24314;&#27169;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#20154;&#21147;&#36164;&#28304;&#19981;&#36275;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#38454;&#27573;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALT&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22810;&#31181;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#25216;&#26415;&#65292;&#37319;&#29992;&#20803;&#23398;&#20064;&#21746;&#23398;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#39044;&#31639;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#31561;&#65292;&#20197;&#25913;&#36827;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20174;&#31995;&#32479;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#22810;&#39033;&#20248;&#21270;&#65292;&#24182;&#21152;&#20837;&#20102;&#24517;&#35201;&#30340;&#27169;&#22359;&#65292;&#20351;&#31995;&#32479;&#26356;&#20855;&#21487;&#34892;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#20013;&#20851;&#38190;&#27169;&#22359;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of long tail scenario modeling with budget limitation, i.e., insufficient human resources for model training stage and limited time and computing resources for model inference stage. This problem is widely encountered in various applications, yet has received deficient attention so far. We present an automatic system named ALT to deal with this problem. Several efforts are taken to improve the algorithms used in our system, such as employing various automatic machine learning related techniques, adopting the meta learning philosophy, and proposing an essential budget-limited neural architecture search method, etc. Moreover, to build the system, many optimizations are performed from a systematic perspective, and essential modules are armed, making the system more feasible and efficient. We perform abundant experiments to validate the effectiveness of our system and demonstrate the usefulness of the critical modules in our system. Moreover, online r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11389</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization Deep Graph Transformation. (arXiv:2305.11389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#27867;&#21270;&#30340;&#28145;&#24230;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#65292;&#36890;&#36807;&#21152;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#35757;&#32451;&#27867;&#21270;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22270;&#24418;&#20174;&#19968;&#31181;&#27169;&#24335;&#36716;&#21464;&#20026;&#21478;&#19968;&#31181;&#27169;&#24335;&#30340;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#37325;&#35201;&#21644;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#34429;&#28982;&#22312;&#24320;&#21457;&#20808;&#36827;&#30340;&#22270;&#24418;&#36716;&#25442;&#25216;&#26415;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#20445;&#25345;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21487;&#29992;&#30340;&#22270;&#24418;&#30340;&#39046;&#22495;&#27867;&#21270;&#22270;&#24418;&#36716;&#25442;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#35299;&#20915;&#22810;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#24403;&#35757;&#32451;&#25152;&#26377;&#36755;&#20837;-&#36755;&#20986;&#27169;&#24335;&#32452;&#21512;&#26102;&#30340;&#26497;&#31471;&#31354;&#38388;&#22797;&#26434;&#24230;&#12289;&#65288;2&#65289;&#36755;&#20837;&#21644;&#36755;&#20986;&#27169;&#24335;&#20043;&#38388;&#30340;&#22270;&#24418;&#25299;&#25169;&#24046;&#24322;&#65292;&#20197;&#21450;(3)&#22914;&#20309;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#23384;&#22312;&#30340;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#30446;&#26631;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#36755;&#20837;-&#22810;&#36755;&#20986;&#12289;&#36229;&#32593;&#32476;&#30340;&#22270;&#24418;&#36716;&#25442;&#26041;&#27861;&#65288;MultiHyperGNN&#65289;&#65292;&#23427;&#21033;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#32534;&#30721;&#36755;&#20837;&#21644;&#36755;&#20986;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#36755;&#20986;&#22270;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#25429;&#25417;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#26469;&#20197;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#20989;&#25968;&#65292;&#35777;&#26126;&#28145;&#24230;&#23398;&#20064;&#20013;ReLU&#28608;&#27963;&#19979;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#65292;&#25361;&#25112;&#20102;&#23545;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#29702;&#35770;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11387</link><description>&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#20844;&#27491;&#20043;&#22768;
&lt;/p&gt;
&lt;p&gt;
Justices for Information Bottleneck Theory. (arXiv:2305.11387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#20989;&#25968;&#65292;&#35777;&#26126;&#28145;&#24230;&#23398;&#20064;&#20013;ReLU&#28608;&#27963;&#19979;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#65292;&#25361;&#25112;&#20102;&#23545;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#35813;&#29702;&#35770;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#29702;&#35770;&#19981;&#26029;&#22686;&#21152;&#30340;&#36136;&#30097;&#20570;&#20986;&#21450;&#26102;&#22238;&#24212;&#65292;&#27880;&#20837;&#26032;&#30340;&#35266;&#28857;&#20197;&#32416;&#27491;&#35823;&#35299;&#24182;&#37325;&#30003;&#20854;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36741;&#21161;&#20989;&#25968;&#65292;&#23558;&#26368;&#22823;&#32534;&#30721;&#29575;&#38477;&#20302;&#27861;&#35299;&#37322;&#20026;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#30340;&#29305;&#27530;&#20294;&#23616;&#37096;&#26368;&#20248;&#24773;&#20917;&#12290;&#36890;&#36807;&#36825;&#31181;&#36741;&#21161;&#20989;&#25968;&#65292;&#25105;&#20204;&#28548;&#28165;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#32593;&#32476;&#20013;&#24212;&#29992;ReLU&#28608;&#27963;&#26102;&#20114;&#20449;&#24687;&#19979;&#38477;&#30340;&#24726;&#35770;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#36741;&#21161;&#20989;&#25968;&#30340;&#35270;&#35282;&#65292;&#35777;&#26126;&#20102;IB&#29702;&#35770;&#35299;&#37322;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#22312;&#38544;&#34255;&#23618;&#20013;&#27809;&#26377;&#21387;&#32553;&#38454;&#27573;&#30340;&#33021;&#21147;&#65292;&#25361;&#25112;&#20102;&#23545;IB&#29702;&#35770;&#36866;&#29992;&#24615;&#30340;&#36136;&#30097;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35266;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;IB&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;DL&#32593;&#32476;&#20869;&#37096;&#32452;&#32455;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#36817;&#30340;&#23454;&#39564;&#35777;&#25454;&#30456;&#19968;&#33268;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26159;&#23545;IB&#29702;&#35770;&#30340;&#20844;&#27491;&#20043;&#22768;&#65292;&#20063;&#20026;&#26410;&#26469;&#35813;&#29702;&#35770;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#21069;&#30651;&#24615;&#30340;&#24605;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, poten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.11364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models. (arXiv:2305.11364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23618;&#21487;&#35270;&#21270;&#26469;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#29983;&#25104;&#26356;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#12289;&#24494;&#35843;&#25110;&#20854;&#20182;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#25968;&#25454;&#38598;&#24456;&#22256;&#38590;&#65292;&#32780;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#22833;&#36133;&#27169;&#24335;&#20173;&#19981;&#20026;&#20154;&#20204;&#25152;&#29702;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#21487;&#33021;&#20197;&#24847;&#22806;&#30340;&#26041;&#24335;&#21464;&#24471;&#37325;&#22797;&#65292;&#19981;&#20165;&#35821;&#20041;&#19978;&#22914;&#27492;&#65292;&#32780;&#19988;&#22312;&#21477;&#27861;&#12289;&#35789;&#27719;&#26041;&#38754;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LinguisticLens&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#21644;&#20998;&#26512;LLM&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#21477;&#27861;&#22810;&#26679;&#24615;&#12290; LinguisticLens&#27839;&#30528;&#21477;&#27861;&#12289;&#35789;&#27719;&#21644;&#35821;&#20041;&#36724;&#23558;&#25991;&#26412;&#32858;&#31867;&#12290;&#23427;&#25903;&#25345;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#21487;&#35270;&#21270;&#65292;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#27983;&#35272;&#27010;&#36848;&#21644;&#26816;&#26597;&#21508;&#20010;&#31034;&#20363;&#12290;&#23454;&#26102;&#28436;&#31034;&#21487;&#22312;shorturl.at/zHOUV&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLM-generated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel inter-active visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at shorturl.at/zHOUV.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;CATE&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#24182;&#20351;&#29992;&#38381;&#24335;&#27714;&#35299;&#22120;&#33719;&#24471;&#21442;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#20248;&#21270;CATE&#20272;&#35745;&#34920;&#29616;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.11353</link><description>&lt;p&gt;
&#20855;&#26377;&#38381;&#24335;&#27714;&#35299;&#22120;&#30340;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-learning for heterogeneous treatment effect estimation with closed-form solvers. (arXiv:2305.11353v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;CATE&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#24182;&#20351;&#29992;&#38381;&#24335;&#27714;&#35299;&#22120;&#33719;&#24471;&#21442;&#25968;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#21644;&#20248;&#21270;CATE&#20272;&#35745;&#34920;&#29616;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23569;&#37327;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#20174;&#22810;&#20010;&#20219;&#21153;&#20013;&#20272;&#35745;CATE&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#36827;&#34892;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;CATE&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20219;&#21153;&#20849;&#20139;&#21644;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#26500;&#24314;&#25105;&#20204;&#30340;&#20272;&#35745;&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20844;&#24335;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#21487;&#24494;&#20998;&#30340;&#38381;&#24335;&#30340;&#26368;&#20248;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#36825;&#20123;&#21442;&#25968;&#33021;&#22815;&#30456;&#23545;&#20110;&#20219;&#21153;&#20849;&#20139;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#20803;&#23398;&#20064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20219;&#21153;&#20849;&#20139;&#21442;&#25968;&#65292;&#20197;&#20351;&#23569;&#31034;&#28857;&#35774;&#32622;&#19979;&#30340;CATE&#20272;&#35745;&#34920;&#29616;&#36890;&#36807;&#23558;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#20272;&#35745;&#30340;CATE&#19982;&#20165;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#20272;&#35745;&#30340;CATE&#20043;&#38388;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#24471;&#21040;&#25913;&#21892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CATE&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22810;&#20809;&#35889;&#25968;&#25454;&#19981;&#33021;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.11347</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#20809;&#35889;&#20998;&#21106;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#21644;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning. (arXiv:2305.11347v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20809;&#35889;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#22810;&#20809;&#35889;&#25968;&#25454;&#19981;&#33021;&#25552;&#39640;&#27169;&#22411;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21462;&#20915;&#20110;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33322;&#31354;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;RGB&#36890;&#36947;&#20043;&#22806;&#65292;&#21253;&#25324;&#26356;&#22810;&#20809;&#35889;&#27874;&#27573;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39069;&#22806;&#25968;&#25454;&#32435;&#20837;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#33258;&#28982;&#25200;&#21160;&#30340;&#25269;&#25239;&#21147;&#22914;&#20309;&#24433;&#21709;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#26088;&#22312;&#34920;&#24449;&#22810;&#20809;&#35889;&#65288;RGB&#21644;&#36817;&#32418;&#22806;&#65289;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#33258;&#28982;&#25200;&#21160;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#34429;&#28982;&#22810;&#20809;&#35889;&#25968;&#25454;&#33021;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#33021;&#25552;&#39640;&#20854;&#23545;&#33258;&#28982;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#25915;&#20987;&#26041;&#27861;&#21644;&#20351;&#29992;&#30340;&#29305;&#23450;&#20809;&#35889;&#27874;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In overhead image segmentation tasks, including additional spectral bands beyond the traditional RGB channels can improve model performance. However, it is still unclear how incorporating this additional data impacts model robustness to adversarial attacks and natural perturbations. For adversarial robustness, the additional information could improve the model's ability to distinguish malicious inputs, or simply provide new attack avenues and vulnerabilities. For natural perturbations, the additional information could better inform model decisions and weaken perturbation effects or have no significant influence at all. In this work, we seek to characterize the performance and robustness of a multispectral (RGB and near infrared) image segmentation model subjected to adversarial attacks and natural perturbations. While existing adversarial and natural robustness research has focused primarily on digital perturbations, we prioritize on creating realistic perturbations designed with physi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#21487;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Tree-Search&#37319;&#26679;&#25216;&#26415;&#26377;&#21161;&#20110;&#20174;&#25552;&#31034;&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#32780;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21487;&#20351;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#29983;&#25104;&#26356;&#22909;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#25552;&#39640;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11334</link><description>&lt;p&gt;
&#32534;&#20889;&#33258;&#24049;&#30340;&#20070;&#65306;&#19968;&#31181;&#20174;&#38381;&#21512;&#21040;&#24320;&#25918;&#24335;&#20070;&#26412;QA&#30340;&#26041;&#27861;&#65292;&#25913;&#21892;&#36739;&#23567;LLM&#30340;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs. (arXiv:2305.11334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#21487;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Tree-Search&#37319;&#26679;&#25216;&#26415;&#26377;&#21161;&#20110;&#20174;&#25552;&#31034;&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#32780;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21487;&#20351;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#29983;&#25104;&#26356;&#22909;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#25552;&#39640;&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;Tree-Search&#21644;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290; Tree-Search&#26159;&#19968;&#31181;&#37319;&#26679;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#20174;&#32473;&#23450;&#25552;&#31034;&#30340;LLM&#20013;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#33258;&#25105;&#19978;&#19979;&#25991;QA&#21033;&#29992;Tree-Search&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#21508;&#31181;&#20449;&#24687;&#21019;&#24314;&#33258;&#24049;&#30340;&#19978;&#19979;&#25991;&#65292;&#26126;&#30830;&#35780;&#20272;&#24182;&#36820;&#22238;&#21021;&#22987;&#25552;&#31034;&#30340;&#24320;&#25918;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25353;&#29031;&#21508;&#31181;&#25351;&#26631;&#65288;&#21253;&#25324;GPT3.5&#65288;text-davinci-003&#65289;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12289;&#20449;&#24687;&#37327;&#12289;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#65289;&#35780;&#20272;&#30340;&#29983;&#25104;&#31572;&#26696;&#36136;&#37327;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#22686;&#21152;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#26641;&#22823;&#23567;&#21576;&#27491;&#30456;&#20851;&#65292;&#20174;&#32780;&#26377;&#30410;&#20110;&#31572;&#26696;&#36136;&#37327;&#21644;&#20581;&#22766;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Tree-Search&#30340;&#20854;&#20182;&#26377; promising &#24212;&#29992;&#65292;&#31361;&#20986;&#20102;&#20854;&#25552;&#39640;&#36739;&#23567;LLM&#20581;&#22766;&#24615;&#21644;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce two novel methods, Tree-Search and Self-contextualizing QA, designed to enhance the performance of large language models (LLMs) in question-answering tasks. Tree-Search is a sampling technique specifically created to extract diverse information from an LLM for a given prompt. Self-contextualizing QA leverages Tree-Search to enable the model to create its own context using a wide range of information relevant to the prompt, evaluate it explicitly and return a open book answer to the initial prompt . We demonstrate that the quality of generated answers improves according to various metrics, including accuracy, informativeness, coherence, and consistency, as evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods result in increased robustness and that performance is positively correlated with tree size, benefiting both answer quality and robustness. Finally, we discuss other promising applications of Tree-Search, highlighting its potential to enhance a b
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65288;PEL&#65289;&#65292;&#21033;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#35821;&#38899;&#21512;&#25104;&#21475;&#38899;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;OT&#30340;&#36741;&#21161;&#26080;&#30417;&#30563;&#25439;&#22833;&#26469;&#26368;&#22823;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11320</link><description>&lt;p&gt;
&#38024;&#23545;&#35821;&#38899;&#21512;&#25104;&#21475;&#38899;&#36866;&#24212;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Learning for Text-to-Speech Accent Adaptation. (arXiv:2305.11320v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65288;PEL&#65289;&#65292;&#21033;&#29992;&#29702;&#35770;&#22522;&#30784;&#30340;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26469;&#23454;&#29616;&#20302;&#36164;&#28304;&#35821;&#38899;&#21512;&#25104;&#21475;&#38899;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;OT&#30340;&#36741;&#21161;&#26080;&#30417;&#30563;&#25439;&#22833;&#26469;&#26368;&#22823;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65288;PEL&#65289;&#26469;&#24320;&#21457;&#20302;&#36164;&#28304;&#30340;&#35821;&#38899;&#21512;&#25104;&#21475;&#38899;&#36866;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;1.2&#65285;&#33267;0.8&#65285;&#65292;&#20174;&#24050;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;TTS&#27169;&#22411;&#20013;&#24320;&#21457;&#20986;&#36164;&#28304;&#39640;&#25928;&#30340;&#36866;&#24212;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26377;&#31454;&#20105;&#21147;&#30340;&#35821;&#38899;&#21512;&#25104;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#36827;&#34892;TTS&#30340;PEL&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;OT&#30340;&#36741;&#21161;&#26080;&#30417;&#30563;&#25439;&#22833;&#65292;&#20197;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#22495;&#21644;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#38500;&#20102;&#26377;&#30417;&#30563;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26080;&#30417;&#30563;&#30340;&#25439;&#22833;&#32454;&#21270;&#26469;&#36890;&#36807;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#25110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#30340;&#20248;&#28857;&#26159;&#22312;&#35780;&#20272;&#26222;&#36890;&#35805;&#21475;&#38899;&#36866;&#24212;&#26102;&#65292;&#36890;&#36807;&#20351;&#29992;&#27531;&#24046;&#36866;&#37197;&#22120;&#23398;&#20064;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;PEL&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a parameter-efficient learning (PEL) to develop a low-resource accent adaptation for text-to-speech (TTS). A resource-efficient adaptation from a frozen pre-trained TTS model is developed by using only 1.2\% to 0.8\% of original trainable parameters to achieve competitive performance in voice synthesis. Motivated by a theoretical foundation of optimal transport (OT), this study carries out PEL for TTS where an auxiliary unsupervised loss based on OT is introduced to maximize a difference between the pre-trained source domain and the (unseen) target domain, in addition to its supervised training loss. Further, we leverage upon this unsupervised loss refinement to boost system performance via either sliced Wasserstein distance or maximum mean discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions based on residual adapter learning, and model reprogramming when evaluating the Mandarin accent adaptation. Experiment results show that the proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.11311</link><description>&lt;p&gt;
BELLA: &#36890;&#36807;&#26412;&#22320;&#32447;&#24615;&#36924;&#36817;&#36827;&#34892;&#40657;&#30418;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
BELLA: Black box model Explanations by Local Linear Approximations. (arXiv:2305.11311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20107;&#21518;&#26041;&#27861;BELLA&#65292;&#29992;&#20110;&#35299;&#37322;&#22238;&#24402;&#40657;&#30418;&#27169;&#22411;&#30340;&#20010;&#21035;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#20013;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#30340;&#31995;&#25968;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#35745;&#31639;&#29305;&#24449;&#20540;&#30340;&#39044;&#27979;&#20540;&#12290;&#27492;&#22806;&#65292;BELLA&#26368;&#22823;&#21270;&#20102;&#32447;&#24615;&#27169;&#22411;&#36866;&#29992;&#30340;&#39046;&#22495;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29702;&#35299;&#40657;&#30418;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#19981;&#20165;&#25104;&#20026;&#27861;&#24459;&#35201;&#27714;&#65292;&#20063;&#25104;&#20026;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#21478;&#19968;&#31181;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#24182;&#21487;&#33021;&#25439;&#23475;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#23427;&#20204; tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;
&lt;p&gt;
In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35774;&#35745;&#24072;&#35782;&#21035;&#35774;&#35745;&#20462;&#25913;&#65292;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#22312;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11308</link><description>&lt;p&gt;
&#35774;&#35745;&#20013;&#30340;&#21453;&#20107;&#23454;&#65306;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#35774;&#35745;&#24314;&#35758;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations. (arXiv:2305.11308v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35774;&#35745;&#24072;&#35782;&#21035;&#35774;&#35745;&#20462;&#25913;&#65292;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#25913;&#36827;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#65292;&#35777;&#26126;&#20854;&#22312;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35774;&#35745;&#38382;&#39064;&#21453;&#20107;&#23454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#22810;&#30446;&#26631;&#35774;&#35745;&#21453;&#20107;&#23454;(MCD)&#12290;&#21453;&#20107;&#23454;&#26159;&#25351;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#20915;&#31574;&#25110;&#36873;&#25321;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#23558;&#21453;&#20107;&#23454;&#25628;&#32034;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#35774;&#35745;&#24314;&#35758;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#23545;&#35774;&#35745;&#36827;&#34892;&#20462;&#25913;&#65292;&#20174;&#32780;&#25552;&#39640;&#21151;&#33021;&#24615;&#33021;&#12290;MCD&#36890;&#36807;&#25903;&#25345;&#22810;&#30446;&#26631;&#26597;&#35810;&#21644;&#35299;&#32806;&#21453;&#20107;&#23454;&#25628;&#32034;&#21644;&#37319;&#26679;&#36807;&#31243;&#26469;&#25552;&#39640;&#25928;&#29575;&#24182;&#20419;&#36827;&#30446;&#26631;&#26435;&#34913;&#21487;&#35270;&#21270;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#25628;&#32034;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20108;&#32500;&#27979;&#35797;&#26696;&#20363;&#35777;&#26126;&#20102;MCD&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#19977;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;MCD&#22312;&#23454;&#38469;&#35774;&#35745;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;MCD&#22312;&#25512;&#33616;&#23545;&#26597;&#35810;&#35774;&#35745;&#36827;&#34892;&#20462;&#25913;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#34892;&#36710;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method for counterfactual optimization in design problems. Counterfactuals are hypothetical situations that can lead to a different decision or choice. In this paper, the authors frame the counterfactual search problem as a design recommendation tool that can help identify modifications to a design, leading to better functional performance. MCD improves upon existing counterfactual search methods by supporting multi-objective queries, which are crucial in design problems, and by decoupling the counterfactual search and sampling processes, thus enhancing efficiency and facilitating objective tradeoff visualization. The paper demonstrates MCD's core functionality using a two-dimensional test case, followed by three case studies of bicycle design that showcase MCD's effectiveness in real-world design problems. In the first case study, MCD excels at recommending modifications to query designs that can significantly enha
&lt;/p&gt;</description></item><item><title>NeuSTIP&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38142;&#25509;&#21644;&#26102;&#38388;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;TKGC&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11301</link><description>&lt;p&gt;
NeuSTIP: &#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#21644;&#26102;&#38388;&#39044;&#27979;&#30340;&#26032;&#22411;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeuSTIP: A Novel Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs. (arXiv:2305.11301v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11301
&lt;/p&gt;
&lt;p&gt;
NeuSTIP&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38142;&#25509;&#21644;&#26102;&#38388;&#39044;&#27979;&#65292;&#19988;&#22312;&#20004;&#20010;TKGC&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#38745;&#24577;&#20107;&#23454;&#19978;&#30340;&#30693;&#35782;&#22270;&#23436;&#25104;&#24050;&#32463;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#20294;&#26159;&#22312;&#23558;&#26377;&#25928;&#26102;&#38388;&#32435;&#20837;&#38745;&#24577;&#20107;&#23454;&#20013;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#65288;TKGC&#65289;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#30693;&#35782;&#22270;&#23436;&#25104;&#26041;&#27861;&#26377;&#22810;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#22522;&#20110;&#23884;&#20837;&#12289;&#22522;&#20110;&#35268;&#21017;&#12289;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31561;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;TKGC&#20013;&#23578;&#26410;&#25506;&#32034;&#36825;&#20123;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;NeuSTIP&#65292;&#23427;&#21487;&#20197;&#22312;TKG&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#21644;&#26102;&#38388;&#38388;&#38548;&#39044;&#27979;&#12290;NeuSTIP&#22312;Allan&#35859;&#35789;&#30340;&#23384;&#22312;&#19979;&#23398;&#20064;&#26102;&#38388;&#35268;&#21017;&#65292;&#20197;&#30830;&#20445;&#32473;&#23450;&#35268;&#21017;&#20013;&#30456;&#37051;&#35859;&#35789;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#35268;&#21017;&#26469;&#35780;&#20272;&#20505;&#36873;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#65292;&#20174;&#32780;&#25191;&#34892;&#38142;&#25509;&#39044;&#27979;&#21644;&#26102;&#38388;&#38388;&#38548;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20110;&#26102;&#38388;&#38388;&#38548;&#30340;TKGC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Knowledge Graph Completion (KGC) on static facts is a matured field, Temporal Knowledge Graph Completion (TKGC), that incorporates validity time into static facts is still in its nascent stage. The KGC methods fall into multiple categories including embedding-based, rule-based, GNN-based, pretrained Language Model based approaches. However, such dimensions have not been explored in TKG. To that end, we propose a novel temporal neuro-symbolic model, NeuSTIP, that performs link prediction and time interval prediction in a TKG. NeuSTIP learns temporal rules in the presence of the Allen predicates that ensure the temporal consistency between neighboring predicates in a given rule. We further design a unique scoring function that evaluates the confidence of the candidate answers while performing link prediction and time interval prediction by utilizing the learned rules. Our empirical evaluation on two time interval based TKGC datasets suggests that our model outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#24314;&#31435;&#31561;&#24335;FOL&#65292;&#20351;&#29992;Mace4&#35745;&#31639;&#25152;&#26377;&#21487;&#33021;&#30340;&#27169;&#22411;&#21644;&#26377;&#21033;&#27169;&#22411;&#25968;&#37327;&#65292;&#26368;&#21518;&#26681;&#25454;&#23450;&#20041;&#35745;&#31639;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#35753;&#36923;&#36753;&#23398;&#29983;&#20351;&#29992;&#20182;&#20204;&#29087;&#24713;&#30340;&#24037;&#20855;&#35299;&#20915;&#27010;&#29575;&#35868;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11294</link><description>&lt;p&gt;
&#29992;&#36923;&#36753;&#24037;&#20855;&#31665;&#35299;&#20915;&#27010;&#29575;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving probability puzzles with logic toolkit. (arXiv:2305.11294v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11294
&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#31561;&#24335;FOL&#65292;&#20351;&#29992;Mace4&#35745;&#31639;&#25152;&#26377;&#21487;&#33021;&#30340;&#27169;&#22411;&#21644;&#26377;&#21033;&#27169;&#22411;&#25968;&#37327;&#65292;&#26368;&#21518;&#26681;&#25454;&#23450;&#20041;&#35745;&#31639;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#35753;&#36923;&#36753;&#23398;&#29983;&#20351;&#29992;&#20182;&#20204;&#29087;&#24713;&#30340;&#24037;&#20855;&#35299;&#20915;&#27010;&#29575;&#35868;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#31561;&#24335;FOL&#24418;&#24335;&#21270;&#27010;&#29575;&#35868;&#39064;&#24182;&#35299;&#20915;&#30340;&#26041;&#27861;&#12290;&#38656;&#35201;&#20004;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#38024;&#23545;&#32473;&#23450;&#30340;&#35868;&#39064;&#27169;&#22411;&#30340;&#25152;&#26377;&#29702;&#35770;&#65292;&#21478;&#19968;&#31181;&#26159;&#38024;&#23545;&#26377;&#21033;&#27169;&#22411;&#30340;&#29702;&#35770;&#12290;&#28982;&#21518;&#20004;&#27425;&#35843;&#29992;Mace4&#65292;&#31532;&#19968;&#27425;&#35745;&#31639;&#25152;&#26377;&#21487;&#33021;&#30340;&#27169;&#22411;Mp&#65292;&#31532;&#20108;&#27425;&#21152;&#19978;&#39069;&#22806;&#30340;&#32422;&#26463;&#65292;Mace4&#21482;&#35745;&#31639;&#26377;&#21033;&#27169;&#22411;Mf&#12290;&#26368;&#21518;&#65292;&#24212;&#29992;&#27010;&#29575;&#23450;&#20041;&#65306;&#26377;&#21033;&#27169;&#22411;&#25968;&#38500;&#20197;&#21487;&#33021;&#27169;&#22411;&#25968;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35753;&#36923;&#36753;&#39046;&#22495;&#30340;&#23398;&#29983;&#36890;&#36807;&#20351;&#29992;&#24314;&#27169;&#21644;&#24418;&#24335;&#21270;&#31561;&#21916;&#29233;&#30340;&#24037;&#20855;&#65292;&#25214;&#21040;&#35299;&#20915;&#27010;&#29575;&#35868;&#39064;&#30340;&#27491;&#30830;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proposed approach is to formalise the probabilistic puzzle in equational FOL. Two formalisations are needed: one theory for all models of the given puzzle, and a second theory for the favorable models. Then Mace4 - that computes all the interpretation models of a FOL theory - is called twice. First, it is asked to compute all the possible models M p .Second, the additional constraint is added, and Mace4 computes only favourabile models M f. Finally, the definition of probability is applied: the number of favorable models is divided by the number of possible models. The proposed approach equips students from the logic tribe to find the correct solution for puzzles from the probabilitistic tribe, by using their favourite instruments: modelling and formalisation. I have exemplified here five probabilistic puzzles and how they can be solved by translating the min FOL and then find the corresponding interpretation models. Mace4 was the tool of choice here. Ongoing work is investigating 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11284</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#27169;&#22411;&#30340;&#23433;&#20840;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Federated learning for secure development of AI models for Parkinson's disease detection using speech from different languages. (arXiv:2305.11284v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#65292;&#23454;&#29616;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#19977;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#24433;&#21709;&#20154;&#31867;&#35828;&#35805;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#24085;&#37329;&#26862;&#30149;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20005;&#26684;&#30340;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#38459;&#30861;&#20102;&#26426;&#26500;&#38388;&#20849;&#20139;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19981;&#20849;&#20139;&#24739;&#32773;&#25968;&#25454;&#30340;&#21069;&#25552;&#19979;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#22312;&#24503;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#25463;&#20811;&#35821;&#31561;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#26412;&#22320;&#27169;&#22411;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurological disorder impacting a person's speech. Among automatic PD assessment methods, deep learning models have gained particular interest. Recently, the community has explored cross-pathology and cross-language models which can improve diagnostic accuracy even further. However, strict patient data privacy regulations largely prevent institutions from sharing patient speech data with each other. In this paper, we employ federated learning (FL) for PD detection using speech signals from 3 real-world language corpora of German, Spanish, and Czech, each from a separate institution. Our results indicate that the FL model outperforms all the local models in terms of diagnostic accuracy, while not performing very differently from the model based on centrally combined training sets, with the advantage of not requiring any data sharing among collaborators. This will simplify inter-institutional collaborations, resulting in enhancement of patient outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#29983;&#29289;&#29305;&#24615;&#26500;&#24314;CNNs&#26550;&#26500;&#65292;&#25104;&#21151;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11275</link><description>&lt;p&gt;
&#29992;&#29983;&#29289;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#35299;&#37322;V1&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture. (arXiv:2305.11275v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#29983;&#29289;&#29305;&#24615;&#26500;&#24314;CNNs&#26550;&#26500;&#65292;&#25104;&#21151;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32570;&#20047;&#29983;&#29289;&#23398;&#30340;&#29305;&#24322;&#24615;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#33145;&#20391;&#35270;&#35273;&#36890;&#36335;&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;V1&#27169;&#22411;&#26159;&#36890;&#36807;&#23545;&#25239;&#24615;&#20363;&#23376;&#30340;&#35757;&#32451;&#21644;&#24191;&#27867;&#22686;&#24378;&#30340;&#25968;&#25454;&#28014;&#29616;&#20986;&#26469;&#30340;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#26080;&#27861;&#35299;&#37322;V1&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#31070;&#32463;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#26469;&#33258;&#20110;&#29983;&#29289;&#30005;&#36335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23558;&#31070;&#32463;&#31185;&#23398;&#30340;&#26550;&#26500;&#32452;&#20214;&#32435;&#20837;CNNs&#20013;&#65292;&#20197;&#35782;&#21035;&#19968;&#32452;&#20840;&#38754;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#30340;&#26426;&#21046;&#21644;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#20013;&#24515;-&#21608;&#22260;&#25326;&#25239;&#12289;&#23616;&#37096;&#24863;&#21463;&#37326;&#12289;&#35843;&#35856;&#24402;&#19968;&#21270;&#21644;&#30382;&#23618;&#25918;&#22823;&#30340;&#26550;&#26500;&#32452;&#20214;&#26469;&#25512;&#21160;&#27169;&#22411;-V1&#23545;&#40784;&#30340;&#24040;&#22823;&#25913;&#36827;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#19987;&#38376;&#30340;&#32452;&#20214;&#22686;&#24378;&#20219;&#21153;&#39537;&#21160;&#30340;CNNs&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#20135;&#29983;&#20102;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.11271</link><description>&lt;p&gt;
&#38754;&#21521;&#24773;&#22659;&#23545;&#35805;&#20013;&#30340;&#24515;&#26234;&#24314;&#27169;&#65292;&#23454;&#29616;&#21327;&#21516;&#35745;&#21010;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue. (arXiv:2305.11271v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#35753;&#20195;&#29702;&#20154;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20219;&#21153;&#36890;&#24120;&#22987;&#20110;&#21452;&#26041;&#25317;&#26377;&#19981;&#23436;&#20840;&#30340;&#20219;&#21153;&#30693;&#35782;&#21644;&#19981;&#23436;&#25972;&#30340;&#21021;&#22987;&#35745;&#21010;&#12290;&#20026;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#20195;&#29702;&#20154;&#38656;&#35201;&#19982;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#23454;&#22320;&#20132;&#27969;&#65292;&#24182;&#21327;&#35843;&#20182;&#20204;&#30340;&#37096;&#20998;&#35745;&#21010;&#20197;&#23454;&#29616;&#32852;&#21512;&#20219;&#21153;&#30446;&#26631;&#12290;&#34429;&#28982;&#36825;&#31181;&#21327;&#20316;&#22312;&#20154;&#19982;&#20154;&#30340;&#22242;&#38431;&#20013;&#20284;&#20046;&#36731;&#32780;&#26131;&#20030;&#65292;&#20294;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21327;&#20316;&#26469;&#35828;&#21364;&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#35745;&#21010;&#33719;&#21462;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#20195;&#29702;&#20154;&#21162;&#21147;&#23398;&#20064;&#24182;&#30456;&#20114;&#20132;&#27969;&#65292;&#20197;&#33719;&#21462;&#32852;&#21512;&#20219;&#21153;&#30340;&#23436;&#25972;&#35745;&#21010;&#12290;&#20855;&#20307;&#22320;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#35753;&#20195;&#29702;&#20154;&#22522;&#20110;&#20016;&#23500;&#30340;&#24863;&#30693;&#21644;&#23545;&#35805;&#21382;&#21490;&#65292;&#39044;&#27979;&#20182;&#20204;&#33258;&#24049;&#21644;&#21512;&#20316;&#20249;&#20276;&#32570;&#22833;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#19977;&#32500;&#26041;&#22359;&#19990;&#30028;&#30340;&#23545;&#31216;&#21327;&#20316;&#20219;&#21153;&#20013;&#25193;&#23637;&#20102;&#19968;&#20010;&#24773;&#22659;&#23545;&#35805;&#22522;&#20934;&#65292;&#24182;&#30740;&#31350;&#20102;&#35745;&#21010;&#33719;&#21462;&#30340;&#35745;&#31639;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#27979;&#20219;&#21153;&#30693;&#35782;&#26159;&#35745;&#21010;&#33719;&#21462;&#36807;&#31243;&#20013;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;/&#36719;&#35745;&#31639;&#25216;&#26415;&#35774;&#35745;&#26234;&#33021;&#20581;&#22766;&#25511;&#21046;&#31995;&#32479;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#30528;&#37325;&#20110;&#22686;&#21152;&#26234;&#33021;&#25511;&#21046;&#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11254</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#37327;&#23376;&#25511;&#21046;&#22120;&#65306;&#22522;&#20110;&#37327;&#23376;&#36719;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#37327;&#23376;&#20449;&#24687;--&#28909;&#21147;&#23398;&#28508;&#22312;&#21147;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robust Quantum Controllers: Quantum Information -- Thermodynamic Hidden Force Control in Intelligent Robotics based on Quantum Soft Computing. (arXiv:2305.11254v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;/&#36719;&#35745;&#31639;&#25216;&#26415;&#35774;&#35745;&#26234;&#33021;&#20581;&#22766;&#25511;&#21046;&#31995;&#32479;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#24182;&#30528;&#37325;&#20110;&#22686;&#21152;&#26234;&#33021;&#25511;&#21046;&#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;/&#36719;&#35745;&#31639;&#25216;&#26415;&#35774;&#35745;&#26234;&#33021;&#20581;&#22766;&#25511;&#21046;&#31995;&#32479;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;&#25552;&#20379;&#33258;&#32452;&#32455;&#19981;&#23436;&#21892;&#30693;&#35782;&#24211;&#30340;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#28151;&#21512;&#26234;&#33021;&#25511;&#21046;&#22120;&#30340;&#21487;&#38752;&#24615;&#12290;&#20027;&#35201;&#20851;&#27880;&#22312;&#19981;&#21487;&#39044;&#27979;&#30340;&#25511;&#21046;&#24773;&#20917;&#19979;&#22686;&#21152;&#26234;&#33021;&#25511;&#21046;&#31995;&#32479;&#30340;&#20581;&#22766;&#24615;&#65292;&#24182;&#20030;&#20363;&#36827;&#34892;&#28436;&#31034;&#12290;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#24314;&#27169;&#37327;&#23376;&#31639;&#27861;&#30340;SW&#65286;HW&#24179;&#21488;&#21644;&#25903;&#25345;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
A generalized strategy for the design of intelligent robust control systems based on quantum / soft computing technologies is described. The reliability of hybrid intelligent controllers increase by providing the ability to self-organize of imperfect knowledge bases. The main attention is paid to increasing the level of robustness of intelligent control systems in unpredictable control situations with the demonstration by illustrative examples. A SW &amp; HW platform and support tools for a supercomputer accelerator for modeling quantum algorithms on a classical computer are described.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#25214;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.11252</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;: &#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired learning in artificial neural networks: a review. (arXiv:2305.11252v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#25214;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24037;&#20855;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#12289;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#20316;&#26426;&#21046;&#19982;&#29983;&#29289;&#22823;&#33041;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#23588;&#20854;&#26159;&#23398;&#20064;&#36807;&#31243;&#26041;&#38754;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33041;&#21551;&#21457;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25506;&#35752;&#20102;&#25972;&#21512;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#26426;&#21046;&#65288;&#22914;&#31361;&#35302;&#21487;&#22609;&#24615;&#65289;&#20197;&#25552;&#39640;&#36825;&#20123;&#32593;&#32476;&#33021;&#21147;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#25361;&#25112;&#65292;&#25214;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#21487;&#33021;&#20351;&#25105;&#20204;&#26356;&#21152;&#25509;&#36817;&#29702;&#35299;&#26234;&#33021;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs' operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to enhance these networks' capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. Ultimately, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11244</link><description>&lt;p&gt;
&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24102;&#26377;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Efficient Learning Approach to Arabic Dialect Identification with Pre-Trained General-Purpose Speech Model. (arXiv:2305.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#31934;&#24230;&#65292;&#21516;&#26102;&#20351;&#29992;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#65288;PEL&#65289;&#25216;&#26415;&#65292;&#20197;&#37325;&#26032;&#21033;&#29992;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#65288;GSM&#65289;&#36827;&#34892;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65288;ADI&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#21495;&#30340;&#26631;&#31614;&#26144;&#23556;&#65292;&#23558;GSM&#36866;&#24212;&#20110;&#38463;&#25289;&#20271;&#26041;&#35328;&#35782;&#21035;&#65292;&#36890;&#36807;&#27531;&#24046;&#36866;&#37197;&#22120;&#21644;&#27169;&#22411;&#37325;&#32534;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;vanilla fine-tuning&#22312;ADI-17&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;PEL&#26041;&#27861;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#65292;&#20351;&#29992;&#39069;&#22806;2.5&#65285;&#30340;&#32593;&#32476;&#21487;&#35757;&#32451;&#21442;&#25968;&#21363;&#21487;&#36798;&#21040;fine-tuning&#31934;&#24230;&#30340;1.86&#65285;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#26469;&#35782;&#21035;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore Parameter-Efficient-Learning (PEL) techniques to repurpose a General-Purpose-Speech (GSM) model for Arabic dialect identification (ADI). Specifically, we investigate different setups to incorporate trainable features into a multi-layer encoder-decoder GSM formulation under frozen pre-trained settings. Our architecture includes residual adapter and model reprogramming (input-prompting). We design a token-level label mapping to condition the GSM for Arabic Dialect Identification (ADI). This is challenging due to the high variation in vocabulary and pronunciation among the numerous regional dialects. We achieve new state-of-the-art accuracy on the ADI-17 dataset by vanilla fine-tuning. We further reduce the training budgets with the PEL method, which performs within 1.86% accuracy to fine-tuning using only 2.5% of (extra) network trainable parameters. Our study demonstrates how to identify Arabic dialects using a small dataset and limited computation with open sou
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11243</link><description>&lt;p&gt;
&#27604;&#36739;&#26426;&#22120;&#21644;&#20799;&#31461;&#65306;&#20351;&#29992;&#21457;&#23637;&#24515;&#29702;&#23398;&#23454;&#39564;&#35780;&#20272;LaMDA&#21709;&#24212;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses. (arXiv:2305.11243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11243
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#23454;&#39564;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#21516;&#26102;&#27604;&#36739;LLMs&#21644;&#20799;&#31461;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#20154;&#31867;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24515;&#29702;&#23398;&#23478;&#33457;&#36153;&#20102;&#20960;&#21313;&#24180;&#30340;&#26102;&#38388;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#23156;&#20799;&#21644;&#20799;&#31461;&#30340;&#26234;&#21147;&#21644;&#30693;&#35782;&#65292;&#36861;&#28335;&#37325;&#35201;&#27010;&#24565;&#21644;&#33021;&#21147;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#20799;&#31461;&#21457;&#23637;&#30340;&#32463;&#20856;&#23454;&#39564;&#26159;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;LLM&#27169;&#22411;&#30340;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#20043;&#19968;&#12290;&#20854;&#27425;&#65292;&#23558;LLM&#19982;&#20799;&#31461;&#36827;&#34892;&#27604;&#36739;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#20855;&#26377;&#20154;&#31867;&#29305;&#28857;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#23884;&#20837;&#21040;&#38656;&#35201;&#19982;&#20154;&#20132;&#20114;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models, in general, and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on othe
&lt;/p&gt;</description></item><item><title>&#26412;&#27425;&#35770;&#25991;&#24635;&#32467;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#20013;&#20851;&#20110;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.11239</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#30340;&#37324;&#31243;&#30865; Part I&#65306;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Milestones in Autonomous Driving and Intelligent Vehicles Part \uppercase\expandafter{\romannumeral1}: Control, Computing System Design, Communication, HD Map, Testing, and Human Behaviors. (arXiv:2305.11239v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#27425;&#35770;&#25991;&#24635;&#32467;&#20102;&#33258;&#21160;&#39550;&#39542;&#21644;&#26234;&#33021;&#36710;&#36742;&#20013;&#20851;&#20110;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#31934;&#24230;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;(AD)&#21644;&#26234;&#33021;&#36710;&#36742;(IV)&#30340;&#20852;&#36259;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#65292;&#22240;&#20026;&#23427;&#20204;&#24102;&#26469;&#30340;&#20415;&#25463;&#12289;&#23433;&#20840;&#21644;&#32463;&#27982;&#25928;&#30410;&#12290;&#34429;&#28982;&#19968;&#20123;&#35843;&#26597;&#24050;&#32463;&#22238;&#39038;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38480;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#65292;&#24182;&#32570;&#20047;&#22312;&#26410;&#26469;&#30340;&#31995;&#32479;&#24635;&#32467;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20998;&#20026;3&#20010;&#29420;&#31435;&#30340;&#25991;&#31456;&#65292;&#31532;&#19968;&#37096;&#20998;&#26159;&#23545;AD&#21644;IV&#30340;&#24635;&#20307;&#25216;&#26415;&#36827;&#34892;&#35843;&#26597;&#65292;&#21253;&#25324;&#21382;&#21490;&#12289;&#24635;&#32467;&#37324;&#31243;&#30865;&#20197;&#21450;&#25552;&#20379;&#21069;&#30651;&#24615;&#12289;&#20262;&#29702;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#26159;&#31532;&#20108;&#37096;&#20998;(Part I)&#30340;&#25216;&#26415;&#35843;&#26597;&#65292;&#29992;&#20110;&#23457;&#26597;IV&#20013;&#25511;&#21046;&#12289;&#35745;&#31639;&#26426;&#31995;&#32479;&#35774;&#35745;&#12289;&#36890;&#20449;&#12289;&#39640;&#28165;&#22320;&#22270;&#12289;&#27979;&#35797;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#21457;&#23637;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#31532;&#19977;&#37096;&#20998;(Part II)&#23558;&#23457;&#26597;&#24863;&#30693;&#21644;&#35268;&#21010;&#37096;&#20998;&#12290;&#26412;&#27425;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#20851;&#20110;AD&#21644;IV&#39046;&#22495;&#25216;&#26415;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#21644;&#26368;&#26032;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in autonomous driving (AD) and intelligent vehicles (IVs) is growing at a rapid pace due to the convenience, safety, and economic benefits. Although a number of surveys have reviewed research achievements in this field, they are still limited in specific tasks and lack systematic summaries and research directions in the future. Our work is divided into 3 independent articles and the first part is a Survey of Surveys (SoS) for total technologies of AD and IVs that involves the history, summarizes the milestones, and provides the perspectives, ethics, and future research directions. This is the second part (Part \uppercase\expandafter{\romannumeral1} for this technical survey) to review the development of control, computing system design, communication, High Definition map (HD map), testing, and human behaviors in IVs. In addition, the third part (Part \uppercase\expandafter{\romannumeral2} for this technical survey) is to review the perception and planning sections. The objecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#26377;&#25928;&#30340;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#31446;&#30452;&#25968;&#25454;&#38598;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#22823;&#37327;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2305.11236</link><description>&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#30340;&#39640;&#25928;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Vertical Federated Learning with Secure Aggregation. (arXiv:2305.11236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#26377;&#25928;&#30340;&#31446;&#21521;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#32858;&#21512;&#65292;&#35299;&#20915;&#20102;&#31446;&#30452;&#25968;&#25454;&#38598;&#19979;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#22823;&#37327;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#27700;&#24179;&#20998;&#21306;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#20013;&#23458;&#25143;&#20849;&#20139;&#30456;&#21516;&#30340;&#29305;&#24449;&#38598;&#24182;&#21487;&#20197;&#29420;&#31435;&#22320;&#35757;&#32451;&#23436;&#25972;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#38382;&#39064;&#20013;&#65292;&#20363;&#22914;&#37329;&#34701;&#27450;&#35784;&#21644;&#30142;&#30149;&#26816;&#27979;&#65292;&#20010;&#21035;&#25968;&#25454;&#28857;&#25955;&#33853;&#22312;&#31446;&#30452;&#32852;&#37030;&#23398;&#20064;&#30340;&#19981;&#21516;&#23458;&#25143;/&#32452;&#32455;&#20013;&#12290;&#38024;&#23545;&#36825;&#31181;FL&#30340;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#26799;&#24230;&#20132;&#25442;&#65292;&#24456;&#23569;&#32771;&#34385;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#27169;&#22359;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#65292;&#20197;&#23433;&#20840;&#26377;&#25928;&#22320;&#36827;&#34892;&#31446;&#21521;FL&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21516;&#24577;&#21152;&#23494;(HE)&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#20102;9.1e2~3.8e4&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, such as financial fraud detection and disease detection, individual data points are scattered across different clients/organizations in vertical federated learning. Solutions for this type of FL require the exchange of gradients between participants and rarely consider privacy and security concerns, posing a potential risk of privacy leakage. In this work, we present a novel design for training vertical FL securely and efficiently using state-of-the-art security modules for secure aggregation. We demonstrate empirically that our method does not impact training performance whilst obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11206</link><description>&lt;p&gt;
LIMA: &#23545;&#40784;&#30340;&#26356;&#23569;&#21363;&#20026;&#26356;&#20248;&#65288;Less Is More for Alignment&#65289;
&lt;/p&gt;
&lt;p&gt;
LIMA: Less Is More for Alignment. (arXiv:2305.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#22768;&#35843;&#23398;&#20064;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#24494;&#35843;&#30340;&#26041;&#27861;&#65288;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#27169;&#22411;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#20063;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;(1)&#26080;&#30417;&#30563;&#30340;&#21407;&#22987;&#25991;&#26412;&#39044;&#35757;&#32451;&#65292;&#20197;&#23398;&#20064;&#36890;&#29992;&#34920;&#31034;&#65307;(2)&#22823;&#35268;&#27169;&#30340;&#25351;&#20196;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#26368;&#32456;&#20219;&#21153;&#21644;&#29992;&#25143;&#20559;&#22909;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;LIMA&#65292;&#19968;&#20010;&#20351;&#29992;&#26631;&#20934;&#30417;&#30563;&#25439;&#22833;&#20540;&#36827;&#34892;&#30340;65B&#21442;&#25968;LLaMa&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;1000&#20010;&#32463;&#36807;&#31579;&#36873;&#30340;&#25552;&#31034;&#21644;&#22238;&#22797;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#25110;&#20154;&#31867;&#20559;&#22909;&#24314;&#27169;&#65292;&#34913;&#37327;&#20102;&#36825;&#20004;&#20010;&#38454;&#27573;&#20043;&#38388;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290; LIMA&#34920;&#29616;&#20986;&#20102;&#26497;&#24378;&#30340;&#24615;&#33021;&#65292;&#20165;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#22914;&#20309;&#36981;&#24490;&#29305;&#23450;&#30340;&#21709;&#24212;&#26684;&#24335;&#65292;&#21253;&#25324;&#20174;&#35268;&#21010;&#26053;&#34892;&#34892;&#31243;&#21040;&#25512;&#27979;&#26367;&#20195;&#21382;&#21490;&#30340;&#22797;&#26434;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#20542;&#21521;&#20110;&#33391;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#19968;&#39033;&#25511;&#21046;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#19982;GPT-4&#30456;&#27604;&#65292;LIMA&#30340;&#21709;&#24212;&#22312;43%&#30340;&#24773;&#20917;&#19979;&#31561;&#25928;&#25110;&#20005;&#26684;&#20248;&#20808;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of c
&lt;/p&gt;</description></item><item><title>PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.11203</link><description>&lt;p&gt;
PDP&#65306;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#21363;&#21487;&#25630;&#23450;
&lt;/p&gt;
&lt;p&gt;
PDP: Parameter-free Differentiable Pruning is All You Need. (arXiv:2305.11203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11203
&lt;/p&gt;
&lt;p&gt;
PDP&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#21487;&#24494;&#21098;&#26525;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#21098;&#26525;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#25552;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#26368;&#23567;&#21270;DNN&#21152;&#36895;&#22120;&#19978;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#12289;&#26114;&#36149;&#25110;&#26080;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#35270;&#35273;/&#35821;&#35328;&#20219;&#21153;&#12289;DNN&#20307;&#31995;&#32467;&#26500;&#24182;&#36981;&#23432;&#32467;&#26500;&#21270;&#21098;&#26525;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;&#26102;&#38388;&#21098;&#26525;&#26041;&#26696;&#8212;&#8212;PDP&#65288;&#21442;&#25968;&#33258;&#30001;&#21487;&#24494;&#21098;&#26525;&#65289;&#65292;&#23427;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;PDP&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#26435;&#37325;&#30340;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21442;&#25968;&#26080;&#20851;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#30340;&#21098;&#26525;&#30446;&#26631;&#29983;&#25104;&#36719;&#21098;&#26525;&#25513;&#30721;&#12290;&#34429;&#28982;&#26159;&#21487;&#24494;&#30340;&#65292;&#20294;&#26159;PDP&#30340;&#31616;&#21333;&#21644;&#39640;&#25928;&#20351;&#20854;&#36275;&#22815;&#26222;&#36941;&#65292;&#20197;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;/&#32467;&#26500;&#21270;/&#36890;&#36947;&#21098;&#26525;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;MobileNet-v1&#65292;PDP&#21487;&#20197;&#22312;86.6%&#30340;&#31232;&#30095;&#24230;&#19979;&#36798;&#21040;68.2%&#30340;ImageNet1k top-1&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNN pruning is a popular way to reduce the size of a model, improve the inference latency, and minimize the power consumption on DNN accelerators. However, existing approaches might be too complex, expensive or ineffective to apply to a variety of vision/language tasks, DNN architectures and to honor structured pruning constraints. In this paper, we propose an efficient yet effective train-time pruning scheme, Parameter-free Differentiable Pruning (PDP), which offers state-of-the-art qualities in model size, accuracy, and training cost. PDP uses a dynamic function of weights during training to generate soft pruning masks for the weights in a parameter-free manner for a given pruning target. While differentiable, the simplicity and efficiency of PDP make it universal enough to deliver state-of-the-art random/structured/channel pruning results on various vision and natural language tasks. For example, for MobileNet-v1, PDP can achieve 68.2% top-1 ImageNet1k accuracy at 86.6% sparsity, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#32570;&#22833;&#27169;&#24335;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#30721;&#30340;&#19981;&#21464;&#26368;&#20248;&#39044;&#27979;&#22120;&#23454;&#29616;&#27867;&#21270;&#65292;&#36890;&#36807;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#36991;&#20813;&#25351;&#25968;&#29190;&#28856;&#65292;&#21516;&#26102;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#20445;&#35777;&#39044;&#27979;&#22120;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11197</link><description>&lt;p&gt;
&#22312;&#19981;&#30693;&#36947;&#36974;&#30422;&#20998;&#24067;&#31227;&#20301;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Prediction with Incomplete Data under Agnostic Mask Distribution Shift. (arXiv:2305.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#24773;&#20917;&#65292;&#22312;&#32570;&#22833;&#27169;&#24335;&#20998;&#24067;&#21487;&#33021;&#21457;&#29983;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#30721;&#30340;&#19981;&#21464;&#26368;&#20248;&#39044;&#27979;&#22120;&#23454;&#29616;&#27867;&#21270;&#65292;&#36890;&#36807;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#36991;&#20813;&#25351;&#25968;&#29190;&#28856;&#65292;&#21516;&#26102;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#20445;&#35777;&#39044;&#27979;&#22120;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#25968;&#25454;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#20204;&#23545;&#20165;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#21644;&#25351;&#31034;&#32570;&#22833;&#27169;&#24335;&#30340;&#25513;&#30721;&#30340;&#19981;&#23436;&#25972;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#30340;&#20851;&#27880;&#19981;&#26029;&#22686;&#21152;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30456;&#21516;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#21487;&#33021;&#20250;&#34987;&#36829;&#21453;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#23436;&#25972;&#25968;&#25454;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#30784;&#23436;&#25972;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#19981;&#21464;&#65292;&#20294;&#32570;&#22833;&#27169;&#24335;&#21363;&#25513;&#30721;&#20998;&#24067;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#19981;&#30693;&#36947;&#22320;&#21457;&#29983;&#21464;&#21270;&#12290;&#20026;&#20102;&#23454;&#29616;&#27867;&#21270;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#23545;&#20110;&#27599;&#20010;&#25513;&#30721;&#65292;&#37117;&#26377;&#19968;&#20010;&#19981;&#21464;&#30340;&#26368;&#20248;&#39044;&#27979;&#22120;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#21333;&#29420;&#23398;&#20064;&#23427;&#20204;&#26102;&#20986;&#29616;&#25351;&#25968;&#29190;&#28856;&#65292;&#22312;&#20351;&#29992;&#21452;&#21442;&#25968;&#21270;&#25216;&#26415;&#32852;&#21512;&#36817;&#20284;&#26368;&#20248;&#39044;&#27979;&#22120;&#12290;&#36825;&#20855;&#26377;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#65292;&#21363;&#20801;&#35768;&#23398;&#20064;&#21040;&#30340;&#39044;&#27979;&#22120;&#23545;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#36827;&#23545;&#21442;&#25968;&#21270;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data with missing values is ubiquitous in many applications. Recent years have witnessed increasing attention on prediction with only incomplete data consisting of observed features and a mask that indicates the missing pattern. Existing methods assume that the training and testing distributions are the same, which may be violated in real-world scenarios. In this paper, we consider prediction with incomplete data in the presence of distribution shift. We focus on the case where the underlying joint distribution of complete features and label is invariant, but the missing pattern, i.e., mask distribution may shift agnostically between training and testing. To achieve generalization, we leverage the observation that for each mask, there is an invariant optimal predictor. To avoid the exponential explosion when learning them separately, we approximate the optimal predictors jointly using a double parameterization technique. This has the undesirable side effect of allowing the learned pred
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992; AISecOps &#23545;&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102; AISecOps &#26159;&#22914;&#20309;&#23558; IT &#36816;&#33829;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#39046;&#22495;&#25972;&#21512;&#36215;&#26469;&#21327;&#21516;&#36816;&#20316;&#20197;&#30830;&#20445;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.11189</link><description>&lt;p&gt;
&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340; AISecOps &#23041;&#32961;&#24314;&#27169;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots. (arXiv:2305.11189v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992; AISecOps &#23545;&#20113;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#30340;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102; AISecOps &#26159;&#22914;&#20309;&#23558; IT &#36816;&#33829;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#39046;&#22495;&#25972;&#21512;&#36215;&#26469;&#21327;&#21516;&#36816;&#20316;&#20197;&#30830;&#20445;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25216;&#26415;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#21253;&#25324;&#32593;&#32476;&#23433;&#20840;&#12290;&#21307;&#30103;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#20132;&#20114;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#38656;&#35201;&#21450;&#26102;&#33719;&#24471;&#21307;&#30103;&#25588;&#21161;&#30340;&#24739;&#32773;&#25552;&#20379;&#26041;&#20415;&#12290;&#30001;&#20110;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#28041;&#21450;&#22823;&#37327;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#20113;&#20027;&#26426;&#36164;&#20135;&#30340;&#26426;&#23494;&#24615;&#65292;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992; AISecOps&#65288;&#20026;&#23433;&#20840; IT &#36816;&#33829;&#32780;&#35774;&#35745;&#30340;&#20154;&#24037;&#26234;&#33021;&#65289;&#23545;&#21307;&#30103;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#30417;&#25511;&#12290; AISecOps &#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#39046;&#22495;&#65292;&#23558; IT &#36816;&#33829;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#23433;&#20840;&#19977;&#20010;&#19981;&#21516;&#20294;&#23494;&#20999;&#30456;&#20851;&#30340;&#39046;&#22495;&#25972;&#21512;&#20026;&#19968;&#20010;&#22495;&#65292;&#22312;&#27492;&#22495;&#20013;&#65292;&#26469;&#33258;&#36825;&#19977;&#20010;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#34987;&#21327;&#21516;&#20351;&#29992;&#20197;&#20445;&#25252;&#32593;&#32476;&#23433;&#20840;&#36164;&#20135;&#12290;&#23427;&#32771;&#34385;&#21040;&#20113;&#25805;&#20316;&#21644;&#23433;&#20840;&#24615;&#22240;&#32032;&#65292;&#37319;&#29992;&#32508;&#21512;&#26694;&#26550;&#25910;&#38598;&#35780;&#20272;&#23433;&#20840;&#23041;&#32961;&#25152;&#38656;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35757;&#32451; AI &#27169;&#22411;&#20197;&#31435;&#21363;&#37319;&#21462;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is playing a vital role in all aspects of technology including cyber security. Application of Conversational AI like the chatbots are also becoming very popular in the medical field to provide timely and immediate medical assistance to patients in need. As medical chatbots deal with a lot of sensitive information, the security of these chatbots is crucial. To secure the confidentiality, integrity, and availability of cloud-hosted assets like these, medical chatbots can be monitored using AISecOps (Artificial Intelligence for Secure IT Operations). AISecOPs is an emerging field that integrates three different but interrelated domains like the IT operation, AI, and security as one domain, where the expertise from all these three domains are used cohesively to secure the cyber assets. It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#23545;&#24212;&#20110;&#20854;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11120</link><description>&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#22797;&#21512;&#39640;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Compound Gaussian Network for Solving Linear Inverse Problems. (arXiv:2305.11120v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#23545;&#24212;&#20110;&#20854;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#37325;&#24314;&#21644;&#21387;&#32553;&#24863;&#30693;&#20013;&#20986;&#29616;&#30340;&#32447;&#24615;&#21453;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#20998;&#24067;&#30340;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#22797;&#21512;&#39640;&#26031;&#20808;&#39564;&#21253;&#21547;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#26041;&#27861;&#31561;&#12290;&#36825;&#20010;&#36845;&#20195;&#31639;&#27861;&#20026;&#26412;&#25991;&#30340;&#31532;&#20108;&#31181;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#21363;&#19968;&#20010;&#23545;&#24212;&#20110;&#36845;&#20195;&#36807;&#31243;&#8220;&#23637;&#24320;&#8221;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#23637;&#24320;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#21487;&#35299;&#37322;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#19988;&#20248;&#20110;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#21253;&#25324;&#35814;&#32454;&#30340;&#35745;&#31639;&#29702;&#35770;&#65292;&#28145;&#20837;&#25506;&#31350;&#20102;&#20004;&#31181;&#31639;&#27861;&#30340;&#26500;&#36896;&#21644;&#24615;&#33021;&#12290;&#26368;&#32456;&#32467;&#35770;&#26159;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#32447;&#24615;&#21453;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving linear inverse problems, particularly of the type that appear in tomographic imaging and compressive sensing, this paper develops two new approaches. The first approach is an iterative algorithm that minimizers a regularized least squares objective function where the regularization is based on a compound Gaussian prior distribution. The Compound Gaussian prior subsumes many of the commonly used priors in image reconstruction, including those of sparsity-based approaches. The developed iterative algorithm gives rise to the paper's second new approach, which is a deep neural network that corresponds to an "unrolling" or "unfolding" of the iterative algorithm. Unrolled deep neural networks have interpretable layers and outperform standard deep learning methods. This paper includes a detailed computational theory that provides insight into the construction and performance of both algorithms. The conclusion is that both algorithms outperform other state-of-the-art approaches to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26032;&#22411;SSR&#26694;&#26550;mdctGAN&#65292;&#22522;&#20110;MDCT&#35889;&#65292;&#20197;&#30456;&#20301;&#24863;&#30693;&#30340;&#26041;&#24335;&#37325;&#24314;HR&#35821;&#38899;&#65292;&#26080;&#38656;&#22768;&#30721;&#22120;&#25110;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#20445;&#35777;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.11104</link><description>&lt;p&gt;
&#22522;&#20110;MDCT&#35889;&#30340;&#25913;&#36827;&#22411;transformer GAN&#29992;&#20110;&#35821;&#38899;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;: mdctGAN
&lt;/p&gt;
&lt;p&gt;
mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra. (arXiv:2305.11104v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26032;&#22411;SSR&#26694;&#26550;mdctGAN&#65292;&#22522;&#20110;MDCT&#35889;&#65292;&#20197;&#30456;&#20301;&#24863;&#30693;&#30340;&#26041;&#24335;&#37325;&#24314;HR&#35821;&#38899;&#65292;&#26080;&#38656;&#22768;&#30721;&#22120;&#25110;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#65292;&#24182;&#20445;&#35777;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36229;&#20998;&#36776;&#29575;(SSR)&#26088;&#22312;&#20174;&#20854;&#23545;&#24212;&#30340;&#20302;&#20998;&#36776;&#29575;(LR)&#35821;&#38899;&#20013;&#24674;&#22797;&#39640;&#20998;&#36776;&#29575;(HR)&#35821;&#38899;&#12290;&#26368;&#36817;&#30340;SSR&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#24133;&#24230;&#35889;&#30340;&#37325;&#24314;&#65292;&#24573;&#30053;&#20102;&#30456;&#20301;&#37325;&#24314;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24674;&#22797;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25913;&#36827;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(MDCT)&#30340;&#26032;&#22411;SSR&#26694;&#26550;mdctGAN&#12290;&#36890;&#36807;&#22312;MDCT&#22495;&#20013;&#36827;&#34892;&#23545;&#25239;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#30456;&#20301;&#24863;&#30693;&#30340;&#26041;&#24335;&#37325;&#24314;HR&#35821;&#38899;&#65292;&#26080;&#38656;&#22768;&#30721;&#22120;&#25110;&#39069;&#22806;&#30340;&#21518;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#30340;&#39057;&#29575;&#19968;&#33268;&#29305;&#24449;&#65292;mdctGAN&#20445;&#35777;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#37325;&#24314;&#12290;&#22312;VCTK&#35821;&#26009;&#24211;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33258;&#28982;&#30340;&#21548;&#35273;&#36136;&#37327;&#65292;MOS&#21644;PESQ&#24471;&#20998;&#39640;&#12290; &#23427;&#36824;&#22312;48kHz&#30446;&#26631;&#20998;&#36776;&#29575;&#19978;&#20174;&#21508;&#31181;&#36755;&#20837;&#36895;&#29575;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;log-spectral-distance(LSD)&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech super-resolution (SSR) aims to recover a high resolution (HR) speech from its corresponding low resolution (LR) counterpart. Recent SSR methods focus more on the reconstruction of the magnitude spectrogram, ignoring the importance of phase reconstruction, thereby limiting the recovery quality. To address this issue, we propose mdctGAN, a novel SSR framework based on modified discrete cosine transform (MDCT). By adversarial learning in the MDCT domain, our method reconstructs HR speeches in a phase-aware manner without vocoders or additional post-processing. Furthermore, by learning frequency consistent features with self-attentive mechanism, mdctGAN guarantees a high quality speech reconstruction. For VCTK corpus dataset, the experiment results show that our model produces natural auditory quality with high MOS and PESQ scores. It also achieves the state-of-the-art log-spectral-distance (LSD) performance on 48 kHz target resolution from various input rates. Code is available fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11067</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#20869;&#23481;&#20016;&#23500;&#12289;&#25925;&#20107;&#36830;&#36143;&#30340;&#28459;&#30011;
&lt;/p&gt;
&lt;p&gt;
Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#20445;&#25345;&#38899;&#20048;&#23478;&#38899;&#20048;&#39118;&#26684;&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#26410;&#23436;&#25104;&#30340;&#38899;&#20048;&#20316;&#21697;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#28459;&#30011;&#25925;&#20107;&#65292;&#24182;&#20445;&#25345;&#33402;&#26415;&#23478;&#30340;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24773;&#33410;&#21644;&#23545;&#35805;&#65292;&#28982;&#21518;&#20351;&#29992;stable diffusion&#29983;&#25104;&#28459;&#30011;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;AI&#29983;&#25104;&#25925;&#20107;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#23545;stable diffusion&#36827;&#34892;fine-tuning&#65292;&#36798;&#21040;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.11019</link><description>&lt;p&gt;
&#26080;&#26631;&#27880;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20998;&#21106;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20934;&#30830;&#22320;&#39044;&#27979;&#20687;&#32032;&#32423;&#20998;&#21106;&#25513;&#30721;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#23450;&#20301;&#22768;&#38899;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#31867;&#21035;&#26631;&#31614;&#12289;&#22270;&#20687;&#25513;&#27169;&#23545;&#21644;&#38899;&#39057;&#26679;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#32452;&#21512;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#65288;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#25513;&#27169;&#65289;&#19977;&#20803;&#32452;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#24863;&#30693;&#21464;&#21387;&#22120;&#65288;AuTR&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#26550;&#26500;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#65307;&#65288;iii&#65289;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Audio-Visual Segmentation (AVS) is to locate sounding objects within visual scenes by accurately predicting pixelwise segmentation masks. In this paper, we present the following contributions: (i), we propose a scalable and annotation-free pipeline for generating artificial data for the AVS task. We leverage existing image segmentation and audio datasets to draw links between category labels, image-mask pairs, and audio samples, which allows us to easily compose (image, audio, mask) triplets for training AVS models; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecture that features an audio-aware query-based transformer decoder. This architecture enables the model to search for sounding objects with the guidance of audio signals, resulting in more accurate segmentation; (iii), we present extensive experiments conducted on both synthetic and real datasets, which demonstrate the effectiveness of training AVS models with synthetic data generated by our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09062</link><description>&lt;p&gt;
SuSana Distance&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification. (arXiv:2305.09062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20165;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#12290;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#36817;&#24037;&#20316;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25903;&#25345;&#65288;&#35757;&#32451;&#65289;&#21644;&#26597;&#35810;&#38598;&#65288;&#27979;&#35797;&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#36825;&#20123;&#38598;&#21512;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#24230;&#37327;&#12290;&#30001;&#20110;&#25968;&#25454;&#32570;&#20047;&#65292;&#23884;&#20837;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24213;&#23618;&#28508;&#22312;&#31354;&#38388;&#30340;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#24182;&#26410;&#23436;&#20840;&#24378;&#21046;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#31532;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;Proto-Triplet Loss&#65292;&#23427;&#22522;&#20110;&#21407;&#22987;&#19977;&#20803;&#32452;&#25439;&#22833;&#65292;&#24182;&#28155;&#21152;&#20102;&#21407;&#22411;&#21521;&#37327;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;SuSana Distance Loss&#65292;&#23427;&#32771;&#34385;&#20102;&#21516;&#31867;&#26679;&#26412;&#21644;&#19981;&#21516;&#31867;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#31867;&#21035;&#21487;&#20998;&#24615;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08285</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65306;&#22522;&#20110;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;LoRA&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence modeling. (arXiv:2305.08285v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#22312;&#20445;&#25345;&#36229;&#36807;92%&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35843;&#25972;&#20165;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25104;&#21151;&#20943;&#23569;&#20102;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;&#20102;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#30340;&#19981;&#26029;&#22686;&#38271;&#24341;&#36215;&#20102;&#23545;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#24494;&#35843;&#26041;&#27861;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;LoRA&#21644;&#32467;&#26500;&#21270;&#23618;&#21098;&#26525;&#26041;&#27861;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312; MIMIC-IV-Note&#19978;&#30340;&#20004;&#20010;&#21307;&#30103;&#25253;&#21578;&#27010;&#36848;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#21307;&#30103;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#30340;0.6%&#30340;&#21442;&#25968;&#24182;&#21098;&#26525;&#36229;&#36807;30%&#30340;Transformer&#23618;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20943;&#23569;50%&#30340;GPU&#20869;&#23384;&#20351;&#29992;&#24182;&#25552;&#21319;100%&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#33258;&#30001;&#25991;&#26412;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36229;&#36807;92%&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#30340;&#36827;&#23637;&#65292;&#35813;&#27169;&#22411;&#25171;&#30772;&#20102;&#20998;&#21106;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.08196</link><description>&lt;p&gt;
&#8220;&#35270;&#35273;&#21644;&#26356;&#22810;&#39046;&#22495;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#32508;&#21512;&#35843;&#26597;&#8221;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Segment Anything Model for Vision and Beyond. (arXiv:2305.08196v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8221;&#65288;SAM&#65289;&#30340;&#36827;&#23637;&#65292;&#35813;&#27169;&#22411;&#25171;&#30772;&#20102;&#20998;&#21106;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27491;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#28436;&#36827;&#65292;&#21363;AI&#31995;&#32479;&#20855;&#22791;&#20687;&#20154;&#19968;&#26679;&#30340;&#24191;&#27867;&#20219;&#21153;&#21644;&#26234;&#33021;&#27700;&#24179;&#12290;&#36825;&#19982;&#31364;&#21270;&#25110;&#19987;&#29992;AI&#30456;&#23545;&#24212;&#65292;&#21518;&#32773;&#26088;&#22312;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35774;&#35745;&#24191;&#27867;&#25968;&#25454;&#35757;&#32451;&#30340;&#12289;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#8220;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#8220;&#65288;SAM&#65289;&#22312;&#25171;&#30772;&#20998;&#21106;&#30028;&#38480;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20805;&#20998;&#20102;&#35299;SAM&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32508;&#21512;&#35843;&#26597;&#30740;&#31350;&#12290;&#20316;&#20026;&#31532;&#19968;&#31687;&#22522;&#20110;SAM&#22522;&#30784;&#27169;&#22411;&#20840;&#38754;&#22238;&#39038;&#35270;&#35273;&#21450;&#20854;&#20182;&#39046;&#22495;&#20219;&#24847;&#20998;&#21106;&#20219;&#21153;&#36827;&#23637;&#30340;&#25991;&#31456;&#65292;&#26412;&#25991;&#30528;&#37325;&#35752;&#35770;&#20102;&#23427;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.07889</link><description>&lt;p&gt;
&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32467;&#26500;&#24037;&#31243;&#30456;&#32467;&#21512;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29992;&#20110;&#21069;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#27169;&#25311;&#65289;&#21644;&#21453;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65289;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#25552;&#20986;&#20102;VINO&#65288;&#36710;&#36742;-&#26725;&#26753;&#30456;&#20114;&#20316;&#29992;&#31070;&#32463;&#36816;&#31639;&#22120;&#65289;&#65292;&#20316;&#20026;&#26725;&#26753;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;VINO&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36816;&#34892;&#21442;&#25968;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#27169;&#25311;&#65292;&#32771;&#34385;&#32467;&#26500;&#21021;&#22987;&#25439;&#20260;&#22330;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;VBI-FE&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#22312;&#22235;&#31181;&#25439;&#20260;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#20135;&#29983;&#20102;VBI-EXP&#25968;&#25454;&#38598;&#12290;&#22312;VINO&#36890;&#36807;VBI-FE&#39044;&#35757;&#32451;&#24182;&#22312;&#20581;&#24247;&#29366;&#24577;&#19979;&#36890;&#36807;VBI-EXP&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23454;&#29616;&#20102;&#20197;&#19979;&#20004;&#20010;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#21069;&#21521;&#30340;VINO&#27604;FE&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#20174;&#25439;&#20260;&#22330;&#36755;&#20837;&#39044;&#27979;&#32467;&#26500;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#21453;&#21521;&#30340;VINO&#21487;&#20197;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
&lt;/p&gt;</description></item><item><title>FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06590</link><description>&lt;p&gt;
FactKG: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06590
&lt;/p&gt;
&lt;p&gt;
FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#24212;&#29992;&#21644;&#23545;&#35805;&#20195;&#29702;&#65289;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#65292;KG&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#20316;&#20026;&#30693;&#35782;&#28304;&#12290;KG&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#39564;&#35777;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;KG&#30001;&#33410;&#28857;&#21644;&#36793;&#32452;&#25104;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20351;&#24471;&#26426;&#22120;&#21487;&#20197;&#25512;&#29702;&#20986;&#19968;&#31995;&#21015;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#36825;&#20123;&#26426;&#22120;&#21487;&#35835;&#30340;&#27010;&#24565;&#22914;&#20309;&#26144;&#23556;&#21040;&#25991;&#26412;&#20013;&#30340;&#20449;&#24687;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#31038;&#21306;&#26356;&#22909;&#22320;&#21033;&#29992;KG&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;FactKG:&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#23427;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#20197;&#21450;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65306;&#21333;&#36339;&#12289;&#21512;&#21462;&#12289;&#23384;&#22312;&#12289;&#22810;&#36339;&#21644;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;FactKG&#21253;&#21547;&#21508;&#31181;&#35821;&#35328;&#27169;&#24335;&#65292;&#21253;&#25324;&#21475;&#35821;&#39118;&#26684;&#30340;&#22768;&#26126;&#21644;&#20070;&#38754;&#39118;&#26684;&#30340;&#22768;&#26126;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05560</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Distributional Multi-Objective Decision Making. (arXiv:2305.05560v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#30446;&#26631;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#21644;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#21487;&#20197;&#21253;&#21547;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#21487;&#20197;&#21521;&#20915;&#31574;&#32773;&#21576;&#29616;&#19968;&#32452;&#21487;&#33021;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25506;&#35752;&#36825;&#20123;&#35299;&#24212;&#35813;&#21253;&#21547;&#21738;&#20123;&#31574;&#30053;&#20197;&#21450;&#22914;&#20309;&#39640;&#25928;&#22320;&#35745;&#31639;&#36825;&#20123;&#35299;&#12290;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21183;&#20934;&#21017;&#65292;&#30452;&#25509;&#20851;&#32852;&#31574;&#30053;&#30340;&#22238;&#25253;&#20998;&#24067;&#12290;&#22522;&#20110;&#36825;&#20010;&#20934;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#20854;&#20013;&#21253;&#21547;&#20102;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#24573;&#30053;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#35777;&#26126;&#23427;&#21253;&#25324;&#25152;&#26377;&#22312;&#22810;&#32500;&#39118;&#38505;&#35268;&#36991;&#20915;&#31574;&#32773;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#25928;&#29992;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#19981;&#25903;&#37197;&#38598;&#65292;&#24182;&#36129;&#29486;&#20102;&#21098;&#26525;&#31639;&#23376;&#26469;&#23558;&#20854;&#20943;&#23569;&#21040;&#20984;&#20998;&#24067;&#19981;&#25903;&#37197;&#38598;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For effective decision support in scenarios with conflicting objectives, sets of potentially optimal solutions can be presented to the decision maker. We explore both what policies these sets should contain and how such sets can be computed efficiently. With this in mind, we take a distributional approach and introduce a novel dominance criterion relating return distributions of policies directly. Based on this criterion, we present the distributional undominated set and show that it contains optimal policies otherwise ignored by the Pareto front. In addition, we propose the convex distributional undominated set and prove that it comprises all policies that maximise expected utility for multivariate risk-averse decision makers. We propose a novel algorithm to learn the distributional undominated set and further contribute pruning operators to reduce the set to the convex distributional undominated set. Through experiments, we demonstrate the feasibility and effectiveness of these metho
&lt;/p&gt;</description></item><item><title>GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05351</link><description>&lt;p&gt;
GPT-NAS: &#20197;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model. (arXiv:2305.05351v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05351
&lt;/p&gt;
&lt;p&gt;
GPT-NAS&#20351;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#25552;&#20986;&#36817;&#20284;&#30340;&#26550;&#26500;&#32452;&#20214;&#20943;&#23567;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#34429;&#28982;&#19968;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;NAS&#26041;&#27861;&#20013;&#24456;&#23569;&#20986;&#29616;&#36825;&#31867;&#25104;&#26524;&#65292;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#31070;&#32463;&#26550;&#26500;&#30340;&#25628;&#32034;&#31354;&#38388;&#22826;&#22823;&#20102;&#65292;&#23548;&#33268;NAS&#31639;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#31216;&#20026;GPT-NAS&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#20248;&#21270;&#31070;&#32463;&#26550;&#26500;&#12290;&#22312;GPT-NAS&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26500;&#24314;&#31070;&#32463;&#26550;&#26500;&#30340;&#22522;&#26412;&#35268;&#24459;&#12290;&#22240;&#27492;&#65292;GPT-NAS&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#20986;&#21512;&#29702;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#25628;&#32034;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#25628;&#32034;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;GPT-NAS&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;NAS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has emerged as one of the effective methods to design the optimal neural network architecture automatically. Although neural architectures have achieved human-level performances in several tasks, few of them are obtained from the NAS method. The main reason is the huge search space of neural architectures, making NAS algorithms inefficient. This work presents a novel architecture search algorithm, called GPT-NAS, that optimizes neural architectures by Generative Pre-Trained (GPT) model. In GPT-NAS, we assume that a generative model pre-trained on a large-scale corpus could learn the fundamental law of building neural architectures. Therefore, GPT-NAS leverages the generative pre-trained (GPT) model to propose reasonable architecture components given the basic one. Such an approach can largely reduce the search space by introducing prior knowledge in the search process. Extensive experimental results show that our GPT-NAS method significantly outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.04288</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#30340;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Achieving Near-optimal Utility for Privacy-Preserving Federated Learning via Data Generation and Parameter Distortion. (arXiv:2305.04288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#19978;&#38480;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#25928;&#29992;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#20316;&#26500;&#24314;&#20855;&#26377;&#25552;&#39640;&#25928;&#29992;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#27844;&#38706;&#31169;&#26377;&#25968;&#25454;&#20449;&#24687;&#12290;&#24517;&#39035;&#37319;&#29992;&#36866;&#24403;&#30340;&#20445;&#25252;&#26426;&#21046;&#26469;&#28385;&#36275;&#20445;&#25252;&#38544;&#31169;&#21644;&#32500;&#25252;&#39640;&#27169;&#22411;&#25928;&#29992;&#30340;&#35201;&#27714;&#12290;&#30446;&#21069;&#37319;&#29992;&#30340;&#20445;&#25252;&#26426;&#21046;&#30340;&#26412;&#36136;&#65292;&#21253;&#25324;&#8220;&#38543;&#26426;&#21270;&#26426;&#21046;&#8221;&#21644;&#8220;&#21387;&#32553;&#26426;&#21046;&#8221;&#65292;&#26159;&#36890;&#36807;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#21644;&#30072;&#21464;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#34913;&#37327;&#25928;&#29992;&#12290;&#25105;&#20204;&#24819;&#35201;&#30830;&#23450;&#22312;&#20160;&#20040;&#26222;&#36941;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#25968;&#25454;&#29983;&#25104;&#21644;&#21442;&#25968;&#30072;&#21464;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#25509;&#36817;&#26368;&#20248;&#25928;&#29992;&#30340;&#36884;&#24452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25928;&#29992;&#25439;&#22833;&#30340;&#19978;&#38480;&#65292;&#29992;&#20004;&#20010;&#20027;&#35201;&#39033;&#31216;&#20026;&#38477;&#20302;&#26041;&#24046;&#21644;&#27169;&#22411;&#21442;&#25968;&#24046;&#24322;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables participating parties to collaboratively build a global model with boosted utility without disclosing private data information. Appropriate protection mechanisms have to be adopted to fulfill the requirements in preserving \textit{privacy} and maintaining high model \textit{utility}. The nature of the widely-adopted protection mechanisms including \textit{Randomization Mechanism} and \textit{Compression Mechanism} is to protect privacy via distorting model parameter. We measure the utility via the gap between the original model parameter and the distorted model parameter. We want to identify under what general conditions privacy-preserving federated learning can achieve near-optimal utility via data generation and parameter distortion. To provide an avenue for achieving near-optimal utility, we present an upper bound for utility loss, which is measured using two main terms called variance-reduction and model parameter discrepancy separately. Our analysis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11954</link><description>&lt;p&gt;
Spikingformer: &#22522;&#20110;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network. (arXiv:2304.11954v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11954
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;SNN&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#35813;&#32467;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;Spikingformer&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#30001;&#20110;&#20854;&#20107;&#20214;&#39537;&#21160;&#30340;&#33033;&#20914;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33410;&#33021;&#26367;&#20195;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21253;&#25324;Spikformer&#21644;SEW ResNet&#22312;&#20869;&#30340;&#26368;&#26032;&#28145;&#24230;SNN&#23384;&#22312;&#38750;&#33033;&#20914;&#35745;&#31639;&#65288;&#25972;&#25968;&#28014;&#28857;&#20056;&#27861;&#65289;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#27531;&#24046;&#36830;&#25509;&#32467;&#26500;&#25152;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;&#38750;&#33033;&#20914;&#35745;&#31639;&#22686;&#21152;&#20102;SNN&#30340;&#21151;&#32791;&#65292;&#24182;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#21482;&#25903;&#25345;&#33033;&#20914;&#25805;&#20316;&#30340;&#20027;&#27969;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#30828;&#20214;&#30340;&#33033;&#20914;&#39537;&#21160;&#27531;&#24046;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#36991;&#20813;&#38750;&#33033;&#20914;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20010;&#27531;&#24046;&#35774;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Spikingformer&#65292;&#36825;&#26159;&#19968;&#20010;&#32431;Transformer&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;ImageNet&#12289;CIFAR10&#12289;CIFAR100&#12289;CIFAR10-DVS&#21644;DVS128 Gesture&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Spikingformer&#65292;&#24182;&#34920;&#26126;&#20316;&#20026;&#20808;&#36827;&#39592;&#24178;&#30340;&#30452;&#25509;&#35757;&#32451;&#30340;&#32431;SNN&#65292;Spikingformer&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;(75.85$\%$ top-1 accuracy on ImageNet)&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, state-of-the-art deep SNNs (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connection. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware, which only supports spike operations. In this paper, we propose a hardware-friendly spike-driven residual learning architecture for SNNs to avoid non-spike computations. Based on this residual design, we develop Spikingformer, a pure transformer-based spiking neural network. We evaluate Spikingformer on ImageNet, CIFAR10, CIFAR100, CIFAR10-DVS and DVS128 Gesture datasets, and demonstrate that Spikingformer outperforms the state-of-the-art in directly trained pure SNNs as a novel advanced backbone (75.85$\
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.04641</link><description>&lt;p&gt;
&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probably Approximately Correct Federated Learning. (arXiv:2304.04641v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#20363;&#65292;&#20854;&#20027;&#35201;&#25903;&#26609;&#20026;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#21516;&#26102;&#23454;&#29616;&#26080;&#31351;&#23567;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#35774;&#35745;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#22914;&#20309;&#25214;&#21040;&#26368;&#20339;&#26435;&#34913;&#35299;&#20915;&#26041;&#26696;&#26159;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26435;&#34913;&#38382;&#39064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30446;&#26631;&#26159;&#22312;&#32422;&#26463;&#38544;&#31169;&#27844;&#38706;&#19981;&#36229;&#36807;&#39044;&#23450;&#20540;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#25928;&#29992;&#25439;&#22833;&#21644;&#25928;&#29575;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26694;&#26550;&#38750;&#24120;&#32791;&#26102;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#23384;&#22312;&#24615;&#65292;&#36825;&#28608;&#21169;&#25105;&#20204;&#23547;&#27714;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22810;&#30446;&#26631;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#30446;&#26631;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26356;&#39640;&#25928;&#12289;&#26356;&#23481;&#26131;&#34987;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;FedPAC&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;PAC&#23398;&#20064;&#29702;&#35770;&#25512;&#23548;&#20986;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#21487;&#20197;&#20445;&#35777;FL&#20043;&#38388;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;FL&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;FL&#31639;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#37319;&#26679;&#27604;&#29575;&#65292;&#20197;&#24179;&#34913;&#20840;&#23616;&#21644;&#26412;&#22320;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#65292;&#26368;&#21518;&#35777;&#26126;FedPAC&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#39640;&#27010;&#29575;&#22320;&#23454;&#29616;&#26368;&#20248;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;FedPAC&#26694;&#26550;&#30340;&#21151;&#25928;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, in this paper, we propose FedPAC, a unified framework that leverages PAC l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.17707</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#21487;&#35299;&#37322;&#24615;&#19982;&#21512;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking AI Explainability and Plausibility. (arXiv:2303.17707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#8212;&#8212;&#35299;&#37322;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#19968;&#30452;&#34987;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#65292;&#20294;&#26159;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#31526;&#21512;&#20154;&#31867;&#20132;&#27969;&#35268;&#33539;&#65292;&#25903;&#25345;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#28385;&#36275;&#20154;&#31867;&#23545;&#20110;AI&#35299;&#37322;&#30340;&#38656;&#27714;&#65292;&#35774;&#23450;&#36866;&#24403;&#30340;&#35780;&#20272;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#37322;&#21512;&#29702;&#24615;&#65292;&#36825;&#26159;XAI&#35780;&#20272;&#20013;&#26368;&#26222;&#36941;&#30340;&#20154;&#20026;&#27010;&#24565;&#12290;&#21512;&#29702;&#24615;&#34913;&#37327;&#26426;&#22120;&#35299;&#37322;&#19982;&#20154;&#31867;&#35299;&#37322;&#30456;&#27604;&#30340;&#21512;&#29702;&#31243;&#24230;&#12290;&#21512;&#29702;&#24615;&#19968;&#30452;&#34987;&#20256;&#32479;&#22320;&#21046;&#23450;&#20026;AI&#21487;&#35299;&#37322;&#24615;&#20219;&#21153;&#30340;&#37325;&#35201;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20248;&#21270;&#21644;&#35780;&#20272;XAI&#30340;&#21512;&#29702;&#24615;&#26377;&#26102;&#26159;&#26377;&#23475;&#30340;&#65292;&#19988;&#26080;&#27861;&#36798;&#21040;&#27169;&#22411;&#21487;&#29702;&#35299;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#20449;&#24230;&#30340;&#30446;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35780;&#20272;XAI&#31639;&#27861;&#30340;&#21512;&#29702;&#24615;&#20250;&#35268;&#33539;&#26426;&#22120;&#35299;&#37322;&#65292;&#20197;&#34920;&#36798;&#19982;&#20154;&#31867;&#35299;&#37322;&#23436;&#20840;&#30456;&#21516;&#30340;&#20869;&#23481;&#65292;&#36825;&#20559;&#31163;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#21160;&#26426;&#65306;&#34920;&#36798;&#33258;&#24049;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this article, we examine explanation plausibility, which is the most pervasive human-grounded concept in XAI evaluation. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective to achieve model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.15714</link><description>&lt;p&gt;
&#26174;&#24335;&#35268;&#21010;&#26377;&#21161;&#20110;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#65292;&#37319;&#29992;&#20102;&#26174;&#24335;&#35268;&#21010;&#26469;&#24110;&#21161;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#27604;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#34920;&#29616;&#26356;&#22909;&#65292;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#26174;&#24335;&#35268;&#21010;&#32435;&#20837;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#23637;&#26395;&#26410;&#26469;&#30340;&#25928;&#26524;&#26469;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20840;&#22871;&#31995;&#32479;&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#39064;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#31995;&#32479;&#65292;&#23613;&#31649;&#21482;&#26377;&#32422;15&#20159;&#20010;&#21442;&#25968;&#65292;&#20294;&#19982;GPT-3-davinci&#34920;&#29616;&#30456;&#24403;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#28040;&#34701;&#30740;&#31350;&#20197;&#35777;&#26126;&#26174;&#24335;&#35268;&#21010;&#22312;&#31995;&#32479;&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#25351;&#21335;&#21644;&#29992;&#25143;&#35843;&#26597;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#25910;&#38598;&#21644;&#24314;&#27169;&#20154;&#24037;&#26234;&#33021;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36719;&#20214;&#38656;&#27714;&#65292;&#24182;&#22312;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;360&#24230;&#35270;&#39057;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.02920</link><description>&lt;p&gt;
&#38754;&#21521;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#30340;&#38656;&#27714;&#24037;&#31243;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Requirements Engineering Framework for Human-centered Artificial Intelligence Software Systems. (arXiv:2303.02920v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#25351;&#21335;&#21644;&#29992;&#25143;&#35843;&#26597;&#24320;&#21457;&#30340;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#25910;&#38598;&#21644;&#24314;&#27169;&#20154;&#24037;&#26234;&#33021;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36719;&#20214;&#38656;&#27714;&#65292;&#24182;&#22312;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#25552;&#39640;360&#24230;&#35270;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
[&#32972;&#26223;] &#36817;&#24180;&#26469;&#65292;&#29992;&#20110;&#26500;&#24314;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#32452;&#20214;&#26174;&#33879;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#20391;&#37325;&#20110;&#25216;&#26415;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#38754;&#12290;[&#30446;&#26631;] &#22312;&#26500;&#24314;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#26102;&#21253;&#21547;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#38754;&#65292;&#21487;&#20197;&#24110;&#21161;&#23454;&#29616;&#26356;&#36127;&#36131;&#20219;&#12289;&#26080;&#20559;&#35265;&#21644;&#21253;&#23481;&#24615;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#12290;[&#26041;&#27861;] &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#25351;&#21335;&#21644;&#29992;&#25143;&#35843;&#26597;&#24320;&#21457;&#65292;&#20197;&#24110;&#21161;&#25910;&#38598;&#20154;&#24037;&#26234;&#33021;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#36719;&#20214;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#30446;&#24405;&#26469;&#24341;&#20986;&#36825;&#20123;&#38656;&#27714;&#65292;&#20197;&#21450;&#19968;&#20010;&#27010;&#24565;&#27169;&#22411;&#26469;&#30452;&#35266;&#22320;&#21576;&#29616;&#23427;&#20204;&#12290;[&#32467;&#26524;] &#26694;&#26550;&#24212;&#29992;&#20110;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20197;&#24341;&#20986;&#21644;&#24314;&#27169;&#22686;&#24378;360&#24230;&#35270;&#39057;&#36136;&#37327;&#30340;&#38656;&#27714;&#65292;&#35813;&#35270;&#39057;&#26088;&#22312;&#20026;&#34394;&#25311;&#29616;&#23454;&#29992;&#25143;&#25552;&#20379;&#12290;[&#32467;&#35770;] &#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#20805;&#20998;&#20102;&#35299;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#38754;&#65292;&#24182;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#36719;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
[Context] Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. [Objective] Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. [Method] In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. [Results] The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360 degree~videos intended for virtual reality (VR) users. [Conclusion] We found that our proposed approach helped the project team fully understand the human-ce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;</title><link>http://arxiv.org/abs/2302.07849</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Batch-Level Anomaly Detection. (arXiv:2302.07849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;AD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#22312;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36866;&#24212;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#28418;&#31227;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#35843;&#25972;&#65292;&#29305;&#21035;&#26159;&#24403;&#27809;&#26377;&#38024;&#23545;&#8220;&#26032;&#27491;&#24120;&#8221;&#36827;&#34892;&#35757;&#32451;&#30340;&#25968;&#25454;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23548;&#33268;&#20135;&#29983;&#20102;&#38646;&#26679;&#26412;AD&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#20013;&#24515;&#34920;&#31034;&#65288;ACR&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#25209;&#27425;&#32423;AD&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#35757;&#32451;&#29616;&#25104;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;SVDD&#65289;&#26469;&#36866;&#24212;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#38646;&#26679;&#26412;&#27867;&#21270;&#20026;&#26410;&#35265;&#36807;&#30340;AD&#20219;&#21153;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#21152;&#20803;&#35757;&#32451;&#65292;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#21644;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#38646;&#26679;&#26412;AD&#32467;&#26524;&#65292;&#24182;&#22312;&#26469;&#33258;&#19987;&#19994;&#39046;&#22495;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#21644;&#20998;&#27573;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a crucial role in many safety-critical application domains. The challenge of adapting an anomaly detector to drift in the normal data distribution, especially when no training data is available for the "new normal," has led to the development of zero-shot AD techniques. In this paper, we propose a simple yet effective method called Adaptive Centered Representations (ACR) for zero-shot batch-level AD. Our approach trains off-the-shelf deep anomaly detectors (such as deep SVDD) to adapt to a set of inter-related training data distributions in combination with batch normalization, enabling automatic zero-shot generalization for unseen AD tasks. This simple recipe, batch normalization plus meta-training, is a highly effective and versatile tool. Our results demonstrate the first zero-shot AD results for tabular data and outperform existing methods in zero-shot anomaly detection and segmentation on image data from specialized domains.
&lt;/p&gt;</description></item><item><title>PubGraph&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#20840;&#38754;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#21644;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#31185;&#23398;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.02231</link><description>&lt;p&gt;
PubGraph: &#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
PubGraph: A Large-Scale Scientific Knowledge Graph. (arXiv:2302.02231v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02231
&lt;/p&gt;
&lt;p&gt;
PubGraph&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#20840;&#38754;&#30340;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#21253;&#21547;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#21644;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#31185;&#23398;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#30740;&#20986;&#29256;&#29289;&#26159;&#20998;&#20139;&#26032;&#21457;&#29616;&#12289;&#26032;&#26041;&#27861;&#12289;&#26032;&#25216;&#26415;&#21644;&#27934;&#35265;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#36164;&#28304;&#26469;&#25429;&#25417;&#20986;&#29256;&#29289;&#12289;&#20316;&#32773;&#21644;&#26399;&#21002;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#65292;&#32473;&#23545;&#31185;&#23398;&#26377;&#26356;&#28145;&#20837;&#29702;&#35299;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PubGraph&#65292;&#23427;&#26159;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#31185;&#23398;&#36827;&#23637;&#30340;&#36164;&#28304;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20855;&#26377;&#36229;&#36807;3.85&#20159;&#20010;&#23454;&#20307;&#12289;130&#20159;&#20010;&#20027;&#35201;&#36793;&#32536;&#21644;15&#20159;&#20010;&#38480;&#23450;&#35789;&#36793;&#32536;&#12290; PubGraph&#26159;&#20840;&#38754;&#30340;&#65292;&#24182;&#20351;&#29992;Wikidata&#26412;&#20307;&#35770;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#65288;&#21253;&#25324;Wikidata&#12289;OpenAlex&#21644;Semantic Scholar&#65289;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#36825;&#20123;&#26469;&#28304;&#30340;&#20803;&#25968;&#25454;&#22806;&#65292;PubGraph&#36824;&#21253;&#25324;&#26469;&#33258;&#36741;&#21161;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25903;&#25345;&#20851;&#20110;&#31185;&#23398;&#32593;&#32476;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20960;&#20010;&#22823;&#35268;&#27169;&#8203;&#8203;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research publications are the primary vehicle for sharing scientific progress in the form of new discoveries, methods, techniques, and insights. Unfortunately, the lack of a large-scale, comprehensive, and easy-to-use resource capturing the myriad relationships between publications, their authors, and venues presents a barrier to applications for gaining a deeper understanding of science. In this paper, we present PubGraph, a new resource for studying scientific progress that takes the form of a large-scale knowledge graph (KG) with more than 385M entities, 13B main edges, and 1.5B qualifier edges. PubGraph is comprehensive and unifies data from various sources, including Wikidata, OpenAlex, and Semantic Scholar, using the Wikidata ontology. Beyond the metadata available from these sources, PubGraph includes outputs from auxiliary community detection algorithms and large language models. To further support studies on reasoning over scientific networks, we create several large-scale ben
&lt;/p&gt;</description></item><item><title>DiSProD&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#36817;&#20284;&#20256;&#25773;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#22270;&#34920;&#31034;&#31574;&#30053;&#20215;&#20540;&#65292;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#35268;&#21010;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.01491</link><description>&lt;p&gt;
DiSProD&#65306;&#29992;&#20110;&#35268;&#21010;&#30340;&#21487;&#24494;&#20998;&#31526;&#21495;&#20998;&#24067;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
DiSProD: Differentiable Symbolic Propagation of Distributions for Planning. (arXiv:2302.01491v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01491
&lt;/p&gt;
&lt;p&gt;
DiSProD&#26159;&#19968;&#20010;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#22312;&#32447;&#35268;&#21010;&#22120;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#36817;&#20284;&#20256;&#25773;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#31526;&#21495;&#22270;&#34920;&#31034;&#31574;&#30053;&#20215;&#20540;&#65292;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#32447;&#35268;&#21010;&#22120;DiSProD&#65292;&#29992;&#20110;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#27010;&#29575;&#36716;&#31227;&#30340;&#29615;&#22659;&#12290;DiSProD&#24314;&#31435;&#19968;&#20010;&#31526;&#21495;&#22270;&#65292;&#25429;&#25417;&#26410;&#26469;&#36712;&#36857;&#30340;&#20998;&#24067;&#65292;&#22522;&#20110;&#32473;&#23450;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#29420;&#31435;&#20551;&#35774;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#36817;&#20284;&#20256;&#25773;&#12290;&#35813;&#31526;&#21495;&#22270;&#25552;&#20379;&#20102;&#31574;&#30053;&#20215;&#20540;&#30340;&#21487;&#24494;&#20998;&#34920;&#31034;&#65292;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#38271;&#26102;&#38388;&#25628;&#32034;&#30340;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#12290;&#36817;&#20284;&#20998;&#24067;&#30340;&#20256;&#25773;&#21487;&#20197;&#30475;&#20316;&#26159;&#35768;&#22810;&#36712;&#36857;&#30340;&#32858;&#21512;&#65292;&#38750;&#24120;&#36866;&#21512;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#21644;&#38543;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#22312;&#31163;&#25955;&#26102;&#38388;&#35268;&#21010;&#21644;&#23454;&#26102;&#25511;&#21046;&#26426;&#22120;&#20154;&#31995;&#32479;&#26041;&#38754;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#24615;&#35780;&#20272;&#65292;&#35813;&#35770;&#25991;&#23558;DiSProD&#19982;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38543;&#26426;&#29615;&#22659;&#12289;&#23545;&#25628;&#32034;&#28145;&#24230;&#30340;&#25935;&#24863;&#24615;&#12289;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#21644;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#26041;&#38754;&#27604;&#29616;&#26377;&#35268;&#21010;&#22120;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper introduces DiSProD, an online planner developed for environments with probabilistic transitions in continuous state and action spaces. DiSProD builds a symbolic graph that captures the distribution of future trajectories, conditioned on a given policy, using independence assumptions and approximate propagation of distributions. The symbolic graph provides a differentiable representation of the policy's value, enabling efficient gradient-based optimization for long-horizon search. The propagation of approximate distributions can be seen as an aggregation of many trajectories, making it well-suited for dealing with sparse rewards and stochastic environments. An extensive experimental evaluation compares DiSProD to state-of-the-art planners in discrete-time planning and real-time control of robotic systems. The proposed method improves over existing planners in handling stochastic environments, sensitivity to search depth, sparsity of rewards, and large action spaces. Additional
&lt;/p&gt;</description></item><item><title>&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12292</link><description>&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot causal learning. (arXiv:2301.12292v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12292
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#26159;&#19968;&#20010;&#35299;&#20915;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#20010;&#24615;&#21270;&#24433;&#21709;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#23545;&#20219;&#21153;&#30340;&#22788;&#29702;&#36798;&#25104;&#20102;&#30446;&#30340;&#65292;&#33021;&#22815;&#23558;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#20013;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#20844;&#20849;&#25919;&#31574;&#21644;&#22312;&#32447;&#33829;&#38144;&#31561;&#39046;&#22495;&#65292;&#39044;&#27979;&#19981;&#21516;&#24178;&#39044;&#25514;&#26045;&#23545;&#29305;&#23450;&#20010;&#20307;&#30340;&#22240;&#26524;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#39044;&#27979;&#29616;&#26377;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#26377;&#35768;&#22810;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#25509;&#21463;&#36807;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#20307;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#24433;&#21709;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#20808;&#39564;&#22240;&#26524;&#23398;&#20064;&#65306;&#39044;&#27979;&#26032;&#22411;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CaML&#65292;&#36825;&#26159;&#19968;&#20010;&#22240;&#26524;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#24178;&#39044;&#25514;&#26045;&#30340;&#20010;&#24615;&#21270;&#39044;&#27979;&#25928;&#26524;&#20316;&#20026;&#19968;&#20010;&#20219;&#21153;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;CaML&#22312;&#25968;&#21315;&#20010;&#20219;&#21153;&#20013;&#35757;&#32451;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26159;&#36890;&#36807;&#25277;&#26679;&#29983;&#25104;&#19968;&#20010;&#24178;&#39044;&#25514;&#26045;&#21450;&#20854;&#25509;&#25910;&#32773;&#21644;&#38750;&#25509;&#25910;&#32773;&#26469;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#24178;&#39044;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#30340;&#23646;&#24615;&#65289;&#21644;&#20010;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#29305;&#23450;&#20010;&#20307;&#30340;&#21307;&#30103;&#35760;&#24405;&#65289;&#65292;CaML&#23398;&#20064;&#22914;&#20309;&#23558;&#24050;&#35266;&#23519;&#21040;&#30340;&#24178;&#39044;&#25514;&#26045;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#20256;&#36755;&#32473;&#26410;&#35265;&#36807;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#24178;&#39044;&#25514;&#26045;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting how different interventions will causally affect a specific individual is important in a variety of domains such as personalized medicine, public policy, and online marketing. There are a large number of methods to predict the effect of an existing intervention based on historical data from individuals who received it. However, in many settings it is important to predict the effects of novel interventions (\emph{e.g.}, a newly invented drug), which these methods do not address. Here, we consider zero-shot causal learning: predicting the personalized effects of a novel intervention. We propose CaML, a causal meta-learning framework which formulates the personalized prediction of each intervention's effect as a task. CaML trains a single meta-model across thousands of tasks, each constructed by sampling an intervention, along with its recipients and nonrecipients. By leveraging both intervention information (\emph{e.g.}, a drug's attributes) and individual features~(\emph{e.g.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CaRE&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#24182;&#20272;&#35745;&#36873;&#39033;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#25351;&#26631;&#30340;&#22240;&#26524;&#25928;&#24212;&#26469;&#25277;&#35937;&#21508;&#31181;&#37197;&#32622;&#36873;&#39033;&#19982;&#26426;&#22120;&#20154;&#24615;&#33021;&#30446;&#26631;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#29992;&#20110;&#35786;&#26029;&#39640;&#24230;&#21487;&#37197;&#32622;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#25925;&#38556;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2301.07690</link><description>&lt;p&gt;
CaRE&#65306;&#39640;&#24230;&#21487;&#37197;&#32622;&#26426;&#22120;&#20154;&#37197;&#32622;&#38382;&#39064;&#26681;&#22240;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CaRE: Finding Root Causes of Configuration Issues in Highly-Configurable Robots. (arXiv:2301.07690v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CaRE&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#24182;&#20272;&#35745;&#36873;&#39033;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#25351;&#26631;&#30340;&#22240;&#26524;&#25928;&#24212;&#26469;&#25277;&#35937;&#21508;&#31181;&#37197;&#32622;&#36873;&#39033;&#19982;&#26426;&#22120;&#20154;&#24615;&#33021;&#30446;&#26631;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#29992;&#20110;&#35786;&#26029;&#39640;&#24230;&#21487;&#37197;&#32622;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#25925;&#38556;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31995;&#32479;&#20855;&#26377;&#32452;&#21512;&#21270;&#30340;&#22823;&#37197;&#32622;&#31354;&#38388;&#65292;&#20197;&#21450;&#25968;&#30334;&#25110;&#25968;&#21315;&#31181;&#21487;&#33021;&#30340;&#36719;&#30828;&#20214;&#37197;&#32622;&#36873;&#39033;&#65292;&#36825;&#20123;&#36873;&#39033;&#20043;&#38388;&#30456;&#20114;&#38750;&#24179;&#20961;&#22320;&#20132;&#20114;&#12290;&#37197;&#32622;&#21442;&#25968;&#34987;&#35774;&#32622;&#20026;&#29305;&#23450;&#30446;&#26631;&#65292;&#20294;&#22914;&#26524;&#37197;&#32622;&#19981;&#27491;&#30830;&#65292;&#21017;&#21487;&#33021;&#23548;&#33268;&#21151;&#33021;&#25925;&#38556;&#12290;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#37197;&#32622;&#31354;&#38388;&#20197;&#21450;&#26426;&#22120;&#20154;&#37197;&#32622;&#35774;&#32622;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#25214;&#21040;&#27492;&#31867;&#25925;&#38556;&#30340;&#26681;&#26412;&#21407;&#22240;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CaRE&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20851;&#31995;&#30340;&#35270;&#35282;&#26469;&#35786;&#26029;&#21151;&#33021;&#25925;&#38556;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290; CaRE&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#24182;&#20272;&#35745;&#36873;&#39033;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#25351;&#26631;&#30340;&#22240;&#26524;&#25928;&#24212;&#26469;&#25277;&#35937;&#21508;&#31181;&#37197;&#32622;&#36873;&#39033;&#19982;&#26426;&#22120;&#20154;&#24615;&#33021;&#30446;&#26631;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#35266;&#23519;&#21040;&#30340;&#21151;&#33021;&#25925;&#38556;&#30340;&#26681;&#26412;&#21407;&#22240;&#20197;&#21450;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#25152;&#35786;&#26029;&#30340;&#26681;&#26412;&#21407;&#22240;&#26469;&#23637;&#31034;CaRE&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic systems have subsystems with a combinatorially large configuration space and hundreds or thousands of possible software and hardware configuration options interacting non-trivially. The configurable parameters are set to target specific objectives, but they can cause functional faults when incorrectly configured. Finding the root cause of such faults is challenging due to the exponentially large configuration space and the dependencies between the robot's configuration settings and performance. This paper proposes CaRE -a method for diagnosing the root cause of functional faults through the lens of causality. CaRE abstracts the causal relationships between various configuration options and the robot's performance objectives by learning a causal structure and estimating the causal effects of options on robot performance indicators. We demonstrate CaRE's efficacy by finding the root cause of the observed functional faults and validating the diagnosed root cause by conducting ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00899</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#32500;&#20256;&#24863;&#22120;&#21453;&#39304;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback. (arXiv:2301.00899v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#28748;&#28297;&#35843;&#24230;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#20316;&#29289;&#31995;&#32479;&#20013;&#24212;&#29992;&#20110;&#26681;&#25454;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#30340;&#21508;&#31181;&#27979;&#37327;&#20540;&#33258;&#36866;&#24212;&#22320;&#26045;&#21152;&#27700;&#20998;&#65292;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#28748;&#28297;&#35843;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31243;&#24207;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21046;&#23450;&#33258;&#24049;&#30340;&#20248;&#21270;&#38382;&#39064;&#24182;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#35299;&#20915;&#26041;&#26696;&#31639;&#27861;&#65292;&#20197;&#21152;&#24555;&#25216;&#26415;&#36827;&#27493;&#12290;&#20854;&#26377;&#25928;&#24615;&#24050;&#22312;&#28595;&#22823;&#21033;&#20122;&#19968;&#20010;&#20135;&#20986;&#39640;&#30340;&#22320;&#21306;&#20351;&#29992;&#28748;&#28297;&#23567;&#40614;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has considerable potential to improve irrigation scheduling in many cropping systems by applying adaptive amounts of water based on various measurements over time. The goal is to discover an intelligent decision rule that processes information available to growers and prescribes sensible irrigation amounts for the time steps considered. Due to the technical novelty, however, the research on the technique remains sparse and impractical. To accelerate the progress, the paper proposes a principled framework and actionable procedure that allow researchers to formulate their own optimisation problems and implement solution algorithms based on deep reinforcement learning. The effectiveness of the framework was demonstrated using a case study of irrigated wheat grown in a productive region of Australia where profits were maximised. Specifically, the decision rule takes nine state variable inputs: crop phenological stage, leaf area index, extractable soil water for 
&lt;/p&gt;</description></item><item><title>ODEX&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;CODEX&#21644;CODEGEN&#20998;&#21035;&#34920;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;ODEx&#23558;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#30721;&#29983;&#25104;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#25191;&#34892;&#30340;&#26041;&#27861;&#35780;&#20272;&#24320;&#25918;&#22495;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Execution-Based Evaluation for Open-Domain Code Generation. (arXiv:2212.10481v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10481
&lt;/p&gt;
&lt;p&gt;
ODEX&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;CODEX&#21644;CODEGEN&#20998;&#21035;&#34920;&#29616;&#19981;&#21516;&#30340;&#34892;&#20026;&#12290;ODEx&#23558;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#20195;&#30721;&#29983;&#25104;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23558;&#32534;&#30721;&#26597;&#35810;&#30340;&#33539;&#22260;&#25193;&#23637;&#21040;&#26356;&#21152;&#23454;&#38469;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ODEx&#65292;&#31532;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#22495;&#25191;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;Python&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;ODEx&#20849;&#26377;945&#20010;NL-Code&#23545;&#65292;&#28085;&#30422;79&#20010;&#19981;&#21516;&#30340;&#24211;&#65292;&#20197;&#21450;1,707&#20010;&#20379;&#25191;&#34892;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#20174;StackOverflow&#35770;&#22363;&#33719;&#24471;NL-Code&#23545;&#65292;&#40723;&#21169;&#33258;&#28982;&#21644;&#23454;&#29992;&#30340;&#32534;&#30721;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;ODEx&#25903;&#25345;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#21363;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#26085;&#35821;&#21644;&#20420;&#35821;&#12290;ODEx&#25581;&#31034;&#20102;&#26368;&#39640;&#25191;&#34892;&#25928;&#26524;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26377;&#36259;&#34892;&#20026;&#24046;&#24322;&#12290;&#34429;&#28982;CODEX&#30340;&#24635;&#20307;&#32467;&#26524;&#26356;&#22909;&#65292;&#20294;CODEGEN&#36890;&#36807;&#25193;&#23637;&#32780;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021; - CODEGEN 6.1B&#19982;CODEX 12B&#34920;&#29616;&#30456;&#24403;&#12290;&#20004;&#20010;&#27169;&#22411;&#37117;&#26174;&#31034;&#20986;&#24320;&#25918;&#22495;&#21644;&#23553;&#38381;&#22495;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#20294;CODEGEN&#24046;&#36317;&#24448;&#24448;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#32780;CODEX&#24046;&#36317;&#21017;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#37322;&#25918;ODEx&#20197;&#20419;&#36827;&#23545;&#20195;&#30721;&#29983;&#25104;&#31038;&#21306;&#30340;&#24320;&#25918;&#22495;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2212.09667</link><description>&lt;p&gt;
&#27880;&#37325;&#35270;&#35273;&#12289;&#23646;&#24615;&#21644;&#29702;&#24615;&#65306;&#36808;&#21521;&#29289;&#29702;&#23433;&#20840;&#21644;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI. (arXiv:2212.09667v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09667
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FARM&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#29983;&#25104;&#33021;&#22815;&#34987;&#20449;&#20219;&#30340;&#21407;&#29702;&#65292;&#35299;&#20915;&#20102;&#19981;&#23433;&#20840;&#25991;&#26412;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#20445;&#38556;&#28040;&#36153;&#32773;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#31995;&#32479;&#24066;&#22330;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#29992;&#25143;&#30340;&#36523;&#20307;&#23433;&#20840;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#19981;&#21463;&#38480;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#25512;&#33616;&#21361;&#38505;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#38544;&#34109;&#30340;&#19981;&#23433;&#20840;&#25991;&#26412;&#26159;&#19968;&#20010;&#29305;&#21035;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#20986;&#29616;&#22312;&#26085;&#24120;&#22330;&#26223;&#20013;&#65292;&#24182;&#19988;&#24456;&#38590;&#34987;&#26816;&#27979;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FARM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22312;&#23433;&#20840;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FARM&#27880;&#37325;&#20110;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#30830;&#35748;&#22312;&#29305;&#23450;&#24773;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21487;&#20449;&#28304;&#36827;&#34892;&#24402;&#22240;&#20197;&#33719;&#21462;&#27492;&#20449;&#24687;&#12290;&#36825;&#20123;&#30693;&#35782;&#29992;&#20110;&#20998;&#31867;&#21407;&#22987;&#25991;&#26412;&#30340;&#23433;&#20840;&#24615;&#24182;&#29983;&#25104;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21407;&#29702;&#65292;&#25581;&#31034;&#31995;&#32479;&#23545;&#29305;&#23450;&#29992;&#25143;&#32676;&#20307;&#30340;&#39118;&#38505;&#65292;&#24182;&#24110;&#21161;&#21033;&#30410;&#30456;&#20851;&#32773;&#31649;&#29702;&#20854;&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24110;&#21161;&#25919;&#31574;&#21046;&#23450;&#32773;&#20026;&#28040;&#36153;&#32773;&#23433;&#20840;&#25552;&#20379;&#20855;&#20307;&#30340;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FARM&#22312;&#35782;&#21035;&#19981;&#23433;&#20840;&#25991;&#26412;&#21644;&#29983;&#25104;&#21487;&#20449;&#30340;&#21407;&#29702;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.03000</link><description>&lt;p&gt;
SODA&#65306;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#30740;&#31350;&#20013;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies. (arXiv:2212.03000v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;SODA&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;&#35813;&#21253;&#22312;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#20197;&#29992;&#20110;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#21253;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SODA&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21253;&#65292;&#20854;&#20013;&#21547;&#26377;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#25552;&#21462;&#30284;&#30151;&#24739;&#32773;&#30340;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#26816;&#39564;&#20102;SODA&#22312;&#26032;&#30340;&#30142;&#30149;&#39046;&#22495;&#65288;&#22914;&#20351;&#29992;&#38463;&#29255;&#31867;&#33647;&#29289;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#30284;&#30151;&#20154;&#32676;&#20013;&#25552;&#21462;SDoH&#30340;&#25552;&#21462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: We aim to develop an open-source natural language processing (NLP) package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models to extract social determinants of health (SDoH) for cancer patients, examine the generalizability of SODA to a new disease domain (i.e., opioid use), and evaluate the extraction rate of SDoH using cancer populations.  Methods: We identified SDoH categories and attributes and developed an SDoH corpus using clinical notes from a general cancer cohort. We compared four transformer-based NLP models to extract SDoH, examined the generalizability of NLP models to a cohort of patients prescribed with opioids, and explored customization strategies to improve performance. We applied the best NLP model to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804), and colorectal cancer (n=6,240) cohorts.  Results and Conclusion: We developed a corpus of 629 cancer patients notes with annotations of 13,193 SDoH concepts/attribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.02021</link><description>&lt;p&gt;
&#19982;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#24847;&#22270;&#35782;&#21035;&#30456;&#20851;&#30340;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue. (arXiv:2212.02021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#24847;&#22270;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#32858;&#31867;&#31639;&#27861;&#21644;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#35774;&#35745;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#22270;&#35889;&#20013;&#30340;&#20856;&#22411;&#25361;&#25112;&#65306;&#20026;&#27599;&#20010;&#23545;&#35805;&#36716;&#25240;&#25351;&#23450;&#24847;&#22270;&#26631;&#31614;&#65288;&#24847;&#22270;&#32858;&#31867;&#65289;&#24182;&#22522;&#20110;&#24847;&#22270;&#32858;&#31867;&#26041;&#27861;&#29983;&#25104;&#19968;&#32452;&#24847;&#22270;&#65288;&#24847;&#22270;&#24402;&#32435;&#65289;&#12290;&#25105;&#20204;&#20551;&#35774;&#33258;&#21160;&#24402;&#32435;&#24847;&#22270;&#26377;&#20004;&#20010;&#26174;&#33879;&#22240;&#32032;&#65306;&#65288;1&#65289;&#24847;&#22270;&#26631;&#31614;&#30340;&#32858;&#31867;&#31639;&#27861;&#21644;&#65288;2&#65289;&#29992;&#25143;&#35805;&#35821;&#23884;&#20837;&#31354;&#38388;&#12290; &#25105;&#20204;&#26681;&#25454;DSTC11&#35780;&#20272;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#25104;&#21697;&#32858;&#31867;&#27169;&#22411;&#21644;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35748;&#30495;&#32771;&#34385;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#35805;&#35821;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#32508;&#21512;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MiniLM&#19982;&#23618;&#27425;&#32858;&#31867;&#30456;&#32467;&#21512;&#21487;&#26174;&#33879;&#25552;&#39640;&#24847;&#22270;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;NMI&#65292;ARI&#65292;F1&#65292;&#20934;&#30830;&#24615;&#21644;&#31034;&#20363;&#35206;&#30422;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/Jeiyoon/dstc11-track2&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#30456;&#27604;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#19981;&#26356;&#39640;&#65292;&#20004;&#32773;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#36890;&#36807;&#20851;&#20110;&#26641;&#23485;&#30340;&#36793;&#30028;&#30028;&#23450;&#24471;&#21040;&#36739;&#22909;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2211.13447</link><description>&lt;p&gt;
&#35770;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of Counterfactual Reasoning. (arXiv:2211.13447v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#30456;&#27604;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#19981;&#26356;&#39640;&#65292;&#20004;&#32773;&#30340;&#22797;&#26434;&#24615;&#21487;&#20197;&#36890;&#36807;&#20851;&#20110;&#26641;&#23485;&#30340;&#36793;&#30028;&#30028;&#23450;&#24471;&#21040;&#36739;&#22909;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#19982;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#19978;&#36827;&#34892;&#20851;&#32852;&#21644;&#24178;&#39044;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#35745;&#31639;&#26694;&#26550;&#30340;&#32972;&#26223;&#19979;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#24182;&#19981;&#27604;&#22312;&#23436;&#20840;&#25351;&#23450;&#30340;SCMs&#19978;&#36827;&#34892;&#20851;&#32852;&#25110;&#24178;&#39044;&#25512;&#29702;&#26356;&#38590;&#12290;&#31532;&#19968;&#20010;&#26694;&#26550;&#22522;&#20110;&#26641;&#23485;&#30340;&#27010;&#24565;&#65292;&#24182;&#21253;&#25324;&#32463;&#20856;&#30340;&#21464;&#37327;&#28040;&#38500;&#21644;&#32852;&#32467;&#26641;&#31639;&#27861;&#12290;&#31532;&#20108;&#20010;&#26694;&#26550;&#21017;&#22522;&#20110;&#26356;&#36817;&#26399;&#19988;&#26356;&#31934;&#32454;&#30340;&#22240;&#26524;&#26641;&#23485;&#27010;&#24565;&#65292;&#38024;&#23545;&#20855;&#26377;&#21151;&#33021;&#20381;&#36182;&#24615;&#30340;&#27169;&#22411;&#65292;&#22914;SCMs&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#24314;&#35774;&#24615;&#30340;&#65292;&#22522;&#20110;&#30028;&#23450;&#21452;&#32593;&#32476;&#65288;&#29992;&#20110;&#26631;&#20934;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#21253;&#25324;&#29616;&#23454;&#21644;&#24819;&#35937;&#20004;&#31181;&#24773;&#20917;&#65289;&#30340;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#65292;&#21040;&#22522;&#30784;SCM&#32467;&#26500;&#30340;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;&#21518;&#32773;&#65288;&#22240;&#26524;&#65289;&#26641;&#23485;&#19981;&#20250;&#36229;&#36807;&#21069;&#32773;&#30340;&#20004;&#20493;&#21152;&#19968;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the computational complexity of counterfactual reasoning in relation to the complexity of associational and interventional reasoning on structural causal models (SCMs). We show that counterfactual reasoning is no harder than associational or interventional reasoning on fully specified SCMs in the context of two computational frameworks. The first framework is based on the notion of treewidth and includes the classical variable elimination and jointree algorithms. The second framework is based on the more recent and refined notion of causal treewidth which is directed towards models with functional dependencies such as SCMs. Our results are constructive and based on bounding the (causal) treewidth of twin networks -- used in standard counterfactual reasoning that contemplates two worlds, real and imaginary -- to the (causal) treewidth of the underlying SCM structure. In particular, we show that the latter (causal) treewidth is no more than twice the former plus one. Hence, if a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VATLM&#30340;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#21548;&#25991;&#26412;&#36164;&#26009;&#30340;&#39044;&#22788;&#29702;&#19982;&#19968;&#31181;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#20248;&#31168;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.11275</link><description>&lt;p&gt;
VATLM: &#20351;&#29992;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#36827;&#34892;&#35270;&#21548;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VATLM&#30340;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#35270;&#21548;&#25991;&#26412;&#36164;&#26009;&#30340;&#39044;&#22788;&#29702;&#19982;&#19968;&#31181;&#32479;&#19968;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#36798;&#21040;&#20248;&#31168;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#38899;&#26159;&#20154;&#31867;&#19982;&#22806;&#30028;&#20132;&#27969;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#20294;&#26356;&#30495;&#23454;&#30340;&#35821;&#38899;&#20132;&#20114;&#21253;&#21547;&#22810;&#27169;&#24335;&#20449;&#24687;&#65292;&#20363;&#22914;&#35270;&#35273;&#12289;&#25991;&#26412;&#12290;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#27169;&#24577;&#20449;&#24687;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#36164;&#28304;&#65288;&#20363;&#22914;&#35270;&#21548;&#23545;&#12289;&#38899;&#39057;&#25991;&#26412;&#23545;&#12289;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#65289;&#20419;&#36827;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#36824;&#27809;&#26377;&#34987;&#24456;&#22909;&#22320;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#36328;&#27169;&#24335;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;VATLM&#65288;Visual-Audio-Text&#35821;&#35328;&#27169;&#22411;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;VATLM&#37319;&#29992;&#32479;&#19968;&#30340;&#39592;&#24178;&#32593;&#32476;&#26469;&#24314;&#27169;&#27169;&#24577;&#29420;&#31435;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#19977;&#20010;&#31616;&#21333;&#30340;&#27169;&#24577;&#20381;&#36182;&#27169;&#22359;&#23545;&#35270;&#35273;&#12289;&#35821;&#38899;&#21644;&#25991;&#26412;&#36755;&#20837;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#20026;&#20102;&#23558;&#36825;&#19977;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#20849;&#20139;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;VATLM&#20351;&#29992;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#32479;&#19968;&#20998;&#35789;&#22120;&#32473;&#20986;&#30340;&#32479;&#19968;&#20196;&#29260;&#30340;&#36974;&#34109;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;&#38899;&#39057;-&#35270;&#35273;&#26816;&#32034;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#30340;VATLM&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#23398;&#20064;&#32852;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2211.00251</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Differentiable Model Selection for Ensemble Learning. (arXiv:2211.00251v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#65292;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#21019;&#36896;&#20934;&#30830;&#21644;&#31283;&#20581;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;&#35774;&#35745;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30830;&#23450;&#20219;&#20309;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#30340;&#26368;&#20339;&#20998;&#31867;&#27169;&#22411;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32452;&#21512;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#20026;&#38598;&#25104;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#35813;&#31574;&#30053;&#32467;&#21512;&#20102;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#23558;&#38598;&#25104;&#23398;&#20064;&#20219;&#21153;&#36716;&#21270;&#20026;&#21487;&#24494;&#20998;&#36873;&#25321;&#31243;&#24207;&#65292;&#22312;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#20869;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23398;&#20064;&#20026;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#36873;&#25321;&#21512;&#36866;&#30340;&#38598;&#25104;&#25104;&#21592;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27979;&#35797;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#19988;&#22312;&#21508;&#31181;&#35774;&#32622;&#21644;&#23398;&#20064;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#20849;&#35782;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a strategy aimed at creating accurate and robust models. A key challenge in designing these algorithms is identifying the optimal model for classifying any particular input sample. This paper addresses this challenge and proposes a novel framework for differentiable model selection integrating machine learning and combinatorial optimization. The framework is tailored for ensemble learning, a strategy that combines the outputs of individually pre-trained models, and learns to select appropriate ensemble members for a particular input sample by transforming the ensemble learning task into a differentiable selection program trained end-to-end within the ensemble learning model. Tested on various tasks, the proposed framework demonstrates its versatility and effectiveness, outperforming conventional and advanced consensus rules across a variety of settings and learning tasks.
&lt;/p&gt;</description></item><item><title>Phantom&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#39537;&#21160;RL&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#31616;&#21270;ABM&#35268;&#33539;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#21253;&#25324;&#32463;&#27982;&#31995;&#32479;&#21644;&#24066;&#22330;&#31561;&#12290;</title><link>http://arxiv.org/abs/2210.06012</link><description>&lt;p&gt;
&#12298;Phantom--&#19968;&#31181;&#39537;&#21160;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22797;&#26434;&#31995;&#32479;&#24314;&#27169;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Phantom -- A RL-driven multi-agent framework to model complex systems. (arXiv:2210.06012v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06012
&lt;/p&gt;
&lt;p&gt;
Phantom&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#39537;&#21160;RL&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#31616;&#21270;ABM&#35268;&#33539;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#21253;&#25324;&#32463;&#27982;&#31995;&#32479;&#21644;&#24066;&#22330;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#24314;&#27169;(ABM)&#26159;&#19968;&#31181;&#36890;&#36807;&#25351;&#23450;&#31995;&#32479;&#20013;&#33258;&#27835;&#20915;&#31574;&#32452;&#20214;&#25110;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#24182;&#20801;&#35768;&#31995;&#32479;&#21160;&#24577;&#20174;&#23427;&#20204;&#30340;&#20114;&#21160;&#20013;&#20986;&#29616;&#30340;&#35745;&#31639;&#24314;&#27169;&#26041;&#27861;&#12290;&#36817;&#24180;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#30340;&#36827;&#23637;&#20351;&#24471;&#30740;&#31350;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23398;&#20064;&#30340;&#22797;&#26434;&#29615;&#22659;&#30340;&#22343;&#34913;&#21464;&#24471;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;ABM&#26694;&#26550;&#19981;&#26159;RL&#26412;&#26426;&#30340;&#65292;&#21363;&#23427;&#20204;&#19981;&#25552;&#20379;&#19982;&#20351;&#29992;MARL&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#20860;&#23481;&#30340;&#27010;&#24565;&#21644;&#25509;&#21475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#24320;&#28304;&#26694;&#26550;Phantom&#65292;&#26469;&#24357;&#21512;ABM&#21644;MARL&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;Phantom&#26159;&#19968;&#20010;&#39537;&#21160;&#20195;&#29702;&#24314;&#27169;&#32463;&#27982;&#31995;&#32479;&#21644;&#24066;&#22330;&#31561;&#22797;&#26434;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;RL&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25552;&#20379;&#20197;MARL&#20860;&#23481;&#26041;&#24335;&#31616;&#21270;ABM&#35268;&#33539;&#30340;&#24037;&#20855; - &#21253;&#25324;&#32534;&#30721;&#21160;&#24577;&#37096;&#20998;&#29305;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
Agent based modelling (ABM) is a computational approach to modelling complex systems by specifying the behaviour of autonomous decision-making components or agents in the system and allowing the system dynamics to emerge from their interactions. Recent advances in the field of Multi-agent reinforcement learning (MARL) have made it feasible to study the equilibrium of complex environments where multiple agents learn simultaneously. However, most ABM frameworks are not RL-native, in that they do not offer concepts and interfaces that are compatible with the use of MARL to learn agent behaviours. In this paper, we introduce a new open-source framework, Phantom, to bridge the gap between ABM and MARL. Phantom is an RL-driven framework for agent-based modelling of complex multi-agent systems including, but not limited to economic systems and markets. The framework aims to provide the tools to simplify the ABM specification in a MARL-compatible way - including features to encode dynamic part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20102;&#23545;HTML&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20351;&#20854;&#22312;&#35821;&#20041;&#20998;&#31867;&#12289;&#25551;&#36848;&#29983;&#25104;&#21644;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;HTML&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.03945</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;HTML
&lt;/p&gt;
&lt;p&gt;
Understanding HTML with Large Language Models. (arXiv:2210.03945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#20102;&#23545;HTML&#30340;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#20351;&#20854;&#22312;&#35821;&#20041;&#20998;&#31867;&#12289;&#25551;&#36848;&#29983;&#25104;&#21644;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;HTML&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;HTML&#29702;&#35299;&#26041;&#38754;&#30340;&#33021;&#21147;&#8212;&#8212;&#21363;&#35299;&#26512;&#32593;&#39029;&#30340;&#21407;&#22987;HTML&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#21270;&#32593;&#32476;&#20219;&#21153;&#12289;&#29228;&#21462;&#21644;&#27983;&#35272;&#22120;&#36741;&#21161;&#26816;&#32034;&#31561;&#26041;&#38754;&#8212;&#8212;&#23578;&#26410;&#24471;&#21040;&#23436;&#20840;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HTML&#29702;&#35299;&#27169;&#22411;&#65288;&#24494;&#35843;LLMs&#65289;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#20102;&#23427;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#33021;&#21147;&#65306;&#65288;i&#65289;HTML&#20803;&#32032;&#30340;&#35821;&#20041;&#20998;&#31867;&#65292;&#65288;ii&#65289;HTML&#36755;&#20837;&#30340;&#25551;&#36848;&#29983;&#25104;&#65292;&#20197;&#21450;&#65288;iii&#65289;HTML&#39029;&#38754;&#30340;&#33258;&#20027;&#32593;&#32476;&#23548;&#33322;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#20026;HTML&#29702;&#35299;&#24320;&#21457;&#20102;&#19987;&#29992;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31243;&#24207;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#20110;&#26631;&#20934;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#30340;LLMs&#38750;&#24120;&#36866;&#29992;&#20110;HTML&#29702;&#35299;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#24494;&#35843;&#21518;&#30340;LLMs&#22312;&#35821;&#20041;&#20998;&#31867;&#26041;&#38754;&#27604;&#20165;&#22522;&#20110;&#20219;&#21153;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#29575;&#39640;12%&#12290;&#27492;&#22806;&#65292;&#24403;&#23427;&#20204;&#34987;&#24494;&#35843;&#20110;MiniW&#30340;&#25968;&#25454;&#26102;&#65292;LLMs&#30340;&#25551;&#36848;&#29983;&#25104;&#22312;&#20154;&#31867;&#20027;&#35266;&#36136;&#37327;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#19982;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#8212;&#35299;&#30721;&#22120;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#65292;&#32780;&#19988;&#23427;&#20204;&#33021;&#22815;&#25104;&#21151;&#22320;&#33258;&#20027;&#22320;&#27983;&#35272;HTML&#39029;&#38754;&#65292;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniW
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.00637</link><description>&lt;p&gt;
&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20854;&#20013;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#34920;&#31034;&#12290;&#26412;&#35770;&#25991;&#27491;&#24335;&#21270;&#20102;&#23547;&#25214;&#26368;&#20339;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#25968;&#23398;&#38382;&#39064;&#24182;&#34920;&#24449;&#20854;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;BAE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;BAE&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#19968;&#20010;&#27969;&#22411;&#19978;&#65292;&#20854;&#32500;&#25968;&#20026;&#29983;&#25104;&#38382;&#39064;&#30340;&#26368;&#20339;&#21487;&#21387;&#32553;&#32500;&#24230;&#12290;&#25105;&#20204;&#24378;&#35843;BAE&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#20960;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#26041;&#21521;&#20043;&#38388;&#30340;&#24778;&#20154;&#32852;&#31995;&#65292;&#22914;&#26377;&#26465;&#20214;&#30340;GAN&#65292;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#31283;&#23450;&#25193;&#25955;&#65292;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BAE&#22914;&#20309;&#25214;&#21040;&#26368;&#20248;&#30340;&#20302;&#32500;&#28508;&#22312;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#25552;&#39640;&#37492;&#21035;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21387;&#32553;&#8220;&#24694;&#24615;&#8221;&#25968;&#25454;&#32500;&#24230;&#65292;BAE&#23548;&#33268;&#26799;&#24230;&#26356;&#21152;&#24179;&#28369;&#21644;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedD3&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#20165;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#26469;&#36866;&#24212;&#20351;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2208.11311</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#25552;&#28860;&#30340;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedD3&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#20165;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#36890;&#36807;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#26469;&#36866;&#24212;&#20351;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25152;&#26377;&#30340;&#32852;&#32593;&#23458;&#25143;&#31471;&#21327;&#20316;&#22320;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#21363;&#20351;&#22312;&#36845;&#20195;&#36890;&#20449;&#20013;&#20849;&#20139;&#24050;&#35757;&#32451;&#30340;&#37096;&#20998;&#27169;&#22411;&#65292;&#20063;&#24448;&#24448;&#20250;&#23548;&#33268;&#24213;&#23618;&#32593;&#32476;&#20013;&#30340;&#20005;&#37325;&#36890;&#20449;&#29942;&#39048;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedD3&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#27425;&#36890;&#20449;&#65292;&#24182;&#38598;&#25104;&#20102;&#25968;&#25454;&#38598;&#25552;&#28860;&#23454;&#20363;&#12290;FedD3&#19981;&#21516;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#20849;&#20139;&#27169;&#22411;&#26356;&#26032;&#65292;&#23427;&#20801;&#35768;&#36830;&#25509;&#30340;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#25552;&#28860;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#20174;&#32593;&#32476;&#20013;&#32858;&#21512;&#37027;&#20123;&#20998;&#25955;&#30340;&#25552;&#28860;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#65292;&#19968;&#20123;&#26080;&#27861;&#35782;&#21035;&#30340;&#22270;&#20687;&#65289;&#24182;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;FedD3&#22312;&#38656;&#35201;&#36890;&#20449;&#30340;&#25968;&#25454;&#37327;&#26041;&#38754;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#65292;&#21516;&#26102;&#23427;&#33021;&#22815;&#22312;&#20351;&#29992;&#22330;&#26223;&#20013;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage sc
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#21033;&#29992;&#36710;&#22411;&#25511;&#21046;&#22120;&#30340;&#21103;&#20135;&#21697;&#26465;&#20214;&#19987;&#38376;&#20110;&#26576;&#19968;&#36710;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#27010;&#29575;&#34892;&#20026;&#27169;&#22411;&#29983;&#25104;&#36710;&#22411;&#29305;&#23450;&#30340;&#33322;&#28857;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2208.04987</link><description>&lt;p&gt;
&#36710;&#22411;&#29305;&#23450;&#33322;&#28857;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vehicle Type Specific Waypoint Generation. (arXiv:2208.04987v1 [cs.AI] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04987
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#21033;&#29992;&#36710;&#22411;&#25511;&#21046;&#22120;&#30340;&#21103;&#20135;&#21697;&#26465;&#20214;&#19987;&#38376;&#20110;&#26576;&#19968;&#36710;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#27010;&#29575;&#34892;&#20026;&#27169;&#22411;&#29983;&#25104;&#36710;&#22411;&#29305;&#23450;&#30340;&#33322;&#28857;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#26426;&#21046;&#65292;&#20174;&#34892;&#20026;&#30340;&#27010;&#29575;&#22522;&#30784;&#27169;&#22411;&#20013;&#29983;&#25104;&#29305;&#23450;&#36710;&#22411;&#30340;&#33322;&#28857;&#24207;&#21015;&#12290;&#35768;&#22810;&#34892;&#20026;&#27169;&#22411;&#26159;&#22522;&#20110;&#19981;&#21253;&#25324;&#36710;&#36742;&#20449;&#24687;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35268;&#21010;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20135;&#29983;&#36710;&#22411;&#29305;&#23450;&#25511;&#21046;&#22120;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21103;&#20135;&#21697;&#65292;&#26377;&#26465;&#20214;&#22320;&#23558;&#36825;&#31181;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#19987;&#38376;&#24212;&#29992;&#20110;&#26576;&#19968;&#36710;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36710;&#22411;&#29305;&#23450;&#20540;&#20989;&#25968;&#20272;&#35745;&#19982;&#36890;&#29992;&#30340;&#27010;&#29575;&#34892;&#20026;&#27169;&#22411;&#32452;&#21512;&#65292;&#29983;&#25104;&#27604;&#20854;&#19981;&#32771;&#34385;&#36710;&#22411;&#30340;&#33322;&#28857;&#24207;&#21015;&#26356;&#21487;&#33021;&#26159;&#29289;&#29702;&#21487;&#34892;&#30340;&#36710;&#22411;&#29305;&#23450;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a generic mechanism for generating vehicle-type specific sequences of waypoints from a probabilistic foundation model of driving behavior. Many foundation behavior models are trained on data that does not include vehicle information, which limits their utility in downstream applications such as planning. Our novel methodology conditionally specializes such a behavior predictive model to a vehicle-type by utilizing byproducts of the reinforcement learning algorithms used to produce vehicle specific controllers. We show how to compose a vehicle specific value function estimate with a generic probabilistic behavior model to generate vehicle-type specific waypoint sequences that are more likely to be physically plausible then their vehicle-agnostic counterparts.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.07886</link><description>&lt;p&gt;
&#22522;&#20110;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Experimental Evaluation of Machine Learning Training on a Real Processing-in-Memory System. (arXiv:2207.07886v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#28508;&#33021;&#65292;&#24182;&#35777;&#26126;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#19981;&#26029;&#35775;&#38382;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#36807;&#31243;&#36890;&#24120;&#20250;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;CPU&#65292;GPU&#65289;&#22312;&#20869;&#23384;&#21333;&#20803;&#21644;&#22788;&#29702;&#21333;&#20803;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#26041;&#38754;&#23384;&#22312;&#26114;&#36149;&#30340;&#29942;&#39048;&#65292;&#36825;&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#33021;&#37327;&#21644;&#25191;&#34892;&#21608;&#26399;&#12290;&#20855;&#26377;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#21151;&#33021;&#30340;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20102;&#35299;&#29616;&#20195;&#36890;&#29992;PIM&#26550;&#26500;&#21152;&#36895;ML&#35757;&#32451;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#23454;&#38469;&#36890;&#29992;PIM&#26550;&#26500;&#19978;&#23454;&#29616;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#20256;&#32479;ML&#31639;&#27861;&#65288;&#21363;&#32447;&#24615;&#22238;&#24402;&#65292;&#36923;&#36753;&#22238;&#24402;&#65292;&#20915;&#31574;&#26641;&#65292;K-Means&#32858;&#31867;&#65289;&#65292;&#65288;2&#65289;&#20005;&#26684;&#35780;&#20272;&#21644;&#34920;&#24449;&#36825;&#20123;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#65292;&#24615;&#33021;&#21644;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#65288;3&#65289;&#19982;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#30340;&#30456;&#24212;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#20869;&#23384;&#20013;&#24515;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#30456;&#24212;&#30340;CPU&#21644;GPU&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;PIM&#30340;ML&#35757;&#32451;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#21152;&#36895;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning (ML) algorithms is a computationally intensive process, which is frequently memory-bound due to repeatedly accessing large training datasets. As a result, processor-centric systems (e.g., CPU, GPU) suffer from costly data movement between memory units and processing units, which consumes large amounts of energy and execution cycles. Memory-centric computing systems, i.e., with processing-in-memory (PIM) capabilities, can alleviate this data movement bottleneck.  Our goal is to understand the potential of modern general-purpose PIM architectures to accelerate ML training. To do so, we (1) implement several representative classic ML algorithms (namely, linear regression, logistic regression, decision tree, K-Means clustering) on a real-world general-purpose PIM architecture, (2) rigorously evaluate and characterize them in terms of accuracy, performance and scaling, and (3) compare to their counterpart implementations on CPU and GPU. Our evaluation on a real mem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.03075</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#21307;&#23398;&#39046;&#22495;&#26159;&#37319;&#29992;FL&#30340;&#26368;&#36866;&#21512;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#24517;&#39035;&#23562;&#37325;&#24739;&#32773;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25552;&#20379;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#24212;&#29992;FL&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21363;&#38271;&#26399;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#30382;&#32932;&#30284;&#22270;&#20687;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#25552;&#20986;&#32463;&#39564;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#12290;&#28508;&#22312;&#30340;FL&#29992;&#25143;&#65292;&#22914;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#22522;&#20934;&#20316;&#20026;&#37319;&#29992;FL&#30340;&#25351;&#21335;&#65292;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#35797;&#38169;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#20197;&#20445;&#30041;&#29616;&#23454;&#19990;&#30028;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#38024;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;FL&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#20004;&#31181;&#20856;&#22411;FL&#31639;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;&#22522;&#20110;&#19977;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#25552;&#20379;&#20102;&#22312;&#23433;&#20840;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#65292;&#24212;&#29992;FL&#20174;&#32780;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PECCO&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#38388;&#30340;&#23545;&#31216;&#24615;&#21644;&#33021;&#37327;&#35780;&#20998;&#35268;&#21017;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2205.01927</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21160;&#21147;&#23398;&#30340;&#27010;&#29575;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Symmetry for Multi-Agent Dynamics. (arXiv:2205.01927v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;PECCO&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#38388;&#30340;&#23545;&#31216;&#24615;&#21644;&#33021;&#37327;&#35780;&#20998;&#35268;&#21017;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#24182;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20915;&#31574;&#25552;&#20379;&#37325;&#35201;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#39550;&#39542;&#31561;&#39046;&#22495;&#12290;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#32780;&#20135;&#29983;&#27010;&#29575;&#39044;&#27979;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#19982;&#35780;&#20272;&#39118;&#38505;&#26041;&#38754;&#23545;&#19979;&#28216;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#65292;&#22914;&#36816;&#21160;&#35268;&#21010;&#19982;&#36991;&#30896;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#26234;&#33021;&#20307;&#21160;&#24577;&#36890;&#24120;&#21253;&#21547;&#20869;&#37096;&#23545;&#31216;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#65292;&#29305;&#21035;&#26159;&#26059;&#36716;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#65292;&#36824;&#21487;&#20197;&#25913;&#21892;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#33021;&#37327;&#35780;&#20998;&#20316;&#20026;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#26469;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21363;&#27010;&#29575;&#31561;&#21464;&#36830;&#32493;&#21367;&#31215;&#65288;PECCO&#65289;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;PECCO&#23558;&#31561;&#21464;&#30340;&#36830;&#32493;&#21367;&#31215;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#30340;&#32852;&#21512;&#36895;&#24230;&#20998;&#24067;&#24314;&#27169;&#19978;&#12290;&#23427;&#20351;&#29992;&#21160;&#24577;&#31215;&#20998;&#23558;&#19981;&#30830;&#23450;&#24615;&#20174;&#36895;&#24230;&#20256;&#25773;&#21040;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#22330;&#26223;&#30340;&#32467;&#26524;&#20013;&#39564;&#35777;&#20102;PECCO&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning multi-agent dynamics is a core AI problem with broad applications in robotics and autonomous driving. While most existing works focus on deterministic prediction, producing probabilistic forecasts to quantify uncertainty and assess risks is critical for downstream decision-making tasks such as motion planning and collision avoidance. Multi-agent dynamics often contains internal symmetry. By leveraging symmetry, specifically rotation equivariance, we can improve not only the prediction accuracy but also uncertainty calibration. We introduce Energy Score, a proper scoring rule, to evaluate probabilistic predictions. We propose a novel deep dynamics model, Probabilistic Equivariant Continuous COnvolution (PECCO) for probabilistic prediction of multi-agent trajectories. PECCO extends equivariant continuous convolution to model the joint velocity distribution of multiple agents. It uses dynamics integration to propagate the uncertainty from velocity to position. On both synthetic a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Sparsemax&#30340;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;Transformer&#19981;&#19968;&#23450;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#26377;&#28145;&#21051;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2202.03932</link><description>&lt;p&gt;
Transformer&#26356;&#21152;&#31283;&#20581;&#21527;&#65311;&#38754;&#21521;Transformer&#30340;&#30830;&#20999;&#31283;&#20581;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Are Transformers More Robust? Towards Exact Robustness Verification for Transformers. (arXiv:2202.03932v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Sparsemax&#30340;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;Transformer&#19981;&#19968;&#23450;&#27604;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#23545;&#20110;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#26377;&#28145;&#21051;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26032;&#20852;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65292;Transformer&#34987;&#24212;&#29992;&#20110;&#20247;&#22810;&#39046;&#22495;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21040;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;Transformers&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#65292;&#20302;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22522;&#20110;Sparsemax&#30340;Transformers&#65292;&#23558;&#25214;&#21040;&#23427;&#20204;&#30340;&#26368;&#22823;&#31283;&#20581;&#24615;&#38477;&#20302;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#32422;&#26463;&#32534;&#31243;&#65288;MIQCP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20004;&#20010;&#21487;&#23884;&#20837;MIQCP&#32534;&#30721;&#24182;&#22823;&#24133;&#21152;&#36895;&#20854;&#27714;&#35299;&#30340;&#39044;&#22788;&#29702;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Land Departure Warning&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#22522;&#20110;Sparsemax&#30340;Transformers&#19982;&#26356;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;Transformer&#24182;&#19981;&#19968;&#23450;&#26356;&#21152;&#31283;&#20581;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#36873;&#25321;&#36866;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#30340;NN&#26550;&#26500;&#26041;&#38754;&#30340;&#28145;&#21051;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.
&lt;/p&gt;</description></item></channel></rss>