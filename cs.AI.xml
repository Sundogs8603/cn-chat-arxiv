<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12153</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#30340;&#27010;&#29575;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing. (arXiv:2310.12153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#30340;&#24179;&#34913;K-Means&#32858;&#31867;&#30340;&#27010;&#29575;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#65292;&#23454;&#29616;&#22312;D-Wave AQC&#19978;&#35782;&#21035;&#27169;&#31946;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#65288;AQC&#65289;&#26159;&#19968;&#31181;&#26377;&#26395;&#29992;&#20110;&#31163;&#25955;&#19988;&#36890;&#24120;&#20026;NP&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#30340;&#37327;&#23376;&#35745;&#31639;&#26041;&#27861;&#12290;&#30446;&#21069;&#30340;AQC&#20801;&#35768;&#23454;&#29616;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65292;&#36825;&#20419;&#20351;&#20102;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#24320;&#21457;&#37327;&#23376;&#34920;&#31034;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#38656;&#35201;&#20174;&#22122;&#22768;AQC&#36827;&#34892;&#22810;&#27425;&#27979;&#37327;&#65292;&#20294;&#24403;&#21069;&#26041;&#27861;&#20165;&#21033;&#29992;&#26368;&#20339;&#27979;&#37327;&#65292;&#20002;&#24323;&#20102;&#20854;&#20182;&#27979;&#37327;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#36827;&#34892;&#27010;&#29575;&#24179;&#34913;k-means&#32858;&#31867;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#38750;&#26368;&#20248;&#35299;&#26469;&#35745;&#31639;&#26657;&#20934;&#21518;&#39564;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#20302;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#27169;&#31946;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#25968;&#25454;&#28857;&#65292;&#25105;&#20204;&#22312;D-Wave AQC&#19978;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#65292;&#25105;&#20204;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26032;&#27169;&#22411;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#31574;&#30053;&#65292;&#20197;&#23547;&#25214;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12145</link><description>&lt;p&gt;
&#36890;&#36807;NAS&#23454;&#29616;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#34920;&#26684;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fairer and More Accurate Tabular Models Through NAS. (arXiv:2310.12145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#65292;&#25105;&#20204;&#22312;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26032;&#27169;&#22411;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;&#31574;&#30053;&#65292;&#20197;&#23547;&#25214;&#26356;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#36890;&#36807;&#31639;&#27861;&#20351;&#24471;&#34920;&#26684;&#25968;&#25454;&#30340;&#27169;&#22411;&#26356;&#21152;&#20844;&#24179;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#29616;&#26377;&#30340;&#25216;&#26415;&#36890;&#24120;&#38024;&#23545;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#32467;&#26524;&#30340;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#21464;&#25968;&#25454;&#30340;&#25668;&#20837;&#26041;&#24335;&#12289;&#27169;&#22411;&#26435;&#37325;&#25110;&#36755;&#20986;&#22788;&#29702;&#26041;&#24335;&#26469;&#20462;&#22797;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26032;&#30340;&#31574;&#30053;&#65292;&#22312;&#21435;&#20559;&#36807;&#31243;&#20013;&#32771;&#34385;&#26356;&#26032;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#36229;&#21442;&#25968;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;&#20174;&#19968;&#24320;&#22987;&#22312;&#39044;&#27979;&#32467;&#26524;&#19978;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;MLP&#12289;ResNet&#21644;FT-Transformer&#31561;&#19981;&#21516;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#23545;&#36229;&#21442;&#25968;&#32452;&#21512;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#38024;&#23545;&#20934;&#30830;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20844;&#24179;&#24615;&#30340;&#38477;&#20302;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#36827;&#34892;Java&#31867;&#38169;&#35823;&#39044;&#27979;&#25928;&#21147;&#30340;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20248;&#20110;&#21333;&#19968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#25552;&#21319;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.12133</link><description>&lt;p&gt;
&#23545;&#20110;Bug&#39044;&#27979;&#30340;&#38598;&#25104;&#27169;&#22411;&#25928;&#21147;&#30340;&#21487;&#29702;&#35299;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A comprehensible analysis of the efficacy of Ensemble Models for Bug Prediction. (arXiv:2310.12133v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#36827;&#34892;Java&#31867;&#38169;&#35823;&#39044;&#27979;&#25928;&#21147;&#30340;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20248;&#20110;&#21333;&#19968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#25552;&#21319;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#23545;&#20110;&#20854;&#26377;&#25928;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#21457;&#29616;&#21644;&#20462;&#22797;&#36719;&#20214;&#32570;&#38519;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#24320;&#21457;&#20219;&#21153;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#22686;&#38271;&#23548;&#33268;&#20102;&#35768;&#22810;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#24110;&#21161;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#20195;&#30721;&#20013;&#30340;&#28508;&#22312;&#32570;&#38519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20004;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65288;&#21333;&#19968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65289;&#36827;&#34892;&#30340;Java&#31867;&#38169;&#35823;&#39044;&#27979;&#25928;&#21147;&#30340;&#21487;&#29702;&#35299;&#24615;&#27604;&#36739;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#24320;&#28304;Apache Commons&#39033;&#30446;&#30340;Java&#32452;&#20214;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25928;&#26524;&#20248;&#20110;&#21333;&#19968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#22686;&#24378;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#30340;&#35265;&#35299;&#12290;&#21576;&#29616;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correctness of software systems is vital for their effective operation. It makes discovering and fixing software bugs an important development task. The increasing use of Artificial Intelligence (AI) techniques in Software Engineering led to the development of a number of techniques that can assist software developers in identifying potential bugs in code. In this paper, we present a comprehensible comparison and analysis of the efficacy of two AI-based approaches, namely single AI models and ensemble AI models, for predicting the probability of a Java class being buggy. We used two open-source Apache Commons Project's Java components for training and evaluating the models. Our experimental findings indicate that the ensemble of AI models can outperform the results of applying individual AI models. We also offer insight into the factors that contribute to the enhanced performance of the ensemble AI model. The presented results demonstrate the potential of using ensemble AI models t
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12126</link><description>&lt;p&gt;
SHARCS&#65306;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#30340;&#39640;&#25928;Transformer
&lt;/p&gt;
&lt;p&gt;
SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks. (arXiv:2310.12126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12126
&lt;/p&gt;
&lt;p&gt;
SHARCS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23485;&#24230;&#23376;&#32593;&#32476;&#36827;&#34892;&#36335;&#30001;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#25512;&#29702;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SHARCS&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#25512;&#29702;&#65292;&#32771;&#34385;&#21040;&#36755;&#20837;&#26679;&#26412;&#30340;&#38590;&#24230;&#12290;SHARCS&#21487;&#20197;&#22312;&#20219;&#20309;Transformer&#32593;&#32476;&#19978;&#35757;&#32451;&#19968;&#20010;&#36335;&#30001;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#23558;&#19981;&#21516;&#26679;&#26412;&#25351;&#21521;&#20855;&#26377;&#19981;&#21516;&#23485;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;&#65288;1&#65289;&#22312;&#20934;&#30830;&#24615;&#19982;FLOPs&#20043;&#38388;&#65292;SHARCS&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#25512;&#29702;&#26041;&#27861;&#65307;&#65288;2&#65289;SHARCS&#22312;&#19981;&#21516;&#26550;&#26500;&#20043;&#38388;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#21387;&#32553;&#21644;&#39640;&#25928;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#25928;&#29575;&#65307;&#65288;3&#65289;SHARCS&#21487;&#20197;&#25552;&#20379;2&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#20316;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.12112</link><description>&lt;p&gt;
&#19968;&#21017;&#35686;&#31034;&#25925;&#20107;&#65306;&#20851;&#20110;&#21442;&#32771;&#25968;&#25454;&#22312;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses. (arXiv:2310.12112v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#20316;&#29992;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#23454;&#35777;&#38544;&#31169;&#38450;&#24481;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#27700;&#24179;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38450;&#24481;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25968;&#25454;&#65292;&#21442;&#32771;&#25968;&#25454;&#25351;&#30340;&#26159;&#26469;&#33258;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#65288;&#25110;&#31867;&#20284;&#65289;&#22522;&#30784;&#20998;&#24067;&#30340;&#38468;&#21152;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#21442;&#32771;&#25968;&#25454;&#30340;&#20351;&#29992;&#24456;&#26222;&#36941;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#23545;&#20110;&#23450;&#20041;&#21644;&#35780;&#20272;&#21442;&#32771;&#25968;&#25454;&#38544;&#31169;&#30456;&#24403;&#20445;&#23432;&#12290;&#30001;&#20110;&#27169;&#22411;&#25928;&#29992;&#21644;/&#25110;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#25552;&#21319;&#21487;&#33021;&#20197;&#29306;&#29298;&#21442;&#32771;&#25968;&#25454;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#22240;&#27492;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20043;&#21069;&#30340;&#20316;&#21697;&#20013;&#21442;&#32771;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21450;&#20854;&#38544;&#31169;&#22788;&#29702;&#24773;&#20917;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#20844;&#24179;&#27604;&#36739;&#38450;&#24481;&#26041;&#27861;&#26469;&#35828;&#21442;&#32771;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20934;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the realm of privacy-preserving machine learning, empirical privacy defenses have been proposed as a solution to achieve satisfactory levels of training data privacy without a significant drop in model utility. Most existing defenses against membership inference attacks assume access to reference data, defined as an additional dataset coming from the same (or a similar) underlying distribution as training data. Despite the common use of reference data, previous works are notably reticent about defining and evaluating reference data privacy. As gains in model utility and/or training data privacy may come at the expense of reference data privacy, it is essential that all three aspects are duly considered. In this paper, we first examine the availability of reference data and its privacy treatment in previous works and demonstrate its necessity for fairly comparing defenses. Second, we propose a baseline defense that enables the utility-privacy tradeoff with respect to both trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#38590;&#24230;&#24863;&#30693;&#35821;&#20041;&#22686;&#24378;(DASA)&#26041;&#27861;&#65292;&#36890;&#36807;&#25200;&#21160;&#35821;&#20041;&#26041;&#21521;&#26469;&#22686;&#24378;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#38590;&#24230;&#24863;&#30693;&#30340;&#21152;&#24615;&#36793;&#30028;&#36719;&#26368;&#22823;&#20540;(DAAM-Softmax)&#26469;&#23454;&#29616;&#26368;&#20248;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12111</link><description>&lt;p&gt;
DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification. (arXiv:2310.12111v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification. (arXiv:2310.12111v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#38590;&#24230;&#24863;&#30693;&#35821;&#20041;&#22686;&#24378;(DASA)&#26041;&#27861;&#65292;&#36890;&#36807;&#25200;&#21160;&#35821;&#20041;&#26041;&#21521;&#26469;&#22686;&#24378;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#24341;&#20837;&#20102;&#38590;&#24230;&#24863;&#30693;&#30340;&#21152;&#24615;&#36793;&#30028;&#36719;&#26368;&#22823;&#20540;(DAAM-Softmax)&#26469;&#23454;&#29616;&#26368;&#20248;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#22686;&#24378;&#26041;&#27861;&#25805;&#32437;&#21407;&#22987;&#20449;&#21495;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#19988;&#22686;&#24378;&#26679;&#26412;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#38590;&#24230;&#24863;&#30693;&#35821;&#20041;&#22686;&#24378;(DASA)&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35828;&#35805;&#20154;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#25104;&#26412;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25200;&#21160;&#26469;&#33258;&#35828;&#35805;&#20154;&#30697;&#38453;&#30340;&#35821;&#20041;&#26041;&#21521;&#26469;&#22686;&#24378;&#35757;&#32451;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20174;&#40065;&#26834;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#20013;&#20272;&#35745;&#20934;&#30830;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#25152;&#20197;&#25105;&#20204;&#24341;&#20837;&#20102;&#38590;&#24230;&#24863;&#30693;&#30340;&#21152;&#24615;&#36793;&#30028;&#36719;&#26368;&#22823;&#20540;(DAAM-Softmax)&#26469;&#33719;&#24471;&#26368;&#20248;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20551;&#35774;&#22686;&#24378;&#26679;&#26412;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#65292;&#24182;&#25512;&#23548;&#20986;&#24102;&#26377;DASA&#30340;&#26399;&#26395;&#25439;&#22833;&#30340;&#38381;&#24335;&#19978;&#30028;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is vital to the generalization ability and robustness of deep neural networks (DNNs) models. Existing augmentation methods for speaker verification manipulate the raw signal, which are time-consuming and the augmented samples lack diversity. In this paper, we present a novel difficulty-aware semantic augmentation (DASA) approach for speaker verification, which can generate diversified training samples in speaker embedding space with negligible extra computing cost. Firstly, we augment training samples by perturbing speaker embeddings along semantic directions, which are obtained from speaker-wise covariance matrices. Secondly, accurate covariance matrices are estimated from robust speaker embeddings during training, so we introduce difficultyaware additive margin softmax (DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the number of augmented samples goes to infinity and derive a closed-form upper bound of the expected loss with DASA, which achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12103</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity through Human Feedback. (arXiv:2310.12103v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity through Human Feedback&#65292;QDHF&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;Quality Diversity&#65292;QD&#65289;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;QD&#30456;&#21305;&#37197;&#30340;&#25628;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#22312;&#25552;&#39640;&#23450;&#24615;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#24403;&#20165;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#26368;&#22823;&#21270;&#24179;&#22343;&#20154;&#31867;&#20559;&#22909;&#30340;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26426;&#21046;&#26102;&#65292;&#29305;&#21035;&#26159;&#22312;&#35201;&#27714;&#22810;&#26679;&#21270;&#27169;&#22411;&#21709;&#24212;&#30340;&#22270;&#20687;&#29983;&#25104;&#31561;&#39046;&#22495;&#65292;&#20854;&#25928;&#26524;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33268;&#21147;&#20110;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#23545;&#25163;&#21160;&#23450;&#20041;&#22810;&#26679;&#24615;&#25351;&#26631;&#30340;&#20381;&#36182;&#32422;&#26463;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#34701;&#21512;&#26469;&#33258;&#20004;&#32773;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#20811;&#26381;RLHF&#21644;QD&#30340;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#23454;&#29616;&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QDHF&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#25512;&#26029;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#25193;&#23637;&#20102;QD&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;QD&#26041;&#27861;&#30456;&#27604;&#65292;QDHF&#22312;&#33258;&#21160;&#22810;&#26679;&#24615;&#21457;&#29616;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19982;QD&#30340;&#25628;&#32034;&#33021;&#21147;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.12100</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#33258;&#36866;&#24212;&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#36755;&#20837;&#20013;&#24515;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling. (arXiv:2310.12100v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65288;AdaLink&#65289;&#65292;&#36890;&#36807;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#32780;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24314;&#27169;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#21442;&#25968;&#25968;&#37327;&#20174;O&#65288;10^9&#65289;&#25193;&#23637;&#21040;O&#65288;10^{12}&#65289;&#29978;&#33267;&#26356;&#39640;&#27700;&#24179;&#65292;&#23637;&#29616;&#20986;&#22312;&#24191;&#27867;&#20219;&#21153;&#19978;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#36825;&#26679;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#20351;&#24471;&#22312;&#32473;&#23450;&#24863;&#20852;&#36259;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#23436;&#20840;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#21644;&#37096;&#32626;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25104;&#20026;&#24212;&#23545;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#36866;&#24212;&#21644;&#26381;&#21153;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#23558;PEFT&#25216;&#26415;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20405;&#20837;&#24335;&#21644;&#38750;&#20405;&#20837;&#24335;&#12290;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#30452;&#25509;&#25913;&#21464;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#34429;&#28982;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#26381;&#21153;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#20445;&#25345;&#20869;&#37096;&#32467;&#26500;&#19981;&#21464;&#65292;&#21482;&#35843;&#25972;&#27169;&#22411;&#30340;&#22806;&#37096;&#21442;&#25968;&#65292;&#22914;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;AdaLink&#25551;&#36848;&#20026;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;PEFT&#25216;&#26415;&#65292;&#19982;SoTA&#20405;&#20837;&#24335;PEFT&#65288;LoRA&#65289;&#21644;&#23436;&#25972;&#27169;&#22411;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.12081</link><description>&lt;p&gt;
DHOT-GM&#65306;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#23454;&#29616;&#40065;&#26834;&#22270;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework. (arXiv:2310.12081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHOT-GM&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#21305;&#37197;&#26159;&#26368;&#37325;&#35201;&#30340;&#22270;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19981;&#21516;&#22270;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#21305;&#37197;&#22270;&#26102;&#20381;&#36182;&#20110;&#37051;&#25509;&#30697;&#38453;&#25110;&#33410;&#28857;&#23884;&#20837;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#36234;&#65292;&#22240;&#20026;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22270;&#20013;&#38544;&#34255;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22914;&#33410;&#28857;&#23646;&#24615;&#12289;&#23376;&#22270;&#32467;&#26500;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#30340;&#20998;&#23618;&#26368;&#20248;&#20256;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#31216;&#20026;DHOT-GM&#12290;&#23454;&#36136;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19982;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#23545;&#24212;&#30340;&#19968;&#32452;&#20851;&#31995;&#30697;&#38453;&#12290;&#32473;&#23450;&#20004;&#20010;&#22270;&#65292;&#25105;&#20204;&#26522;&#20030;&#25152;&#26377;&#20851;&#31995;&#30697;&#38453;&#23545;&#65292;&#24182;&#33719;&#21462;&#23427;&#20204;&#30340;&#21305;&#37197;&#32467;&#26524;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#21305;&#37197;&#32467;&#26524;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#25512;&#26029;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20026;&#35745;&#31639;&#20004;&#20010;&#22270;&#20043;&#38388;&#30340;HOT&#36317;&#31163;&#65292;&#27599;&#20010;&#22270;&#37117;&#26159;&#30001;&#20851;&#31995;&#30697;&#38453;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#25915;&#20987;&#26469;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#29256;&#26435;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.12063</link><description>&lt;p&gt;
GAN&#20013;&#40657;&#30418;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#30340;&#25506;&#27979;&#22120;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Black-Box Training Data Identification in GANs via Detector Networks. (arXiv:2310.12063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#26102;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#25915;&#20987;&#26469;&#35782;&#21035;&#35757;&#32451;&#25968;&#25454;&#25104;&#21592;&#36523;&#20221;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#29256;&#26435;&#21644;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23427;&#20204;&#38382;&#19990;&#20197;&#26469;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#19968;&#30452;&#26159;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#35757;&#32451;&#22909;&#30340;GAN&#20197;&#21450;&#26469;&#33258;&#22522;&#30784;&#20998;&#24067;&#30340;&#26032;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#33021;&#21542;&#26377;&#25928;&#22320;&#35782;&#21035;&#32473;&#23450;&#28857;&#26159;&#21542;&#23646;&#20110;GAN&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23545;&#20110;&#29256;&#26435;&#30456;&#20851;&#30340;&#21407;&#22240;&#24456;&#26377;&#24847;&#20041;&#65292;&#29992;&#25143;&#21487;&#33021;&#24819;&#30830;&#23450;&#20182;&#20204;&#30340;&#29256;&#26435;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#26469;&#35757;&#32451;GAN&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#30740;&#31350;&#20063;&#24456;&#26377;&#24847;&#20041;&#65292;&#20854;&#20013;&#26816;&#27979;&#35757;&#32451;&#38598;&#25104;&#21592;&#36523;&#20221;&#30340;&#33021;&#21147;&#34987;&#31216;&#20026;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20351;&#29992;GAN&#30340;&#38544;&#31169;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21482;&#33021;&#35775;&#38382;&#29983;&#25104;&#22120;&#30340;&#26679;&#26412;&#65292;&#32780;&#19981;&#33021;&#35775;&#38382;&#37492;&#21035;&#22120;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#38024;&#23545;&#40657;&#30418;&#35774;&#32622;&#20013;GAN&#30340;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#65292;&#24182;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12052</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#65306;&#19968;&#31181;&#22823;&#35268;&#27169;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-based Nutrient Application's Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach. (arXiv:2310.12052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#20892;&#19994;&#33829;&#20859;&#24212;&#29992;&#26102;&#38388;&#25512;&#33616;&#26041;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#65292;&#24182;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#25968;&#25454;&#20998;&#26512;&#22312;&#30417;&#27979;&#20892;&#20316;&#29289;&#26045;&#32933;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#19981;&#20934;&#30830;&#30340;&#26045;&#32933;&#20915;&#31574;&#20250;&#23548;&#33268;&#26114;&#36149;&#30340;&#21518;&#26524;&#65292;&#38459;&#30861;&#31918;&#39135;&#29983;&#20135;&#65292;&#24182;&#36896;&#25104;&#29615;&#22659;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#30830;&#23450;&#25972;&#20010;&#23395;&#33410;&#25152;&#38656;&#30340;&#32933;&#26009;&#25968;&#37327;&#26469;&#39044;&#27979;&#33829;&#20859;&#24212;&#29992;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#24314;&#35758;&#26681;&#25454;&#22825;&#27668;&#26465;&#20214;&#21644;&#22303;&#22756;&#29305;&#24615;&#35843;&#25972;&#32933;&#26009;&#37327;&#65292;&#20197;&#20419;&#36827;&#32463;&#27982;&#39640;&#25928;&#21644;&#29615;&#22659;&#21451;&#22909;&#30340;&#20892;&#19994;&#12290;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#26159;&#39640;&#32500;&#24230;&#21644;&#24322;&#26500;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#22659;&#20013;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20197;&#20908;&#23567;&#40614;&#20316;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#20102;&#26045;&#32933;&#24212;&#29992;&#19982;&#22825;&#27668;&#25968;&#25454;&#23545;&#20316;&#29289;&#20135;&#37327;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35299;&#26412;&#22320;&#32972;&#26223;&#21644;&#22320;&#29702;&#22240;&#32032;&#65292;&#25105;&#20204;&#24076;&#26395;&#31283;&#23450;&#29978;&#33267;&#20943;&#23569;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the dema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.12036</link><description>&lt;p&gt;
&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Theoretical Paradigm to Understand Learning from Human Preferences. (arXiv:2310.12036v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#23454;&#38469;&#31639;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#32469;&#36807;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#30340;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#37325;&#35201;&#30340;&#36817;&#20284;&#65306;&#31532;&#19968;&#20551;&#35774;&#21487;&#20197;&#29992;&#36880;&#28857;&#22870;&#21169;&#26367;&#20195;&#25104;&#23545;&#20559;&#22909;&#12290;&#31532;&#20108;&#20010;&#20551;&#35774;&#26159;&#22312;&#36825;&#20123;&#36880;&#28857;&#22870;&#21169;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#20174;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#27867;&#21270;&#21040;&#31574;&#30053;&#37319;&#26679;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32469;&#36807;&#20102;&#31532;&#20108;&#20010;&#36817;&#20284;&#65292;&#24182;&#30452;&#25509;&#20174;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#31532;&#19968;&#20010;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23545;&#36825;&#20123;&#23454;&#38469;&#31639;&#27861;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#31216;&#20026;&#936;PO&#65292;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#35813;&#30446;&#26631;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#22240;&#27492;&#32469;&#36807;&#20102;&#36825;&#20004;&#20010;&#36817;&#20284;&#12290;&#36825;&#20010;&#26032;&#30340;&#19968;&#33324;&#30446;&#26631;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31181;&#26032;&#30340;&#20174;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#31574;&#30053;&#30340;&#26041;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22870;&#21169;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.  In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an 
&lt;/p&gt;</description></item><item><title>SegmATRon&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#26102;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.12031</link><description>&lt;p&gt;
SegmATRon: &#29992;&#20110;&#23460;&#20869;&#29615;&#22659;&#30340;&#20855;&#26377;&#33258;&#36866;&#24212;&#35821;&#20041;&#20998;&#21106;&#30340;&#20307;&#39564;&#24863;&#30693;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment. (arXiv:2310.12031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12031
&lt;/p&gt;
&lt;p&gt;
SegmATRon&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#36890;&#36807;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#26102;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20351;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SegmATRon&#30340;&#33258;&#36866;&#24212;&#21464;&#25442;&#27169;&#22411;&#65292;&#29992;&#20110;&#23460;&#20869;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#28151;&#21512;&#22810;&#32452;&#20998;&#25439;&#22833;&#20989;&#25968;&#22312;&#22810;&#24352;&#22270;&#20687;&#19978;&#36827;&#34892;&#25512;&#26029;&#30340;&#27169;&#22411;&#26435;&#37325;&#36866;&#24212;&#24615;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#36924;&#30495;&#30340;Habitat&#21644;AI2-THOR&#21512;&#25104;&#27169;&#25311;&#22120;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#21033;&#29992;&#20195;&#29702;&#30340;&#34892;&#20026;&#33719;&#21462;&#39069;&#22806;&#22270;&#20687;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/wingrune/SegmATRon&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#31867;&#22411;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12008</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs. (arXiv:2310.12008v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#31867;&#22411;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;(KGET)&#26088;&#22312;&#25512;&#26029;&#30693;&#35782;&#22270;&#35889;&#20013;&#23454;&#20307;&#30340;&#21487;&#33021;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;KGET&#26041;&#27861;&#20391;&#37325;&#20110;&#22914;&#20309;&#26356;&#22909;&#22320;&#23558;&#23454;&#20307;&#30340;&#37051;&#23621;&#21644;&#31867;&#22411;&#25552;&#20379;&#30340;&#30693;&#35782;&#32534;&#30721;&#21040;&#20854;&#34920;&#31034;&#20013;&#65292;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#31867;&#22411;&#22914;&#20309;&#20197;&#32858;&#31867;&#26041;&#24335;&#25552;&#20379;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#30340;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#31867;&#22411;&#21028;&#26029;(MCLET)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#32858;&#31867;&#25552;&#20379;&#30340;&#31895;&#31890;&#24230;&#30693;&#35782;&#32534;&#30721;&#21040;&#23454;&#20307;&#21644;&#31867;&#22411;&#23884;&#20837;&#20013;&#12290;MCLET&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;i) &#22810;&#35270;&#22270;&#29983;&#25104;&#21644;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#32534;&#30721;&#23454;&#20307;&#31867;&#22411;&#12289;&#23454;&#20307;&#32858;&#31867;&#21644;&#32858;&#31867;&#31867;&#22411;&#35270;&#22270;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65307;ii) &#36328;&#35270;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#65292;&#40723;&#21169;&#19981;&#21516;&#35270;&#22270;&#20849;&#21516;&#25913;&#36827;&#23454;&#20307;&#21644;&#31867;&#22411;&#30340;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#65307;iii) &#23454;&#20307;&#31867;&#22411;&#21028;&#26029;&#27169;&#22359;&#65292;&#38598;&#25104;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.12007</link><description>&lt;p&gt;
KI-PMF&#65306;&#30693;&#35782;&#32508;&#21512;&#30340;&#21512;&#29702;&#21160;&#20316;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KI-PMF: Knowledge Integrated Plausible Motion Forecasting. (arXiv:2310.12007v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KI-PMF&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#26410;&#26469;&#34892;&#21160;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#36981;&#24490;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36890;&#36807;&#26465;&#20214;&#21270;&#32593;&#32476;&#20197;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#21160;&#23545;&#22823;&#35268;&#27169;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#19981;&#31526;&#21512;&#29289;&#29702;&#23450;&#24459;&#25110;&#36829;&#21453;&#22806;&#37096;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32467;&#21512;&#26126;&#30830;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#65292;&#31526;&#21512;&#36710;&#36742;&#30340;&#36816;&#21160;&#32422;&#26463;&#21644;&#34892;&#39542;&#29615;&#22659;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38750;&#21442;&#25968;&#21098;&#26525;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#26469;&#25972;&#21512;&#23450;&#20041;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20132;&#36890;&#21442;&#19982;&#32773;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#21040;&#36798;&#21487;&#36798;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#26465;&#20214;&#21270;&#20026;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#39044;&#27979;&#65292;&#36825;&#23545;&#20110;&#22312;&#23454;&#38469;&#19990;&#30028;&#29615;&#22659;&#20013;&#32500;&#25252;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11986</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#25216;&#26415;&#23433;&#20840;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Sociotechnical Safety Evaluation of Generative AI Systems. (arXiv:2310.11986v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#23545;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#29616;&#29366;&#35843;&#26597;&#21457;&#29616;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#31995;&#32479;&#20250;&#20135;&#29983;&#19968;&#31995;&#21015;&#39118;&#38505;&#12290;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#65292;&#38656;&#35201;&#23545;&#36825;&#20123;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26694;&#26550;&#65292;&#37319;&#29992;&#32467;&#26500;&#21270;&#30340;&#31038;&#20250;&#25216;&#26415;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#33021;&#21147;&#35780;&#20272;&#65292;&#36825;&#26159;&#30446;&#21069;&#20027;&#35201;&#30340;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#22312;&#31995;&#32479;&#23433;&#20840;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#35748;&#35782;&#21040;&#19978;&#19979;&#25991;&#20915;&#23450;&#20102;&#29305;&#23450;&#33021;&#21147;&#26159;&#21542;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#20026;&#20102;&#32771;&#34385;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22686;&#21152;&#20102;&#20154;&#26426;&#20114;&#21160;&#21644;&#31995;&#32479;&#24433;&#21709;&#20316;&#20026;&#39069;&#22806;&#30340;&#35780;&#20272;&#23618;&#38754;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#23433;&#20840;&#35780;&#20272;&#30340;&#29616;&#29366;&#65292;&#24182;&#21019;&#24314;&#20102;&#29616;&#26377;&#35780;&#20272;&#30340;&#24211;&#12290;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19977;&#20010;&#26174;&#33879;&#30340;&#35780;&#20272;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#30340;&#21069;&#36827;&#26041;&#24335;&#65292;&#27010;&#36848;&#20102;&#23454;&#38469;&#27493;&#39588;&#21644;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles
&lt;/p&gt;</description></item><item><title>InfoDiffusion&#26159;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#65292;&#20197;&#21450;&#32467;&#21512;&#33258;&#25105;&#26465;&#20214;&#21644;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.11976</link><description>&lt;p&gt;
InfoDiffusion: &#38024;&#23545;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#30340;&#20449;&#24687;&#29109;&#24863;&#30693;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation. (arXiv:2310.11976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11976
&lt;/p&gt;
&lt;p&gt;
InfoDiffusion&#26159;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#21644;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#65292;&#20197;&#21450;&#32467;&#21512;&#33258;&#25105;&#26465;&#20214;&#21644;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25688;&#35201;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25193;&#25955;&#27169;&#22411;&#30340;&#8220;easy-first&#8221;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#19982;&#20154;&#31867;&#30340;&#8220;keyword-first&#8221;&#33258;&#28982;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#36825;&#24341;&#36215;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;InfoDiffusion&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#8220;keyinfo-first&#8221;&#29983;&#25104;&#31574;&#30053;&#65292;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#37327;&#30340;&#22122;&#22768;&#35843;&#24230;&#12290;&#27492;&#22806;&#65292;InfoDiffusion&#32467;&#21512;&#20102;&#33258;&#25105;&#26465;&#20214;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#37096;&#20998;&#21152;&#22122;&#27169;&#22411;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfoDiffusion&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20986;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the "easy-first" text generation process of current diffusion models and the "keyword-first" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a "keyinfo-first" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.11959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11959
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#29420;&#29305;&#30340;&#32452;&#25104;&#21644;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#20854;&#20998;&#26512;&#20013;&#29305;&#21035;&#32771;&#34385;&#20998;&#35299;&#21644;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#23545;&#23376;&#24207;&#21015;&#32423;&#21035;&#30340;&#24314;&#27169;&#21644;&#20998;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSD-Mixer&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#20998;&#35299;&#30340;MLP-Mixer&#65292;&#23427;&#23398;&#20250;&#20102;&#23558;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#26126;&#30830;&#22320;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20026;&#22810;&#23610;&#24230;&#23376;&#24207;&#21015;&#65292;&#21363;patches&#65292;&#24182;&#20351;&#29992;MLPs&#26469;&#32452;&#21512;patches&#20869;&#37096;&#21644;patches&#38388;&#30340;&#21464;&#21270;&#20197;&#21450;&#36890;&#36947;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#20998;&#35299;&#27531;&#24046;&#30340;&#24133;&#24230;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#23436;&#25972;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#23384;&#22312;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#21644;&#20132;&#21449;&#39564;&#35777;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#20559;&#35265;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26368;&#26032;&#30340;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#20250;&#23548;&#33268;&#25253;&#21578;&#36739;&#20302;&#20934;&#30830;&#24230;&#30340;&#35770;&#25991;&#26356;&#38590;&#21457;&#34920;&#12290;</title><link>http://arxiv.org/abs/2310.11950</link><description>&lt;p&gt;
&#22826;&#22909;&#19981;&#20687;&#26159;&#30495;&#30340;&#65306;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition. (arXiv:2310.11950v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#23384;&#22312;&#24615;&#33021;&#35780;&#20272;&#36807;&#39640;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#20998;&#21106;&#21644;&#20132;&#21449;&#39564;&#35777;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#20559;&#35265;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26368;&#26032;&#30340;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#19981;&#27491;&#30830;&#30340;&#32467;&#26524;&#20250;&#23548;&#33268;&#25253;&#21578;&#36739;&#20302;&#20934;&#30830;&#24230;&#30340;&#35770;&#25991;&#26356;&#38590;&#21457;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#39046;&#22495;&#26377;&#19968;&#20123;&#26631;&#20934;&#21644;&#26082;&#23450;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20256;&#32479;&#26041;&#27861;&#20250;&#23548;&#33268;&#31934;&#24230;&#34987;&#39640;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#36827;&#34892;&#25968;&#25454;&#20998;&#21106;&#30340;&#26041;&#27861;&#20197;&#21450;&#26631;&#20934;&#30340;&#38543;&#26426;k&#25240;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20250;&#20135;&#29983;&#20559;&#35265;&#32467;&#26524;&#12290;&#23545;&#36807;&#21435;&#30340;&#25991;&#29486;&#21644;&#29616;&#20195;&#30740;&#31350;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;HAR&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#26377;&#24517;&#35201;&#24341;&#36215;&#31185;&#23398;&#30028;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#30340;&#36127;&#38754;&#24433;&#21709;&#34987;&#24573;&#35270;&#20102;&#12290;&#21542;&#21017;&#65292;&#21457;&#34920;&#20559;&#35265;&#32467;&#26524;&#30340;&#35770;&#25991;&#23558;&#20250;&#25253;&#21578;&#36739;&#20302;&#30340;&#20934;&#30830;&#24230;&#65292;&#27491;&#30830;&#30340;&#26080;&#20559;&#26041;&#27861;&#26356;&#38590;&#20197;&#21457;&#34920;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#30340;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#35770;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#22914;&#20309;&#65292;&#36825;&#20010;&#38382;&#39064;&#37117;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2310.11917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs. (arXiv:2310.11917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#20013;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65292;&#22522;&#20110;Wikidata5M&#36827;&#34892;&#25193;&#23637;&#12290;&#36890;&#36807;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#20449;&#24687;&#32452;&#21512;&#65292;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#25972;&#21512;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#26159;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#39044;&#27979;&#26032;&#30340;&#12289;&#20043;&#21069;&#26410;&#35265;&#30340;&#23454;&#20307;&#30340;&#20107;&#23454;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#21644;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#35780;&#20272;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#22522;&#20110;&#24182;&#25193;&#23637;&#20102;Wikidata5M&#65306;&#23427;&#25552;&#20379;&#20102;&#36716;&#23548;&#24335;&#12289;k-shot&#21644;0-shot&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#20250;&#26681;&#25454;&#21487;&#29992;&#30340;&#20449;&#24687;&#24773;&#20917;&#20174;&#65288;i&#65289;&#20165;&#26377;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#65292;&#21040;&#65288;ii&#65289;&#21253;&#21547;&#25991;&#26412;&#25552;&#21450;&#65292;&#20877;&#21040;&#65288;iii&#65289;&#23454;&#20307;&#30340;&#35814;&#32454;&#25551;&#36848;&#36827;&#34892;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26368;&#36817;&#26041;&#27861;&#30340;&#23567;&#22411;&#30740;&#31350;&#65292;&#21457;&#29616;&#21322;&#24863;&#24212;&#24335;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#36828;&#36828;&#20302;&#20110;&#36716;&#23548;&#24335;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#23454;&#39564;&#20013;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#38271;&#23614;&#23454;&#20307;&#30340;&#19981;&#36275;&#12290;&#35813;&#22522;&#20934;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#20449;&#24687;&#25972;&#21512;&#21040;&#38142;&#25509;&#39044;&#27979;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#21476;&#20195;&#28779;&#26143;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#36716;&#25442;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.11888</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#36741;&#21161;&#29702;&#35299;&#28779;&#26143;&#30340;&#36807;&#21435;&#36866;&#23621;&#24615;&#24182;&#25552;&#20379;&#26410;&#26469;&#20219;&#21153;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions. (arXiv:2310.11888v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#21476;&#20195;&#28779;&#26143;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#36716;&#25442;&#12289;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#36136;&#35889;&#25968;&#25454;&#20197;&#26816;&#27979;&#28779;&#26143;&#21476;&#20195;&#36866;&#23621;&#24615;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#25968;&#25454;&#26159;&#38024;&#23545;&#28779;&#26143;&#25910;&#38598;&#30340;&#65292;&#20294;&#21516;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22826;&#38451;&#31995;&#20013;&#30340;&#20219;&#20309;&#22320;&#29699;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#20219;&#20309;&#20351;&#29992;&#36136;&#35889;&#30340;&#39046;&#22495;&#12290;&#30740;&#31350;&#38598;&#20013;&#20110;&#20004;&#31181;&#36136;&#35889;&#25216;&#26415;&#65288;&#36827;&#21270;&#27668;&#20307;&#20998;&#26512;-&#36136;&#35889;&#21644;&#27668;&#30456;&#33394;&#35889;-&#36136;&#35889;&#65289;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#35782;&#21035;&#22320;&#36136;&#26679;&#21697;&#20013;&#30340;&#29305;&#23450;&#21270;&#23398;&#21270;&#21512;&#29289;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#36827;&#21270;&#27668;&#20307;&#20998;&#26512;-&#36136;&#35889;&#21644;&#27668;&#30456;&#33394;&#35889;-&#36136;&#35889;&#25968;&#25454;&#22312;&#22806;&#26143;&#29289;&#36136;&#20998;&#26512;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#21253;&#25324;&#36136;&#35889;&#20540;&#30340;&#24179;&#26041;&#26681;&#36716;&#25442;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#20108;&#32500;&#20809;&#35889;&#22270;&#65292;&#24182;&#21033;&#29992;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25216;&#26415;&#20197;&#36991;&#20813;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36807;&#24230;&#25311;&#21512;&#12290;EGA-MS&#21644;GC-MS&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
This paper presents an application of artificial intelligence on mass spectrometry data for detecting habitability potential of ancient Mars. Although data was collected for planet Mars the same approach can be replicated for any terrestrial object of our solar system. Furthermore, proposed methodology can be adapted to any domain that uses mass spectrometry. This research is focused in data analysis of two mass spectrometry techniques, evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to identify specific chemical compounds in geological material samples. The study demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial material analysis. Most important features of proposed methodology includes square root transformation of mass spectrometry values, conversion of raw data to 2D sprectrograms and utilization of specific machine learning models and techniques to avoid overfitting on relative small datasets. Both EGA-MS and GC-MS datasets c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.11884</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#28608;&#27963;&#21040;&#27010;&#24565;: &#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#27010;&#24565;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#33258;&#28982;&#26725;&#26753;&#65306;&#19968;&#26086;&#30830;&#23450;&#20102;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#23601;&#21487;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#25512;&#29702;&#31995;&#32479;&#25972;&#21512;&#65292;&#29992;&#20110;&#25512;&#29702;&#25110;&#20351;&#29992;&#25512;&#29702;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#25110;&#22686;&#24378;&#20197;&#25913;&#21892;&#23398;&#20064;&#31995;&#32479;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#20165;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#23558;&#27010;&#24565;&#30693;&#35782;&#25554;&#20837;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#30001;&#20110;&#25972;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#26159;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#25152;&#20197;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#33719;&#24471;&#30340;&#35265;&#35299;&#21487;&#20197;&#25104;&#20026;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22899;&#20070;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;AI Nushu&#65292;&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35270;&#35282;&#65292;&#32467;&#21512;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#20004;&#20010;AI&#20195;&#29702;&#20154;&#30340;&#21512;&#20316;&#21019;&#36896;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.11870</link><description>&lt;p&gt;
AI Nushu: &#35745;&#31639;&#35821;&#35328;&#23398;&#35270;&#35282;&#19979;&#22992;&#22969;&#22242;&#32467;&#20013;&#30340;&#35821;&#35328;&#24418;&#25104;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics. (arXiv:2310.11870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21463;&#22899;&#20070;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;AI Nushu&#65292;&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35270;&#35282;&#65292;&#32467;&#21512;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#20004;&#20010;AI&#20195;&#29702;&#20154;&#30340;&#21512;&#20316;&#21019;&#36896;&#20102;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;AI Nushu&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#22899;&#20070;&#65288;&#22899;&#24615;&#19987;&#29992;&#25991;&#23383;&#65289;&#21551;&#21457;&#30340;&#26032;&#20852;&#35821;&#35328;&#31995;&#32479;&#65292;&#22899;&#20070;&#26159;&#21476;&#20195;&#20013;&#22269;&#22899;&#24615;&#22312;&#19968;&#20010;&#29238;&#26435;&#31038;&#20250;&#20013;&#34987;&#35748;&#20026;&#26159;&#25991;&#30450;&#32780;&#21019;&#36896;&#24182;&#29420;&#33258;&#20351;&#29992;&#30340;&#29420;&#29305;&#35821;&#35328;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#24335;&#35013;&#32622;&#20013;&#65292;&#20004;&#20010;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#20154;&#36890;&#36807;&#23545;&#20013;&#25991;&#23383;&#20856;&#21644;&#22899;&#20070;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#65292;&#19981;&#26029;&#35266;&#23519;&#29615;&#22659;&#24182;&#36827;&#34892;&#20132;&#27969;&#65292;&#21512;&#20316;&#21019;&#36896;&#19968;&#20010;&#26631;&#20934;&#30340;&#20013;&#25991;&#20889;&#20316;&#31995;&#32479;&#12290;&#23427;&#20174;&#35745;&#31639;&#35821;&#35328;&#23398;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#31181;&#33402;&#26415;&#24615;&#35299;&#37322;&#65292;&#23558;AI&#25216;&#26415;&#19982;&#20013;&#22269;&#25991;&#21270;&#36951;&#20135;&#21644;&#22899;&#26435;&#20027;&#20041;&#35270;&#35282;&#30456;&#32467;&#21512;&#65292;&#21019;&#36896;&#20102;&#19968;&#31181;&#38750;&#35199;&#26041;&#25991;&#23383;&#30340;&#21019;&#20316;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents "AI Nushu," an emerging language system inspired by Nushu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nushu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20215;&#20540;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#20849;&#21516;&#35774;&#35745;&#26694;&#26550;&#65292;&#26088;&#22312;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21512;&#20316;&#35774;&#35745;&#20855;&#26377;&#20215;&#20540;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.11848</link><description>&lt;p&gt;
&#20215;&#20540;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#20849;&#21516;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Value-Sensitive Conversational Agent Co-Design Framework. (arXiv:2310.11848v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20215;&#20540;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#20849;&#21516;&#35774;&#35745;&#26694;&#26550;&#65292;&#26088;&#22312;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21512;&#20316;&#35774;&#35745;&#20855;&#26377;&#20215;&#20540;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#27491;&#22312;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#38543;&#30528;&#36825;&#20123;&#20195;&#29702;&#22312;&#20844;&#20247;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#20197;&#21450;&#25215;&#25285;&#20102;&#19968;&#31995;&#21015;&#20851;&#38190;&#30340;&#29992;&#20363;&#21644;&#31038;&#20250;&#35282;&#33394;&#65292;&#32771;&#34385;&#21040;&#36825;&#20123;&#31995;&#32479;&#20013;&#23884;&#20837;&#30340;&#20215;&#20540;&#35266;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20010;&#32771;&#34385;&#21253;&#25324;&#22238;&#31572;&#35832;&#22914;&#8220;&#35841;&#30340;&#20215;&#20540;&#35266;&#34987;&#23884;&#20837;&#21040;&#36825;&#20123;&#20195;&#29702;&#20013;&#65311;&#8221;&#21644;&#8220;&#36825;&#20123;&#20215;&#20540;&#35266;&#22312;&#27491;&#22312;&#35774;&#35745;&#30340;&#20195;&#29702;&#20013;&#22914;&#20309;&#20307;&#29616;&#65311;&#8221;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20171;&#32461;&#19968;&#31181;&#20215;&#20540;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#20849;&#21516;&#35774;&#35745;&#26694;&#26550;&#65292;&#20197;&#20415;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#20849;&#21516;&#35774;&#35745;&#20215;&#20540;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#39318;&#20808;&#65292;&#22312;&#27492;&#24635;&#32467;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#35774;&#35745;&#20215;&#20540;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#30340;&#20849;&#21516;&#35774;&#35745;&#35201;&#27714;&#12290;&#20854;&#27425;&#65292;&#20171;&#32461;&#24182;&#35752;&#35770;&#20102;&#23454;&#38469;&#26694;&#26550;&#65292;&#21253;&#25324;&#20854;&#22312;&#35774;&#35745;&#24037;&#20855;&#21253;&#20013;&#30340;&#25805;&#20316;&#21270;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#20849;&#21516;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational agents (CAs) are gaining traction in both industry and academia, especially with the advent of generative AI and large language models. As these agents are used more broadly by members of the general public and take on a number of critical use cases and social roles, it becomes important to consider the values embedded in these systems. This consideration includes answering questions such as 'whose values get embedded in these agents?' and 'how do those values manifest in the agents being designed?' Accordingly, the aim of this paper is to present the Value-Sensitive Conversational Agent (VSCA) Framework for enabling the collaborative design (co-design) of value-sensitive CAs with relevant stakeholders. Firstly, requirements for co-designing value-sensitive CAs which were identified in previous works are summarised here. Secondly, the practical framework is presented and discussed, including its operationalisation into a design toolkit. The framework facilitates the co-d
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#26694;&#26550;&#37319;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.11846</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Masked Pretraining for Multi-Agent Decision Making. (arXiv:2310.11846v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11846
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#36825;&#20010;&#26694;&#26550;&#37319;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#20915;&#31574;&#39046;&#22495;&#65292;&#26500;&#24314;&#20855;&#26377;&#38646;&#26679;&#26412;&#33021;&#21147;&#30340;&#36890;&#29992;&#21333;&#26234;&#33021;&#20307;&#26085;&#30410;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#23384;&#22312;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#38646;&#26679;&#26412;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21407;&#22240;&#26159;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#23384;&#22312;&#20004;&#20010;&#29305;&#23450;&#25361;&#25112;&#65306;&#38598;&#20013;&#24335;&#39044;&#35757;&#32451;&#21644;&#20998;&#25955;&#24335;&#25191;&#34892;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#25968;&#37327;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20351;&#24471;&#38590;&#20197;&#21019;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#34920;&#31034;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26694;&#26550;(MaskMA)&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#37319;&#29992;&#36866;&#21512;&#20110;&#24102;&#26377;&#37096;&#20998;&#35266;&#27979;&#30340;&#20998;&#25955;&#24335;&#25191;&#34892;&#30340;&#22522;&#20110;&#25513;&#30721;&#30340;&#21327;&#20316;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;MaskMA&#36890;&#36807;&#23558;&#21160;&#20316;&#31354;&#38388;&#21010;&#20998;&#20026;&#38754;&#21521;&#33258;&#36523;&#20449;&#24687;&#30340;&#21160;&#20316;&#21644;&#19982;&#20182;&#20154;&#30456;&#20851;&#30340;&#21160;&#20316;&#65292;&#34701;&#20837;&#20102;&#21487;&#27867;&#21270;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a single generalist agent with zero-shot capability has recently sparked significant advancements in decision-making. However, extending this capability to multi-agent scenarios presents challenges. Most current works struggle with zero-shot capabilities, due to two challenges particular to the multi-agent settings: a mismatch between centralized pretraining and decentralized execution, and varying agent numbers and action spaces, making it difficult to create generalizable representations across diverse downstream tasks. To overcome these challenges, we propose a \textbf{Mask}ed pretraining framework for \textbf{M}ulti-\textbf{a}gent decision making (MaskMA). This model, based on transformer architecture, employs a mask-based collaborative learning strategy suited for decentralized execution with partial observation. Moreover, MaskMA integrates a generalizable action representation by dividing the action space into actions toward self-information and actions related to other 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#27599;&#20010;&#20844;&#27665;&#20027;&#26435;&#21644;&#29420;&#31435;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#31181;&#29420;&#35009;&#21046;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35777;&#26126;&#25216;&#26415;&#26469;&#35299;&#20915;&#20004;&#20010;&#31867;&#21035;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#35782;&#21035;&#20986;&#20004;&#20010;&#31867;&#21035;&#21644;&#20004;&#20010;&#29289;&#20307;&#24773;&#20917;&#19979;&#30340;&#25152;&#26377;&#29420;&#31435;&#21644;&#19968;&#33268;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.11841</link><description>&lt;p&gt;
&#26080;&#38656;&#19968;&#33268;&#24615;&#30340;&#20998;&#31867;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Classification Aggregation without Unanimity. (arXiv:2310.11841v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11841
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#27599;&#20010;&#20844;&#27665;&#20027;&#26435;&#21644;&#29420;&#31435;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#31181;&#29420;&#35009;&#21046;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35777;&#26126;&#25216;&#26415;&#26469;&#35299;&#20915;&#20004;&#20010;&#31867;&#21035;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#35782;&#21035;&#20986;&#20004;&#20010;&#31867;&#21035;&#21644;&#20004;&#20010;&#29289;&#20307;&#24773;&#20917;&#19979;&#30340;&#25152;&#26377;&#29420;&#31435;&#21644;&#19968;&#33268;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#26159;&#20174;&#19968;&#32452;&#29289;&#20307;&#21040;&#19968;&#32452;&#31867;&#21035;&#30340;&#28385;&#23556;&#26144;&#23556;&#12290;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#23558;&#27599;&#20010;&#20998;&#31867;&#21521;&#37327;&#32858;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#27599;&#20010;&#20844;&#27665;&#20027;&#26435;&#21644;&#29420;&#31435;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#26412;&#36136;&#19978;&#37117;&#26159;&#19968;&#31181;&#29420;&#35009;&#21046;&#24230;&#12290;&#36825;&#31181;&#19981;&#21487;&#33021;&#24615;&#25512;&#23548;&#20986;&#20102;Maniquet&#21644;Mongin&#65288;2016&#24180;&#65289;&#30340;&#19968;&#20010;&#36739;&#26089;&#32467;&#26524;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#19968;&#33268;&#19988;&#29420;&#31435;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;&#37117;&#26159;&#19968;&#31181;&#29420;&#35009;&#21046;&#24230;&#12290;&#36825;&#20004;&#31181;&#19981;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#20284;&#20110;&#20559;&#22909;&#32858;&#21512;&#20013;Wilson&#21644;Arrow&#30340;&#19981;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;Maniquet-Mongin&#30340;&#19981;&#21487;&#33021;&#24615;&#26159;&#24314;&#31435;&#22312;&#33267;&#23569;&#19977;&#20010;&#31867;&#21035;&#23384;&#22312;&#30340;&#22522;&#30784;&#19978;&#30340;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#35777;&#26126;&#25216;&#26415;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#31867;&#21035;&#30340;&#24773;&#20917;&#65292;&#38500;&#38750;&#29289;&#20307;&#30340;&#25968;&#37327;&#20063;&#26159;&#20004;&#20010;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20004;&#20010;&#31867;&#21035;&#21644;&#20004;&#20010;&#29289;&#20307;&#24773;&#20917;&#19979;&#30340;&#25152;&#26377;&#29420;&#31435;&#21644;&#19968;&#33268;&#30340;&#20998;&#31867;&#32858;&#21512;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A classification is a surjective mapping from a set of objects to a set of categories. A classification aggregation function aggregates every vector of classifications into a single one. We show that every citizen sovereign and independent classification aggregation function is essentially a dictatorship. This impossibility implies an earlier result of Maniquet and Mongin (2016), who show that every unanimous and independent classification aggregation function is a dictatorship. The relationship between the two impossibilities is reminiscent to the relationship between Wilson's and Arrow's impossibilities in preference aggregation. Moreover, while the Maniquet-Mongin impossibility rests on the existence of at least three categories, we propose an alternative proof technique that covers the case of two categories, except when the number of objects is also two. We also identify all independent and unanimous classification aggregation functions for the case of two categories and two objec
&lt;/p&gt;</description></item><item><title>IntentDial&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#65292;&#22312;&#23454;&#29616;&#24847;&#22270;&#26816;&#27979;&#21644;&#35782;&#21035;&#26102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#25512;&#29702;&#36335;&#24452;&#21487;&#35270;&#21270;&#32452;&#20214;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11818</link><description>&lt;p&gt;
IntentDial:&#19968;&#31181;&#22522;&#20110;&#24847;&#22270;&#22270;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#25512;&#29702;&#36335;&#24452;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization. (arXiv:2310.11818v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11818
&lt;/p&gt;
&lt;p&gt;
IntentDial&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#65292;&#22312;&#23454;&#29616;&#24847;&#22270;&#26816;&#27979;&#21644;&#35782;&#21035;&#26102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#25512;&#29702;&#36335;&#24452;&#21487;&#35270;&#21270;&#32452;&#20214;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#21644;&#35782;&#21035;&#24050;&#25104;&#20026;&#23545;&#35805;&#20195;&#29702;&#65288;&#20363;&#22914;&#22768;&#38899;&#21161;&#25163;&#21644;&#26234;&#33021;&#23458;&#26381;&#65289;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#25216;&#26415;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#23558;&#24847;&#22270;&#25366;&#25496;&#36807;&#31243;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38382;&#39064;&#24120;&#24120;&#38459;&#30861;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#22810;&#36718;&#23545;&#35805;&#31995;&#32479;&#65292;&#31216;&#20026;IntentDial&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#21160;&#24577;&#26500;&#24314;&#21644;&#21487;&#25193;&#23637;&#30340;&#24847;&#22270;&#22270;&#20013;&#35782;&#21035;&#29992;&#25143;&#30340;&#24847;&#22270;&#65292;&#21363;&#24847;&#22270;&#20803;&#32032;&#21644;&#26631;&#20934;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#32452;&#20214;&#65292;&#29992;&#20110;&#30417;&#35270;&#23545;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#30452;&#25509;&#25512;&#29702;&#36335;&#24452;&#65292;&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#31995;&#32479;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent detection and identification from multi-turn dialogue has become a widely explored technique in conversational agents, for example, voice assistants and intelligent customer services. The conventional approaches typically cast the intent mining process as a classification task. Although neural classifiers have proven adept at such classification tasks, the issue of neural network models often impedes their practical deployment in real-world settings. We present a novel graph-based multi-turn dialogue system called , which identifies a user's intent by identifying intent elements and a standard query from a dynamically constructed and extensible intent graph using reinforcement learning. In addition, we provide visualization components to monitor the immediate reasoning path for each turn of a dialogue, which greatly facilitates further improvement of the system.
&lt;/p&gt;</description></item><item><title>&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11815</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conservative Predictions on Noisy Financial Data. (arXiv:2310.11815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11815
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#30340;&#37329;&#34701;&#25968;&#25454;&#19978;&#36827;&#34892;&#20445;&#23432;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#21098;&#26525;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#24066;&#22330;&#30340;&#20215;&#26684;&#27874;&#21160;&#34987;&#35748;&#20026;&#26159;&#38750;&#24120;&#22024;&#26434;&#30340;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20598;&#23572;&#33021;&#22815;&#25429;&#25417;&#21040;&#21487;&#21033;&#29992;&#30340;&#27169;&#24335;&#65292;&#30001;&#20110;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#22122;&#22768;&#65292;&#36825;&#20123;&#27169;&#24335;&#24448;&#24448;&#34987;&#25513;&#30422;&#65292;&#20351;&#24471;&#39044;&#27979;&#21464;&#24471;&#19981;&#22826;&#26377;&#29992;&#19988;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#26041;&#27861;&#65292;&#21363;&#27169;&#22411;&#22312;&#23545;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#28857;&#19978;&#19981;&#36827;&#34892;&#39044;&#27979;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19968;&#31995;&#21015;&#36825;&#26679;&#30340;&#27169;&#22411;&#25353;&#24207;&#36827;&#34892;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#35268;&#21017;&#21015;&#34920;&#65292;&#27599;&#20010;&#27169;&#22411;&#20165;&#22312;&#21069;&#38754;&#30340;&#27169;&#22411;&#23545;&#20854;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#27979;&#35797;&#26102;&#20063;&#20250;&#36827;&#34892;&#31867;&#20284;&#30340;&#25968;&#25454;&#21098;&#26525;&#65292;&#21482;&#23545;&#27979;&#35797;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#65288;&#25903;&#25345;&#38598;&#65289;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decis
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.11807</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learning and Discovering Quantum Properties with Multi-Task Neural Networks. (arXiv:2310.11807v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21457;&#29616;&#37327;&#23376;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#21644;&#21457;&#29616;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#26029;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#21644;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#20174;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#37327;&#23376;&#24577;&#24615;&#36136;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#37327;&#23376;&#24615;&#36136;&#65292;&#21253;&#25324;&#37327;&#23376;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#20197;&#21450;&#37327;&#23376;&#29366;&#24577;&#30340;&#19968;&#33324;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#32416;&#32544;&#29109;&#21644;&#22810;&#20307;&#25299;&#25169;&#19981;&#21464;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#22312;&#32473;&#23450;&#24615;&#36136;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#21457;&#29616;&#36229;&#20986;&#35813;&#38598;&#21512;&#30340;&#26032;&#24615;&#36136;&#12290;&#22810;&#21151;&#33021;&#35757;&#32451;&#36824;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#23616;&#37096;&#27979;&#37327;&#20013;&#25512;&#26029;&#20986;&#22810;&#20307;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#23616;&#24615;&#36136;&#65292;&#20998;&#31867;&#20445;&#25252;&#23545;&#31216;&#30340;&#25299;&#25169;&#29289;&#36136;&#30456;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30456;&#20043;&#38388;&#30340;&#26410;&#30693;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11798</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Auction-Based Scheduling. (arXiv:2310.11798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#30446;&#26631;&#20915;&#31574;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#23558;&#27599;&#20010;&#30446;&#26631;&#30340;&#23454;&#29616;&#20998;&#37197;&#32473;&#21333;&#29420;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#36825;&#20123;&#31574;&#30053;&#12290;&#20351;&#29992;&#25293;&#21334;&#26426;&#21046;&#26469;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#30830;&#20445;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#38656;&#35201;&#28385;&#36275;&#22810;&#20010;&#37096;&#20998;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#26041;&#27861;&#26159;&#25972;&#20307;&#21270;&#30340;&#65292;&#21363;&#36890;&#36807;&#19968;&#20010;&#20989;&#25968;&#26469;&#36873;&#25321;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#28385;&#36275;&#25152;&#26377;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25293;&#21334;&#30340;&#35843;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#12290;&#27599;&#20010;&#30446;&#26631;&#37117;&#20351;&#29992;&#21333;&#29420;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29420;&#31435;&#21019;&#24314;&#12289;&#20462;&#25913;&#21644;&#26367;&#25442;&#12290;&#21487;&#20197;&#29702;&#35299;&#30340;&#26159;&#65292;&#20855;&#26377;&#20914;&#31361;&#30446;&#26631;&#30340;&#19981;&#21516;&#31574;&#30053;&#21487;&#33021;&#22312;&#32473;&#23450;&#26102;&#38388;&#36873;&#25321;&#20914;&#31361;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#20914;&#31361;&#21644;&#32452;&#21512;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25293;&#21334;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#32473;&#27599;&#20010;&#31574;&#30053;&#20998;&#37197;&#19968;&#20010;&#26377;&#38480;&#30340;&#39044;&#31639;&#65292;&#22312;&#27599;&#19968;&#27493;&#65292;&#31574;&#30053;&#21516;&#26102;&#20174;&#21487;&#29992;&#30340;&#39044;&#31639;&#20013;&#20986;&#20215;&#26469;&#33719;&#21462;&#35843;&#24230;&#21644;&#36873;&#25321;&#21160;&#20316;&#30340;&#29305;&#26435;&#12290;&#31574;&#30053;&#20351;&#29992;&#20854;&#20986;&#20215;&#26469;&#34920;&#36798;&#35843;&#24230;&#30340;&#32039;&#36843;&#24615;&#65292;&#26377;&#38480;&#30340;&#39044;&#31639;&#30830;&#20445;&#20102;&#38271;&#26399;&#30340;&#35843;&#24230;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay 
&lt;/p&gt;</description></item><item><title>&#30005;&#20449;&#34892;&#19994;&#35797;&#22270;&#23558;&#29983;&#25104;&#24335;AI&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#25972;&#21512;&#36827;&#20135;&#21697;&#24320;&#21457;&#20013;&#65292;&#24418;&#25104;AI&#21407;&#29983;&#30340;&#30005;&#20449;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20262;&#29702;&#12289;&#30417;&#31649;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#25361;&#25112;&#38656;&#35201;&#24910;&#37325;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2310.11770</link><description>&lt;p&gt;
&#30005;&#20449;AI&#21407;&#29983;&#31995;&#32479;&#22312;&#29983;&#25104;&#24335;AI&#26102;&#20195;&#30340;&#24037;&#31243;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Telecom AI Native Systems in the Age of Generative AI -- An Engineering Perspective. (arXiv:2310.11770v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11770
&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#34892;&#19994;&#35797;&#22270;&#23558;&#29983;&#25104;&#24335;AI&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#25972;&#21512;&#36827;&#20135;&#21697;&#24320;&#21457;&#20013;&#65292;&#24418;&#25104;AI&#21407;&#29983;&#30340;&#30005;&#20449;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20262;&#29702;&#12289;&#30417;&#31649;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#25361;&#25112;&#38656;&#35201;&#24910;&#37325;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#24335;AI&#21644;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#27491;&#22312;&#19981;&#21516;&#34892;&#19994;&#24341;&#36215;&#36716;&#22411;&#24615;&#21464;&#38761;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;FM&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#36719;&#20214;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;FMs&#25972;&#21512;&#21040;&#30005;&#20449;&#34892;&#19994;&#20013;&#30340;&#21487;&#33021;&#24615;&#65292;&#25581;&#31034;&#20102;AI&#21407;&#29983;&#30005;&#20449;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;AI&#26080;&#32541;&#34701;&#20837;&#30005;&#20449;&#20135;&#21697;&#30340;&#26500;&#26550;&#20013;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#23558;FMs&#23454;&#26045;&#21040;&#36719;&#20214;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24037;&#31243;&#32771;&#34385;&#21644;&#29420;&#29305;&#25361;&#25112;&#65292;&#24378;&#35843;&#20102;AI&#21407;&#29983;&#20248;&#20808;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;FMs&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#20851;&#20046;&#20219;&#21153;&#20851;&#38190;&#30340;&#30005;&#20449;&#29615;&#22659;&#20013;&#65292;&#20262;&#29702;&#12289;&#30417;&#31649;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#25361;&#25112;&#38656;&#35201;&#24910;&#37325;&#32771;&#34385;&#12290;&#38543;&#30528;&#30005;&#20449;&#34892;&#19994;&#35797;&#22270;&#21033;&#29992;AI&#30340;&#21147;&#37327;&#65292;&#19968;&#20010;&#32452;&#24314;AI&#21407;&#29983;&#22242;&#38431;&#30340;&#32508;&#21512;&#21644;&#23454;&#38469;&#26041;&#27861;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in Artificial Intelligence (AI), particularly in generative AI and foundational models (FMs), have ushered in transformative changes across various industries. Large language models (LLMs), a type of FM, have demonstrated their prowess in natural language processing tasks and content generation, revolutionizing how we interact with software products and services. This article explores the integration of FMs in the telecommunications industry, shedding light on the concept of AI native telco, where AI is seamlessly woven into the fabric of telecom products. It delves into the engineering considerations and unique challenges associated with implementing FMs into the software life cycle, emphasizing the need for AI native-first approaches. Despite the enormous potential of FMs, ethical, regulatory, and operational challenges require careful consideration, especially in mission-critical telecom contexts. As the telecom industry seeks to harness the power of AI, a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#26448;&#26009;&#24615;&#36136;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#19979;&#65292;&#36890;&#36807;&#24314;&#27169;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#21644;&#37096;&#20998;&#35780;&#20272;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.11749</link><description>&lt;p&gt;
&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#30340;&#26448;&#26009;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Estimating Material Properties of Interacting Objects Using Sum-GP-UCB. (arXiv:2310.11749v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-GP-UCB&#26041;&#27861;&#20272;&#35745;&#20114;&#21160;&#29289;&#20307;&#26448;&#26009;&#24615;&#36136;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#19979;&#65292;&#36890;&#36807;&#24314;&#27169;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#21644;&#37096;&#20998;&#35780;&#20272;&#26469;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#36890;&#36807;&#35266;&#23519;&#26469;&#20934;&#30830;&#27169;&#25311;&#29289;&#20307;&#30340;&#26448;&#26009;&#21644;&#21160;&#24577;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#19968;&#32452;&#35266;&#23519;&#26469;&#35782;&#21035;&#29289;&#20307;&#30340;&#26448;&#26009;&#23646;&#24615;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;&#19981;&#21516;&#32452;&#20114;&#21160;&#29289;&#20307;&#22330;&#26223;&#30340;&#35266;&#23519;&#26469;&#20272;&#35745;&#36825;&#20123;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22870;&#21169;&#20989;&#25968;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21035;&#24314;&#27169;&#27599;&#20010;&#35266;&#23519;&#30340;&#22870;&#21169;&#65292;&#24182;&#20165;&#20351;&#29992;&#35813;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#21442;&#25968;&#20316;&#20026;&#36755;&#20837;&#12290;&#24471;&#21040;&#30340;&#20302;&#32500;&#27169;&#22411;&#22312;&#21442;&#25968;&#31354;&#38388;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#24555;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#20943;&#23569;&#23547;&#25214;&#20248;&#31168;&#21442;&#25968;&#20540;&#25152;&#38656;&#30340;&#20223;&#30495;&#27425;&#25968;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22870;&#21169;&#20989;&#25968;&#30340;&#37096;&#20998;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#20013;&#36873;&#25321;&#30340;&#21442;&#25968;&#20165;&#22312;&#19968;&#37096;&#20998;&#30495;&#23454;&#19990;&#30028;&#35780;&#20272;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21160;&#20316;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;VQ-VAE&#26469;&#23398;&#20064;&#29366;&#24577;&#26465;&#20214;&#19979;&#30340;&#21160;&#20316;&#37327;&#21270;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#31163;&#25955;&#21160;&#20316;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11731</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#21160;&#20316;&#37327;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning. (arXiv:2310.11731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#21160;&#20316;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;VQ-VAE&#26469;&#23398;&#20064;&#29366;&#24577;&#26465;&#20214;&#19979;&#30340;&#21160;&#20316;&#37327;&#21270;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#31163;&#25955;&#21160;&#20316;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#38745;&#24577;&#34892;&#20026;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#27604;&#25910;&#38598;&#25968;&#25454;&#30340;&#31574;&#30053;&#34920;&#29616;&#26356;&#22909;&#30340;&#31574;&#30053;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#23613;&#31649;&#31574;&#30053;&#32422;&#26463;&#12289;&#20445;&#23432;&#24615;&#21644;&#20854;&#20182;&#32531;&#35299;&#20998;&#24067;&#20559;&#31227;&#30340;&#26041;&#27861;&#20351;&#24471;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#22312;&#36830;&#32493;&#21160;&#20316;&#35774;&#32622;&#20013;&#65292;&#24448;&#24448;&#38656;&#35201;&#21508;&#31181;&#36817;&#20284;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#12290;&#35768;&#22810;&#36825;&#20123;&#25361;&#25112;&#22312;&#31163;&#25955;&#21160;&#20316;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#32531;&#35299;&#65292;&#31163;&#32447;RL&#32422;&#26463;&#21644;&#35268;&#21017;&#21270;&#22120;&#24448;&#24448;&#21487;&#20197;&#26356;&#31934;&#30830;&#25110;&#29978;&#33267;&#23436;&#20840;&#35745;&#31639;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21160;&#20316;&#37327;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#20351;&#29992;VQ-VAE&#26469;&#23398;&#20064;&#29366;&#24577;&#26465;&#20214;&#19979;&#30340;&#21160;&#20316;&#37327;&#21270;&#65292;&#36991;&#20813;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#26420;&#32032;&#31163;&#25955;&#21270;&#25152;&#24102;&#26469;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#65292;&#22914;IQL&#12289;CQL&#21644;BRAC&#65292;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#21518;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.11730</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#30340;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19978;&#21327;&#21516;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#36890;&#36807;&#20803;&#36335;&#24452;&#25551;&#36848;&#20016;&#23500;&#30340;&#35821;&#20041;&#65292;&#24050;&#25104;&#20026;&#32531;&#35299;&#25512;&#33616;&#31995;&#32479;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;HIN&#30340;&#25512;&#33616;&#31995;&#32479;&#25345;&#26377;&#25968;&#25454;&#30340;&#38598;&#20013;&#23384;&#20648;&#20551;&#35774;&#65292;&#24182;&#36827;&#34892;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#23384;&#20648;&#65292;&#23548;&#33268;&#38598;&#20013;&#24335;HIN&#25512;&#33616;&#26080;&#27861;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;HIN&#20998;&#20026;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#31169;&#26377;HIN&#21644;&#26381;&#21153;&#22120;&#31471;&#30340;&#20849;&#20139;HIN&#12290;&#22312;&#27492;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FedHGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;HIN&#19978;&#21327;&#20316;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#27844;&#38706;&#29992;&#25143;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#22522;&#20110;HIN&#30340;&#32852;&#21512;&#25512;&#33616;&#65292;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#20809;&#19979;&#30830;&#23450;&#20102;&#38544;&#31169;&#23450;&#20041;&#65292;&#26088;&#22312;&#20445;&#25252;&#31169;&#26377;HIN&#30340;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#65292;&#20197;&#21450;&#29992;&#25143;&#30340;&#38544;&#31169;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#25581;&#31034;&#20102;&#33258;&#21160;&#26412;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#25104;&#39046;&#22495;&#30340;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11723</link><description>&lt;p&gt;
&#33258;&#21160;&#26412;&#20307;&#21305;&#37197;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#20174;&#23454;&#35777;&#23454;&#39564;&#20013;&#33719;&#24471;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation. (arXiv:2310.11723v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#25581;&#31034;&#20102;&#33258;&#21160;&#26412;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#25104;&#39046;&#22495;&#30340;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#25104;&#34987;&#35748;&#20026;&#26159;&#20449;&#24687;&#31185;&#23398;&#30028;&#20013;&#30340;&#19968;&#20010;&#32463;&#20856;&#30740;&#31350;&#39046;&#22495;&#21644;&#19968;&#20010;&#32039;&#36843;&#30340;&#38656;&#27714;&#12290;&#26412;&#20307;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#25198;&#28436;&#20102;&#20851;&#38190;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#25552;&#20379;&#33391;&#22909;&#24041;&#22266;&#30340;&#25903;&#25345;&#26469;&#36890;&#36807;&#20114;&#25805;&#20316;&#24615;&#38142;&#25509;&#21644;&#35821;&#20041;&#38598;&#25104;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#20174;&#24212;&#29992;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22522;&#20110;&#26412;&#20307;&#21305;&#37197;&#30340;&#25216;&#26415;&#26469;&#36827;&#34892;&#25968;&#25454;&#38598;&#25104;&#12290;&#22522;&#20110;&#26412;&#20307;&#30340;&#36807;&#31243;&#21482;&#26377;&#36890;&#36807;&#20551;&#35774;&#25163;&#21160;&#21305;&#37197;&#19981;&#21516;&#20449;&#24687;&#28304;&#25165;&#33021;&#34987;&#35748;&#20026;&#26159;&#36866;&#24403;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#31995;&#32479;&#25193;&#22823;&#35268;&#27169;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#21305;&#37197;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#21464;&#24471;&#36843;&#20999;&#38656;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#31185;&#23398;&#30028;&#29616;&#26377;&#30340;&#33258;&#21160;&#26412;&#20307;&#21305;&#37197;&#24037;&#20855;&#23545;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#21363;&#20351;&#32771;&#34385;&#21040;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#26696;&#20363;&#30740;&#31350;&#65288;&#21363;&#20840;&#29699;&#25351;&#26631;&#30340;&#26102;&#31354;&#23545;&#40784;&#65289;&#65292;&#32467;&#26524;&#28165;&#26970;&#22320;&#26174;&#31034;&#20986;&#30001;&#20110;&#38169;&#35823;&#21644;&#19981;&#20934;&#30830;&#24615;&#20135;&#29983;&#30340;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data integration is considered a classic research field and a pressing need within the information science community. Ontologies play a critical role in such a process by providing well-consolidated support to link and semantically integrate datasets via interoperability. This paper approaches data integration from an application perspective, looking at techniques based on ontology matching. An ontology-based process may only be considered adequate by assuming manual matching of different sources of information. However, since the approach becomes unrealistic once the system scales up, automation of the matching process becomes a compelling need. Therefore, we have conducted experiments on actual data with the support of existing tools for automatic ontology matching from the scientific community. Even considering a relatively simple case study (i.e., the spatio-temporal alignment of global indicators), outcomes clearly show significant uncertainty resulting from errors and inaccuracie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.11722</link><description>&lt;p&gt;
&#37327;&#21270;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#65306;&#19968;&#39033;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis. (arXiv:2310.11722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#65292;&#37327;&#21270;&#20102;&#20013;&#25991;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21407;&#23376;&#30693;&#35782;&#30340;&#23384;&#20648;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#36890;&#29992;LLMs&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#31867;&#22411;&#30340;LLMs&#37117;&#20542;&#21521;&#20110;&#36814;&#21512;&#29992;&#25143;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#21644;&#39640;&#25928;&#22320;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#35786;&#26029;&#24314;&#35758;&#65292;&#20174;&#32780;&#38761;&#26032;&#29992;&#25143;&#33258;&#35786;&#26029;&#30340;&#26041;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;GPT-4&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#25110;&#20854;&#36890;&#36807;&#21307;&#23398;&#32771;&#35797;&#30340;&#33021;&#21147;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#37327;&#21270;&#23384;&#20648;&#22312;LLMs&#35760;&#24518;&#20013;&#30340;&#20581;&#24247;&#30456;&#20851;&#21407;&#23376;&#30693;&#35782;&#30340;&#31243;&#24230;&#65292;&#32780;&#36825;&#26159;LLMs&#25552;&#20379;&#26356;&#20934;&#30830;&#24314;&#35758;&#30340;&#22522;&#30784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#29992;&#25143;&#33258;&#35786;&#26029;&#26597;&#35810;&#20013;&#26368;&#24120;&#35265;&#30340;&#21407;&#23376;&#30693;&#35782;&#31867;&#22411;&#65292;&#20849;17&#31181;&#21407;&#23376;&#31867;&#22411;&#21644;14048&#26465;&#21407;&#23376;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#22312;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21407;&#23376;&#30693;&#35782;&#21644;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#26041;&#38754;&#65292;&#36890;&#29992;LLMs&#30340;&#34920;&#29616;&#20248;&#20110;&#19987;&#19994;LLMs&#12290;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;&#36890;&#29992;&#21644;&#19987;&#19994;LLMs&#37117;&#26159;&#39532;&#23617;&#31934;&#65292;&#21363;&#22312;&#28041;&#21450;&#29992;&#25143;&#35201;&#27714;&#26102;&#24635;&#26159;&#36814;&#21512;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11715</link><description>&lt;p&gt;
&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets. (arXiv:2310.11715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11715
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#26144;&#23556;&#30697;&#38453;&#26469;&#26174;&#24335;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20302;&#36164;&#28304;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#32454;&#31890;&#24230;NER&#22330;&#26223;&#19979;&#24120;&#24120;&#38754;&#20020;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24212;&#29992;K-shot&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#24403;&#27880;&#37322;&#25968;&#37327;&#36229;&#36807;&#20960;&#21313;&#20010;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#24448;&#24448;&#36798;&#21040;&#39281;&#21644;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#22823;&#37327;&#30340;&#26631;&#27880;&#12290;&#19968;&#31181;&#30452;&#25509;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#39044;&#35757;&#32451;&#65292;&#23427;&#21033;&#29992;&#31895;&#31890;&#24230;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#26080;&#27861;&#30452;&#25509;&#21033;&#29992;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23613;&#31649;&#32454;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#24456;&#21487;&#33021;&#26159;&#31895;&#31890;&#24230;&#23454;&#20307;&#31867;&#22411;&#30340;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;-&#31895;&#31890;&#24230;&#65288;F2C&#65289;&#26144;&#23556;&#30697;&#38453;&#30340;&#32454;&#31890;&#24230;NER&#27169;&#22411;&#65292;&#20197;&#26174;&#24335;&#22320;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#19968;&#33268;&#24615;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#19982;&#32454;&#31890;&#24230;&#19981;&#19968;&#33268;&#30340;&#31895;&#31890;&#24230;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-gr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.11710</link><description>&lt;p&gt;
&#23398;&#20064;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#29992;&#20110;&#22810;&#27169;&#24577;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Co-Speech Gesture for Multimodal Aphasia Type Detection. (arXiv:2310.11710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11710
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35821;&#38899;&#21644;&#25163;&#21183;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#22833;&#35821;&#31867;&#22411;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#35821;&#26159;&#19968;&#31181;&#30001;&#33041;&#25439;&#20260;&#24341;&#36215;&#30340;&#35821;&#35328;&#38556;&#30861;&#65292;&#38656;&#35201;&#20934;&#30830;&#35782;&#21035;&#29305;&#23450;&#30340;&#22833;&#35821;&#31867;&#22411;&#65292;&#22914;Broca&#22833;&#35821;&#21644;Wernicke&#22833;&#35821;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#31867;&#22411;&#22833;&#35821;&#30340;&#26041;&#27861;&#65292;&#20154;&#20204;&#24182;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#20998;&#26512;&#21516;&#26102;&#35821;&#35328;&#25163;&#21183;&#23545;&#20110;&#21306;&#20998;&#22833;&#35821;&#31867;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35821;&#38899;&#21644;&#30456;&#24212;&#30340;&#25163;&#21183;&#27169;&#24335;&#36827;&#34892;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#31181;&#22833;&#35821;&#31867;&#22411;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#23545;&#25163;&#21183;&#20449;&#24687;&#25935;&#24863;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#22833;&#35821;&#31867;&#22411;&#26816;&#27979;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65288;F1 84.2%&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25163;&#21183;&#29305;&#24449;&#20248;&#20110;&#22768;&#23398;&#29305;&#24449;&#65292;&#31361;&#26174;&#20102;&#25163;&#21183;&#34920;&#36798;&#22312;&#26816;&#27979;&#22833;&#35821;&#31867;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;NFT&#30340;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#65292;&#20026;&#20102;&#24357;&#34917;&#23545;&#26032;&#20852;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#30340;&#32570;&#21475;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;</title><link>http://arxiv.org/abs/2310.11709</link><description>&lt;p&gt;
Live Graph Lab:&#26397;&#21521;&#20855;&#26377;NFT&#30340;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#20132;&#26131;&#22270;
&lt;/p&gt;
&lt;p&gt;
Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT. (arXiv:2310.11709v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;NFT&#30340;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#65292;&#20026;&#20102;&#24357;&#34917;&#23545;&#26032;&#20852;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#30340;&#32570;&#21475;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35843;&#26597;&#22823;&#35268;&#27169;&#26102;&#38388;&#22270;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#36825;&#20123;&#22270;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21644;&#25216;&#26415;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#25972;&#20010;&#23454;&#26102;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#8220;&#23454;&#26102;&#22270;&#23454;&#39564;&#23460;&#8221;&#27010;&#24565;&#65292;&#35813;&#23454;&#39564;&#23460;&#21487;&#20197;&#20174;&#21306;&#22359;&#38142;&#20013;&#25552;&#21462;&#24320;&#25918;&#12289;&#21160;&#24577;&#21644;&#23454;&#26102;&#30340;&#20132;&#26131;&#22270;&#12290;&#20854;&#20013;&#65292;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#25104;&#20026;&#21306;&#22359;&#38142;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#20043;&#19968;&#12290;&#36825;&#20010;&#20998;&#25955;&#21270;&#29983;&#24577;&#31995;&#32479;&#20855;&#26377;&#36229;&#36807;400&#20159;&#32654;&#20803;&#30340;&#24066;&#20540;&#65292;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21311;&#21517;&#21644;&#23454;&#26102;&#20132;&#26131;&#27963;&#21160;&#65292;&#33258;&#28982;&#24418;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20132;&#26131;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20174;&#26102;&#38388;&#22270;&#20998;&#26512;&#30340;&#35282;&#24230;&#23545;&#36825;&#20010;&#26032;&#20852;&#30340;NFT&#29983;&#24577;&#31995;&#32479;&#30340;&#29305;&#24615;&#20102;&#35299;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20351;&#29992;NFT&#20132;&#26131;&#32593;&#32476;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23454;&#26102;&#22270;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#24102;&#26469;&#30340;&#26032;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2310.11703</link><description>&lt;p&gt;
&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#20840;&#38754;&#35843;&#26597;&#65306;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#65292;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge. (arXiv:2310.11703v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#21521;&#37327;&#25968;&#25454;&#24211;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20171;&#32461;&#20102;&#23384;&#20648;&#21644;&#26816;&#32034;&#25216;&#26415;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#23545;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#24102;&#26469;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#29992;&#20110;&#23384;&#20648;&#26080;&#27861;&#29992;&#20256;&#32479;&#30340;DBMS&#26469;&#25551;&#36848;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;&#34429;&#28982;&#30446;&#21069;&#23545;&#29616;&#26377;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#26550;&#26500;&#30340;&#25551;&#36848;&#25110;&#26032;&#30340;&#24341;&#20837;&#30340;&#25991;&#31456;&#24182;&#19981;&#22810;&#65292;&#20294;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#22312;&#21521;&#37327;&#25968;&#25454;&#24211;&#32972;&#21518;&#24050;&#32463;&#34987;&#38271;&#26102;&#38388;&#30740;&#31350;&#65292;&#30456;&#20851;&#30340;&#31639;&#27861;&#25991;&#31456;&#22312;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#30456;&#24403;&#22810;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;&#30456;&#20851;&#31639;&#27861;&#65292;&#20197;&#25552;&#20379;&#23545;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26222;&#36941;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#36825;&#20123;&#30740;&#31350;&#20998;&#20026;&#22522;&#20110;&#21704;&#24076;&#12289;&#22522;&#20110;&#26641;&#12289;&#22522;&#20110;&#22270;&#12289;&#21644;&#22522;&#20110;&#37327;&#21270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#38754;&#20020;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#22914;&#20309;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.11700</link><description>&lt;p&gt;
&#21333;&#35270;&#35282;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
Runner re-identification from single-view video in the open-world setting. (arXiv:2310.11700v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36305;&#32773;&#34987;&#26694;&#36873;&#20986;&#22810;&#27425;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#36305;&#32773;&#30340;&#20877;&#35782;&#21035;&#23545;&#20110;&#33258;&#21160;&#35270;&#39057;&#22788;&#29702;&#21644;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#22810;&#35270;&#35282;&#25110;&#21333;&#35270;&#35282;&#20307;&#32946;&#35270;&#39057;&#20013;&#36305;&#32773;&#20877;&#35782;&#21035;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#35760;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23553;&#38381;&#19990;&#30028;&#35774;&#23450;&#30340;&#20877;&#35782;&#21035;&#19978;&#65292;&#32780;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36827;&#34892;&#33258;&#21160;&#35270;&#39057;&#20998;&#26512;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#24182;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#22788;&#29702;&#21333;&#35270;&#35282;&#35270;&#39057;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#30340;&#36305;&#32773;&#20877;&#35782;&#21035;&#31995;&#32479;&#12290;&#22312;&#24320;&#25918;&#19990;&#30028;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26080;&#27861;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#24517;&#39035;&#30452;&#25509;&#22788;&#29702;&#35270;&#39057;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#21407;&#22987;&#35270;&#39057;&#20316;&#20026;&#36755;&#20837;&#26469;&#35782;&#21035;&#36305;&#32773;&#65292;&#21363;&#20351;&#36305;&#32773;&#20986;&#29616;&#22810;&#27425;&#34987;&#26694;&#36873;&#20986;&#65292;&#31995;&#32479;&#20063;&#33021;&#36827;&#34892;&#35782;&#21035;&#12290;&#23545;&#20110;&#33258;&#21160;&#22788;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;YOLOv8&#21644;&#24494;&#35843;&#30340;EfficientNet&#26469;&#26816;&#27979;&#35270;&#39057;&#20013;&#30340;&#36305;&#32773;&#12290;&#28982;&#21518;&#20351;&#29992;ByteTrack&#26469;&#36319;&#36394;&#36305;&#32773;&#65292;&#24182;&#20351;&#29992;&#24494;&#35843;&#30340;YOLO&#26469;&#26816;&#27979;&#20182;&#20204;&#30340;&#38795;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YO
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26500;&#24314;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21477;&#23376;&#24182;&#36827;&#34892;&#20851;&#31995;&#25628;&#32034;&#21644;&#23548;&#33322;&#12290;&#24182;&#19988;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;COVID-19&#30740;&#31350;&#20013;&#24212;&#29992;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11681</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Descriptive Knowledge Graph in Biomedical Domain. (arXiv:2310.11681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#26500;&#24314;&#25551;&#36848;&#24615;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21477;&#23376;&#24182;&#36827;&#34892;&#20851;&#31995;&#25628;&#32034;&#21644;&#23548;&#33322;&#12290;&#24182;&#19988;&#35813;&#31995;&#32479;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#25551;&#36848;&#24615;&#21477;&#23376;&#30340;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;COVID-19&#30740;&#31350;&#20013;&#24212;&#29992;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#29983;&#25104;&#26377;&#20449;&#24687;&#37327;&#21644;&#25551;&#36848;&#24615;&#30340;&#21477;&#23376;&#65292;&#24182;&#20419;&#36827;&#26377;&#25928;&#30340;&#20851;&#31995;&#30693;&#35782;&#25628;&#32034;&#12290;&#19982;&#20043;&#21069;&#26816;&#32034;&#38750;&#30456;&#20851;&#27573;&#33853;&#30340;&#25628;&#32034;&#24341;&#25806;&#25110;&#25506;&#32034;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23558;&#25551;&#36848;&#24615;&#21477;&#23376;&#32452;&#32455;&#20026;&#19968;&#20010;&#20851;&#31995;&#22270;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#30456;&#20851;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#30001;&#21270;&#23398;&#29289;&#36136;&#27835;&#30103;&#30340;&#30142;&#30149;&#65289;&#25110;&#38388;&#25509;&#30456;&#20851;&#30340;&#23454;&#20307;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#27835;&#30103;&#26576;&#31181;&#30142;&#30149;&#30340;&#28508;&#22312;&#33647;&#29289;&#65289;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#20351;&#29992;ChatGPT&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#20851;&#31995;&#21512;&#25104;&#27169;&#22411;&#20174;&#26816;&#32034;&#21040;&#30340;&#20449;&#24687;&#20013;&#29983;&#25104;&#31616;&#27905;&#21487;&#38752;&#30340;&#25551;&#36848;&#24615;&#21477;&#23376;&#65292;&#20943;&#23569;&#20102;&#20154;&#24037;&#38405;&#35835;&#30340;&#38656;&#27714;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#33719;&#24471;&#39640;&#32423;&#30693;&#35782;&#21644;&#35814;&#32454;&#21442;&#32771;&#65292;&#24182;&#21487;&#20197;&#20132;&#20114;&#22320;&#23548;&#33322;&#21040;&#24863;&#20852;&#36259;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#25105;&#20204;&#31995;&#32479;&#22312;COVID-19&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#35828;&#26126;&#20854;&#22312;&#30456;&#20851;&#39046;&#22495;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#36716;&#21270;&#20026;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11678</link><description>&lt;p&gt;
&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Using Experience Classification for Training Non-Markovian Tasks. (arXiv:2310.11678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#39564;&#20998;&#31867;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#36716;&#21270;&#20026;&#26377;&#38480;&#36712;&#36857;&#19978;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#34920;&#36798;&#65292;&#24182;&#21033;&#29992;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20110;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#26159;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#65292;&#20854;&#22870;&#21169;&#26159;&#22522;&#20110;&#29366;&#24577;&#21382;&#21490;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#29366;&#24577;&#12290;&#35299;&#20915;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#37329;&#34701;&#20132;&#26131;&#21644;&#21307;&#23398;&#35786;&#26029;&#65289;&#20013;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36712;&#36857;&#19978;&#34920;&#36798;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#30340;&#30446;&#26631;&#36923;&#36753;LTL$_f$&#65288;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20174;LTL$_f$&#21040;MDPs&#65288;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65289;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#32534;&#30721;&#65292;&#20197;&#21033;&#29992;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#33258;&#21160;&#26426;&#32467;&#26500;&#65288;&#35821;&#20041;&#31561;&#20215;&#20110;LTL$_f$&#35268;&#33539;&#65289;&#30340;&#20248;&#20808;&#21270;&#32463;&#39564;&#22238;&#25918;&#25216;&#26415;&#26469;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#24102;&#26377;&#38750;&#39532;&#23572;&#21487;&#22827;&#20219;&#21153;&#30340;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11677</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes. (arXiv:2310.11677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ANPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;ANPG&#23454;&#29616;&#20102;&#26679;&#26412;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35774;&#35745;&#26679;&#26412;&#39640;&#25928;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#38388;&#26080;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;ANPG&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;&#21152;&#36895;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#26469;&#33719;&#21462;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;ANPG&#31639;&#27861;&#22312;&#19968;&#33324;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;O(&#949;^{-2})&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;O(&#949;^{-1})&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#949;&#23450;&#20041;&#20102;&#26368;&#20248;&#24615;&#35823;&#24046;&#12290;&#36825;&#23558;&#26679;&#26412;&#22797;&#26434;&#24230;&#25552;&#39640;&#20102;&#19968;&#20010;log(1/&#949;)&#30340;&#22240;&#23376;&#12290;ANPG&#26159;&#19968;&#20010;&#19968;&#38454;&#31639;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#29616;&#26377;&#25991;&#29486;&#20013;&#21487;&#33021;&#26080;&#27861;&#39564;&#35777;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;(IS)&#26435;&#37325;&#26041;&#24046;&#19978;&#30028;&#30340;&#20551;&#35774;&#12290;&#22312;&#26080;Hessian&#21644;&#26080;IS&#31639;&#27861;&#31867;&#20013;&#65292;ANPG&#36229;&#36807;&#20102;&#24050;&#30693;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19968;&#20010;O(&#949;^{-\frac{1}{2}})&#30340;&#22240;&#23376;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#20102;&#23427;&#20204;&#30340;&#26368;&#26032;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2}})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2}})$ and simultaneously matches their state-of-the-art it
&lt;/p&gt;</description></item><item><title>PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11676</link><description>&lt;p&gt;
PREM:&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11676
&lt;/p&gt;
&lt;p&gt;
PREM&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#31616;&#21270;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#36807;&#31243;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33410;&#28857;&#32423;&#22270;&#24322;&#24120;&#26816;&#27979;&#22312;&#35782;&#21035;&#21307;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#26631;&#27880;&#25968;&#25454;&#30340;&#21294;&#20047;&#65292;&#24050;&#26377;&#30340;&#22522;&#20110;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#24448;&#24448;&#22312;&#25928;&#29575;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#36825;&#28304;&#20110;&#23427;&#20204;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#32321;&#29712;&#30340;&#27169;&#22359;&#12290;&#20026;&#20102;&#25552;&#39640;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#25928;&#29575;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;PREprocessing and Matching&#65288;&#31616;&#31216;PREM&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#22270;&#24322;&#24120;&#26816;&#27979;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#21644;&#20869;&#23384;&#30340;&#28040;&#32791;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#12290;PREM&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#39044;&#22788;&#29702;&#27169;&#22359;&#21644;&#37051;&#23621;&#21305;&#37197;&#27169;&#22359;&#12290;PREM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#28040;&#24687;&#20256;&#36882;&#20256;&#25773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#20869;&#23384;&#20351;&#29992;&#37327;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11670</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning. (arXiv:2310.11670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11670
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#21487;&#27604;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29978;&#33267;&#22312;&#25968;&#25454;&#37327;&#36739;&#23567;&#26102;&#20063;&#33021;&#36229;&#36807;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#32463;&#35777;&#26126;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#26377;&#25928;&#65292;&#21516;&#26102;&#21482;&#26356;&#26032;&#20102;&#23569;&#37327;&#21442;&#25968;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#36866;&#24212;&#27599;&#20010;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36755;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20302;&#25968;&#25454;&#24773;&#26223;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#36229;&#36866;&#37197;&#22120;&#65288;PHA&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#36866;&#37197;&#22120;&#35843;&#25972;&#21644;&#36229;&#32593;&#32476;&#22522;&#30784;&#19978;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20363;&#23494;&#38598;&#30340;&#26816;&#32034;&#22120;&#21644;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#21407;&#22411;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#26465;&#20214;&#27169;&#22359;&#12290;&#36825;&#23548;&#33268;&#19982;&#29616;&#26377;PEFT&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#19978;&#30456;&#24403;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#21487;&#29992;&#25968;&#25454;&#37327;&#21464;&#23567;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#22522;&#32447;&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;&#22522;&#20110;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PHA&#22312;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-
&lt;/p&gt;</description></item><item><title>Hetero$^2$Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24230;&#37327;&#25351;&#26631;&#26469;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#65292;&#20197;&#35299;&#20915;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.11664</link><description>&lt;p&gt;
Hetero$^2$Net: &#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs. (arXiv:2310.11664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11664
&lt;/p&gt;
&lt;p&gt;
Hetero$^2$Net&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#24230;&#37327;&#25351;&#26631;&#26469;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#65292;&#20197;&#35299;&#20915;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#30340;&#22270;&#36890;&#24120;&#38750;&#24120;&#22797;&#26434;&#65292;&#20840;&#23616;&#32467;&#26500;&#20013;&#23384;&#22312;&#24322;&#26500;&#24615;&#65292;&#32780;&#23616;&#37096;&#37051;&#22495;&#20869;&#21017;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#24322;&#36136;&#24615;&#12290;&#23613;&#31649;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;&#24120;&#35265;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#30340;&#21516;&#26500;&#22270;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#20294;&#22312;&#30740;&#31350;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#23646;&#24615;&#26041;&#38754;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;&#20803;&#36335;&#24452;&#35782;&#21035;&#20102;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#24230;&#37327;&#25351;&#26631;&#26469;&#23450;&#37327;&#25551;&#36848;&#24322;&#36136;&#24615;&#27700;&#24179;&#12290;&#36890;&#36807;&#23545;&#20960;&#20010;&#23637;&#31034;&#19981;&#21516;&#24322;&#36136;&#24615;&#27700;&#24179;&#30340;&#30495;&#23454;&#19990;&#30028;&#24322;&#26500;&#22270;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#32487;&#25215;&#20102;&#24456;&#22810;&#35774;&#35745;&#29992;&#20110;&#21516;&#26500;&#22270;&#30340;GNNs&#26426;&#21046;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#22312;&#22788;&#29702;&#20855;&#26377;&#24322;&#36136;&#24615;&#25110;&#20302;&#24230;&#21516;&#36136;&#24615;&#30340;&#24322;&#26500;&#22270;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hetero$^2$Net&#65292;&#19968;&#20010;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a h
&lt;/p&gt;</description></item><item><title>Cloud-MRI&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;MRI&#31995;&#32479;&#65292;&#22312;6G&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#24212;&#29992;&#30340;&#20113;&#30913;&#20849;&#25391;&#25104;&#20687;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;MRI&#25968;&#25454;&#23384;&#20648;&#23433;&#20840;&#24615;&#12289;&#20256;&#36755;&#36895;&#24230;&#12289;AI&#31639;&#27861;&#32500;&#25252;&#12289;&#30828;&#20214;&#21319;&#32423;&#21644;&#21327;&#20316;&#24037;&#20316;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.11641</link><description>&lt;p&gt;
&#20113;&#30913;&#20849;&#25391;&#25104;&#20687;&#31995;&#32479;&#65306;&#22312;6G&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial Intelligence. (arXiv:2310.11641v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11641
&lt;/p&gt;
&lt;p&gt;
Cloud-MRI&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;MRI&#31995;&#32479;&#65292;&#22312;6G&#21644;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#24212;&#29992;&#30340;&#20113;&#30913;&#20849;&#25391;&#25104;&#20687;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;MRI&#25968;&#25454;&#23384;&#20648;&#23433;&#20840;&#24615;&#12289;&#20256;&#36755;&#36895;&#24230;&#12289;AI&#31639;&#27861;&#32500;&#25252;&#12289;&#30828;&#20214;&#21319;&#32423;&#21644;&#21327;&#20316;&#24037;&#20316;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#21307;&#23398;&#35786;&#26029;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27599;&#24180;&#22312;&#22823;&#22411;&#21307;&#38498;&#20135;&#29983;&#20102;&#25968;PB&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#27969;&#38656;&#35201;&#22823;&#37327;&#30340;&#32593;&#32476;&#24102;&#23485;&#21644;&#24222;&#22823;&#30340;&#23384;&#20648;&#22522;&#30784;&#35774;&#26045;&#12290;&#27492;&#22806;&#65292;&#26412;&#22320;&#25968;&#25454;&#22788;&#29702;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#30828;&#20214;&#25237;&#36164;&#12290;&#19981;&#21516;&#21307;&#30103;&#26426;&#26500;&#20043;&#38388;&#30340;&#25968;&#25454;&#38548;&#31163;&#38459;&#30861;&#20102;&#20020;&#24202;&#21644;&#30740;&#31350;&#30340;&#36328;&#26426;&#26500;&#21512;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39044;&#26399;&#23558;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#20113;&#35745;&#31639;&#12289;6G&#24102;&#23485;&#12289;&#36793;&#32536;&#35745;&#31639;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#21306;&#22359;&#38142;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;MRI&#31995;&#32479;&#21450;&#20854;&#22235;&#20195;&#12290;&#35813;&#31995;&#32479;&#34987;&#31216;&#20026;Cloud-MRI&#65292;&#26088;&#22312;&#35299;&#20915;MRI&#25968;&#25454;&#23384;&#20648;&#23433;&#20840;&#24615;&#12289;&#20256;&#36755;&#36895;&#24230;&#12289;AI&#31639;&#27861;&#32500;&#25252;&#12289;&#30828;&#20214;&#21319;&#32423;&#21644;&#21327;&#20316;&#24037;&#20316;&#30340;&#38382;&#39064;&#12290;&#24037;&#20316;&#27969;&#31243;&#20174;&#23558;k&#31354;&#38388;&#21407;&#22987;&#25968;&#25454;&#36716;&#21270;&#20026;&#26631;&#20934;&#21270;&#30340;&#30913;&#20849;&#25391;&#25104;&#20687;&#23398;&#20250;&#30340;&#22270;&#20687;&#26684;&#24335;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetic Resonance Imaging (MRI) plays an important role in medical diagnosis, generating petabytes of image data annually in large hospitals. This voluminous data stream requires a significant amount of network bandwidth and extensive storage infrastructure. Additionally, local data processing demands substantial manpower and hardware investments. Data isolation across different healthcare institutions hinders cross-institutional collaboration in clinics and research. In this work, we anticipate an innovative MRI system and its four generations that integrate emerging distributed cloud computing, 6G bandwidth, edge computing, federated learning, and blockchain technology. This system is called Cloud-MRI, aiming at solving the problems of MRI data storage security, transmission speed, AI algorithm maintenance, hardware upgrading, and collaborative work. The workflow commences with the transformation of k-space raw data into the standardized Imaging Society for Magnetic Resonance in Med
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#65292;&#20351;&#29992;&#20102;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;StratiFOILed&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#21644;&#20840;&#23616;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.11636</link><description>&lt;p&gt;
&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
A Symbolic Language for Interpreting Decision Trees. (arXiv:2310.11636v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11636
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#65292;&#20351;&#29992;&#20102;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;StratiFOILed&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#21508;&#31181;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#21644;&#20840;&#23616;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#21457;&#23637;&#30340;&#27491;&#24335;&#21487;&#35299;&#37322;&#30340;AI&#25361;&#25112;&#20102;&#8220;&#20915;&#31574;&#26641;&#26159;&#26131;&#35299;&#37322;&#30340;&#27169;&#22411;&#8221;&#30340;&#27969;&#34892;&#35828;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#20915;&#31574;&#26641;&#19978;&#36827;&#34892;&#35299;&#37322;&#24615;&#26597;&#35810;&#30340;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#35299;&#37322;&#24615;&#26597;&#35810;&#25110;&#35780;&#20998;&#36866;&#29992;&#20110;&#27599;&#20010;&#24773;&#22659;&#21644;&#26368;&#32456;&#29992;&#25143;&#12290;&#36825;&#33258;&#28982;&#22320;&#25552;&#20986;&#20102;&#8220;&#21487;&#35299;&#37322;&#24615;&#35821;&#35328;&#8221;&#30340;&#21487;&#33021;&#24615;&#65292;&#20854;&#20013;&#21487;&#20197;&#34920;&#36798;&#21508;&#31181;&#26597;&#35810;&#65292;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#23450;&#21046;&#26597;&#35810;&#30340;&#25511;&#21046;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20171;&#32461;&#20102;&#35299;&#37322;&#20915;&#31574;&#26641;&#30340;&#31526;&#21495;&#35821;&#35328;ExplainDT&#12290;ExplainDT&#26681;&#26893;&#20110;&#25105;&#20204;&#31216;&#20043;&#20026;StratiFOILed&#30340;&#31934;&#24515;&#26500;&#24314;&#30340;&#19968;&#38454;&#36923;&#36753;&#30340;&#29255;&#27573;&#12290;StratiFOILed&#24179;&#34913;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#35780;&#20272;&#22797;&#26434;&#24230;&#65292;&#20801;&#35768;&#35745;&#31639;&#20986;&#35768;&#22810;&#20107;&#21518;&#35299;&#37322;&#65292;&#21253;&#25324;&#23616;&#37096;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#35748;&#20026;&#21644;&#21453;&#21521;&#25512;&#29702;&#65289;&#21644;&#20840;&#23616;&#35299;&#37322;&#65288;&#20363;&#22914;&#65292;&#25512;&#24191;&#21644;&#23545;&#25239;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of formal explainable AI has disputed the folklore claim that "decision trees are readily interpretable models", showing different interpretability queries that are computationally hard on decision trees, as well as proposing different methods to deal with them in practice. Nonetheless, no single explainability query or score works as a "silver bullet" that is appropriate for every context and end-user. This naturally suggests the possibility of "interpretability languages" in which a wide variety of queries can be expressed, giving control to the end-user to tailor queries to their particular needs. In this context, our work presents ExplainDT, a symbolic language for interpreting decision trees. ExplainDT is rooted in a carefully constructed fragment of first-ordered logic that we call StratiFOILed. StratiFOILed balances expressiveness and complexity of evaluation, allowing for the computation of many post-hoc explanations--both local (e.g., abductive and contr
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;"&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;"&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#25913;&#21892;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11628</link><description>&lt;p&gt;
&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#35789;&#27744;&#21270;&#26631;&#35760;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learn Your Tokens: Word-Pooled Tokenization for Language Modeling. (arXiv:2310.11628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11628
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;"&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;"&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#20197;&#25913;&#21892;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23558;&#25991;&#26412;&#26631;&#35760;&#21270;&#20026;&#23376;&#35789;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#30340;&#12289;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#23383;&#31526;&#32452;&#21512;&#25104;&#26356;&#38271;&#30340;&#34920;&#23618;&#23383;&#31526;&#20018;&#65288;&#22914; 'ing'&#65289;&#25110;&#25972;&#20010;&#21333;&#35789;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#21453;&#22797;&#23637;&#31034;&#20102;&#36825;&#31181;&#26631;&#35760;&#21270;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#33521;&#25991;&#30340;&#25991;&#26723;&#21644;&#34920;&#31034;&#25968;&#23383;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23383;&#33410;/&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#21046;&#36739;&#23569;&#65292;&#20294;&#22312;&#33258;&#25105;&#27880;&#24847;&#35745;&#31639;&#20013;&#23384;&#22312;&#24207;&#21015;&#25551;&#36848;&#38271;&#24230;&#22686;&#21152;&#21644;&#21518;&#32493;&#20108;&#27425;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#23545;&#36825;&#20123;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#22266;&#23450;&#22823;&#23567;&#21367;&#31215;&#21387;&#32553;&#21644;&#38480;&#21046;&#30340;&#23581;&#35797;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#23436;&#20840;&#24573;&#30053;&#20102;&#21333;&#35789;&#36793;&#30028;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#8220;&#23398;&#20064;&#24744;&#30340;&#26631;&#35760;&#8221;&#26041;&#26696;&#65292;&#21033;&#29992;&#21333;&#35789;&#36793;&#30028;&#23558;&#23383;&#33410;/&#23383;&#31526;&#27719;&#32858;&#25104;&#21333;&#35789;&#34920;&#31034;&#24418;&#24335;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#20027;&#35201;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20877;&#23545;&#27599;&#20010;&#21333;&#35789;&#24182;&#34892;&#35299;&#30721;&#20010;&#21035;&#30340;&#23383;&#31526;/&#23383;&#33410;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Language models typically tokenize text into subwords, using a deterministic, hand-engineered heuristic of combining characters into longer surface-level strings such as 'ing' or whole words. Recent literature has repeatedly shown the limitations of such a tokenization strategy, particularly for documents not written in English and for representing numbers. On the other extreme, byte/character-level language models are much less restricted but suffer from increased sequence description lengths and a subsequent quadratic expansion in self-attention computation. Recent attempts to compress and limit these context lengths with fixed size convolutions is helpful but completely ignores the word boundary. This paper considers an alternative 'learn your tokens' scheme which utilizes the word boundary to pool bytes/characters into word representations, which are fed to the primary language model, before again decoding individual characters/bytes per word in parallel. We find that our moderatel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.11616</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;&#65306;&#19968;&#31181;&#24515;&#29702;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unveiling the General Intelligence Factor in Language Models: A Psychometric Approach. (arXiv:2310.11616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#21457;&#29616;&#20102;&#35813;&#22240;&#23376;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#24515;&#29702;&#27979;&#37327;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#26222;&#36941;&#26234;&#33021;&#22240;&#23376;g&#30340;&#23384;&#22312;&#65292;&#24182;&#25193;&#23637;&#20102;&#35813;&#29702;&#35770;&#22312;&#20154;&#31867;&#21644;&#26576;&#20123;&#21160;&#29289;&#29289;&#31181;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;Open LLM Leaderboard&#65288;&#21253;&#21547;1,232&#20010;&#27169;&#22411;&#65289;&#21644;General Language Understanding Evaluation&#65288;GLUE&#65289;Leaderboard&#65288;&#21253;&#21547;88&#20010;&#27169;&#22411;&#65289;&#36827;&#34892;&#22240;&#23376;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20855;&#26377;&#19968;&#32500;&#24615;&#21644;&#39640;&#24230;&#31283;&#23450;&#24615;&#30340;g&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#26041;&#24046;&#30340;85%&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;g&#20043;&#38388;&#30340;&#20013;&#24230;&#30456;&#20851;&#24615;&#20026;0.48&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;g&#22240;&#23376;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25351;&#26631;&#65292;&#20026;&#26356;&#24378;&#22823;&#12289;&#22522;&#20110;g&#22240;&#23376;&#30340;&#27169;&#22411;&#33021;&#21147;&#35780;&#20272;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#20174;&#24515;&#29702;&#27979;&#37327;&#30340;&#35282;&#24230;&#29702;&#35299;&#21644;&#26410;&#26469;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#23545;&#27169;&#22411;&#35780;&#20272;&#21644;&#24320;&#21457;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uncovers the factor of general intelligence, or g, in language models, extending the psychometric theory traditionally applied to humans and certain animal species. Utilizing factor analysis on two extensive datasets Open LLM Leaderboard with 1,232 models and General Language Understanding Evaluation (GLUE) Leaderboard with 88 models - we find compelling evidence for a unidimensional, highly stable g factor that accounts for 85% of the variance in model performance. The study also finds a moderate correlation of .48 between model size and g. The discovery of g in language models offers a unified metric for model evaluation and opens new avenues for more robust, g-based model ability assessment. These findings lay the foundation for understanding and future research on artificial general intelligence from a psychometric perspective and have practical implications for model evaluation and development.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#31243;&#23398;&#20064;&#19982;&#23618;&#27425;&#35268;&#21010;&#22120;&#32467;&#21512;&#30340;&#33258;&#28982;&#32534;&#31243;&#24211;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#25945;&#23548;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#24182;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.11614</link><description>&lt;p&gt;
&#20174;&#22810;&#20195;&#20154;&#20013;&#23398;&#20064;&#30340;&#23618;&#27425;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning a Hierarchical Planner from Humans in Multiple Generations. (arXiv:2310.11614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11614
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32534;&#31243;&#23398;&#20064;&#19982;&#23618;&#27425;&#35268;&#21010;&#22120;&#32467;&#21512;&#30340;&#33258;&#28982;&#32534;&#31243;&#24211;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#25945;&#23548;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#24182;&#36866;&#24212;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20174;&#20154;&#31867;&#33719;&#24471;&#30693;&#35782;&#30340;&#19968;&#31181;&#20856;&#22411;&#26041;&#24335;&#26159;&#36890;&#36807;&#32534;&#31243;&#12290;&#19982;&#20174;&#28436;&#31034;&#25110;&#32463;&#39564;&#20013;&#23398;&#20064;&#30456;&#27604;&#65292;&#32534;&#31243;&#23398;&#20064;&#20801;&#35768;&#26426;&#22120;&#22312;&#32534;&#20889;&#31243;&#24207;&#26102;&#23601;&#33021;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#31243;&#24207;&#24211;&#65292;&#26426;&#22120;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#22914;&#20309;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31243;&#24207;&#24448;&#24448;&#40664;&#35748;&#25191;&#34892;&#29615;&#22659;&#19981;&#21464;&#65292;&#24403;&#29615;&#22659;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#22797;&#26434;&#31243;&#24207;&#30340;&#36866;&#24212;&#24615;&#21464;&#24046;&#65292;&#20351;&#24471;&#23558;&#22797;&#26434;&#31243;&#24207;&#36866;&#24212;&#26032;&#29615;&#22659;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32534;&#31243;&#23398;&#20064;&#19982;&#23618;&#27425;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#30340;&#33258;&#28982;&#32534;&#31243;&#24211;&#23398;&#20064;&#31995;&#32479;&#12290;&#33258;&#28982;&#32534;&#31243;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#21253;&#21547;&#30446;&#26631;&#12289;&#30446;&#26631;&#22914;&#20309;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#65292;&#20197;&#21450;&#20855;&#20307;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#30340;&#23454;&#20363;&#30340;&#20998;&#35299;&#24211;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35838;&#31243;&#26500;&#24314;&#30340;&#26041;&#24335;&#25945;&#23548;&#31995;&#32479;&#65292;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#20294;&#19981;&#26159;&#19981;&#21487;&#33021;&#36798;&#21040;&#30340;&#30446;&#26631;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#22914;&#20309;&#36798;&#25104;&#36825;&#19968;&#30446;&#26631;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical way in which a machine acquires knowledge from humans is by programming. Compared to learning from demonstrations or experiences, programmatic learning allows the machine to acquire a novel skill as soon as the program is written, and, by building a library of programs, a machine can quickly learn how to perform complex tasks. However, as programs often take their execution contexts for granted, they are brittle when the contexts change, making it difficult to adapt complex programs to new contexts. We present natural programming, a library learning system that combines programmatic learning with a hierarchical planner. Natural programming maintains a library of decompositions, consisting of a goal, a linguistic description of how this goal decompose into sub-goals, and a concrete instance of its decomposition into sub-goals. A user teaches the system via curriculum building, by identifying a challenging yet not impossible goal along with linguistic hints on how this goal may
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11604</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Zero-Shot Trajectory Generators. (arXiv:2310.11604v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#38646;-shot&#36712;&#36857;&#29983;&#25104;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#32473;&#20104;LLM&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;LLMs&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#25805;&#20316;&#25216;&#33021;&#20013;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#24207;&#21015;&#65292;&#24182;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;&#36825;&#19968;&#30740;&#31350;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#65292;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20104;&#20302;&#32423;&#25216;&#33021;&#36873;&#25321;&#26102;&#33021;&#22815;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#39640;&#32423;&#35268;&#21010;&#22120;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35748;&#20026;LLMs&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#29992;&#20110;&#20302;&#32423;&#36712;&#36857;&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#35843;&#26597;&#20102;&#24403;&#32473;&#20104;LLM&#65288;GPT-4&#65289;&#20165;&#33021;&#35775;&#38382;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#35270;&#35273;&#27169;&#22411;&#26102;&#65292;&#23427;&#33021;&#21542;&#30452;&#25509;&#39044;&#27979;&#19968;&#31995;&#21015;&#23494;&#38598;&#30340;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#29992;&#20110;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#25552;&#31034;&#65292;&#27809;&#26377;&#20219;&#20309;&#19978;&#19979;&#25991;&#31034;&#20363;&#12289;&#36816;&#21160;&#21407;&#35821;&#25110;&#22806;&#37096;&#36712;&#36857;&#20248;&#21270;&#22120;&#65292;&#23427;&#22312;26&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22522;&#20110;&#35821;&#35328;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#22914;&#8220;&#25171;&#24320;&#29942;&#30422;&#8221;&#21644;&#8220;&#29992;&#28023;&#32501;&#25830;&#25325;&#30424;&#23376;&#8221;&#65292;&#20197;&#21450;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20010;&#25552;&#31034;&#20013;&#21738;&#20123;&#35774;&#35745;&#36873;&#25321;&#26368;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#31361;&#30772;&#20102;&#23545;LLMs&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#19978;&#30340;&#38480;&#21046;&#65292;&#24182;&#39318;&#27425;&#25581;&#31034;&#20102;LLMs&#30830;&#23454;&#20855;&#26377;&#23545;&#25805;&#20316;&#20219;&#21153;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently shown promise as high-level planners for robots when given access to a selection of low-level skills. However, it is often assumed that LLMs do not possess sufficient knowledge to be used for the low-level trajectories themselves. In this work, we address this assumption thoroughly, and investigate if an LLM (GPT-4) can directly predict a dense sequence of end-effector poses for manipulation skills, when given access to only object detection and segmentation vision models. We study how well a single task-agnostic prompt, without any in-context examples, motion primitives, or external trajectory optimisers, can perform across 26 real-world language-based tasks, such as "open the bottle cap" and "wipe the plate with the sponge", and we investigate which design choices in this prompt are the most effective. Our conclusions raise the assumed limit of LLMs for robotics, and we reveal for the first time that LLMs do indeed possess an understanding o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Transformer&#23545;&#20110;&#31995;&#32479;&#38450;&#24481;&#32773;&#21644;&#31995;&#32479;&#25915;&#20987;&#32773;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#22312;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#23646;&#24615;&#21644;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.11597</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Efficacy of Transformer-based Adversarial Attacks in Security Domains. (arXiv:2310.11597v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Transformer&#23545;&#20110;&#31995;&#32479;&#38450;&#24481;&#32773;&#21644;&#31995;&#32479;&#25915;&#20987;&#32773;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;Transformer&#22312;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#20013;&#30340;&#23646;&#24615;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#23433;&#20840;&#20381;&#36182;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#26816;&#27979;&#23041;&#32961;&#12289;&#35782;&#21035;&#28431;&#27934;&#24182;&#20445;&#25252;&#31995;&#32479;&#20813;&#21463;&#25915;&#20987;&#12290;&#26368;&#36817;&#65292;Transformer&#26550;&#26500;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#21644;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#25913;&#21892;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25918;&#24323;&#24403;&#21069;&#30340;Transformer&#26041;&#27861;&#20043;&#21069;&#65292;&#20102;&#35299;&#23427;&#20204;&#23545;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#23646;&#24615;&#21644;&#24433;&#21709;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Transformer&#23545;&#20110;&#31995;&#32479;&#38450;&#24481;&#32773;&#65288;&#21363;&#23545;&#20110;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#26550;&#26500;&#19978;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#38887;&#24615;&#65289;&#21644;&#31995;&#32479;&#25915;&#20987;&#32773;&#65288;&#21363;&#23558;&#30001;Transformer&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#65289;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#19968;&#22871;&#39044;&#35757;&#32451;&#30340;Transformer&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#28151;&#21512;&#65288;Transformer&#21644;CNN&#30340;&#38598;&#25104;&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, the security of many domains rely on the use of Machine Learning to detect threats, identify vulnerabilities, and safeguard systems from attacks. Recently, transformer architectures have improved the state-of-the-art performance on a wide range of tasks such as malware detection and network intrusion detection. But, before abandoning current approaches to transformers, it is crucial to understand their properties and implications on cybersecurity applications. In this paper, we evaluate the robustness of transformers to adversarial samples for system defenders (i.e., resiliency to adversarial perturbations generated on different types of architectures) and their adversarial strength for system attackers (i.e., transferability of adversarial samples generated by transformers to other target models). To that effect, we first fine-tune a set of pre-trained transformer, Convolutional Neural Network (CNN), and hybrid (an ensemble of transformer and CNN) models to solve different down
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveAttack&#30340;&#26032;&#22411;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#26469;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;</title><link>http://arxiv.org/abs/2310.11595</link><description>&lt;p&gt;
WaveAttack&#65306;&#22522;&#20110;&#19981;&#23545;&#31216;&#39057;&#29575;&#28151;&#28102;&#30340;&#22522;&#20110;&#32972;&#38376;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks Against Deep Neural Networks. (arXiv:2310.11595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WaveAttack&#30340;&#26032;&#22411;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#26469;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#35768;&#22810;&#23545;&#25163;&#35774;&#35745;&#20102;&#32972;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#25805;&#32437;&#35757;&#32451;&#26679;&#26412;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#32972;&#38376;&#25915;&#20987;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#37117;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#34987;&#29616;&#26377;&#30340;&#32972;&#38376;&#26816;&#27979;&#31639;&#27861;&#36731;&#26131;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#34920;&#29616;&#20026;&#27602;&#21270;&#26679;&#26412;&#30340;&#20302;&#20445;&#30495;&#24615;&#21644;&#38544;&#34255;&#31354;&#38388;&#20013;&#30340;&#38750;&#21487;&#24573;&#30053;&#36716;&#25442;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#24369;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39057;&#29575;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;WaveAttack&#65292;&#23427;&#36890;&#36807;&#31163;&#25955;&#23567;&#27874;&#21464;&#25442;&#65288;DWT&#65289;&#33719;&#21462;&#22270;&#20687;&#30340;&#39640;&#39057;&#29305;&#24449;&#26469;&#29983;&#25104;&#32972;&#38376;&#35302;&#21457;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#39057;&#29575;&#28151;&#28102;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#28155;&#21152;&#33258;&#36866;&#24212;&#27531;&#24046;&#65292;&#20197;&#25552;&#39640;&#35302;&#21457;&#22120;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;WaveAttack&#30340;&#26377;&#25928;&#24615;&#12290;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WaveAttack&#19981;&#20165;&#25552;&#39640;&#20102;&#32972;&#38376;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#24182;&#19988;&#19981;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the popularity of Artificial Intelligence (AI) technology, numerous backdoor attacks are designed by adversaries to mislead deep neural network predictions by manipulating training samples and training processes. Although backdoor attacks are effective in various real scenarios, they still suffer from the problems of both low fidelity of poisoned samples and non-negligible transfer in latent space, which make them easily detectable by existing backdoor detection algorithms. To overcome the weakness, this paper proposes a novel frequency-based backdoor attack method named WaveAttack, which obtains image high-frequency features through Discrete Wavelet Transform (DWT) to generate backdoor triggers. Furthermore, we introduce an asymmetric frequency obfuscation method, which can add an adaptive residual in the training and inference stage to improve the impact of triggers and further enhance the effectiveness of WaveAttack. Comprehensive experimental results show that WaveAttack not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.11594</link><description>&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#29615;&#22659;&#20013;&#65292;&#32500;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37322;&#25918;&#25968;&#25454;&#28508;&#21147;&#20043;&#38388;&#24494;&#22937;&#30340;&#24179;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20197;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36825;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#24102;&#26469;&#20102;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#23454;&#20307;&#27880;&#20837;&#25439;&#22351;&#25968;&#25454;&#30340;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#21021;&#21463;&#21040;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#12290;ARU&#34987;&#19968;&#37096;&#20998;&#23545;&#25163;&#20351;&#29992;&#65292;&#20197;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;ARU&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#23545;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11593</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Evaluation of Personalized Text Generation using Large Language Models. (arXiv:2310.11593v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35780;&#20215;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#36136;&#37327;&#30340;&#24494;&#22937;&#24046;&#21035;&#65292;&#32780;&#20154;&#24037;&#21028;&#26029;&#21448;&#26114;&#36149;&#19988;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#27979;&#37327;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#36825;&#19977;&#20010;&#37325;&#35201;&#35821;&#20041;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#29992;&#25143;&#20010;&#20154;&#32972;&#26223;&#20132;&#20184;&#20869;&#23481;&#30340;&#19987;&#38376;&#26426;&#21046;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#36805;&#36895;&#65292;&#20294;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#65288;&#22914;BLEU&#21644;ROUGE&#65289;&#20027;&#35201;&#34913;&#37327;&#19982;&#20154;&#24037;&#21442;&#32771;&#25991;&#26412;&#30340;&#35789;&#27719;&#30456;&#20284;&#24230;&#65292;&#24182;&#19981;&#33021;&#21306;&#20998;&#20010;&#24615;&#21270;&#19982;&#20854;&#20182;&#24494;&#22937;&#30340;&#35821;&#20041;&#26041;&#38754;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#20010;&#24615;&#21270;&#29983;&#25104;&#20869;&#23481;&#36136;&#37327;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#24037;&#21028;&#26029;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#20010;&#24615;&#21270;&#35780;&#20272;&#39046;&#22495;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35780;&#20272;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#26816;&#39564;&#23427;&#20204;&#29702;&#35299;&#32454;&#33268;&#30340;&#29992;&#25143;&#32972;&#26223;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AuPEL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#25991;&#26412;&#30340;&#20010;&#24615;&#21270;&#12289;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#19977;&#20010;&#20027;&#35201;&#35821;&#20041;&#26041;&#38754;&#25552;&#21462;&#24182;&#33258;&#21160;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized text generation presents a specialized mechanism for delivering content that is specific to a user's personal context. While the research progress in this area has been rapid, evaluation still presents a challenge. Traditional automated metrics such as BLEU and ROUGE primarily measure lexical similarity to human-written references, and are not able to distinguish personalization from other subtle semantic aspects, thus falling short of capturing the nuances of personalized generated content quality. On the other hand, human judgments are costly to obtain, especially in the realm of personalized evaluation. Inspired by these challenges, we explore the use of large language models (LLMs) for evaluating personalized text generation, and examine their ability to understand nuanced user context. We present AuPEL, a novel evaluation method that distills three major semantic aspects of the generated text: personalization, quality and relevance, and automatically measures these as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.11589</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#33719;&#21462;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Eliciting Human Preferences with Language Models. (arXiv:2310.11589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#21644;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#65292;&#36890;&#36807;GATE&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26631;&#27880;&#31034;&#20363;&#25110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#25191;&#34892;&#30446;&#26631;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#22312;&#36873;&#25321;&#31034;&#20363;&#25110;&#25776;&#20889;&#25552;&#31034;&#26102;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#8212;&#8212;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#24322;&#24120;&#24773;&#20917;&#12289;&#35201;&#27714;&#31934;&#30830;&#34920;&#36798;&#27169;&#31946;&#20559;&#22909;&#25110;&#38656;&#35201;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#35748;&#30693;&#30340;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;*&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;*&#26469;&#24341;&#23548;&#20219;&#21153;&#35268;&#33539;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;**&#29983;&#25104;&#24335;&#20027;&#21160;&#20219;&#21153;&#24341;&#23548;&#65288;GATE&#65289;**&#65306;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#30340;&#12289;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#26469;&#24341;&#23548;&#24182;&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#39046;&#22495;&#30740;&#31350;&#20102;GATE&#65306;&#30005;&#23376;&#37038;&#20214;&#39564;&#35777;&#12289;&#20869;&#23481;&#25512;&#33616;&#21644;&#36947;&#24503;&#25512;&#29702;&#12290;&#22312;&#39044;&#20808;&#27880;&#20876;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#31034;&#25191;&#34892;GATE&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#36890;&#36807;&#29983;&#25104;&#24320;&#25918;&#24335;&#38382;&#39064;&#25110;&#21512;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#36793;&#30028;&#26696;&#20363;&#65289;&#25152;&#24341;&#21457;&#30340;&#21709;&#24212;&#36890;&#24120;&#27604;&#29992;&#25143;&#32534;&#20889;&#30340;&#25552;&#31034;&#25110;&#26631;&#31614;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#29992;&#25143;&#25253;&#21578;&#31216;&#65292;&#20132;&#20114;&#24335;&#20219;&#21153;&#24341;&#23548;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#24110;&#21161;&#20182;&#20204;&#34920;&#36798;&#20559;&#22909;&#21644;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) can be directed to perform target tasks by using labeled examples or natural language prompts. But selecting examples or writing prompts for can be challenging--especially in tasks that involve unusual edge cases, demand precise articulation of nebulous preferences, or require an accurate mental model of LM behavior. We propose to use *LMs themselves* to guide the task specification process. In this paper, we introduce **Generative Active Task Elicitation (GATE)**: a learning framework in which models elicit and infer intended behavior through free-form, language-based interaction with users. We study GATE in three domains: email validation, content recommendation, and moral reasoning. In preregistered experiments, we show that LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels. Users report that interactive task elicitat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11569</link><description>&lt;p&gt;
&#24403;&#21018;&#24615;&#25104;&#20026;&#38382;&#39064;&#65306;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#20307;&#65292;&#20854;&#30446;&#26631;&#26159;&#23545;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#26410;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#20063;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#20013;&#26045;&#21152;&#23618;&#27425;&#20851;&#31995;&#65292;&#20294;&#26410;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20063;&#40664;&#35748;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#23618;&#27425;&#20851;&#31995;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#26410;&#36866;&#24212;&#26174;&#31034;&#20986;&#20559;&#31163;&#27492;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHiT&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#25972;&#20010;&#23618;&#27425;&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;PROFHiT&#20351;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;3D&#22478;&#24066;&#25968;&#25454;&#65292;&#33021;&#22815;&#21033;&#29992;CityGML&#30340;&#35821;&#20041;&#21644;&#25299;&#25169;&#23646;&#24615;&#23545;&#35813;&#25968;&#25454;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#30340;&#20998;&#26512;&#12290;&#30446;&#21069;&#20851;&#20110;&#26597;&#35810;CityGML&#25968;&#25454;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#24320;&#21457;&#65292;&#24120;&#35265;&#30340;&#22788;&#29702;&#26041;&#24335;&#26159;&#23558;&#20854;&#23384;&#20648;&#22312;3DCityDB&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;SQL&#26597;&#35810;&#35821;&#35328;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#32456;&#31471;&#29992;&#25143;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21046;&#23450;&#26597;&#35810;&#26102;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;CityGML&#30340;&#27010;&#24565;&#35821;&#20041;&#19982;&#20851;&#31995;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.11555</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;3D&#22478;&#24066;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Integrating 3D City Data through Knowledge Graphs. (arXiv:2310.11555v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11555
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25972;&#21512;3D&#22478;&#24066;&#25968;&#25454;&#65292;&#33021;&#22815;&#21033;&#29992;CityGML&#30340;&#35821;&#20041;&#21644;&#25299;&#25169;&#23646;&#24615;&#23545;&#35813;&#25968;&#25454;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#30340;&#20998;&#26512;&#12290;&#30446;&#21069;&#20851;&#20110;&#26597;&#35810;CityGML&#25968;&#25454;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#24320;&#21457;&#65292;&#24120;&#35265;&#30340;&#22788;&#29702;&#26041;&#24335;&#26159;&#23558;&#20854;&#23384;&#20648;&#22312;3DCityDB&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;SQL&#26597;&#35810;&#35821;&#35328;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#32456;&#31471;&#29992;&#25143;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21046;&#23450;&#26597;&#35810;&#26102;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;CityGML&#30340;&#27010;&#24565;&#35821;&#20041;&#19982;&#20851;&#31995;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CityGML&#26159;&#30001;&#24320;&#25918;&#22320;&#29702;&#31354;&#38388;&#32852;&#30431;&#65288;OGC&#65289;&#24191;&#27867;&#37319;&#29992;&#30340;&#29992;&#20110;&#34920;&#31034;&#21644;&#20132;&#25442;3D&#22478;&#24066;&#27169;&#22411;&#30340;&#26631;&#20934;&#12290;CityGML&#20013;&#35821;&#20041;&#21644;&#25299;&#25169;&#23646;&#24615;&#30340;&#34920;&#31034;&#20351;&#24471;&#21487;&#20197;&#23545;&#36825;&#20123;3D&#22478;&#24066;&#25968;&#25454;&#36827;&#34892;&#26597;&#35810;&#65292;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#30340;&#20998;&#26512;&#65292;&#20363;&#22914;&#23433;&#20840;&#31649;&#29702;&#21644;&#24212;&#24613;&#21709;&#24212;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#20272;&#35745;&#20197;&#21450;&#21344;&#29992;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#26597;&#35810;CityGML&#25968;&#25454;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#24320;&#21457;&#12290;CityGML&#30340;&#23448;&#26041;GML/XML&#32534;&#30721;&#21482;&#26159;&#29992;&#20316;&#20132;&#25442;&#26684;&#24335;&#65292;&#19981;&#36866;&#21512;&#36827;&#34892;&#26597;&#35810;&#22238;&#31572;&#12290;&#22788;&#29702;CityGML&#25968;&#25454;&#30340;&#26368;&#24120;&#35265;&#26041;&#24335;&#26159;&#23558;&#20854;&#23384;&#20648;&#22312;3DCityDB&#31995;&#32479;&#20013;&#20316;&#20026;&#20851;&#31995;&#34920;&#65292;&#28982;&#21518;&#20351;&#29992;&#26631;&#20934;&#30340;SQL&#26597;&#35810;&#35821;&#35328;&#36827;&#34892;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#30452;&#25509;&#22312;3DCityDB&#19978;&#20026;&#20182;&#20204;&#30340;&#29305;&#23450;&#20998;&#26512;&#20219;&#21153;&#21046;&#23450;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;CityGML&#30340;&#27010;&#24565;&#35821;&#20041;&#19982;&#25152;&#37319;&#29992;&#30340;&#20851;&#31995;&#27169;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
CityGML is a widely adopted standard by the Open Geospatial Consortium (OGC) for representing and exchanging 3D city models. The representation of semantic and topological properties in CityGML makes it possible to query such 3D city data to perform analysis in various applications, e.g., security management and emergency response, energy consumption and estimation, and occupancy measurement. However, the potential of querying CityGML data has not been fully exploited. The official GML/XML encoding of CityGML is only intended as an exchange format but is not suitable for query answering. The most common way of dealing with CityGML data is to store them in the 3DCityDB system as relational tables and then query them with the standard SQL query language. Nevertheless, for end users, it remains a challenging task to formulate queries over 3DCityDB directly for their ad-hoc analytical tasks, because there is a gap between the conceptual semantics of CityGML and the relational schema adopte
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#32447;&#24615;MDPs&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#21644;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11550</link><description>&lt;p&gt;
&#38754;&#21521;&#24102;&#26377;&#24378;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#23545;&#25239;&#24615;&#32447;&#24615;MDPs&#30340;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback. (arXiv:2310.11550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11550
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#30340;&#32447;&#24615;MDPs&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#21644;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32771;&#34385;&#20102;&#23545;&#25239;&#24615;&#25439;&#22833;&#21644;&#24378;&#30423;&#21453;&#39304;&#65292;&#27809;&#26377;&#20107;&#20808;&#20102;&#35299;&#36716;&#25442;&#25110;&#35775;&#38382;&#27169;&#25311;&#22120;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#20204;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#36951;&#25022;&#24615;&#33021;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#34429;&#28982;&#35745;&#31639;&#25928;&#29575;&#20302;&#65292;&#20294;&#33021;&#20445;&#35777;$\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#20854;&#20013;$K$&#26159;&#22238;&#21512;&#25968;&#12290;&#36825;&#26159;&#35813;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;$K$&#20381;&#36182;&#24615;&#30340;&#32467;&#26524;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#20445;&#35777;$\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$&#30340;&#36951;&#25022;&#24615;&#33021;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#32467;&#26524;&#37117;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65306;Kong&#31561;&#20154;[2023]&#30340;&#35745;&#31639;&#25928;&#29575;&#20302;&#30340;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#24615;&#33021;&#20026;$\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$&#65292;&#20854;&#20013;$\lambda_{\min}$&#26159;&#38382;&#39064;&#30456;&#20851;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback, without prior knowledge on transitions or access to simulators. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, ensures a regret of $\widetilde{\mathcal{O}}\left(\sqrt{K}\right)$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, which is based on the policy optimization framework, guarantees a regret of $\widetilde{\mathcal{O}}\left(K^{\frac{3}{4}} \right)$ and is computationally efficient. Both our results significantly improve over the state-of-the-art: a computationally inefficient algorithm by Kong et al. [2023] with $\widetilde{\mathcal{O}}\left(K^{\frac{4}{5}}+poly\left(\frac{1}{\lambda_{\min}}\right) \right)$ regret, for some problem-dependent constant $\lam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;</title><link>http://arxiv.org/abs/2310.11541</link><description>&lt;p&gt;
MUST&amp;P-SRL: &#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MUST&amp;P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning. (arXiv:2310.11541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#32479;&#19968;&#38899;&#33410;&#26631;&#35760;&#30340;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#30340;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#24182;&#29983;&#25104;&#23453;&#36149;&#27880;&#37322;&#65292;&#36866;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22810;&#31181;&#35821;&#35328;&#20013;&#33258;&#21160;&#38899;&#33410;&#21270;&#21333;&#35789;&#65292;&#24182;&#35774;&#35745;&#19982;&#24378;&#21046;&#23545;&#40784;&#24037;&#20855;Montreal Forced Aligner&#65288;MFA&#65289;&#20860;&#23481;&#12290;&#22312;&#25991;&#26412;&#21644;&#38899;&#38901;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#38899;&#26631;&#36716;&#24405;&#12289;&#37325;&#38899;&#26631;&#35760;&#21644;&#32479;&#19968;&#30340;&#33258;&#21160;&#38899;&#33410;&#21270;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;&#24320;&#28304;&#32452;&#20214;&#21644;&#36164;&#28304;&#26500;&#24314;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#38899;&#33410;&#21270;&#22810;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#30340;&#21333;&#35789;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;CMU ARCTIC&#25968;&#25454;&#38598;&#30340;&#36716;&#24405;&#20013;&#65292;&#29983;&#25104;&#20102;&#26377;&#21161;&#20110;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#12289;&#35821;&#38899;&#21333;&#20803;&#21457;&#29616;&#21644;&#35821;&#38899;&#22240;&#32032;&#35299;&#32544;&#30340;&#23453;&#36149;&#27880;&#37322;&#65292;&#22312;&#32447;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\footnote{\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.11531</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#26102;&#22495;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#23398;&#20064;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Learning with Offline Datasets for Infinite Horizon MDPs: A Bayesian Approach. (arXiv:2310.11531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23384;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23567;&#32047;&#31215;&#36951;&#25022;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#25552;&#20379;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#26469;&#32467;&#21512;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#23384;&#22312;&#19968;&#20010;&#31163;&#32447;&#25968;&#25454;&#38598;&#26102;&#65292;&#22914;&#20309;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#31163;&#32447;&#25968;&#25454;&#38598;&#26159;&#30001;&#19968;&#20010;&#19987;&#23478;&#29983;&#25104;&#30340;&#65292;&#20294;&#20854;&#33021;&#21147;&#27700;&#24179;&#26410;&#30693;&#65292;&#21363;&#23427;&#19981;&#26159;&#23436;&#32654;&#30340;&#65292;&#20063;&#19981;&#19968;&#23450;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#23398;&#20064;&#20195;&#29702;&#27169;&#25311;&#19987;&#23478;&#20351;&#29992;&#30340;&#34892;&#20026;&#31574;&#30053;&#65288;&#30001;&#33021;&#21147;&#21442;&#25968;&#21442;&#25968;&#21270;&#65289;&#65292;&#22312;&#32047;&#31215;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#38754;&#33021;&#21462;&#24471;&#26126;&#26174;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20197; $\tilde{O}(\sqrt{T})$ &#20026;&#32553;&#25918;&#30340;&#31934;&#30830;&#26377;&#29992;PSRL&#31639;&#27861;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#36825;&#38656;&#35201;&#23545;&#36125;&#21494;&#26031;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#22312;&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#26032;&#39062;&#30340;&#20808;&#39564;&#30456;&#20851;&#36951;&#25022;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#30340;Informed RLSVI&#31639;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#28982;&#21518;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of efficient online reinforcement learning in the infinite horizon setting when there is an offline dataset to start with. We assume that the offline dataset is generated by an expert but with unknown level of competence, i.e., it is not perfect and not necessarily using the optimal policy. We show that if the learning agent models the behavioral policy (parameterized by a competence parameter) used by the expert, it can do substantially better in terms of minimizing cumulative regret, than if it doesn't do that. We establish an upper bound on regret of the exact informed PSRL algorithm that scales as $\tilde{O}(\sqrt{T})$. This requires a novel prior-dependent regret analysis of Bayesian online learning algorithms for the infinite horizon setting. We then propose an approximate Informed RLSVI algorithm that we can interpret as performing imitation learning with the offline dataset, and then performing online learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11523</link><description>&lt;p&gt;
&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Group Preference Optimization: Few-Shot Alignment of Large Language Models. (arXiv:2310.11523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#30340;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#22312;&#22522;&#26412;LLM&#19978;&#21152;&#20837;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#39044;&#27979;&#32676;&#20307;&#20559;&#22909;&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;GPO&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#65292;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#21019;&#24847;&#20889;&#20316;&#65292;&#37117;&#38656;&#35201;&#32454;&#33268;&#20837;&#24494;&#30340;&#20027;&#35266;&#21028;&#26029;&#65292;&#36825;&#20123;&#21028;&#26029;&#22312;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#31639;&#27861;&#22312;&#27599;&#20010;&#32676;&#20307;&#19978;&#23545;&#40784;&#30340;&#25104;&#26412;&#24456;&#39640;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#32780;&#35328;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#32676;&#20307;&#29305;&#23450;&#20559;&#22909;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20559;&#22909;&#20248;&#21270;&#65288;GPO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23558;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#21040;&#20010;&#21035;&#32676;&#20307;&#30340;&#20559;&#22909;&#12290;&#22312;GPO&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;transformer&#27169;&#22359;&#26469;&#25193;&#20805;&#22522;&#26412;LLM&#65292;&#29992;&#20110;&#39044;&#27979;&#32676;&#20307;&#23545;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20559;&#22909;&#12290;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22359;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#33258;&#22238;&#24402;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#22810;&#20010;&#32676;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#22312;&#19977;&#20010;&#20154;&#31867;&#24847;&#35265;&#36866;&#24212;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;GPO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of large language models (LLMs), ranging from chatbots to creative writing, require nuanced subjective judgments that can differ significantly across different groups. Existing alignment algorithms can be expensive to align for each group, requiring prohibitive amounts of group-specific preference data and computation for real-world use cases. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict the preferences of a group for the LLM generations. For few-shot learning, we parameterize this module as an in-context autoregressive transformer and train it via meta-learning on several groups. We empirically validate the efficacy of GPO through rigorous evaluations using LLMs with varied sizes on three human opinion adaptation tasks. These tasks involve adapting to the preferences
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11518</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23545;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11518
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#33258;&#25105;&#23545;&#25239;&#30340;&#20445;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#30697;&#38453;&#21487;&#20998;&#35299;&#24615;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#31639;&#27861;&#33021;&#22815;&#20135;&#29983;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#25239;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#21103;&#26412;&#20132;&#20114;&#26469;&#23398;&#20064;&#12290;&#33258;&#25105;&#23545;&#25239;&#23545;&#20110;&#29983;&#25104;&#22823;&#37327;&#30340;&#23398;&#20064;&#25968;&#25454;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#30340;&#32570;&#28857;&#26159;&#35757;&#32451;&#21518;&#23398;&#20064;&#32773;&#23558;&#38754;&#23545;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#19982;&#36890;&#36807;&#19982;&#33258;&#36523;&#20132;&#20114;&#26102;&#25152;&#26399;&#26395;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#25130;&#28982;&#19981;&#21516;&#12290;&#23545;&#20110;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36798;&#21040;&#32435;&#20160;&#22343;&#34913;&#30340;&#33258;&#25105;&#23545;&#25239;&#33021;&#22815;&#20445;&#35777;&#20135;&#29983;&#23545;&#20219;&#20309;&#35757;&#32451;&#21518;&#23545;&#25163;&#34920;&#29616;&#33391;&#22909;&#30340;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#20154;&#28216;&#25103;&#26469;&#35828;&#27809;&#26377;&#36825;&#26679;&#30340;&#20445;&#35777;&#23384;&#22312;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36817;&#20284;&#20998;&#35299;&#20026;&#19968;&#32452;&#20004;&#20154;&#24120;&#21644;&#28216;&#25103;&#65288;&#31216;&#20026;&#22810;&#30697;&#38453;&#28216;&#25103;&#65289;&#30340;&#28216;&#25103;&#20013;&#65292;&#20854;&#20013;&#20840;&#23616; $\epsilon$-&#32435;&#20160;&#22343;&#34913;&#22312;&#27599;&#20010;&#23376;&#28216;&#25103;&#20013;&#37117;&#19982;&#32435;&#20160;&#22343;&#34913;&#26377;&#26377;&#30028;&#36317;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#33258;&#25105;&#23545;&#25239;&#23398;&#20064;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#23558;&#20135;&#29983;&#19968;&#20010;&#26377;&#30028;&#33030;&#24369;&#24615;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#27425;&#30830;&#23450;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Self-play is a technique for machine learning in multi-agent systems where a learning algorithm learns by interacting with copies of itself. Self-play is useful for generating large quantities of data for learning, but has the drawback that the agents the learner will face post-training may have dramatically different behavior than the learner came to expect by interacting with itself. For the special case of two-player constant-sum games, self-play that reaches Nash equilibrium is guaranteed to produce strategies that perform well against any post-training opponent; however, no such guarantee exists for multi-player games. We show that in games that approximately decompose into a set of two-player constant-sum games (called polymatrix games) where global $\epsilon$-Nash equilibria are boundedly far from Nash-equilibria in each subgame, any no-external-regret algorithm that learns by self-play will produce a strategy with bounded vulnerability. For the first time, our results identify 
&lt;/p&gt;</description></item><item><title>Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.11511</link><description>&lt;p&gt;
Self-RAG: &#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#23398;&#20064;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. (arXiv:2310.11511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11511
&lt;/p&gt;
&lt;p&gt;
Self-RAG&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23427;&#20204;&#25152;&#21253;&#21547;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#22240;&#27492;&#24448;&#24448;&#20250;&#20135;&#29983;&#21547;&#26377;&#20107;&#23454;&#19981;&#20934;&#30830;&#24615;&#30340;&#21709;&#24212;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#22686;&#24378;LM&#30340;&#20020;&#26102;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#21152;&#36873;&#25321;&#22320;&#26816;&#32034;&#24182;&#32467;&#21512;&#19968;&#23450;&#25968;&#37327;&#30340;&#26816;&#32034;&#27573;&#33853;&#65292;&#32780;&#19981;&#32771;&#34385;&#26816;&#32034;&#26159;&#21542;&#24517;&#35201;&#25110;&#27573;&#33853;&#26159;&#21542;&#30456;&#20851;&#65292;&#20250;&#38477;&#20302;LM&#30340;&#22810;&#21151;&#33021;&#24615;&#25110;&#23548;&#33268;&#26080;&#25928;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Self-Reflective Retrieval-Augmented Generation &#65288;Self-RAG&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#33258;&#25105;&#21453;&#24605;&#25552;&#39640;LM&#30340;&#36136;&#37327;&#21644;&#20107;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#29420;&#30340;&#20219;&#24847;LM&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#38656;&#27714;&#33258;&#36866;&#24212;&#22320;&#26816;&#32034;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#30340;&#26631;&#35760;&#65292;&#31216;&#20026;&#21453;&#24605;&#26631;&#35760;&#65292;&#29983;&#25104;&#21644;&#21453;&#24605;&#26816;&#32034;&#30340;&#27573;&#33853;&#21644;&#33258;&#36523;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;CoMPosT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#27169;&#25311;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20854;&#26159;&#21542;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.11501</link><description>&lt;p&gt;
CoMPosT: LLM&#27169;&#25311;&#20013;&#30340;&#28459;&#30011;&#34920;&#29616;&#29305;&#24449;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. (arXiv:2310.11501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11501
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;CoMPosT&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;LLM&#27169;&#25311;&#36827;&#34892;&#29305;&#24449;&#21270;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272;&#20854;&#26159;&#21542;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27169;&#25311;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#22312;&#31038;&#20250;&#31185;&#23398;&#23454;&#39564;&#21644;&#33286;&#35770;&#35843;&#26597;&#31561;&#24773;&#22659;&#20013;&#30340;&#21453;&#24212;&#65292;&#20197;&#25429;&#25417;&#20154;&#31867;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#30830;&#23450;&#30340;&#26041;&#27861;&#26469;&#35752;&#35770;&#25110;&#35780;&#20272;&#36825;&#31181;LLM&#27169;&#25311;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;LLM&#27169;&#25311;&#26159;&#23545;&#20182;&#20204;&#25152;&#27169;&#25311;&#30340;&#20154;&#29289;&#36827;&#34892;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#34920;&#29616;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#20154;&#30340;&#22810;&#32500;&#24615;&#24182;&#24310;&#32493;&#20102;&#21051;&#26495;&#21360;&#35937;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoMPosT&#26694;&#26550;&#65292;&#29992;&#22235;&#20010;&#32500;&#24230;&#26469;&#25551;&#36848;LLM&#27169;&#25311;&#65306;&#35821;&#22659;&#12289;&#27169;&#22411;&#12289;&#35282;&#33394;&#21644;&#20027;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#34913;&#37327;&#24320;&#25918;&#24335;LLM&#27169;&#25311;&#23545;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#20004;&#20010;&#26631;&#20934;&#26469;&#23450;&#20041;&#65306;&#20010;&#24615;&#21270;&#21644;&#22840;&#24352;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;LLM&#27169;&#25311;&#24037;&#20316;&#20013;&#30340;&#22330;&#26223;&#20013;&#22840;&#24352;&#21051;&#26495;&#21270;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;GPT-4&#65292;&#29305;&#23450;&#32676;&#20307;&#65288;&#25919;&#27835;&#21644;&#36793;&#32536;&#32676;&#20307;&#65289;&#21644;
&lt;/p&gt;
&lt;p&gt;
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20799;&#31461;&#38405;&#35835;&#30340;&#23454;&#26102;&#36319;&#36394;&#22120;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#36739;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21046;&#23545;&#40784;&#29983;&#25104;&#35757;&#32451;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#65292;&#24182;&#22312;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11486</link><description>&lt;p&gt;
&#20799;&#31461;&#38405;&#35835;&#23454;&#26102;&#36319;&#36394;&#30340;&#31471;&#21040;&#31471;&#25351;&#38024;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
End-to-End real time tracking of children's reading with pointer network. (arXiv:2310.11486v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20799;&#31461;&#38405;&#35835;&#30340;&#23454;&#26102;&#36319;&#36394;&#22120;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#36739;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#25351;&#38024;&#32593;&#32476;&#21644;&#24378;&#21046;&#23545;&#40784;&#29983;&#25104;&#35757;&#32451;&#20449;&#21495;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#65292;&#24182;&#22312;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#26500;&#24314;&#19968;&#20010;&#29992;&#20110;&#20799;&#31461;&#35821;&#38899;&#30340;&#23454;&#26102;&#38405;&#35835;&#36319;&#36394;&#22120;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#38405;&#35835;&#36319;&#36394;&#22120;&#20027;&#35201;&#22522;&#20110;ASR&#30340;&#32423;&#32852;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#35821;&#38899;&#36319;&#36394;&#30340;&#24310;&#36831;&#26356;&#23569;&#12290;&#25105;&#20204;&#37319;&#29992;&#25351;&#38024;&#32593;&#32476;&#65292;&#30452;&#25509;&#23398;&#20064;&#22312;&#27969;&#24335;&#35821;&#38899;&#20013;&#39044;&#27979;&#19982;&#30495;&#23454;&#25991;&#26412;&#20301;&#32622;&#23545;&#24212;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#25351;&#38024;&#32593;&#32476;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21046;&#23545;&#40784;&#22312;&#35757;&#32451;&#38598;&#19978;&#29983;&#25104;&#30495;&#23454;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#20174;&#32780;&#23545;&#35835;&#20986;&#30340;&#35821;&#38899;&#21644;&#34987;&#35835;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#25506;&#32034;&#19981;&#21516;&#30340;&#24378;&#21046;&#23545;&#40784;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#31070;&#32463;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22312;&#23545;&#40784;&#20934;&#30830;&#24615;&#19978;&#33267;&#23569;&#19982;&#33945;&#29305;&#21033;&#23572;&#24378;&#21046;&#23545;&#40784;&#22120;&#30456;&#24403;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#26356;&#36866;&#21512;&#29992;&#20316;&#25351;&#38024;&#32593;&#32476;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25253;&#21578;&#20102;&#19968;&#20010;&#25104;&#20154;&#35821;&#38899;&#25968;&#25454;&#38598;(TIMIT)&#21644;&#20004;&#20010;&#20799;&#31461;&#35821;&#38899;&#25968;&#25454;&#38598;(CMU Kids&#21644;Reading Races)&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#33021;&#22815;&#20197;87.8%&#30340;&#20934;&#30830;&#29575;&#36319;&#36394;&#25104;&#20154;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we explore how a real time reading tracker can be built efficiently for children's voices. While previously proposed reading trackers focused on ASR-based cascaded approaches, we propose a fully end-to-end model making it less prone to lags in voice tracking. We employ a pointer network that directly learns to predict positions in the ground truth text conditioned on the streaming speech. To train this pointer network, we generate ground truth training signals by using forced alignment between the read speech and the text being read on the training set. Exploring different forced alignment models, we find a neural attention based model is at least as close in alignment accuracy to the Montreal Forced Aligner, but surprisingly is a better training signal for the pointer network. Our results are reported on one adult speech data (TIMIT) and two children's speech datasets (CMU Kids and Reading Races). Our best model can accurately track adult speech with 87.8% accuracy and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.11482</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Class-incremental Learning in the Era of Large Pre-trained Models via Test-Time Adaptation. (arXiv:2310.11482v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20102;&#22312;&#27599;&#20010;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#25345;&#32493;&#23398;&#20064;&#23558;&#31867;&#21035;&#21010;&#20998;&#21040;&#26032;&#20219;&#21153;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#21152;&#24555;&#20102;&#22686;&#37327;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#22240;&#20026;&#39640;&#24230;&#21487;&#20256;&#36755;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#34920;&#31034;&#20351;&#24471;&#22312;&#35843;&#25972;&#19968;&#23567;&#32452;&#21442;&#25968;&#26102;&#65292;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#21453;&#22797;&#24494;&#35843;&#20250;&#30772;&#22351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#24182;&#23548;&#33268;&#36951;&#24536;&#20043;&#21069;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#30452;&#25509;&#22312;&#27979;&#35797;&#23454;&#20363;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22686;&#37327;&#23398;&#20064;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#8221;&#65288;TTACIL&#65289;&#65292;&#23427;&#39318;&#20808;&#22312;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23618;&#24402;&#19968;&#21270;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) is a challenging task that involves continually learning to categorize classes into new tasks without forgetting previously learned information. The advent of the large pre-trained models (PTMs) has fast-tracked the progress in CIL due to the highly transferable PTM representations, where tuning a small set of parameters results in state-of-the-art performance when compared with the traditional CIL methods that are trained from scratch. However, repeated fine-tuning on each task destroys the rich representations of the PTMs and further leads to forgetting previous tasks. To strike a balance between the stability and plasticity of PTMs for CIL, we propose a novel perspective of eliminating training on every new task and instead performing test-time adaptation (TTA) directly on the test instances. Concretely, we propose "Test-Time Adaptation for Class-Incremental Learning" (TTACIL) that first fine-tunes Layer Norm parameters of the PTM on each test instan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21512;&#21516;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#19982;&#21560;&#25910;&#33258;&#21160;&#26426;&#65288;TA&#65289;&#29366;&#24577;&#30340;&#31232;&#30095;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#21560;&#25910;&#24615;&#30340;&#25490;&#38500;&#21644;&#21253;&#21547;&#29366;&#24577;&#65292;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2310.11481</link><description>&lt;p&gt;
&#29992;&#21560;&#25910;&#33258;&#21160;&#26426;&#25193;&#23637;&#30340;&#21512;&#21516;Tsetlin&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Contracting Tsetlin Machine with Absorbing Automata. (arXiv:2310.11481v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21512;&#21516;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#19982;&#21560;&#25910;&#33258;&#21160;&#26426;&#65288;TA&#65289;&#29366;&#24577;&#30340;&#31232;&#30095;&#25193;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;&#21560;&#25910;&#24615;&#30340;&#25490;&#38500;&#21644;&#21253;&#21547;&#29366;&#24577;&#65292;&#21152;&#36895;&#23398;&#20064;&#36807;&#31243;&#24182;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21560;&#25910;Tsetlin&#33258;&#21160;&#26426;&#65288;TA&#65289;&#29366;&#24577;&#30340;&#31232;&#30095;Tsetlin&#26426;&#22120;&#65288;TM&#65289;&#12290;&#31616;&#21333;&#26469;&#35828;&#65292;&#27599;&#20010;&#26465;&#27454;&#25991;&#23383;&#30340;TA&#20855;&#26377;&#21560;&#25910;&#24615;&#30340;&#25490;&#38500;&#65288;Exclude&#65289;&#29366;&#24577;&#21644;&#21560;&#25910;&#24615;&#30340;&#21253;&#21547;&#65288;Include&#65289;&#29366;&#24577;&#65292;&#20351;&#24471;&#23398;&#20064;&#26041;&#26696;&#20855;&#26377;&#21560;&#25910;&#24615;&#32780;&#19981;&#26159;&#36941;&#21382;&#24615;&#12290;&#24403;TA&#36798;&#21040;&#19968;&#20010;&#21560;&#25910;&#29366;&#24577;&#26102;&#65292;&#23427;&#23558;&#27704;&#36828;&#19981;&#20250;&#20877;&#31163;&#24320;&#35813;&#29366;&#24577;&#12290;&#22914;&#26524;&#21560;&#25910;&#29366;&#24577;&#26159;&#25490;&#38500;&#29366;&#24577;&#65292;&#33258;&#21160;&#26426;&#21644;&#25991;&#23383;&#37117;&#21487;&#20197;&#20174;&#36827;&#19968;&#27493;&#32771;&#34385;&#20013;&#21024;&#38500;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#35813;&#25991;&#23383;&#22312;&#35813;&#26465;&#27454;&#20013;&#23558;&#27704;&#36828;&#19981;&#20250;&#21442;&#19982;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#21560;&#25910;&#29366;&#24577;&#26159;&#21253;&#21547;&#29366;&#24577;&#65292;&#35813;&#25991;&#23383;&#23558;&#20316;&#20026;&#26465;&#27454;&#30340;&#27704;&#20037;&#37096;&#20998;&#23384;&#20648;&#65292;&#32780;TA&#23558;&#34987;&#20002;&#24323;&#12290;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25968;&#25454;&#32467;&#26500;&#36890;&#36807;&#19977;&#20010;&#21160;&#20316;&#21015;&#34920;&#26469;&#25903;&#25345;&#36825;&#20123;&#26356;&#26032;&#65306;&#21560;&#25910;&#30340;&#21253;&#21547;&#65288;Absorbed Include&#65289;&#12289;&#21253;&#21547;&#65288;Include&#65289;&#21644;&#25490;&#38500;&#65288;Exclude&#65289;&#12290;&#36890;&#36807;&#26356;&#26032;&#36825;&#20123;&#21015;&#34920;&#65292;TM&#38543;&#30528;&#25991;&#23383;&#21450;&#20854;TA&#30340;&#25764;&#22238;&#21464;&#24471;&#36234;&#26469;&#36234;&#23567;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#36807;&#31243;&#21152;&#36895;&#65292;&#23548;&#33268;&#26356;&#24555;&#30340;&#23398;&#20064;&#21644;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a sparse Tsetlin Machine (TM) with absorbing Tsetlin Automata (TA) states. In brief, the TA of each clause literal has both an absorbing Exclude- and an absorbing Include state, making the learning scheme absorbing instead of ergodic. When a TA reaches an absorbing state, it will never leave that state again. If the absorbing state is an Exclude state, both the automaton and the literal can be removed from further consideration. The literal will as a result never participates in that clause. If the absorbing state is an Include state, on the other hand, the literal is stored as a permanent part of the clause while the TA is discarded. A novel sparse data structure supports these updates by means of three action lists: Absorbed Include, Include, and Exclude. By updating these lists, the TM gets smaller and smaller as the literals and their TA withdraw. In this manner, the computation accelerates during learning, leading to faster learning and less energy cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP)&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;&#26469;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;ASP&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.11478</link><description>&lt;p&gt;
ASP: &#29992;&#20110;&#39640;&#25928;AutoML&#30340;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ASP: Automatic Selection of Proxy dataset for efficient AutoML. (arXiv:2310.11478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP)&#65292;&#36890;&#36807;&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;&#26469;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;ASP&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#33719;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#21644;&#22810;&#26679;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;, &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;, &#36825;&#20063;&#32473;&#35745;&#31639;&#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36127;&#25285;, &#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#37327;&#19982;&#35757;&#32451;&#26102;&#38388;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;, &#19968;&#20010;&#33391;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#37325;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#32467;&#26500;&#35774;&#35745;&#21644;&#36229;&#21442;&#25968;, &#21363;&#20351;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;, &#36825;&#21487;&#33021;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#36873;&#25321;&#20195;&#29702;&#25968;&#25454;&#38598;&#26694;&#26550; (ASP), &#26088;&#22312;&#22312;&#27599;&#20010;epoch&#21160;&#24577;&#22320;&#25214;&#21040;&#20449;&#24687;&#20016;&#23500;&#30340;&#20195;&#29702;&#23376;&#38598;, &#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#24182;&#33410;&#30465;AutoML&#22788;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;CIFAR10&#12289;CIFAR100&#12289;ImageNet16-120&#21644;ImageNet-1k&#19978;&#39564;&#35777;&#20102;ASP&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;, &#36890;&#36807;&#23545;&#19981;&#21516;&#20844;&#20849;&#27169;&#22411;&#22522;&#20934;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ASP&#21487;&#20197;&#33719;&#24471;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have gained great success due to the increasing amounts of data, and diverse effective neural network designs. However, it also brings a heavy computing burden as the amount of training data is proportional to the training time. In addition, a well-behaved model requires repeated trials of different structure designs and hyper-parameters, which may take a large amount of time even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms and neural architecture search (NAS) algorithms. In this paper, we propose an Automatic Selection of Proxy dataset framework (ASP) aimed to dynamically find the informative proxy subsets of training data at each epoch, reducing the training data size as well as saving the AutoML processing time. We verify the effectiveness and generalization of ASP on CIFAR10, CIFAR100, ImageNet16-120, and ImageNet-1k, across various public model benchmarks. The experiment results show that ASP can obtain better results than other 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11477</link><description>&lt;p&gt;
Robust-MBFD&#65306;&#20351;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#30340;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Robust-MBFD: A Robust Deep Learning System for Motor Bearing Faults Detection Using Multiple Deep Learning Training Strategies and A Novel Double Loss Function. (arXiv:2310.11477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#29992;&#20110;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65292;&#37319;&#29992;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#21644;&#19968;&#31181;&#26032;&#30340;&#21452;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#19981;&#21516;&#31995;&#32479;&#24182;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#23545;&#21508;&#31181;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#30005;&#26426;&#36724;&#25215;&#25925;&#38556;&#26816;&#27979;&#65288;MBFD&#65289;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25391;&#21160;&#20449;&#21495;&#35782;&#21035;&#30005;&#26426;&#36724;&#25215;&#30340;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MBFD&#31995;&#32479;&#65292;&#20998;&#21035;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36825;&#19977;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#23545;&#25552;&#20986;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#24182;&#25214;&#20986;&#20102;&#36866;&#29992;&#20110;MBFD&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#32654;&#22269;&#26426;&#26800;&#25925;&#38556;&#39044;&#38450;&#25216;&#26415;&#21327;&#20250;&#65288;MFPT&#65289;&#12289;&#20975;&#26031;&#35199;&#20648;&#22823;&#23398;&#36724;&#25215;&#20013;&#24515;&#65288;CWRU&#65289;&#21644;&#24085;&#24503;&#21338;&#24681;&#22823;&#23398;&#30340;&#30005;&#26426;&#39537;&#21160;&#31995;&#32479;&#36724;&#25215;&#25439;&#20260;&#29366;&#24577;&#30417;&#27979;&#31561;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive analysis of motor bearing fault detection (MBFD), which involves the task of identifying faults in a motor bearing based on its vibration. To this end, we first propose and evaluate various machine learning based systems for the MBFD task. Furthermore, we propose three deep learning based systems for the MBFD task, each of which explores one of the following training strategies: supervised learning, semi-supervised learning, and unsupervised learning. The proposed machine learning based systems and deep learning based systems are evaluated, compared, and then they are used to identify the best model for the MBFD task. We conducted extensive experiments on various benchmark datasets of motor bearing faults, including those from the American Society for Mechanical Failure Prevention Technology (MFPT), Case Western Reserve University Bearing Center (CWRU), and the Condition Monitoring of Bearing Damage in Electromechanical Drive Systems from Paderborn U
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#20027;&#35201;&#20171;&#32461;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#20013;&#20171;&#32461;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#21450;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.11470</link><description>&lt;p&gt;
&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classic machine learning methods. (arXiv:2310.11470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#20027;&#35201;&#20171;&#32461;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#12290;&#20854;&#20013;&#20171;&#32461;&#20102;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20197;&#21450;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#19968;&#31456;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#31456;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#37117;&#29992;&#20110;&#20171;&#32461;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#26368;&#36817;&#37051;&#26041;&#27861;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22522;&#20110;&#26641;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#36807;&#25311;&#21512;&#38382;&#39064;&#20197;&#21450;&#20811;&#26381;&#36807;&#25311;&#21512;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#27010;&#36848;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#29992;&#20110;&#32858;&#31867;&#21644;&#38477;&#32500;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11467</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#65306;&#25972;&#21512;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Binary Code Comment Quality Classification: Integrating Generative AI for Improved Accuracy. (arXiv:2310.11467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#25913;&#36827;&#20108;&#36827;&#21046;&#20195;&#30721;&#27880;&#37322;&#36136;&#37327;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;9048&#23545;&#29992;C&#35821;&#35328;&#32534;&#20889;&#30340;&#20195;&#30721;&#21644;&#27880;&#37322;&#65292;&#27599;&#23545;&#37117;&#34987;&#27880;&#37322;&#20026;&#8220;&#26377;&#29992;&#8221;&#25110;&#8220;&#26080;&#29992;&#8221;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#29983;&#25104;&#20195;&#30721;&#21644;&#27880;&#37322;&#23545;&#65292;&#24182;&#23545;&#36825;&#20123;&#29983;&#25104;&#30340;&#23545;&#36827;&#34892;&#26631;&#35760;&#20197;&#25351;&#31034;&#20854;&#23454;&#29992;&#24615;&#12290;&#27492;&#39033;&#24037;&#20316;&#30340;&#25104;&#26524;&#21253;&#25324;&#20004;&#20010;&#20998;&#31867;&#27169;&#22411;&#65306;&#19968;&#20010;&#21033;&#29992;&#21407;&#22987;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#26032;&#29983;&#25104;&#30340;&#20195;&#30721;&#27880;&#37322;&#23545;&#21644;&#26631;&#35760;&#30340;&#25193;&#20805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report focuses on enhancing a binary code comment quality classification model by integrating generated code and comment pairs, to improve model accuracy. The dataset comprises 9048 pairs of code and comments written in the C programming language, each annotated as "Useful" or "Not Useful." Additionally, code and comment pairs are generated using a Large Language Model Architecture, and these generated pairs are labeled to indicate their utility. The outcome of this effort consists of two classification models: one utilizing the original dataset and another incorporating the augmented dataset with the newly generated code comment pairs and labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11466</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#19977;&#32500;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#29289;&#23398;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#21644;&#20122;&#32454;&#32990;&#23450;&#20301;&#20272;&#35745;&#65289;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;AlphaFold2&#65289;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26500;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#26126;&#26174;&#19979;&#38477;&#12290;&#34429;&#28982;&#31867;&#20284;&#29616;&#35937;&#24050;&#32463;&#22312;&#19968;&#33324;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#39044;&#27979;&#30340;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11465</link><description>&lt;p&gt;
BaitBuster-Bangla:&#19968;&#20010;&#21253;&#21547;&#22810;&#29305;&#24449;&#21644;&#22810;&#27169;&#24577;&#20998;&#26512;&#30340;&#29992;&#20110;&#23391;&#21152;&#25289;&#35821;Clickbait&#26816;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BaitBuster-Bangla: A Comprehensive Dataset for Clickbait Detection in Bangla with Multi-Feature and Multi-Modal Analysis. (arXiv:2310.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#24314;&#27169;clickbait&#29616;&#35937;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#24182;&#19988;&#21487;&#20197;&#24320;&#21457;&#20986;&#26356;&#22797;&#26434;&#30340;&#36328;&#35821;&#35328;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;YouTube clickbait&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20351;&#29992;YouTube API&#21644;Python&#32593;&#32476;&#33258;&#21160;&#21270;&#26694;&#26550;&#33258;&#21160;&#25910;&#38598;&#20102;253,070&#20010;&#25968;&#25454;&#28857;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;58&#20010;&#23391;&#21152;&#25289;&#35821;YouTube&#39057;&#36947;&#30340;&#21333;&#20010;&#35270;&#39057;&#30340;18&#20010;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#20998;&#31867;&#20026;&#20803;&#25968;&#25454;&#12289;&#20027;&#35201;&#20869;&#23481;&#12289;&#21442;&#19982;&#32479;&#35745;&#21644;&#26631;&#31614;&#12290;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#39044;&#22788;&#29702;&#65292;&#21435;&#22122;&#22768;&#12289;&#21435;&#37325;&#22797;&#21644;&#21435;&#20559;&#24046;&#65292;&#30830;&#20445;&#20102;&#26080;&#20559;&#20506;&#21644;&#21487;&#38752;&#30340;&#20998;&#26512;&#12290;&#20316;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#19988;&#26368;&#24378;&#22823;&#30340;&#23391;&#21152;&#25289;&#35821;clickbait&#35821;&#26009;&#24211;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20182;&#20204;&#24076;&#26395;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25512;&#36827;clickbait&#29616;&#35937;&#30340;&#24314;&#27169;&#12290;&#23427;&#30340;&#22810;&#27169;&#24577;&#24615;&#36136;&#20351;&#24471;&#21487;&#20197;&#23545;clickbait&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#28085;&#30422;&#20869;&#23481;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#35821;&#35328;&#32500;&#24230;&#65292;&#20197;&#24320;&#21457;&#20855;&#26377;&#36328;&#35821;&#35328;&#24212;&#29992;&#30340;&#26356;&#22797;&#26434;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a large multi-modal Bangla YouTube clickbait dataset consisting of 253,070 data points collected through an automated process using the YouTube API and Python web automation frameworks. The dataset contains 18 diverse features categorized into metadata, primary content, engagement statistics, and labels for individual videos from 58 Bangla YouTube channels. A rigorous preprocessing step has been applied to denoise, deduplicate, and remove bias from the features, ensuring unbiased and reliable analysis. As the largest and most robust clickbait corpus in Bangla to date, this dataset provides significant value for natural language processing and data science researchers seeking to advance modeling of clickbait phenomena in low-resource languages. Its multi-modal nature allows for comprehensive analyses of clickbait across content, user interactions, and linguistic dimensions to develop more sophisticated detection methods with cross-linguistic applications.
&lt;/p&gt;</description></item><item><title>GPT-4&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#35745;&#31639;&#36719;&#20214;&#20043;&#38388;&#25509;&#21475;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#31185;&#23398;&#36719;&#20214;&#30340;&#21487;&#29992;&#24615;&#21644;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11458</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#35745;&#31639;&#36719;&#20214;&#20043;&#38388;&#30340;&#25509;&#21475;&#65306;&#25552;&#39640;&#21487;&#29992;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an interface between researchers and computational software: improving usability and reproducibility. (arXiv:2310.11458v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11458
&lt;/p&gt;
&lt;p&gt;
GPT-4&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#35745;&#31639;&#36719;&#20214;&#20043;&#38388;&#25509;&#21475;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#31185;&#23398;&#36719;&#20214;&#30340;&#21487;&#29992;&#24615;&#21644;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#35299;&#26512;&#21644;&#29702;&#35299;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#35821;&#35328;&#30340;&#33021;&#21147;&#20351;&#23427;&#20204;&#25104;&#20026;&#24378;&#22823;&#30340;&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#31561;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#34987;&#20805;&#20998;&#35760;&#24405;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;GPT-4 LLM&#22312;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#20013;&#25913;&#21892;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#33021;&#21147;&#65306;i) &#19982;&#20351;&#29992;&#33258;&#23450;&#20041;&#36755;&#20837;&#35821;&#35328;&#30456;&#20851;&#30340;&#31185;&#23398;&#36719;&#20214;&#37319;&#29992;&#30340;&#39640;&#38376;&#27099;&#65292;&#20197;&#21450;ii) &#30001;&#20110;&#23545;&#27169;&#25311;&#26041;&#27861;&#30340;&#25551;&#36848;&#32454;&#33410;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#24050;&#21457;&#34920;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#24046;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#36719;&#20214;&#8212;&#8212;&#22823;&#35268;&#27169;&#21407;&#23376;/&#20998;&#23376;&#24182;&#34892;&#27169;&#25311;&#22120;(LAMMPS)&#65292;&#24182;&#37327;&#21270;GPT-4&#20174;&#33521;&#25991;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#30340;&#36755;&#20837;&#25991;&#20214;&#30340;&#23454;&#29992;&#24615;&#20197;&#21450;&#20174;&#36755;&#20837;&#25991;&#20214;&#29983;&#25104;&#35745;&#31639;&#20219;&#21153;&#30340;&#35814;&#32454;&#25551;&#36848;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-4&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#30340;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are playing an increasingly important role in science and engineering. For example, their ability to parse and understand human and computer languages makes them powerful interpreters and their use in applications like code generation are well-documented. We explore the ability of the GPT-4 LLM to ameliorate two major challenges in computational materials science: i) the high barriers for adoption of scientific software associated with the use of custom input languages, and ii) the poor reproducibility of published results due to insufficient details in the description of simulation methods. We focus on a widely used software for molecular dynamics simulations, the Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS), and quantify the usefulness of input files generated by GPT-4 from task descriptions in English and its ability to generate detailed descriptions of computational tasks from input files. We find that GPT-4 can generate correct an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11211</link><description>&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#20102;&#20195;&#29702;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#24046;&#36317;&#12290;&#36825;&#20010;&#24046;&#36317;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#33021;&#21542;&#36866;&#24403;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#24182;&#23454;&#29616;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#65292;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#28041;&#21450;&#20844;&#24179;&#24615;&#23450;&#20041;&#30340;&#20195;&#29702;&#20989;&#25968;&#65292;&#24182;&#35299;&#20915;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#36825;&#31181;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#20026;&#20102;&#28145;&#20837;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20197;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#23450;&#20041;&#8212;&#8212;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#8212;&#8212;&#20026;&#20363;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20844;&#24179;&#24615;&#23450;&#20041;&#21644;&#20844;&#24179;&#24615;&#20195;&#29702;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20195;&#29702;-&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#36825;&#20010;"&#24046;&#36317;"&#30452;&#25509;&#20915;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20989;&#25968;&#26159;&#21542;&#36866;&#21512;&#26367;&#20195;&#19968;&#20010;&#20844;&#24179;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#36825;&#20010;"&#24046;&#36317;"&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#28608;&#21457;&#20102;&#25105;&#20204;&#30340;&#20852;&#36259;&#65292;&#34920;&#26126;&#26080;&#38480;&#21046;&#30340;&#20195;&#29702;&#20989;&#25968;&#23558;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#36828;&#31163;&#30340;&#28857;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#65292;&#24182;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#21307;&#30103;&#27835;&#30103;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.11147</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#30340;&#27969;&#20307;&#27969;&#21160;&#27979;&#37327;&#20013;&#25581;&#31034;&#22721;&#21098;&#20999;&#24212;&#21147;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Uncovering wall-shear stress dynamics from neural-network enhanced fluid flow measurements. (arXiv:2310.11147v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#65292;&#24182;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#21307;&#30103;&#27835;&#30103;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#27969;&#20307;&#22312;&#29289;&#20307;&#38468;&#36817;&#25110;&#20869;&#37096;&#31227;&#21160;&#30340;&#28237;&#27969;&#30340;&#25705;&#25830;&#38459;&#21147;&#22312;&#20132;&#36890;&#36816;&#36755;&#12289;&#20844;&#29992;&#20107;&#19994;&#22522;&#30784;&#35774;&#26045;&#12289;&#33021;&#28304;&#25216;&#26415;&#21644;&#20154;&#31867;&#20581;&#24247;&#31561;&#39046;&#22495;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20316;&#20026;&#21098;&#20999;&#24341;&#36215;&#30340;&#25705;&#25830;&#21147;&#30340;&#30452;&#25509;&#27979;&#37327;&#65292;&#20934;&#30830;&#39044;&#27979;&#22721;&#21098;&#20999;&#24212;&#21147;&#21487;&#20197;&#20419;&#36827;&#27665;&#33322;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#12289;&#36164;&#28304;&#20445;&#25252;&#21644;&#30899;&#20013;&#21644;&#65292;&#20197;&#21450;&#25913;&#36827;&#34880;&#31649;&#30142;&#30149;&#21644;&#30284;&#30151;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#23613;&#31649;&#23545;&#29616;&#20195;&#31038;&#20250;&#22914;&#27492;&#37325;&#35201;&#65292;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#36275;&#22815;&#30340;&#23454;&#39564;&#26041;&#27861;&#26469;&#25429;&#25417;&#30636;&#26102;&#22721;&#21098;&#20999;&#24212;&#21147;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;&#26041;&#27861;&#65292;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#30693;&#35782;&#30340;&#28145;&#24230;&#20809;&#27969;&#20272;&#35745;&#22120;&#20174;&#27969;&#21160;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#36895;&#24230;&#21644;&#22721;&#21098;&#20999;&#24212;&#21147;&#22330;&#12290;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25512;&#23548;&#27969;&#21160;&#37327;&#30340;&#26377;&#25928;&#24615;&#21644;&#29289;&#29702;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Friction drag from a turbulent fluid moving past or inside an object plays a crucial role in domains as diverse as transportation, public utility infrastructure, energy technology, and human health. As a direct measure of the shear-induced friction forces, an accurate prediction of the wall-shear stress can contribute to sustainability, conservation of resources, and carbon neutrality in civil aviation as well as enhanced medical treatment of vascular diseases and cancer. Despite such importance for our modern society, we still lack adequate experimental methods to capture the instantaneous wall-shear stress dynamics. In this contribution, we present a holistic approach that derives velocity and wall-shear stress fields with impressive spatial and temporal resolution from flow measurements using a deep optical flow estimator with physical knowledge. The validity and physical correctness of the derived flow quantities is demonstrated with synthetic and real-world experimental data cover
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#20102;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11029</link><description>&lt;p&gt;
&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65306;&#19979;&#19968;&#20195;&#22320;&#29702;&#31354;&#38388;GPT&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Core Building Blocks: Next Gen Geo Spatial GPT Application. (arXiv:2310.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#30340;&#26680;&#24515;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#29983;&#25104;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#20102;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MapGPT&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#19982;&#31354;&#38388;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;MapGPT&#26088;&#22312;&#36890;&#36807;&#24378;&#35843;&#30456;&#20851;&#30340;&#26680;&#24515;&#26500;&#24314;&#27169;&#22359;&#65292;&#24357;&#21512;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#31354;&#38388;&#25968;&#25454;&#20998;&#26512;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#32467;&#21512;LLMs&#21644;&#22320;&#29702;&#31354;&#38388;&#20998;&#26512;&#30340;&#20248;&#21183;&#65292;MapGPT&#33021;&#22815;&#23545;&#22522;&#20110;&#20301;&#32622;&#30340;&#26597;&#35810;&#36827;&#34892;&#26356;&#31934;&#30830;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21709;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24378;&#35843;&#22312;&#31354;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#19978;&#26500;&#24314;LLMs&#65292;&#21033;&#29992;&#29305;&#23450;&#20110;&#31354;&#38388;&#20449;&#24687;&#30340;&#26631;&#35760;&#21270;&#21644;&#21521;&#37327;&#34920;&#31034;&#12290;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#29983;&#25104;&#31354;&#38388;&#21521;&#37327;&#34920;&#31034;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;MapGPT&#20869;&#30340;&#35745;&#31639;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#25191;&#34892;&#22320;&#29702;&#31354;&#38388;&#35745;&#31639;&#24182;&#33719;&#24471;&#21487;&#35270;&#21270;&#36755;&#20986;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;&#22522;&#20110;&#22320;&#29702;&#31354;&#38388;&#30340;GPT&#24212;&#29992;&#25152;&#38656;&#35201;&#30340;&#26680;&#24515;&#27169;&#22359;&#30340;&#26041;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes MapGPT which is a novel approach that integrates the capabilities of language models, specifically large language models (LLMs), with spatial data processing techniques. This paper introduces MapGPT, which aims to bridge the gap between natural language understanding and spatial data analysis by highlighting the relevant core building blocks. By combining the strengths of LLMs and geospatial analysis, MapGPT enables more accurate and contextually aware responses to location-based queries. The proposed methodology highlights building LLMs on spatial and textual data, utilizing tokenization and vector representations specific to spatial information. The paper also explores the challenges associated with generating spatial vector representations. Furthermore, the study discusses the potential of computational capabilities within MapGPT, allowing users to perform geospatial computations and obtain visualized outputs. Overall, this research paper presents the building bl
&lt;/p&gt;</description></item><item><title>&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2310.10780</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;
&lt;/p&gt;
&lt;p&gt;
Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10780
&lt;/p&gt;
&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#25581;&#24320;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#36890;&#36807;&#35780;&#20272;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#30830;&#23450;&#20102;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#12289;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#21521;&#20197;&#21450;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#20381;&#36182;&#26085;&#30410;&#22686;&#38271;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#21644;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20013;&#27602;&#21518;&#38376;&#25915;&#20987;&#30001;&#20110;&#20854;&#38544;&#34109;&#24615;&#21644;&#28508;&#22312;&#30340;&#20005;&#37325;&#21518;&#26524;&#32780;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31867;&#25915;&#20987;&#28041;&#21450;&#23558;&#35302;&#21457;&#22120;&#23884;&#20837;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#22312;&#23384;&#22312;&#27963;&#21160;&#35302;&#21457;&#22120;&#26102;&#24341;&#36215;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#22312;&#27809;&#26377;&#35302;&#21457;&#22120;&#30340;&#24773;&#20917;&#19979;&#32500;&#25345;&#27491;&#24120;&#21151;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#20026;&#21463;&#25439;&#27169;&#22411;&#22312;&#28165;&#27905;&#21644;&#21518;&#38376;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#24314;&#31435;&#20005;&#26684;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#35780;&#20272;&#20102;&#20219;&#20309;&#21253;&#21547;&#24658;&#23450;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24320;&#21457;&#30340;&#29702;&#35770;&#22238;&#31572;&#20102;&#19968;&#31995;&#21015;&#22522;&#26412;&#20294;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;1&#65289;&#21518;&#38376;&#25915;&#20987;&#25104;&#21151;&#30340;&#20915;&#23450;&#22240;&#32032;&#26159;&#20160;&#20040;&#65292;&#65288;2&#65289;&#26368;&#26377;&#25928;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#21521;&#26159;&#20160;&#20040;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#20154;&#31867;&#35302;&#21457;&#22120;&#20309;&#26102;&#20250;&#25104;&#21151;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#29702;&#35299;...
&lt;/p&gt;
&lt;p&gt;
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2310.10706</link><description>&lt;p&gt;
&#21457;&#25381;LLMs&#30340;&#33021;&#37327;&#65306;&#36890;&#36807;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#35270;&#35282;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of LLMs: Evaluating Human-AI text Co-Creation through the Lens of News Headline Generation. (arXiv:2310.10706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#33021;&#22815;&#24102;&#26469;&#26368;&#22823;&#30340;&#25928;&#30410;&#65292;&#24182;&#19988;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#24182;&#19981;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#20889;&#20316;&#65292;&#24182;&#20102;&#35299;&#19982;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#22914;&#20309;&#24433;&#21709;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#21644;&#20449;&#20219;&#24230;&#65292;&#25105;&#20204;&#22312;LLM&#36741;&#21161;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#30340;&#32972;&#26223;&#19979;&#27604;&#36739;&#20102;&#24120;&#35265;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#24341;&#23548;&#31995;&#32479;&#65292;&#20174;&#31995;&#32479;&#36755;&#20986;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#21518;&#26399;&#32534;&#36753;&#36755;&#20986;&#65289;&#12290;&#23613;&#31649;LLMs&#21333;&#29420;&#21487;&#20197;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;&#26032;&#38395;&#26631;&#39064;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#20154;&#31867;&#30340;&#25511;&#21046;&#26159;&#38656;&#35201;&#30340;&#65292;&#20197;&#20462;&#22797;&#19981;&#21487;&#21462;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#21508;&#31181;&#20132;&#20114;&#26041;&#27861;&#20013;&#65292;&#24341;&#23548;&#21644;&#36873;&#25321;&#27169;&#22411;&#36755;&#20986;&#22686;&#21152;&#20102;&#26368;&#22810;&#30340;&#25928;&#30410;&#65292;&#20195;&#20215;&#26368;&#20302;&#65288;&#26102;&#38388;&#21644;&#31934;&#21147;&#65289;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#21327;&#21161;&#24182;&#27809;&#26377;&#25439;&#23475;&#21442;&#19982;&#32773;&#23545;&#25511;&#21046;&#30340;&#24863;&#30693;&#65292;&#19982;&#33258;&#30001;&#32534;&#36753;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.10537</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#25193;&#23637;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31364;&#20301;&#23485;&#25968;&#25454;&#26684;&#24335;&#23545;&#20110;&#38477;&#20302;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#27599;&#20010;&#22359;&#30340;&#32553;&#25918;&#22240;&#23376;&#19982;&#31364;&#28014;&#28857;&#21644;&#25972;&#25968;&#31867;&#22411;&#30456;&#32467;&#21512;&#30340;&#24494;&#25193;&#23637;&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#65292;&#20197;&#28385;&#36275;&#30828;&#20214;&#25928;&#29575;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#25705;&#25830;&#20043;&#38388;&#30340;&#31454;&#20105;&#38656;&#27714;&#12290;&#23545;&#20110;AI&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;MX&#25968;&#25454;&#26684;&#24335;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#26102;&#29992;&#25143;&#25705;&#25830;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#26080;&#38656;&#20462;&#25913;&#35757;&#32451;&#37197;&#26041;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#35757;&#32451;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#20110;8&#20301;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#28176;&#21464;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.10449</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;: MPT-7b-instruct&#12289;Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models. (arXiv:2310.10449v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;MPT-7b-instruct, Falcon-7b-instruct&#21644;OpenAI Chat-GPT&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#25991;&#26412;&#25688;&#35201;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;text-davinci-003&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#21644;&#20869;&#23481;&#29983;&#25104;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#21319;&#25688;&#35201;&#25216;&#26415;&#26041;&#38754;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;MPT-7b-instruct&#65292;falcon-7b-instruct&#21644;OpenAI ChatGPT text-davinci-003&#27169;&#22411;&#65289;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;&#21452;&#35821;&#35780;&#20272;&#34913;&#37327;&#65288;BLEU&#65289;&#20998;&#25968;&#65292;&#38754;&#21521;&#22238;&#24518;&#30340;&#35270;&#35282;&#35780;&#20272;&#65288;ROUGE&#65289;&#20998;&#25968;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#20998;&#25968;&#31561;&#24191;&#27867;&#25509;&#21463;&#30340;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;&#26681;&#25454;&#23454;&#39564;&#65292;text-davinci-003&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#26412;&#27425;&#30740;&#31350;&#28041;&#21450;CNN Daily Mail&#21644;XSum&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#30446;&#26631;&#26159;&#20840;&#38754;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large
&lt;/p&gt;</description></item><item><title>TRANSOM&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#39640;&#25928;&#23481;&#38169;&#31995;&#32479;&#65292;&#21253;&#25324;&#35757;&#32451;&#27969;&#27700;&#32447;&#33258;&#21160;&#23481;&#38169;&#21644;&#24674;&#22797;&#26426;&#21046;&#65288;TOL&#65289;&#12289;&#35757;&#32451;&#20219;&#21153;&#22810;&#32500;&#24230;&#24230;&#37327;&#30340;&#33258;&#21160;&#20915;&#31574;&#21644;&#35843;&#25972;&#26426;&#21046;&#65288;ADAM&#65289;&#20197;&#21450;&#22312;&#32676;&#38598;&#24674;&#22797;&#20043;&#38388;&#33258;&#21160;&#20915;&#31574;&#21644;&#31649;&#29702;&#20219;&#21153;&#31227;&#21160;&#30340;&#27169;&#22411;&#65288;RMM&#65289;&#12290;</title><link>http://arxiv.org/abs/2310.10046</link><description>&lt;p&gt;
TRANSOM:&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#39640;&#25928;&#23481;&#38169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TRANSOM: An Efficient Fault-Tolerant System for Training LLMs. (arXiv:2310.10046v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10046
&lt;/p&gt;
&lt;p&gt;
TRANSOM&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#39640;&#25928;&#23481;&#38169;&#31995;&#32479;&#65292;&#21253;&#25324;&#35757;&#32451;&#27969;&#27700;&#32447;&#33258;&#21160;&#23481;&#38169;&#21644;&#24674;&#22797;&#26426;&#21046;&#65288;TOL&#65289;&#12289;&#35757;&#32451;&#20219;&#21153;&#22810;&#32500;&#24230;&#24230;&#37327;&#30340;&#33258;&#21160;&#20915;&#31574;&#21644;&#35843;&#25972;&#26426;&#21046;&#65288;ADAM&#65289;&#20197;&#21450;&#22312;&#32676;&#38598;&#24674;&#22797;&#20043;&#38388;&#33258;&#21160;&#20915;&#31574;&#21644;&#31649;&#29702;&#20219;&#21153;&#31227;&#21160;&#30340;&#27169;&#22411;&#65288;RMM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;chatGPT&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#34920;&#26126;&#25317;&#26377;&#25968;&#30334;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#21442;&#25968;&#30340;LLMs&#23558;&#32487;&#32493;&#25913;&#21464;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;GPU&#38598;&#32676;&#21644;&#25345;&#32493;&#25968;&#26376;&#30340;&#35757;&#32451;&#21608;&#26399;&#12290;&#22312;&#36825;&#26679;&#24222;&#22823;&#30340;&#38598;&#32676;&#20013;&#65292;&#30001;&#20110;&#30828;&#20214;&#21644;&#36719;&#20214;&#25925;&#38556;&#65292;&#20445;&#25345;&#19981;&#20013;&#26029;&#21644;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21464;&#24471;&#24322;&#24120;&#22256;&#38590;&#12290;&#30456;&#24403;&#22810;&#30340;&#35757;&#32451;&#26102;&#38388;&#34987;&#29992;&#20110;&#20219;&#21153;&#26816;&#26597;&#28857;&#30340;&#20445;&#23384;&#21644;&#21152;&#36733;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#37325;&#21551;&#65292;&#23548;&#33268;&#25972;&#20307;&#35757;&#32451;&#25928;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TRANSOM&#30340;&#26032;&#22411;&#23481;&#38169;&#22823;&#35268;&#27169;&#27169;&#22411;&#35757;&#32451;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;:&#35757;&#32451;&#27969;&#27700;&#32447;&#33258;&#21160;&#23481;&#38169;&#21644;&#24674;&#22797;&#26426;&#21046;&#65288;TOL&#65289;&#12289;&#35757;&#32451;&#20219;&#21153;&#22810;&#32500;&#24230;&#24230;&#37327;&#30340;&#33258;&#21160;&#20915;&#31574;&#21644;&#35843;&#25972;&#26426;&#21046;&#65288;ADAM&#65289;&#20197;&#21450;&#22312;&#32676;&#38598;&#24674;&#22797;&#20043;&#38388;&#33258;&#21160;&#20915;&#31574;&#21644;&#31649;&#29702;&#20219;&#21153;&#31227;&#21160;&#30340;&#27169;&#22411;&#65288;RMM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by chatGPT, have made significant strides in various domains, indicating that LLMs with hundreds of billions or even trillions of parameters will continue to revolutionize our daily lives. However, training such super-large-scale models demands even more powerful GPU clusters and extended training periods spanning months. Maintaining uninterrupted and long-duration training has become exceptionally challenging due to hardware and software failures in these extensive clusters. A substantial amount of training time is devoted to tasks checkpointing saving and loading, ananomaly detection and restarts, leading to a notable reduction in overall training efficiency.To address these challenges, we introduce novel fault-tolerant large-scale model training system named TRANSOM. This system comprises three integral components: the training pipeline automatic fault tolerance and recovery mechanism (TOL), the training task multi-dimensional metric automat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09978</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Chinese Painting Style Transfer Using Deep Generative Models. (arXiv:2310.09978v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#39118;&#26684;&#36716;&#25442;&#26088;&#22312;&#22312;&#20445;&#30041;&#22270;&#20687;&#20869;&#23481;&#30340;&#21516;&#26102;&#20462;&#25913;&#20854;&#39118;&#26684;&#12290;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#22823;&#22810;&#25968;&#24212;&#29992;&#37117;&#38598;&#20013;&#22312;&#20687;&#26805;&#39640;&#12289;&#33707;&#22856;&#12289;&#22622;&#23578;&#36825;&#26679;&#30340;&#29305;&#23450;&#33402;&#26415;&#23478;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#32479;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#26041;&#38754;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36739;&#23569;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;&#21644;&#21033;&#29992;&#19981;&#21516;&#30340;&#26368;&#26032;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20013;&#22269;&#32472;&#30011;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#38754;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20960;&#31181;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25226;&#20256;&#32479;&#20013;&#22269;&#32472;&#30011;&#30340;&#20004;&#31181;&#20027;&#35201;&#39118;&#26684;&#65292;&#21363;&#8220;&#24037;&#31508;&#8221;&#21644;&#8220;&#27700;&#22696;&#8221;&#65292;&#24212;&#29992;&#21040;&#29616;&#20195;&#22270;&#20687;&#20013;&#65292;&#22914;&#33258;&#28982;&#23545;&#35937;&#12289;&#32918;&#20687;&#21644;&#39118;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artistic style transfer aims to modify the style of the image while preserving its content. Style transfer using deep learning models has been widely studied since 2015, and most of the applications are focused on specific artists like Van Gogh, Monet, Cezanne. There are few researches and applications on traditional Chinese painting style transfer. In this paper, we will study and leverage different state-of-the-art deep generative models for Chinese painting style transfer and evaluate the performance both qualitatively and quantitatively. In addition, we propose our own algorithm that combines several style transfer models for our task. Specifically, we will transfer two main types of traditional Chinese painting style, known as "Gong-bi" and "Shui-mo" (to modern images like nature objects, portraits and landscapes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.09716</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#30340;&#20449;&#24687;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting. (arXiv:2310.09716v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09716
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#20197;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#37325;&#20889;&#22312;&#25552;&#21319;&#23545;&#35805;&#24335;&#25628;&#32034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#29992;&#25143;&#26597;&#35810;&#36716;&#21270;&#20026;&#29420;&#31435;&#24418;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20154;&#24037;&#37325;&#20889;&#30340;&#26597;&#35810;&#20316;&#20026;&#26631;&#31614;&#26469;&#35757;&#32451;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#37325;&#20889;&#21487;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#20449;&#24687;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#26816;&#32034;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#37325;&#35201;&#29305;&#24615;&#26469;&#23450;&#20041;&#35268;&#33539;&#30340;&#37325;&#20889;&#65292;&#24182;&#23558;&#20854;&#20840;&#37096;&#32435;&#20837;&#25351;&#20196;&#20013;&#12290;&#27492;&#22806;&#65292;&#24403;&#21021;&#22987;&#26597;&#35810;&#37325;&#20889;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLMs&#30340;&#37325;&#20889;&#32534;&#36753;&#22120;&#30340;&#35282;&#33394;&#65292;&#24418;&#25104;&#19968;&#20010;&#8220;&#37325;&#20889;-&#32534;&#36753;&#8221;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;LLMs&#30340;&#37325;&#20889;&#33021;&#21147;&#25552;&#28860;&#25104;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#37325;&#20889;&#24310;&#36831;&#12290;&#25105;&#20204;&#22312;QReCC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#20449;&#24687;&#20016;&#23500;&#30340;&#26597;&#35810;&#37325;&#20889;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query rewriting plays a vital role in enhancing conversational search by transforming context-dependent user queries into standalone forms. Existing approaches primarily leverage human-rewritten queries as labels to train query rewriting models. However, human rewrites may lack sufficient information for optimal retrieval performance. To overcome this limitation, we propose utilizing large language models (LLMs) as query rewriters, enabling the generation of informative query rewrites through well-designed instructions. We define four essential properties for well-formed rewrites and incorporate all of them into the instruction. In addition, we introduce the role of rewrite editors for LLMs when initial query rewrites are available, forming a "rewrite-then-edit" process. Furthermore, we propose distilling the rewriting capabilities of LLMs into smaller models to reduce rewriting latency. Our experimental evaluation on the QReCC dataset demonstrates that informative query rewrites can y
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Edge-InversionNet&#65292;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#24471;&#21040;&#20102;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#22312;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.09667</link><description>&lt;p&gt;
Edge-InversionNet&#65306;&#20351;InversionNet&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Edge-InversionNet: Enabling Efficient Inference of InversionNet on Edge Devices. (arXiv:2310.09667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Edge-InversionNet&#65292;&#36890;&#36807;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#24471;&#21040;&#20102;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#22312;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38663;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#26159;&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#22320;&#38663;&#25968;&#25454;&#20013;&#25512;&#26029;&#22320;&#19979;&#32467;&#26500;&#12290;&#32780;InversionNet&#26159;&#26368;&#25104;&#21151;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#24212;&#29992;&#20110;&#22320;&#38663;FWI&#12290;&#28982;&#32780;&#65292;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;InversionNet&#38590;&#20197;&#26377;&#25928;&#37096;&#32626;&#21040;&#36890;&#24120;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#32467;&#26500;&#21270;&#20462;&#21098;&#31639;&#27861;&#33719;&#24471;InversionNet&#30340;&#36731;&#37327;&#21270;&#29256;&#26412;&#65292;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#26641;&#33683;&#27966;&#21046;&#20316;&#20102;&#19968;&#20010;&#36816;&#34892;&#36731;&#37327;&#21270;InversionNet&#30340;&#21407;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#21518;&#30340;InversionNet&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;98.2%&#30340;&#35745;&#31639;&#36164;&#28304;&#20943;&#23569;&#65292;&#32780;&#27169;&#22411;&#24615;&#33021;&#30053;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seismic full waveform inversion (FWI) is a widely used technique in geophysics for inferring subsurface structures from seismic data. And InversionNet is one of the most successful data-driven machine learning models that is applied to seismic FWI. However, the high computing costs to run InversionNet have made it challenging to be efficiently deployed on edge devices that are usually resource-constrained. Therefore, we propose to employ the structured pruning algorithm to get a lightweight version of InversionNet, which can make an efficient inference on edge devices. And we also made a prototype with Raspberry Pi to run the lightweight InversionNet. Experimental results show that the pruned InversionNet can achieve up to 98.2 % reduction in computing resources with moderate model performance degradation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#21644;&#20998;&#31867;&#20102;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#12290;&#26088;&#22312;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2310.08565</link><description>&lt;p&gt;
AI-&#26426;&#22120;&#20154;&#20013;&#30340;&#23433;&#20840;&#32771;&#34385;&#65306;&#24403;&#21069;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26426;&#20250;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities. (arXiv:2310.08565v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#21644;&#20998;&#31867;&#20102;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#12290;&#26088;&#22312;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#23427;&#20204;&#35806;&#29983;&#20197;&#26469;&#65292;&#26426;&#22120;&#20154;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23601;&#23494;&#19981;&#21487;&#20998;&#22320;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22914;&#20170;&#65292;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#26426;&#22120;&#20154;&#21560;&#23576;&#22120;&#21040;&#21322;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#36825;&#20123;&#31995;&#32479;&#24314;&#31435;&#22312;&#19977;&#20010;&#22522;&#26412;&#30340;&#26550;&#26500;&#20803;&#32032;&#19978;&#65306;&#24863;&#30693;&#12289;&#23548;&#33322;&#19982;&#35268;&#21010;&#65292;&#20197;&#21450;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25972;&#21512;&#25552;&#39640;&#20102;&#25105;&#20204;&#29983;&#27963;&#30340;&#36136;&#37327;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064; - &#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#25915;&#20987;&#12290;&#26500;&#25104;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#29289;&#29702;&#32452;&#20214;&#12289;&#31639;&#27861;&#21644;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#22522;&#20110;&#24212;&#23545;AI-&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#22312;&#25915;&#20987;&#38754;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#20154;&#26426;&#20132;&#20114;&#23433;&#20840;&#19977;&#20010;&#32500;&#24230;&#19978;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#29992;&#25143;&#12289;&#24320;&#21457;&#32773;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotics and Artificial Intelligence (AI) have been inextricably intertwined since their inception. Today, AI-Robotics systems have become an integral part of our daily lives, from robotic vacuum cleaners to semi-autonomous cars. These systems are built upon three fundamental architectural elements: perception, navigation and planning, and control. However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks. The physical components, algorithms, and data that make up AI-Robotics systems can be exploited by malicious actors, potentially leading to dire consequences. Motivated by the need to address the security concerns in AI-Robotics systems, this paper presents a comprehensive survey and taxonomy across three dimensions: attack surfaces, ethical and legal concerns, and Human-Robot Interaction (HRI) security. Our goal is to provide users, developers and other stakehol
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07177</link><description>&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Online Speculative Decoding. (arXiv:2310.07177v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07177
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#25345;&#32493;&#26356;&#26032;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#36890;&#36807;&#21033;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#26469;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#25991;&#26412;&#36755;&#20837;&#21644;&#33609;&#31295;&#27169;&#22411;&#19982;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#26174;&#33879;&#33021;&#21147;&#24046;&#36317;&#26102;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#32447;&#25512;&#27979;&#35299;&#30721;&#65288;OSD&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#20016;&#23500;&#30340;&#22810;&#20313;&#35745;&#31639;&#33021;&#21147;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#29992;&#25143;&#26597;&#35810;&#25968;&#25454;&#25345;&#32493;&#26356;&#26032;&#65288;&#22810;&#20010;&#65289;&#33609;&#31295;&#27169;&#22411;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#21463;&#20869;&#23384;&#38480;&#21046;&#65292;&#20856;&#22411;&#30340;LLM&#26381;&#21153;&#38598;&#32676;&#20013;&#30340;&#21097;&#20313;&#35745;&#31639;&#33021;&#21147;&#21487;&#20197;&#29992;&#20110;&#22312;&#32447;&#37325;&#26032;&#35757;&#32451;&#33609;&#31295;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#25104;&#26412;&#20445;&#25345;&#20013;&#24615;&#12290;&#30001;&#20110;LLM&#26381;&#21153;&#30340;&#26597;&#35810;&#20998;&#24067;&#30456;&#23545;&#31616;&#21333;&#65292;&#26681;&#25454;&#26597;&#35810;&#20998;&#24067;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#20351;&#33609;&#31295;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative decoding is a pivotal technique to accelerate the inference of large language models (LLMs) by employing a smaller draft model to predict the target model's outputs. However, its efficacy can be limited due to the low predictive accuracy of the draft model, particularly when faced with diverse text inputs and a significant capability gap between the draft and target models. We introduce online speculative decoding (OSD) to address this challenge. The main idea is to continually update (multiple) draft model(s) on observed user query data using the abundant excess computational power in an LLM serving cluster. Given that LLM inference is memory-bounded, the surplus computational power in a typical LLM serving cluster can be repurposed for online retraining of draft models, thereby making the training cost-neutral. Since the query distribution of an LLM service is relatively simple, retraining on query distribution enables the draft model to more accurately predict the target
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04921</link><description>&lt;p&gt;
Crystal: &#20197;&#33258;&#25105;&#21453;&#39304;&#20026;&#22686;&#24378;&#30340;&#20869;&#30465;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Crystal: Introspective Reasoners Reinforced with Self-Feedback. (arXiv:2310.04921v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Crystal&#30340;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#20869;&#30465;&#30693;&#35782;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24120;&#35782;&#25512;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20854;&#20013;&#25512;&#29702;&#36807;&#31243;&#30340;&#22522;&#30784;&#30693;&#35782;&#26126;&#30830;&#34920;&#36798;&#21644;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23454;&#29616;&#65292;&#21253;&#25324;"&#24605;&#32500;&#38142;"&#21450;&#20854;&#21464;&#31181;&#65292;&#26410;&#33021;&#25429;&#25417;&#21040;&#24120;&#35782;&#25512;&#29702;&#20013;&#25152;&#38656;&#30340;&#20869;&#30465;&#24615;&#36136;&#65292;&#20063;&#26410;&#33021;&#35299;&#37322;&#30693;&#35782;&#29983;&#25104;&#21644;&#21033;&#29992;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#20869;&#30465;&#22411;&#24120;&#35782;&#25512;&#29702;&#22120; Crystal&#12290;&#20026;&#20102;&#35299;&#20915;&#24120;&#35782;&#38382;&#39064;&#65292;&#23427;&#39318;&#20808;&#20869;&#30465;&#19982;&#32473;&#23450;&#38382;&#39064;&#30456;&#20851;&#30340;&#30693;&#35782;&#38472;&#36848;&#65292;&#28982;&#21518;&#22522;&#20110;&#20808;&#21069;&#20869;&#30465;&#30340;&#30693;&#35782;&#36827;&#34892;&#30693;&#24773;&#39044;&#27979;&#12290;&#27169;&#22411;&#30340;&#30693;&#35782;&#20869;&#30465;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#27169;&#24335;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35843;&#25972;&#65292;&#20854;&#20013;&#22870;&#21169;&#26469;&#33258;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extensive work has shown that the performance and interpretability of commonsense reasoning can be improved via knowledge-augmented reasoning methods, where the knowledge that underpins the reasoning process is explicitly verbalized and utilized. However, existing implementations, including "chain-of-thought" and its variants, fall short in capturing the introspective nature of knowledge required in commonsense reasoning, and in accounting for the mutual adaptation between the generation and utilization of knowledge. We propose a novel method to develop an introspective commonsense reasoner, Crystal. To tackle commonsense problems, it first introspects for knowledge statements related to the given question, and subsequently makes an informed prediction that is grounded in the previously introspected knowledge. The knowledge introspection and knowledge-grounded reasoning modes of the model are tuned via reinforcement learning to mutually adapt, where the reward derives from the feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20256;&#24863;&#22120;&#25968;&#37327;&#22686;&#21152;&#24102;&#26469;&#30340;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00627</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Intelligent Client Selection for Federated Learning using Cellular Automata. (arXiv:2310.00627v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20256;&#24863;&#22120;&#25968;&#37327;&#22686;&#21152;&#24102;&#26469;&#30340;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#38544;&#31169;&#22686;&#24378;&#21644;&#24310;&#36831;&#26368;&#23567;&#21270;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#22914;&#20132;&#36890;&#12289;&#36890;&#20449;&#21644;&#21307;&#30103;&#65289;&#20013;&#30340;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;FL&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25968;&#30334;&#19975;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24341;&#20837;&#36793;&#32536;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#24555;&#36895;&#21709;&#24212;&#65292;&#24182;&#20135;&#29983;&#39640;&#24230;&#20010;&#24615;&#21270;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#24212;&#29992;&#20013;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#22312;&#36890;&#20449;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#25152;&#26377;&#35774;&#22791;&#21442;&#19982;&#32852;&#37030;&#36807;&#31243;&#30340;&#33021;&#21147;&#65292;&#36827;&#32780;&#38656;&#35201;&#26377;&#25928;&#30340;FL&#23458;&#25143;&#31471;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#32990;&#33258;&#21160;&#26426;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#65288;CA-CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#20316;&#20026;&#27169;&#22411;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#24555;&#36895;&#28436;&#21464;&#29615;&#22659;&#20013;&#30340;&#26102;&#31354;&#21464;&#21270;&#12290;CA-CS&#32771;&#34385;&#20102;&#35745;&#31639;&#36164;&#28304;&#21644;&#36890;&#20449;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising solution for privacy-enhancement and latency minimization in various real-world applications, such as transportation, communications, and healthcare. FL endeavors to bring Machine Learning (ML) down to the edge by harnessing data from million of devices and IoT sensors, thus enabling rapid responses to dynamic environments and yielding highly personalized results. However, the increased amount of sensors across diverse applications poses challenges in terms of communication and resource allocation, hindering the participation of all devices in the federated process and prompting the need for effective FL client selection. To address this issue, we propose Cellular Automaton-based Client Selection (CA-CS), a novel client selection algorithm, which leverages Cellular Automata (CA) as models to effectively capture spatio-temporal changes in a fast-evolving environment. CA-CS considers the computational resources and communication capacity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.16661</link><description>&lt;p&gt;
SA2-Net: &#29992;&#20110;&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#30340;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SA2-Net: Scale-aware Attention Network for Microscopic Image Segmentation. (arXiv:2309.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16661
&lt;/p&gt;
&lt;p&gt;
SA2-Net&#26159;&#19968;&#31181;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#32467;&#26500;&#12290;&#23427;&#32467;&#21512;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#21644;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26174;&#24494;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#20026;&#32473;&#23450;&#30340;&#26174;&#24494;&#22270;&#20687;&#20013;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#26631;&#31614;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#26159;&#35768;&#22810;&#29616;&#26377;&#26694;&#26550;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#26126;&#30830;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#21464;&#21387;&#22120;&#26368;&#21021;&#26159;&#20026;&#20102;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#26174;&#24494;&#22270;&#20687;&#20013;&#65292;&#21253;&#25324;&#24418;&#29366;&#12289;&#22823;&#23567;&#12289;&#22806;&#35266;&#21644;&#30446;&#26631;&#21306;&#22495;&#23494;&#24230;&#30340;&#21508;&#31181;&#25361;&#25112;&#20013;&#65292;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SA2-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#23398;&#20064;&#26469;&#26377;&#25928;&#22788;&#29702;&#26174;&#24494;&#22270;&#20687;&#20013;&#22810;&#26679;&#32467;&#26500;&#30340;&#27880;&#24847;&#24341;&#23548;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23610;&#24230;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;SA2&#65289;&#27169;&#22359;&#65292;&#29992;&#20110;&#25429;&#25417;&#26174;&#24494;&#21306;&#22495;&#65288;&#22914;&#32454;&#32990;&#65289;&#23610;&#24230;&#21644;&#24418;&#29366;&#30340;&#22266;&#26377;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;&#36825;&#20010;&#27169;&#22359;&#32467;&#21512;&#20102;&#23616;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Microscopic image segmentation is a challenging task, wherein the objective is to assign semantic labels to each pixel in a given microscopic image. While convolutional neural networks (CNNs) form the foundation of many existing frameworks, they often struggle to explicitly capture long-range dependencies. Although transformers were initially devised to address this issue using self-attention, it has been proven that both local and global features are crucial for addressing diverse challenges in microscopic images, including variations in shape, size, appearance, and target region density. In this paper, we introduce SA2-Net, an attention-guided method that leverages multi-scale feature learning to effectively handle diverse structures within microscopic images. Specifically, we propose scale-aware attention (SA2) module designed to capture inherent variations in scales and shapes of microscopic regions, such as cells, for accurate segmentation. This module incorporates local attention
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15028</link><description>&lt;p&gt;
&#35753;PPO&#21464;&#24471;&#26356;&#22909;&#65306;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Making PPO even better: Value-Guided Monte-Carlo Tree Search decoding. (arXiv:2309.15028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#23548;&#21521;&#30340;Monte-Carlo Tree Search&#35299;&#30721;&#31639;&#27861;PPO-MCTS&#65292;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#35299;&#20915;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#35780;&#20998;&#26426;&#21046;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;Proximal Policy Optimization (PPO)&#65292;&#22240;&#27492;&#21487;&#20197;&#35748;&#20026;&#25512;&#29702;&#26102;&#38388;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#22914;Monte-Carlo Tree Search (MCTS) &#26159;&#19981;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;PPO&#20043;&#19978;&#38598;&#25104;MCTS&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;PPO&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#35299;&#30721;&#25991;&#26412;&#26102;&#65292;&#19981;&#35201;&#20002;&#24323;&#20540;&#32593;&#32476;&#65292;&#21363;PPO&#35757;&#32451;&#26102;&#29992;&#20110;&#35780;&#20272;&#37096;&#20998;&#36755;&#20986;&#24207;&#21015;&#30340;&#21103;&#20135;&#21697;&#65292;&#32780;&#26159;&#23558;&#20854;&#19982;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PPO-MCTS&#30340;&#26032;&#39062;&#30340;&#20540;&#23548;&#21521;&#35299;&#30721;&#31639;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;PPO&#30340;&#20540;&#32593;&#32476;&#19982;&#25512;&#29702;&#26102;&#38388;&#20135;&#29983;&#30340;&#31574;&#30053;&#32593;&#32476;&#32039;&#23494;&#32467;&#21512;&#12290;&#19982;&#22522;&#20110;MCTS&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#20943;&#23569;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#37096;&#20998;&#36755;&#20986;&#30340;&#35780;&#20998;&#26426;&#21046;&#30340;&#22522;&#26412;&#19981;&#21305;&#37197;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;PPO-MCTS&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS grea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2309.13365</link><description>&lt;p&gt;
&#20915;&#31574;&#26641;&#31574;&#30053;&#22312;IBMDP&#20013;&#30340;Actor-Critic&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65288;arXiv:2309.13365v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in IBMDPs. (arXiv:2309.13365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13365
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;IBMDP&#20013;&#20351;&#29992;Actor-Critic&#31639;&#27861;&#23398;&#20064;&#20915;&#31574;&#26641;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#23433;&#20840;&#26816;&#26597;&#26469;&#24314;&#31435;&#23545;&#36825;&#20123;AI&#30340;&#20449;&#20219;&#12290;&#29305;&#21035;&#26159;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#25552;&#20379;&#20102;&#23545;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#35270;&#35282;&#65292;&#24182;&#36879;&#26126;&#22320;&#25581;&#31034;&#20102;&#21738;&#20123;&#36755;&#20837;&#29305;&#24449;&#23545;&#20110;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20915;&#31574;&#26641;&#36807;&#22823;&#65292;&#21487;&#35299;&#37322;&#24615;&#23601;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#23398;&#20064;&#32039;&#20945;&#30340;&#20915;&#31574;&#26641;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;RL&#25506;&#32034;DT&#30340;&#31354;&#38388;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22686;&#21152;&#21160;&#20316;&#26469;&#25910;&#38598;&#20851;&#20110;&#38544;&#34255;&#36755;&#20837;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#36866;&#24403;&#22320;&#23545;&#36825;&#20123;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#65292;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#22312;&#26641;&#30340;&#22823;&#23567;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#26435;&#34913;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#38656;&#35201;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#36825;&#19968;&#31867;&#31616;&#21333;&#30340;&#29609;&#20855;&#20219;&#21153;&#19978;&#65292;&#28145;&#24230;RL&#20063;&#21487;&#33021;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of AI models allows for user safety checks to build trust in such AIs. In particular, Decision Trees (DTs) provide a global look at the learned model and transparently reveal which features of the input are critical for making a decision. However, interpretability is hindered if the DT is too large. To learn compact trees, a recent Reinforcement Learning (RL) framework has been proposed to explore the space of DTs using deep RL. This framework augments a decision problem (e.g. a supervised classification task) with additional actions that gather information about the features of an otherwise hidden input. By appropriately penalizing these actions, the agent learns to optimally trade-off size and performance of DTs. In practice, a reactive policy for a partially observable Markov decision process (MDP) needs to be learned, which is still an open problem. We show in this paper that deep RL can fail even on simple toy tasks of this class. However, when the underlying deci
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25581;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.15684</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#19982;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#21644;&#20027;&#21160;&#25552;&#38382;
&lt;/p&gt;
&lt;p&gt;
Interactively Robot Action Planning with Uncertainty Analysis and Active Questioning by Large Language Model. (arXiv:2308.15684v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25581;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#24050;&#32463;&#34987;&#31215;&#26497;&#30740;&#31350;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32473;&#20986;&#30340;LLM&#25351;&#20196;&#21487;&#33021;&#23384;&#22312;&#27495;&#20041;&#21644;&#32570;&#20047;&#20449;&#24687;&#65292;&#36825;&#21462;&#20915;&#20110;&#20219;&#21153;&#29615;&#22659;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#25351;&#20196;&#36755;&#20837;&#26356;&#35814;&#32454;&#26469;&#35843;&#25972;LLM&#30340;&#36755;&#20986;&#65307;&#28982;&#32780;&#65292;&#35774;&#35745;&#25104;&#26412;&#24456;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#20801;&#35768;LLM&#36890;&#36807;&#21521;&#20154;&#31867;&#25552;&#38382;&#26469;&#20998;&#26512;&#24182;&#25910;&#38598;&#32570;&#22833;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#29983;&#25104;&#31934;&#30830;&#26426;&#22120;&#20154;&#25351;&#20196;&#30340;&#35774;&#35745;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#28921;&#39274;&#20219;&#21153;&#30340;&#20855;&#20307;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20063;&#25581;&#31034;&#20102;LLM&#22312;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#25552;&#20986;&#19981;&#37325;&#35201;&#30340;&#38382;&#39064;&#21644;&#22312;&#19981;&#35810;&#38382;&#30340;&#24773;&#20917;&#19979;&#20551;&#35774;&#20851;&#38190;&#20449;&#24687;&#12290;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#38416;&#26126;&#20026;&#26410;&#26469;&#21033;&#29992;LLM&#36827;&#34892;&#26426;&#22120;&#20154;&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of the Large Language Model (LLM) to robot action planning has been actively studied. The instructions given to the LLM by natural language may include ambiguity and lack of information depending on the task context. It is possible to adjust the output of LLM by making the instruction input more detailed; however, the design cost is high. In this paper, we propose the interactive robot action planning method that allows the LLM to analyze and gather missing information by asking questions to humans. The method can minimize the design cost of generating precise robot instructions. We demonstrated the effectiveness of our method through concrete examples in cooking tasks. However, our experiments also revealed challenges in robot action planning with LLM, such as asking unimportant questions and assuming crucial information without asking. Shedding light on these issues provides valuable insights for future research on utilizing LLM for robotics.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.14840</link><description>&lt;p&gt;
&#35782;&#21035;&#21644;&#20943;&#36731;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Identifying and Mitigating the Security Risks of Generative AI. (arXiv:2308.14840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14840
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#32508;&#21512;&#25253;&#36947;&#65292;&#35752;&#35770;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25152;&#24102;&#26469;&#30340;&#21452;&#37325;&#29992;&#36884;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#31038;&#21306;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#19968;&#39033;&#37325;&#22823;&#25216;&#26415;&#21457;&#26126;&#37117;&#20250;&#24102;&#26469;&#21452;&#37325;&#29992;&#36884;&#30340;&#22256;&#22659; - &#26032;&#25216;&#26415;&#26082;&#26377;&#21487;&#33021;&#34987;&#29992;&#20110;&#21892;&#33391;&#65292;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#24694;&#24847;&#34892;&#20026;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20195;&#30721;&#34917;&#20840;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65289;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#21516;&#26679;&#21487;&#20197;&#21033;&#29992;GenAI&#29983;&#25104;&#26032;&#30340;&#25915;&#20987;&#65292;&#24182;&#22686;&#21152;&#29616;&#26377;&#25915;&#20987;&#30340;&#36895;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#22312;Google&#20030;&#21150;&#30340;&#19968;&#20010;&#30740;&#35752;&#20250;&#30340;&#21457;&#29616;&#65288;&#30001;&#26031;&#22374;&#31119;&#22823;&#23398;&#21644;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;&#40614;&#36842;&#36874;&#20998;&#26657;&#20849;&#21516;&#32452;&#32455;&#65289;&#12290;&#26412;&#25991;&#24182;&#19981;&#24847;&#21619;&#30528;&#20840;&#38754;&#65292;&#32780;&#26159;&#35797;&#22270;&#32508;&#21512;&#19968;&#20123;&#26377;&#36259;&#30340;&#30740;&#35752;&#20250;&#21457;&#29616;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#26082;&#20026;&#36825;&#20010;&#37325;&#35201;&#20027;&#39064;&#30340;&#35752;&#35770;&#25552;&#20379;&#19968;&#20010;&#36215;&#28857;&#65292;&#20063;&#24341;&#36215;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks.  This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interest
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;TSP&#27714;&#35299;&#22120;NAR4TSP&#20351;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00560</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;&#27714;&#35299;&#22120;&#29992;&#20110;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Non-Autoregressive Solver for Traveling Salesman Problems. (arXiv:2308.00560v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00560
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;TSP&#27714;&#35299;&#22120;NAR4TSP&#20351;&#29992;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#65292;&#24182;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65288;TSP&#65289;&#26159;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;TSP&#27714;&#35299;&#22120;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26102;&#38754;&#20020;&#20302;&#24310;&#36831;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAR4TSP&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#26041;&#24335;&#29983;&#25104;TSP&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;NAR4TSP&#20351;&#29992;&#22686;&#24378;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#22522;&#20110;NAR&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#26114;&#36149;&#26631;&#31614;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;NAR4TSP&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#32467;&#21512;&#20102;RL&#21644;NAR&#35299;&#30721;&#30340;TSP&#27714;&#35299;&#22120;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;TSP&#23454;&#20363;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NAR4TSP&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#12289;&#25512;&#29702;&#24310;&#36831;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NAR4TSP&#35299;&#30721;&#36807;&#31243;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Traveling Salesman Problem (TSP) is a well-known problem in combinatorial optimization with applications in various domains. However, existing TSP solvers face challenges in producing high-quality solutions with low latency. To address this issue, we propose NAR4TSP, which produces TSP solutions in a Non-Autoregressive (NAR) manner using a specially designed Graph Neural Network (GNN), achieving faster inference speed. Moreover, NAR4TSP is trained using an enhanced Reinforcement Learning (RL) strategy, eliminating the dependency on costly labels used to train conventional supervised learning-based NAR models. To the best of our knowledge, NAR4TSP is the first TSP solver that successfully combines RL and NAR decoding. The experimental results on both synthetic and real-world TSP instances demonstrate that NAR4TSP outperforms four state-of-the-art models in terms of solution quality, inference latency, and generalization ability. Lastly, we present visualizations of NAR4TSP's decodin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15895</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#65306;&#22810;&#26679;&#24615;&#21644;&#20559;&#24046;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias. (arXiv:2306.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#24615;&#23646;&#24615;&#30340;&#25552;&#31034;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#24402;&#22240;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#23646;&#24615;&#21270;&#25552;&#31034;&#23545;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#20851;&#20110;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#31995;&#32479;&#24615;&#20559;&#24046;&#23384;&#22312;&#20110;&#29983;&#25104;&#25968;&#25454;&#20013;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#29983;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#32487;&#25215;&#20102;LLM&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#22810;&#26679;&#23646;&#24615;&#30340;&#25552;&#31034;(&#20363;&#22914;&#25351;&#23450;&#38271;&#24230;&#21644;&#39118;&#26684;&#31561;&#23646;&#24615;)&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65292;&#36825;&#26377;&#28508;&#21147;&#20135;&#29983;&#22810;&#26679;&#21644;&#24402;&#22240;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#20855;&#26377;&#39640;&#22522;&#25968;&#21644;&#22810;&#26679;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23646;&#24615;&#21270;&#25552;&#31034;&#22312;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#30340;&#31867;&#21035;&#26465;&#20214;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#39033;&#21253;&#25324;&#20559;&#24046;&#12289;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#31561;&#20851;&#38190;&#26041;&#38754;&#30340;&#20840;&#38754;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#19977;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;&#39318;&#20808;&#65292;&#31995;&#32479;&#24615;&#20559;&#24046;&#22312;&#29983;&#25104;&#25968;&#25454;&#20013;&#23384;&#22312;&#65307;&#20854;&#27425;&#65292;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65307;&#26368;&#21518;&#65292;&#36827;&#34892;&#23646;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, sy
&lt;/p&gt;</description></item><item><title>OpenSTL&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#24490;&#29615;&#21644;&#19981;&#22522;&#20110;&#24490;&#29615;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.11249</link><description>&lt;p&gt;
OpenSTL: &#19968;&#20010;&#20840;&#38754;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning. (arXiv:2306.11249v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11249
&lt;/p&gt;
&lt;p&gt;
OpenSTL&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#27969;&#34892;&#30340;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#24490;&#29615;&#21644;&#19981;&#22522;&#20110;&#24490;&#29615;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#26469;&#24103;&#26469;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#29615;&#22659;&#12289;&#22797;&#26434;&#30340;&#23454;&#29616;&#21644;&#38590;&#20197;&#22797;&#29616;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#30528;&#32570;&#20047;&#31995;&#32479;&#24615;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;&#27809;&#26377;&#26631;&#20934;&#21270;&#65292;&#27604;&#36739;&#21487;&#33021;&#19981;&#20844;&#24179;&#65292;&#27934;&#35265;&#19981;&#30830;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenSTL&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#65292;&#23558;&#27969;&#34892;&#30340;&#26041;&#27861;&#20998;&#20026;&#22522;&#20110;&#24490;&#29615;&#21644;&#19981;&#22522;&#20110;&#24490;&#29615;&#30340;&#27169;&#22411;&#12290;OpenSTL&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;&#21512;&#25104;&#31227;&#21160;&#29289;&#20307;&#36712;&#36857;&#12289;&#20154;&#20307;&#36816;&#21160;&#12289;&#39550;&#39542;&#22330;&#26223;&#12289;&#20132;&#36890;&#27969;&#37327;&#21644;&#22825;&#27668;&#39044;&#25253;&#22312;&#20869;&#30340;&#21508;&#31181;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26631;&#20934;&#35780;&#20272;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35814;&#32454;&#20998;&#26512;&#20102;&#27169;&#22411;&#26550;&#26500;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a comprehensive benchmark for spatio-temporal predictive learning that categorizes prevalent approaches into recurrent-based and recurrent-free models. OpenSTL provides a modular and extensible framework implementing various state-of-the-art methods. We conduct standard evaluations on datasets across various domains, including synthetic moving object trajectory, human motion, driving scenes, traffic flow and weather forecasting. Based on our observations, we provide a detailed analysis of how model arch
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05809</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#20013;&#37319;&#29992;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interactive Explanation with Varying Level of Details in an Explainable Scientific Literature Recommender System. (arXiv:2306.05809v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#36171;&#20104;&#29992;&#25143;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#37319;&#29992;&#19968;&#31181;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#26041;&#27861;&#65292;&#21521;&#27599;&#20010;&#29992;&#25143;&#25552;&#20379;&#30456;&#21516;&#31243;&#24230;&#30340;&#35299;&#37322;&#65292;&#32780;&#19981;&#32771;&#34385;&#20182;&#20204;&#30340;&#20010;&#20307;&#38656;&#27714;&#21644;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#35299;&#37322;&#22823;&#22810;&#20197;&#38745;&#24577;&#21644;&#38750;&#20132;&#20114;&#26041;&#24335;&#21576;&#29616;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#26088;&#22312;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#22522;&#20110;&#20854;&#38656;&#27714;&#21644;&#20559;&#22909;&#36827;&#34892;&#20132;&#20114;&#12289;&#25511;&#21046;&#21644;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19977;&#20010;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65288;&#22522;&#26412;&#12289;&#20013;&#32423;&#21644;&#39640;&#32423;&#65289;&#65292;&#24182;&#22312;&#36879;&#26126;&#30340;&#25512;&#33616;&#21644;&#20852;&#36259;&#24314;&#27169;&#24212;&#29992;&#65288;RIMA&#65289;&#20013;&#23454;&#29616;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#65292;&#20197;&#35843;&#26597;&#25552;&#20379;&#19981;&#21516;&#32454;&#33410;&#32423;&#21035;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#23545;&#29992;&#25143;&#23545;&#31995;&#32479;&#21487;&#35299;&#37322;&#24615;&#30340;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems (RS) have traditionally followed a one-size-fits-all approach, delivering the same explanation level of detail to each user, without considering their individual needs and goals. Further, explanations in RS have so far been presented mostly in a static and non-interactive manner. To fill these research gaps, we aim in this paper to adopt a user-centered, interactive explanation model that provides explanations with different levels of detail and empowers users to interact with, control, and personalize the explanations based on their needs and preferences. We followed a user-centered approach to design interactive explanations with three levels of detail (basic, intermediate, and advanced) and implemented them in the transparent Recommendation and Interest Modeling Application (RIMA). We conducted a qualitative user study (N=14) to investigate the impact of providing interactive explanations with varying level of details on the users' perception of the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00840</link><description>&lt;p&gt;
MuZero&#23398;&#21040;&#20102;&#20160;&#20040;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26377;&#21487;&#33021;&#20174;&#22797;&#26434;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#35268;&#21010;&#33021;&#21147;&#30340;&#25552;&#21319;&#24403;&#21069;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MuZero&#36825;&#20010;&#33879;&#21517;&#30340;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#23454;&#29616;&#20540;&#31561;&#20215;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#19978;&#30340;&#25104;&#23601;&#20197;&#21450;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#23545;&#31574;&#30053;&#25913;&#36827;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#35832;&#22810;&#20854;&#20182;&#35266;&#28857;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;MuZero&#23398;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#35268;&#21010;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#24403;&#21069;&#31574;&#30053;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.11913</link><description>&lt;p&gt;
&#29992;&#20110;&#20174;&#31232;&#30095;&#36828;&#31243;&#20256;&#24863;&#22120;&#25968;&#25454;&#37325;&#24314;&#38750;&#32447;&#24615;&#28023;&#27915;&#27874;&#28010;&#34920;&#38754;&#30456;&#20301;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine learning for phase-resolved reconstruction of nonlinear ocean wave surface elevations from sparse remote sensing data. (arXiv:2305.11913v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#24230;&#29616;&#23454;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#31232;&#30095;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#30456;&#20301;&#30456;&#20851;&#30340;&#27700;&#27874;&#26465;&#20214;&#23545;&#20110;&#28023;&#27915;&#24037;&#31243;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36828;&#31243;&#30417;&#27979;&#27874;&#28010;&#39044;&#27979;&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#39318;&#20808;&#38656;&#35201;&#20174;&#31867;&#20284;&#38647;&#36798;&#30340;&#31232;&#30095;&#27979;&#37327;&#20013;&#37325;&#24314;&#27874;&#28010;&#34920;&#38754;&#12290;&#29616;&#26377;&#30340;&#37325;&#24314;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#31616;&#21270;&#30340;&#27169;&#22411;&#20551;&#35774;&#65292;&#36825;&#20250;&#24433;&#21709;&#25972;&#20010;&#39044;&#27979;&#36807;&#31243;&#30340;&#23454;&#26102;&#24615;&#25110;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U-Net&#21644;Fourier&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#20301;&#30456;&#20851;&#30340;&#27874;&#28010;&#34920;&#38754;&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20855;&#26377;&#39640;&#24230;&#29616;&#23454;&#24615;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#22343;&#21248;&#30340;&#19968;&#32500;&#32593;&#26684;&#19978;&#30001;&#27874;&#28010;&#27169;&#25311;&#30340;&#39640;&#38454;&#35889;&#26041;&#27861;&#21644;&#20960;&#20309;&#38647;&#36798;&#24314;&#27169;&#26041;&#27861;&#29983;&#25104;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#31181;&#27169;&#22411;&#37117;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#27874;&#28010;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate short-term prediction of phase-resolved water wave conditions is crucial for decision-making in ocean engineering. However, the initialization of remote-sensing-based wave prediction models first requires a reconstruction of wave surfaces from sparse measurements like radar. Existing reconstruction methods either rely on computationally intensive optimization procedures or simplistic modeling assumptions that compromise real-time capability or accuracy of the entire prediction process. We therefore address these issues by proposing a novel approach for phase-resolved wave surface reconstruction using neural networks based on the U-Net and Fourier neural operator (FNO) architectures. Our approach utilizes synthetic yet highly realistic training data on uniform one-dimensional grids, that is generated by the high-order spectral method for wave simulation and a geometric radar modeling approach. The investigation reveals that both models deliver accurate wave reconstruction resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2305.11508</link><description>&lt;p&gt;
&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Medical Dialogue System. (arXiv:2305.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11508
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21307;&#30103;&#38382;&#31572;&#21450;&#35786;&#26029;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20026;&#24739;&#32773;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#21307;&#30103;&#38382;&#31572;&#39046;&#22495;&#20855;&#26377;&#26480;&#20986;&#30340;&#33021;&#21147;&#65292;&#34920;&#26126;&#20855;&#22791;&#20102;&#23545;&#24120;&#35782;&#30340;&#20016;&#23500;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#35786;&#26029;&#31574;&#30053;&#65292;LLMs&#26080;&#27861;&#30452;&#25509;&#29992;&#20110;&#35786;&#26029;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#26114;&#36149;&#30340;LLMs&#24494;&#35843;&#12290;&#21478;&#19968;&#31181;&#26356;&#20855;&#21560;&#24341;&#21147;&#30340;&#35299;&#20915;&#26041;&#27861;&#26159;&#24320;&#21457;&#19968;&#20010;&#25554;&#20214;&#65292;&#36171;&#20104;LLMs&#25191;&#34892;&#21307;&#30103;&#23545;&#35805;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PlugMed&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#20004;&#20010;&#27169;&#22359;&#20419;&#36827;&#20102;LLMs&#30340;&#24688;&#24403;&#23545;&#35805;&#21160;&#20316;&#65306;&#25552;&#31034;&#29983;&#25104;&#65288;PG&#65289;&#27169;&#22359;&#21644;&#22238;&#22797;&#25490;&#21517;&#65288;RR&#65289;&#27169;&#22359;&#12290;PG&#27169;&#22359;&#26088;&#22312;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35282;&#24230;&#25429;&#33719;&#23545;&#35805;&#20449;&#24687;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#21305;&#37197;&#24230;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems aim to provide accurate answers to patients, necessitating specific domain knowledge. Recent advancements in Large Language Models (LLMs) have demonstrated their exceptional capabilities in the medical Q&amp;A domain, indicating a rich understanding of common sense. However, LLMs are insufficient for direct diagnosis due to the absence of diagnostic strategies. The conventional approach to address this challenge involves expensive fine-tuning of LLMs. Alternatively, a more appealing solution is the development of a plugin that empowers LLMs to perform medical conversation tasks. Drawing inspiration from in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System that facilitates appropriate dialogue actions by LLMs through two modules: the prompt generation (PG) module and the response ranking (RR) module. The PG module is designed to capture dialogue information from both global and local perspectives. It selects suitable prompts by assessing 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10160</link><description>&lt;p&gt;
&#19981;&#35201;&#29992;&#26126;&#25991;&#19978;&#20256;&#27979;&#35797;&#25968;&#25454;&#65306;&#20943;&#36731;&#25968;&#25454;&#22806;&#27844;&#23545;&#20110;&#35780;&#20272;&#22522;&#20934;&#30340;&#25345;&#32493;&#24433;&#21709;&#30340;&#23454;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks. (arXiv:2305.10160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10160
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#20010;&#36866;&#29992;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#27979;&#35797;&#25968;&#25454;&#65292;&#20165;&#20801;&#35768;&#29305;&#23450;&#27966;&#29983;&#21457;&#24067;&#65307;&#65288;2&#65289;&#23545;&#20110;API&#25345;&#26377;&#26041;&#65292;&#35201;&#27714;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#20445;&#25252;&#27979;&#35797;&#25968;&#25454;&#65292;&#19981;&#20572;&#27490;&#35780;&#20272;&#30452;&#21040;&#36798;&#21040;&#35201;&#27714;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#38656;&#36991;&#20813;&#26576;&#20123;&#32467;&#26524;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#21160;&#29228;&#32593;&#36164;&#26009;&#24211;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#65292;&#25968;&#25454;&#22806;&#27844;&#21464;&#24471;&#24120;&#35265;&#19988;&#37096;&#20998;&#38590;&#20197;&#24212;&#23545;&#12290;&#23545;&#20110;&#37027;&#20123;&#19981;&#20250;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20854;&#25968;&#25454;&#25104;&#20026;&#20102;&#21830;&#19994;&#26426;&#23494;&#65292;&#21363;&#20351;&#22312;&#20844;&#24320;&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#29305;&#23450;&#27979;&#35797;&#23454;&#20363;&#26159;&#21542;&#34987;&#27844;&#38706;&#20063;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#19977;&#20010;&#21487;&#34892;&#30340;&#31574;&#30053;&#65306;&#65288;1&#65289;&#20351;&#29992;&#20844;&#38053;&#21152;&#23494;&#21457;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#24182;&#38480;&#21046;&#27966;&#29983;&#21457;&#24067;&#30340;&#35768;&#21487;&#65307;&#65288;2&#65289;&#35201;&#27714;&#25345;&#26377;API&#35757;&#32451;&#25968;&#25454;&#30340;&#20844;&#21496;&#37319;&#29992;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#65292;&#24182;&#25298;&#32477;&#35780;&#20272;&#65292;&#30452;&#21040;&#35757;&#32451;&#25490;&#38500;&#25511;&#21046;&#26080;&#35823;&#20026;&#27490;&#65307;&#65288;3&#65289;&#22914;&#26524;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20114;&#32852;&#32593;&#25991;&#26412;&#65292;&#37027;&#20040;&#38656;&#36991;&#20813;&#22312;&#32593;&#32476;&#25628;&#32034;&#20013;&#20986;&#29616;&#21253;&#21547;&#27491;&#30830;&#25552;&#21462;&#37096;&#20998;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination has become especially prevalent and challenging with the rise of models pretrained on very large, automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to ascertain whether a particular test instance has been compromised. Strategies such as live leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate until demands are met; (3) in case of test data based on internet text, avoid data which appears with its soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04449</link><description>&lt;p&gt;
DeformerNet: &#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#21452;&#25163;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DeformerNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#23545;&#29289;&#20307;&#24418;&#29366;&#30340;&#25805;&#32437;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#24037;&#29305;&#24449;&#21644;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#21487;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#28436;&#31034;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23478;&#24237;&#25252;&#29702;&#21040;&#20179;&#24211;&#37197;&#36865;&#20877;&#21040;&#22806;&#31185;&#25163;&#26415;&#21161;&#29702;&#31561;&#39046;&#22495;&#65292;&#24212;&#29992;&#38656;&#35201;&#26426;&#22120;&#20154;&#21487;&#38752;&#22320;&#25805;&#32437;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#24418;&#29366;&#12290;&#24377;&#24615;&#19977;&#32500;&#21487;&#22609;&#29289;&#20307;&#30340;&#20998;&#26512;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25551;&#36848;&#20915;&#23450;&#29289;&#20307;&#24418;&#29366;&#30340;&#21487;&#33021;&#26080;&#38480;&#33258;&#30001;&#24230;&#12290;&#20197;&#24448;&#30340;3D&#24418;&#29366;&#25511;&#21046;&#23581;&#35797;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#26469;&#34920;&#31034;&#29289;&#20307;&#24418;&#29366;&#65292;&#24182;&#38656;&#35201;&#35757;&#32451;&#29289;&#20307;&#29305;&#23450;&#30340;&#25511;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;DeformerNet&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26550;&#26500;&#22312;&#34987;&#25805;&#32437;&#29289;&#20307;&#30340;&#37096;&#20998;&#35270;&#22270;&#28857;&#20113;&#21644;&#30446;&#26631;&#24418;&#29366;&#30340;&#28857;&#20113;&#19978;&#36816;&#34892;&#65292;&#23398;&#20064;&#29289;&#20307;&#24418;&#29366;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#36825;&#20010;&#24418;&#29366;&#23884;&#20837;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#35270;&#35273;&#20282;&#26381;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#35745;&#31639;&#20986;&#25152;&#38656;&#30340;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#21160;&#20316;&#65292;&#23558;&#29289;&#20307;&#36845;&#20195;&#22320;&#21464;&#24418;&#21521;&#30446;&#26631;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#28436;&#31034;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.03695</link><description>&lt;p&gt;
Vera&#65306;&#19968;&#20010;&#29992;&#20110;&#36890;&#29992;&#24120;&#35782;&#35821;&#21477;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements. (arXiv:2305.03695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vera&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#22312;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#29616;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#20170;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20986;&#29616;&#33618;&#35884;&#21644;&#24847;&#22806;&#30340;&#24120;&#35782;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#39038;&#24615;&#39564;&#35777;&#26041;&#27861;&#65292;&#21453;&#24605;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;Vera&#65292;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#20272;&#35745;&#38472;&#36848;&#24615;&#35821;&#21477;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;19&#20010;QA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#21019;&#24314;&#30340;&#32422;700&#19975;&#26465;&#24120;&#35782;&#35821;&#21477;&#20197;&#21450;&#19977;&#20010;&#35757;&#32451;&#30446;&#26631;&#30340;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#65292;Vera&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21508;&#31181;&#24120;&#35782;&#39046;&#22495;&#20013;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#35821;&#21477;&#12290;&#24403;&#24212;&#29992;&#20110;&#35299;&#20915;&#39564;&#35777;&#26684;&#24335;&#30340;&#24120;&#35782;&#38382;&#39064;&#26102;&#65292;Vera&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#37325;&#29992;&#20110;&#24120;&#35782;&#39564;&#35777;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#23427;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#26631;&#23450;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;Vera&#22312;&#36807;&#28388;LM&#29983;&#25104;&#30340;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#22686;&#24378;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.02997</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#32988;&#36807;&#22686;&#24378;&#26641;&#65311;
&lt;/p&gt;
&lt;p&gt;
When Do Neural Nets Outperform Boosted Trees on Tabular Data?. (arXiv:2305.02997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02997
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#20998;&#26512;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#25110;&#32773;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;965&#20010;&#20803;&#29305;&#24449;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;GBDT&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#20154;&#20204;&#20173;&#22312;&#31215;&#26497;&#35752;&#35770;NN&#26159;&#21542;&#36890;&#24120;&#20248;&#20110;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#35201;&#20040;&#35748;&#20026;GBDT&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19968;&#36143;&#20248;&#20110;NN&#65292;&#35201;&#20040;&#35748;&#20026;NN&#20248;&#20110;GBDT&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#38382;&#65306;'&#36825;&#37325;&#35201;&#21527;&#65311;'&#25105;&#20204;&#36890;&#36807;&#23545;176&#20010;&#25968;&#25454;&#38598;&#27604;&#36739;19&#31181;&#31639;&#27861;&#65292;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;'NN vs. GBDT'&#20105;&#35770;&#34987;&#36807;&#20998;&#24378;&#35843;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#24403;&#22810;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;GBDT&#21644;NN&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#35201;&#20040;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#35201;&#20040;GBDT&#30340;&#36731;&#24494;&#36229;&#21442;&#25968;&#35843;&#25972;&#27604;&#36873;&#25321;&#26368;&#20339;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;965&#20010;&#20803;&#29305;&#24449;&#65292;&#20197;&#30830;&#23450;&#25968;&#25454;&#38598;&#30340;&#21738;&#20123;&#29305;&#24615;&#20351;NN&#25110;GBDT&#26356;&#36866;&#21512;&#34920;&#29616;&#33391;&#22909;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GBDT&#35201;&#27604;NN&#22312;&#39640;&#32500;&#31232;&#30095;&#25968;&#25454;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and ask, 'does it matter?' We conduct the largest tabular data analysis to date, by comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than selecting the best algorithm. Next, we analyze 965 metafeatures to determine what properties of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.13007</link><description>&lt;p&gt;
&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65306;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;&#38382;&#39064;&#35299;&#31572;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Answering Questions by Meta-Reasoning over Multiple Chains of Thought. (arXiv:2304.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340;Multi-Chain Reasoning (MCR)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26816;&#26597;&#22810;&#20010;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#65292;&#20174;&#32780;&#36229;&#36234;&#22810;&#38142;&#24605;&#32500;&#65292;&#35299;&#20915;&#22810;&#36339;QA&#38382;&#39064;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MCR&#32988;&#36807;&#22810;&#20010;&#24378;&#22522;&#32447;&#65292;&#35299;&#37322;&#36136;&#37327;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#36339;&#38382;&#39064;&#35299;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#36890;&#24120;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#24605;&#32771;&#27493;&#39588;&#65288;CoT&#65289;&#65292;&#28982;&#21518;&#25165;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#22810;&#20010;&#38142;&#26465;&#34987;&#25277;&#26679;&#24182;&#36890;&#36807;&#26368;&#32456;&#31572;&#26696;&#30340;&#25237;&#31080;&#26426;&#21046;&#36827;&#34892;&#32858;&#21512;&#65292;&#20294;&#20013;&#38388;&#27493;&#39588;&#26412;&#36523;&#34987;&#20002;&#24323;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#32771;&#34385;&#38142;&#20043;&#38388;&#30340;&#20013;&#38388;&#27493;&#39588;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#39044;&#27979;&#31572;&#26696;&#30340;&#32479;&#19968;&#35299;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20803;&#25512;&#29702;&#30340; Multi-Chain Reasoning (MCR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36229;&#36234;&#22810;&#20010;&#24605;&#32771;&#38142;&#65292;&#32780;&#19981;&#26159;&#32858;&#21512;&#22238;&#31572;&#12290;MCR&#26816;&#26597;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#28151;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#20449;&#24687;&#24182;&#36873;&#25321;&#22312;&#29983;&#25104;&#35299;&#37322;&#21644;&#39044;&#27979;&#31572;&#26696;&#26102;&#26368;&#30456;&#20851;&#30340;&#20107;&#23454;&#12290;MCR&#22312;7&#20010;&#22810;&#36339;QA&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;MCR&#30340;&#35299;&#37322;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-of-thought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregating their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, en
&lt;/p&gt;</description></item><item><title>SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11060</link><description>&lt;p&gt;
SkillGPT: &#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11060
&lt;/p&gt;
&lt;p&gt;
SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SkillGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20174;&#33258;&#30001;&#39118;&#26684;&#32844;&#20301;&#25551;&#36848;&#21644;&#29992;&#25143;&#36164;&#26009;&#20013;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270; (SES) &#30340;&#24037;&#20855;&#12290;&#19982;&#22823;&#22810;&#25968;&#31867;&#20284;&#20219;&#21153;&#30340;&#20197;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;SkillGPT &#30452;&#25509;&#20351;&#29992;&#26368;&#26032;&#30340;&#23545;&#35805; LLM &#36827;&#34892;&#26631;&#20934;&#25216;&#33021;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#26469;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20813;&#36153; SkillGPT &#35753;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#35805;&#22411; SES&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SkillGPT, a tool for skill extraction and standardization (SES) from free-style job descriptions and user profiles with an open-source Large Language Model (LLM) as backbone. Most previous methods for similar tasks either need supervision or rely on heavy data-preprocessing and feature engineering. Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes a LLM to perform its tasks in steps via summarization and vector similarity search, to balance speed with precision. The backbone LLM of SkillGPT is based on Llama, free for academic use and thus useful for exploratory research and prototype development. Hence, our cost-free SkillGPT gives users the convenience of conversational SES, efficiently and reliably.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.10532</link><description>&lt;p&gt;
Nerfbusters&#65306;&#20174;&#38543;&#24847;&#25429;&#33719;&#30340;NeRF&#20013;&#21435;&#38500;&#24189;&#28789;&#20284;&#30340;&#22270;&#20687;&#20266;&#24433;
&lt;/p&gt;
&lt;p&gt;
Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs. (arXiv:2304.10532v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10532
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;3D&#25193;&#25955;&#20248;&#20808;&#32423;&#21035;&#21152;&#19978;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#38450;&#27490;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#22270;&#24418;&#20266;&#24433;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#24847;&#25429;&#33719;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#22312;&#28210;&#26579;&#25668;&#20687;&#26426;&#36712;&#36857;&#20043;&#22806;&#26102;&#20250;&#20986;&#29616;&#28014;&#28857;&#38169;&#35823;&#25110;&#26377;&#32570;&#38519;&#30340;&#20960;&#20309;&#22270;&#24418;&#31561;&#20266;&#24433;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#21327;&#35758;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#36825;&#20123;&#25928;&#24212;&#65292;&#22240;&#20026;&#36890;&#24120;&#20165;&#22312;&#35757;&#32451;&#25235;&#21462;&#30340;&#27599;&#20010;&#31532;&#20843;&#24103;&#35780;&#20272;&#22270;&#20687;&#36136;&#37327;&#12290;&#20026;&#20102;&#25512;&#21160;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#31243;&#24207;&#65292;&#20854;&#20013;&#35760;&#24405;&#20102;&#22330;&#26223;&#30340;&#20004;&#20010;&#25668;&#20687;&#26426;&#36712;&#36857;&#65306;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#35268;&#21017;&#19981;&#20165;&#19981;&#33021;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#65292;&#32780;&#19988;&#20063;&#19981;&#33021;&#25913;&#21892;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;3D&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;3D&#20808;&#39564;&#21644;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#24471;&#20998;&#33976;&#39311;&#37319;&#26679;&#25439;&#22833;&#65292;&#22312;NeRF&#20248;&#21270;&#36807;&#31243;&#20013;&#38450;&#27490;&#20986;&#29616;&#20266;&#20687;&#29616;&#35937;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#20808;&#32423;&#21035;&#33021;&#22815;&#21435;&#38500;&#28014;&#28857;&#38169;&#35823;&#24182;&#25913;&#21892;&#38543;&#24847;&#25429;&#33719;&#30340;&#22330;&#26223;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Casually captured Neural Radiance Fields (NeRFs) suffer from artifacts such as floaters or flawed geometry when rendered outside the camera trajectory. Existing evaluation protocols often do not capture these effects, since they usually only assess image quality at every 8th frame of the training capture. To push forward progress in novel-view synthesis, we propose a new dataset and evaluation procedure, where two camera trajectories are recorded of the scene: one used for training, and the other for evaluation. In this more challenging in-the-wild setting, we find that existing hand-crafted regularizers do not remove floaters nor improve scene geometry. Thus, we propose a 3D diffusion-based method that leverages local 3D priors and a novel density-based score distillation sampling loss to discourage artifacts during NeRF optimization. We show that this data-driven prior removes floaters and improves scene geometry for casual captures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#20154;&#31867;&#31354;&#38388;&#24863;&#30693;&#30340;3D&#23556;&#24433;&#20960;&#20309;&#23398;&#19982;&#26234;&#33021;&#20307;&#24863;&#30693;&#26041;&#26696;&#20013;&#30340;&#32676;&#27010;&#24565;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#19981;&#21516;&#32676;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.00188</link><description>&lt;p&gt;
&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#20013;&#27431;&#27663;&#32676;&#19982;&#23556;&#24433;&#32676;&#23545;&#20110;&#26234;&#33021;&#20307;&#20869;&#37096;&#31354;&#38388;&#30340;&#20316;&#29992;&#65306;&#24418;&#24335;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Action of the Euclidean versus Projective group on an agent's internal space in curiosity driven exploration: a formal analysis. (arXiv:2304.00188v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#20154;&#31867;&#31354;&#38388;&#24863;&#30693;&#30340;3D&#23556;&#24433;&#20960;&#20309;&#23398;&#19982;&#26234;&#33021;&#20307;&#24863;&#30693;&#26041;&#26696;&#20013;&#30340;&#32676;&#27010;&#24565;&#30456;&#32467;&#21512;&#65292;&#25506;&#35752;&#19981;&#21516;&#32676;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#22909;&#22855;&#24515;&#39537;&#21160;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#31354;&#38388;&#24863;&#30693;&#20013;&#65292;&#20449;&#24687;&#20284;&#20046;&#26159;&#26681;&#25454;&#19977;&#32500;&#23556;&#24433;&#20960;&#20309;&#23398;&#34920;&#31034;&#30340;&#12290;&#23427;&#23558;&#20449;&#24687;&#38598;&#25104;&#21644;&#34892;&#21160;&#35268;&#21010;&#32452;&#32455;&#22312;&#19968;&#20010;&#20869;&#37096;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#19981;&#21516;&#20010;&#20307;&#30340;&#31532;&#19968;&#20154;&#31216;&#35270;&#35282;&#26159;&#22914;&#20309;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#30340;&#21464;&#25442;&#30456;&#20114;&#20851;&#32852;&#65292;&#24182;&#23450;&#20041;&#26234;&#33021;&#20307;&#30340;&#29305;&#23450;&#24863;&#30693;&#26041;&#26696;&#12290;&#36825;&#20123;&#21464;&#25442;&#30340;&#38598;&#21512;&#22312;&#25968;&#23398;&#19978;&#34987;&#31216;&#20026;&#8220;&#32676;&#8221;&#65292;&#23427;&#36890;&#36807;&#23545;&#20960;&#20309;&#31354;&#38388;&#30340;&#25805;&#20316;&#26469;&#34920;&#24449;&#20854;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#25317;&#26377;&#8220;&#20960;&#20309;&#8221;&#32467;&#26500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#32473;&#20986;&#30001;&#32676;&#25552;&#20379;&#30340;&#19968;&#31181;&#26041;&#24335;&#26469;&#25429;&#25417;&#20195;&#29702;&#20043;&#38388;&#19981;&#21516;&#30340;&#24863;&#30693;&#26041;&#26696;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#19990;&#30028;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#22914;&#20309;&#24433;&#21709;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#36825;&#20123;&#20960;&#20309;&#36816;&#31639;&#22914;&#20309;&#36890;&#36807;&#22312;&#39537;&#21160;&#26234;&#33021;&#20307;&#22909;&#22855;&#24515;&#30340;&#20027;&#35266;&#25512;&#26029;&#30340;&#24418;&#24335;&#34920;&#36798;&#19978;&#36827;&#34892;&#36716;&#25442;&#65292;&#24182;&#30456;&#24212;&#22320;&#24433;&#21709;&#25506;&#32034;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#32676;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In human spatial awareness, information appears to be represented according to 3-D projective geometry. It structures information integration and action planning within an internal representation space. The way different first person perspectives of an agent relate to each other, through transformations of a world model, defines a specific perception scheme for the agent. In mathematics, this collection of transformations is called a `group' and it characterizes a geometric space by acting on it. We propose that imbuing world models with a `geometric' structure, given by a group, is one way to capture different perception schemes of agents. We explore how changing the geometric structure of a world model impacts the behavior of an agent.  In particular, we focus on how such geometrical operations transform the formal expression of epistemic value in active inference as driving an agent's curiosity about its environment, and impact exploration behaviors accordingly. We used group action
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#31649;&#36947;&#65292;&#29992;&#20110;&#31163;&#20307;MRI&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#12290;&#22522;&#20110;37&#20010;&#26631;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;7 T MRI&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21487;&#20934;&#30830;&#20998;&#21106;&#25972;&#20010;&#33041;&#21322;&#29699;&#65292;&#21253;&#25324;&#30382;&#23618;&#12289;&#30382;&#36136;&#19979;&#32467;&#26500;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#31070;&#32463;&#30149;&#29702;&#23398;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.12237</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#32467;&#26500;&#30149;&#29702;&#30456;&#20851;&#24615;&#23450;&#37327;&#20998;&#26512;&#30340;&#39640;&#20998;&#36776;&#29575;7T&#31163;&#20307;MRI&#30340;&#33258;&#21160;&#21270;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Automated deep learning segmentation of high-resolution 7 T ex vivo MRI for quantitative analysis of structure-pathology correlations in neurodegenerative diseases. (arXiv:2303.12237v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#31649;&#36947;&#65292;&#29992;&#20110;&#31163;&#20307;MRI&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#20998;&#21106;&#12290;&#22522;&#20110;37&#20010;&#26631;&#26412;&#30340;&#39640;&#20998;&#36776;&#29575;7 T MRI&#22270;&#20687;&#65292;&#35813;&#26041;&#27861;&#21487;&#20934;&#30830;&#20998;&#21106;&#25972;&#20010;&#33041;&#21322;&#29699;&#65292;&#21253;&#25324;&#30382;&#23618;&#12289;&#30382;&#36136;&#19979;&#32467;&#26500;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#12290;&#35813;&#26041;&#27861;&#21487;&#20026;&#31070;&#32463;&#30149;&#29702;&#23398;&#30740;&#31350;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#22312; vivo MRI&#65292;&#22823;&#33041;&#31163;&#20307; MRI &#25552;&#20379;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21487;&#29992;&#20110;&#21487;&#35270;&#21270;&#21644;&#34920;&#24449;&#35814;&#32454;&#30340;&#31070;&#32463;&#35299;&#21078;&#65292;&#21327;&#21161;&#23558;&#24494;&#35266;&#32452;&#32455;&#23398;&#30740;&#31350;&#19982;&#24418;&#24577;&#27979;&#37327;&#30456;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#38480;&#20110;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#25195;&#25551;&#22120;&#30828;&#20214;&#21644;&#37319;&#38598;&#21327;&#35758;&#30340;&#24322;&#36136;&#24615;&#65292;&#31163;&#20307; MRI &#30340;&#33041;&#21306;&#22495;&#20998;&#21106;&#33258;&#21160;&#21270;&#26041;&#27861;&#24182;&#19981;&#21457;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22312;7T&#20840;&#36523;MRI&#25195;&#25551;&#22120;&#19978;&#25195;&#25551;&#30340;37&#20010;&#31163;&#20307;&#20154;&#33041;&#32452;&#32455;&#26631;&#26412;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#26469;&#20998;&#21106;&#30382;&#23618;&#65292;&#36890;&#36807;&#23545;&#20061;&#31181;&#28145;&#24230;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#30382;&#36136;&#19979;&#32467;&#26500;&#65288;&#23614;&#29366;&#26680;&#65292;&#33041;&#35910;&#26680;&#65292;&#33485;&#30333;&#29699;&#21644;&#19992;&#33041;&#65289;&#12289;&#30333;&#36136;&#39640;&#20449;&#21495;&#20197;&#21450;&#27491;&#24120;&#20986;&#29616;&#30340;&#30333;&#36136;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#26631;&#26412;&#30340;&#20840;&#33041;&#21322;&#29699;&#20043;&#38388;&#30340;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20063;&#23637;&#31034;&#20102;&#22312;&#30475;&#19981;&#35265;&#30340;&#30828;&#20214;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#31070;&#32463;&#30149;&#29702;&#23398;&#21644;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#32467;&#26500;&#30149;&#29702;&#30456;&#20851;&#24615;&#30340;&#23450;&#37327;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for visualizing and characterizing detailed neuroanatomy, and helps to link microscale histology studies with morphometric measurements. However, automated segmentation methods for brain mapping in ex vivo MRI are not well developed, primarily due to limited availability of labeled datasets, and heterogeneity in scanner hardware and acquisition protocols. In this work, we present a high resolution dataset of 37 ex vivo post-mortem human brain tissue specimens scanned on a 7T whole-body MRI scanner. We developed a deep learning pipeline to segment the cortical mantle by benchmarking the performance of nine deep neural architectures. We then segment the four subcortical structures: caudate, putamen, globus pallidus, and thalamus; white matter hyperintensities, and the normal appearing white matter. We show excellent generalizing capabilities across whole brain hemispheres in different specimens, and also on unseen i
&lt;/p&gt;</description></item><item><title>Taylor TD&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;TD&#26356;&#26032;&#30340;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#65292;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#20855;&#26377;&#19982;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;TaTD3&#26159;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#25152;&#24418;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.14182</link><description>&lt;p&gt;
Taylor TD&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Taylor TD-learning. (arXiv:2302.14182v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14182
&lt;/p&gt;
&lt;p&gt;
Taylor TD&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;TD&#26356;&#26032;&#30340;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#65292;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#20855;&#26377;&#19982;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;TaTD3&#26159;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#25152;&#24418;&#25104;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#20998;&#65288;TD&#65289;&#23398;&#20064;&#26469;&#23398;&#20064;&#19968;&#20010;&#35780;&#35770;&#23478;&#12290;&#28982;&#32780;&#65292;TD&#23398;&#20064;&#30340;&#26356;&#26032;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#26694;&#26550;&#65292;&#21363;Taylor TD&#65292;&#23427;&#20943;&#23569;&#20102;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#35774;&#32622;&#20013;&#30340;&#26041;&#24046;&#12290;Taylor TD&#20351;&#29992;TD&#26356;&#26032;&#30340;&#19968;&#38454;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#12290;&#35813;&#23637;&#24320;&#20801;&#35768;Taylor TD&#22312;&#34892;&#21160;&#36873;&#25321;&#30340;&#38543;&#26426;&#24615;&#21644;&#27599;&#20010;TD&#26356;&#26032;&#30340;&#21021;&#22987;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#29366;&#24577;&#20998;&#24067;&#30340;&#19968;&#20123;&#38543;&#26426;&#24615;&#19978;&#36827;&#34892;&#20998;&#26512;&#31215;&#20998;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;Taylor TD&#30340;&#26356;&#26032;&#30830;&#23454;&#27604;&#26631;&#20934;&#30340;TD&#26356;&#26032;&#20855;&#26377;&#36739;&#20302;&#30340;&#26041;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;Taylor TD&#20855;&#26377;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#26631;&#20934;TD&#23398;&#20064;&#30456;&#21516;&#30340;&#31283;&#23450;&#23398;&#20064;&#20445;&#35777;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;Taylor TD&#19982;TD3&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;TaTD3&#12290;&#25105;&#20204;&#23637;&#31034;TaTD3&#30340;&#34920;&#29616;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many reinforcement learning approaches rely on temporal-difference (TD) learning to learn a critic. However, TD-learning updates can be high variance. Here, we introduce a model-based RL framework, Taylor TD, which reduces this variance in continuous state-action settings. Taylor TD uses a first-order Taylor series expansion of TD updates. This expansion allows Taylor TD to analytically integrate over stochasticity in the action-choice, and some stochasticity in the state distribution for the initial state and action of each TD update. We include theoretical and empirical evidence that Taylor TD updates are indeed lower variance than standard TD updates. Additionally, we show Taylor TD has the same stable learning guarantees as standard TD-learning with linear function approximation under a reasonable assumption. Next, we combine Taylor TD with the TD3 algorithm, forming TaTD3. We show TaTD3 performs as well, if not better, than several state-of-the art model-free and model-based basel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Lang2LTL&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#36890;&#36807;&#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#20840;&#38754;&#35780;&#20272;Lang2LTL&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#23637;&#31034;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#21487;&#20197;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;Lang2LTL&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.11649</link><description>&lt;p&gt;
&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#20026;&#26102;&#38388;&#20219;&#21153;&#24314;&#31435;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments. (arXiv:2302.11649v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Lang2LTL&#30340;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#36890;&#36807;&#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#20840;&#38754;&#35780;&#20272;Lang2LTL&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#24182;&#19988;&#36890;&#36807;&#23637;&#31034;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#21487;&#20197;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;Lang2LTL&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753; (LTL) &#36827;&#34892;&#20851;&#32852;&#65292;&#21033;&#29992;&#20854;&#26126;&#30830;&#30340;&#35821;&#20041;&#26469;&#25512;&#29702;&#38271;&#31243;&#20219;&#21153;&#21644;&#39564;&#35777;&#26102;&#38388;&#32422;&#26463;&#30340;&#28385;&#36275;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26469;&#33258;&#29305;&#23450;&#29615;&#22659;&#21644;&#29992;&#20110;&#29702;&#35299;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#21629;&#20196;&#30340;&#22320;&#26631;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Lang2LTL&#65292;&#36825;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#31995;&#32479;&#21644;&#19968;&#20010;&#36719;&#20214;&#21253;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#27809;&#26377;&#20808;&#21069;&#35821;&#35328;&#25968;&#25454;&#30340;&#29615;&#22659;&#20013;&#23558;&#26102;&#24577;&#23548;&#33322;&#21629;&#20196;&#19982;LTL&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#23545;Lang2LTL&#36827;&#34892;&#20102;&#20116;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;Lang2LTL&#22312;21&#20010;&#22478;&#24066;&#32423;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#21333;&#20010;&#27169;&#22411;&#23558;&#23548;&#33322;&#21629;&#20196;&#19982;&#21508;&#31181;&#26102;&#24577;&#35268;&#33539;&#36827;&#34892;&#20851;&#32852;&#30340;&#26368;&#20808;&#36827;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20351;&#29992;Lang2LTL&#30340;&#29289;&#29702;&#26426;&#22120;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#29615;&#22659;&#20013;&#36981;&#24490;52&#20010;&#35821;&#20041;&#22810;&#26679;&#30340;&#23548;&#33322;&#21629;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grounding navigational commands to linear temporal logic (LTL) leverages its unambiguous semantics for reasoning about long-horizon tasks and verifying the satisfaction of temporal constraints. Existing approaches require training data from the specific environment and landmarks that will be used in natural language to understand commands in those environments. We propose Lang2LTL, a modular system and a software package that leverages large language models (LLMs) to ground temporal navigational commands to LTL specifications in environments without prior language data. We comprehensively evaluate Lang2LTL for five well-defined generalization behaviors. Lang2LTL demonstrates the state-of-the-art ability of a single model to ground navigational commands to diverse temporal specifications in 21 city-scaled environments. Finally, we demonstrate a physical robot using Lang2LTL can follow 52 semantically diverse navigational commands in two indoor environments.
&lt;/p&gt;</description></item><item><title>SHINE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#65292;&#24182;&#20855;&#22791;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#21151;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.00837</link><description>&lt;p&gt;
SHINE: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SHINE: Deep Learning-Based Accessible Parking Management System. (arXiv:2302.00837v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00837
&lt;/p&gt;
&lt;p&gt;
SHINE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#65292;&#24182;&#20855;&#22791;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#21151;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#25216;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#22478;&#24066;&#21306;&#22495;&#30340;&#19981;&#26029;&#25193;&#24352;&#65292;&#20840;&#19990;&#30028;&#31169;&#20154;&#36710;&#36742;&#25968;&#37327;&#30456;&#24212;&#22686;&#21152;&#65292;&#20854;&#20013;&#38889;&#22269;&#20063;&#19981;&#20363;&#22806;&#12290;&#36825;&#31181;&#36710;&#36742;&#25968;&#37327;&#36880;&#27493;&#22686;&#21152;&#20063;&#24102;&#26469;&#20102;&#19982;&#20572;&#36710;&#26377;&#20851;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20026;&#27531;&#30142;&#20154;&#25351;&#23450;&#30340;&#27531;&#30142;&#20154;&#20572;&#36710;&#20301;&#34987;&#28389;&#29992;&#12290;&#20256;&#32479;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#30001;&#20110;&#30417;&#25511;&#25668;&#20687;&#22836;&#39640;&#24103;&#29575;&#12289;&#33258;&#28982;&#21644;&#20154;&#20026;&#22122;&#22768;&#30340;&#23384;&#22312;&#20197;&#21450;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#23454;&#26102;&#26816;&#27979;&#21644;&#35782;&#21035;&#25928;&#29575;&#20302;&#19979;&#65292;&#38590;&#20197;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SHINE&#65292;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#26080;&#38556;&#30861;&#24615;&#30417;&#27979;&#21644;&#35782;&#21035;&#20572;&#25918;&#22312;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#19978;&#30340;&#36710;&#36742;&#12290;SHINE&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#21644;&#26080;&#38556;&#30861;&#20572;&#36710;&#31649;&#29702;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;4&#20010;&#19981;&#21516;&#26102;&#27573;&#25293;&#25668;&#30340;21&#20010;&#26080;&#38556;&#30861;&#20572;&#36710;&#20301;&#30340;4,780&#24352;&#22270;&#20687;&#65292;&#20197;&#35780;&#20272;SHINE&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHINE&#20248;&#20110;&#29616;&#26377;&#30340;&#36710;&#29260;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;&#35782;&#21035;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30417;&#27979;&#26041;&#38754;&#37117;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing expansion of urban areas facilitated by advancements in science and technology has resulted in a considerable increase in the number of privately owned vehicles worldwide, including in South Korea. However, this gradual increment in the number of vehicles has inevitably led to parking-related issues, including the abuse of disabled parking spaces (hereafter referred to as accessible parking spaces) designated for individuals with disabilities. Traditional license plate recognition (LPR) systems have proven inefficient in addressing such a problem in real-time due to the high frame rate of surveillance cameras, the presence of natural and artificial noise, and variations in lighting and weather conditions that impede detection and recognition by these systems. With the growing concept of parking 4.0, many sensors, IoT and deep learning-based approaches have been applied to automatic LPR and parking management systems. Nonetheless, the studies show a need for a robust and eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2212.10764</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#25490;&#21517;&#30340;&#21015;&#34920;&#32423;&#21035;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning List-Level Domain-Invariant Representations for Ranking. (arXiv:2212.10764v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#30340;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21015;&#34920;&#30340;&#32467;&#26500;&#29305;&#24615;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20013;&#23454;&#29616;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36866;&#24212;&#26088;&#22312;&#23558;&#22312;&#65288;&#25968;&#25454;&#20016;&#23500;&#65289;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#65288;&#36164;&#28304;&#26377;&#38480;&#65289;&#30446;&#26631;&#39046;&#22495;&#65292;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#26159;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#65292;&#23427;&#21305;&#37197;&#24182;&#23545;&#40784;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#20294;&#22312;&#25490;&#21517;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#21364;&#26159;&#38646;&#25955;&#30340;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#20960;&#31181;&#23454;&#29616;&#32570;&#20047;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#29992;&#20110;&#25490;&#21517;&#30340;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#23457;&#26597;&#20043;&#21069;&#30340;&#24037;&#20316;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#23454;&#26045;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#39033;&#30446;&#32423;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32858;&#21512;&#30340;&#25152;&#26377;&#21015;&#34920;&#20013;&#23545;&#36827;&#34892;&#25490;&#21517;&#30340;&#39033;&#30446;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#20294;&#24573;&#30053;&#20102;&#21015;&#34920;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#21015;&#34920;&#30340;&#32467;&#26500;&#24212;&#35813;&#34987;&#21033;&#29992;&#65292;&#22240;&#20026;&#23427;&#26159;&#25490;&#21517;&#38382;&#39064;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#24230;&#37327;&#26159;&#22312;&#21015;&#34920;&#19978;&#23450;&#20041;&#21644;&#35745;&#31639;&#30340;&#65292;&#32780;&#19981;&#26159;&#22312;&#39033;&#30446;&#26412;&#36523;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#19968;&#33268;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21015;&#34920;&#32423;&#21035;&#23545;&#40784;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation aims to transfer the knowledge learned on (data-rich) source domains to (low-resource) target domains, and a popular method is invariant representation learning, which matches and aligns the data distributions on the feature space. Although this method is studied extensively and applied on classification and regression problems, its adoption on ranking problems is sporadic, and the few existing implementations lack theoretical justifications. This paper revisits invariant representation learning for ranking. Upon reviewing prior work, we found that they implement what we call item-level alignment, which aligns the distributions of the items being ranked from all lists in aggregate but ignores their list structure. However, the list structure should be leveraged, because it is intrinsic to ranking problems where the data and the metrics are defined and computed on lists, not the items by themselves. To close this discrepancy, we propose list-level alignment -learning
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#20102;&#24403;&#21069;&#20851;&#20110;&#35268;&#21010;&#12289;&#35843;&#24230;&#21644;&#23398;&#20064;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#21253;&#25324;&#20808;&#36827;&#30340;&#35268;&#21010;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#25104;&#21151;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.13181</link><description>&lt;p&gt;
&#35745;&#21010;&#19982;&#23398;&#20064;&#65306;&#33258;&#20027;&#36710;&#36742;&#30340;&#36335;&#24452;&#35268;&#21010;&#8212;&#8212;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Planning and Learning: Path-Planning for Autonomous Vehicles, a Review of the Literature. (arXiv:2207.13181v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#20102;&#24403;&#21069;&#20851;&#20110;&#35268;&#21010;&#12289;&#35843;&#24230;&#21644;&#23398;&#20064;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#21253;&#25324;&#20808;&#36827;&#30340;&#35268;&#21010;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#25104;&#21151;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#32508;&#36848;&#26088;&#22312;&#35753;&#35835;&#32773;&#29087;&#24713;&#19982;&#35268;&#21010;&#12289;&#35843;&#24230;&#21644;&#23398;&#20064;&#30456;&#20851;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26356;&#35814;&#32454;&#22320;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22788;&#29702;&#22270;&#32467;&#26500;&#36755;&#20837;&#30340;&#26368;&#26032;&#21464;&#20307;&#12290;&#25105;&#20204;&#31616;&#35201;&#25551;&#36848;&#20102;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#30340;&#27010;&#24565;&#21644;&#19968;&#20123;&#36804;&#20170;&#20026;&#27490;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#25104;&#21151;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#36335;&#24452;&#35268;&#21010;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#26102;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short review aims to make the reader familiar with state-of-the-art works relating to planning, scheduling and learning. First, we study state-of-the-art planning algorithms. We give a brief introduction of neural networks. Then we explore in more detail graph neural networks, a recent variant of neural networks suited for processing graph-structured inputs. We describe briefly the concept of reinforcement learning algorithms and some approaches designed to date. Next, we study some successful approaches combining neural networks for path-planning. Lastly, we focus on temporal planning problems with uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2103.03223</link><description>&lt;p&gt;
&#37327;&#21270;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comparative Evaluation of Quantification Methods. (arXiv:2103.03223v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#22312;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#27604;&#36739;&#65292;&#22635;&#34917;&#20102;&#37327;&#21270;&#26041;&#27861;&#27604;&#36739;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#34920;&#29616;&#26368;&#20339;&#65307;&#32780;&#22312;&#22810;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;Generaliz&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#25351;&#22312;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#31867;&#21035;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#23427;&#20063;&#20195;&#34920;&#30528;&#19968;&#20010;&#22312;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20221;&#20840;&#38754;&#30340;&#23454;&#35777;&#27604;&#36739;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#20197;&#25903;&#25345;&#31639;&#27861;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#36229;&#36807;40&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;24&#31181;&#19981;&#21516;&#37327;&#21270;&#26041;&#27861;&#30340;&#24443;&#24213;&#23454;&#35777;&#24615;&#24615;&#33021;&#27604;&#36739;&#65292;&#21253;&#25324;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#37327;&#21270;&#35774;&#32622;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#27809;&#26377;&#21333;&#19968;&#31639;&#27861;&#33021;&#22815;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#22312;&#20108;&#20998;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#38408;&#20540;&#36873;&#25321;&#30340;Median Sweep&#21644;TSMax&#26041;&#27861;&#12289;DyS&#26694;&#26550;&#21644;&#24343;&#37324;&#24503;&#26364;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#22810;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21478;&#19968;&#32452;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#21253;&#25324;Generaliz&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification represents the problem of predicting class distributions in a dataset. It also represents a growing research field in supervised machine learning, for which a large variety of different algorithms has been proposed in recent years. However, a comprehensive empirical comparison of quantification methods that supports algorithm selection is not available yet. In this work, we close this research gap by conducting a thorough empirical performance comparison of 24 different quantification methods on overall more than 40 data sets, considering binary as well as multiclass quantification settings. We observe that no single algorithm generally outperforms all competitors, but identify a group of methods including the threshold selection-based Median Sweep and TSMax methods, the DyS framework, and Friedman's method that performs best in the binary setting. For the multiclass setting, we observe that a different group of algorithms yields good performance, including the Generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;Uppaal&#27169;&#22411;&#26816;&#26597;&#22120;&#23545;&#25237;&#31080;&#21327;&#35758;&#36827;&#34892;&#24314;&#27169;&#21644;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#23545;Pr&#234;ter &#224; Voter&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#30340;&#39564;&#35777;&#65292;&#24182;&#20811;&#26381;&#20102;&#27169;&#22411;&#26816;&#26597;&#22120;&#20013;&#23646;&#24615;&#35268;&#33539;&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2007.12412</link><description>&lt;p&gt;
&#27169;&#22411;&#26816;&#26597;&#22120;&#24456;&#37239;&#65306;&#22914;&#20309;&#22312;Uppaal&#20013;&#23545;&#25237;&#31080;&#21327;&#35758;&#36827;&#34892;&#27169;&#22411;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Model Checkers Are Cool: How to Model Check Voting Protocols in Uppaal. (arXiv:2007.12412v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.12412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;Uppaal&#27169;&#22411;&#26816;&#26597;&#22120;&#23545;&#25237;&#31080;&#21327;&#35758;&#36827;&#34892;&#24314;&#27169;&#21644;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#23545;Pr&#234;ter &#224; Voter&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#30340;&#39564;&#35777;&#65292;&#24182;&#20811;&#26381;&#20102;&#27169;&#22411;&#26816;&#26597;&#22120;&#20013;&#23646;&#24615;&#35268;&#33539;&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#21644;&#23454;&#29616;&#30005;&#23376;&#25237;&#31080;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24418;&#24335;&#21270;&#20998;&#26512;&#21487;&#20197;&#22312;&#36825;&#26041;&#38754;&#25552;&#20379;&#24456;&#22823;&#30340;&#24110;&#21161;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25237;&#31080;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20197;&#21450;&#23545;&#31995;&#32479;&#30340;&#35201;&#27714;&#26159;&#20160;&#20040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26816;&#26597;&#22120;Uppaal&#20026;&#24314;&#27169;&#21644;&#21021;&#27493;&#39564;&#35777;&#25237;&#31080;&#21327;&#35758;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#29615;&#22659;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Pr&#234;ter &#224; Voter&#30340;Uppaal&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#39564;&#35777;&#19968;&#31181;&#21464;&#31181;&#30340;&#26080;&#31080;&#35777;&#24615;&#65292;&#23613;&#31649;&#27169;&#22411;&#26816;&#26597;&#22120;&#20013;&#30340;&#23646;&#24615;&#35268;&#33539;&#35821;&#35328;&#20855;&#26377;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design and implementation of an e-voting system is a challenging task. Formal analysis can be of great help here. In particular, it can lead to a better understanding of how the voting system works, and what requirements on the system are relevant. In this paper, we propose that the state-of-art model checker Uppaal provides a good environment for modelling and preliminary verification of voting protocols. To illustrate this, we present an Uppaal model of Pr\^et \`a Voter, together with some natural extensions. We also show how to verify a variant of receipt-freeness, despite the severe limitations of the property specification language in the model checker.
&lt;/p&gt;</description></item></channel></rss>